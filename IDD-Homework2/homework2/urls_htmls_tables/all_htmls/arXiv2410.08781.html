<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>VideoSAM: Open-World Video Segmentation</title>
<!--Generated on Fri Oct 11 12:50:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.08781v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S1" title="In VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">INTRODUCTION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S2" title="In VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Works</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S2.SS1" title="In II Related Works ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Open-world segmentation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S2.SS2" title="In II Related Works ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Object tracking</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S2.SS3" title="In II Related Works ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Existing tracking-anything model</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S3" title="In VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S3.SS1" title="In III Methods ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Preliminaries of SAM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S3.SS2" title="In III Methods ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">VideoSAM Framework</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S3.SS3" title="In III Methods ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Robust Object Association</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S3.SS4" title="In III Methods ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Granularity Consistent Segmentation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4" title="In VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.SS1" title="In IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Implementation Details</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.SS2" title="In IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Benchmarks</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.SS3" title="In IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Evaluation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.SS4" title="In IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Ablation Study</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S5" title="In VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">VideoSAM: Open-World Video Segmentation
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Pinxue Guo<sup class="ltx_sup" id="id11.9.id1"><span class="ltx_text ltx_font_italic" id="id11.9.id1.1">1,∗</span></sup>,
Zixu Zhao<sup class="ltx_sup" id="id12.10.id2"><span class="ltx_text ltx_font_italic" id="id12.10.id2.1">2,∗</span></sup>,
Jianxiong Gao<sup class="ltx_sup" id="id13.11.id3">3</sup>,
Chongruo Wu<sup class="ltx_sup" id="id14.12.id4">2</sup>,
Tong He<sup class="ltx_sup" id="id15.13.id5">2</sup>,
Zheng Zhang<sup class="ltx_sup" id="id16.14.id6">2</sup>, 
<br class="ltx_break"/>Tianjun Xiao<sup class="ltx_sup" id="id17.15.id7"><span class="ltx_text ltx_font_italic" id="id17.15.id7.1">2,†</span></sup>,
Wenqiang Zhang<sup class="ltx_sup" id="id18.16.id8"><span class="ltx_text ltx_font_italic" id="id18.16.id8.1">1,†</span></sup>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id19.id1">Video segmentation is essential for advancing robotics and autonomous driving, particularly in open-world settings where continuous perception and object association across video frames are critical. While the Segment Anything Model (SAM) has excelled in static image segmentation, extending its capabilities to video segmentation poses significant challenges. We tackle two major hurdles: a) SAM’s embedding limitations in associating objects across frames, and b) granularity inconsistencies in object segmentation. To this end, we introduce VideoSAM, an end-to-end framework designed to address these challenges by improving object tracking and segmentation consistency in dynamic environments. VideoSAM integrates an agglomerated backbone, RADIO, enabling object association through similarity metrics and introduces Cycle-ack-Pairs Propagation with a memory mechanism for stable object tracking. Additionally, we incorporate an autoregressive object-token mechanism within the SAM decoder to maintain consistent granularity across frames. Our method is extensively evaluated on the UVO and BURST benchmarks, and robotic videos from RoboTAP, demonstrating its effectiveness and robustness in real-world scenarios. All codes will be available.</p>
</div>
<div class="ltx_logical-block" id="id10">
<div class="ltx_para" id="id10.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="126" id="id9.g1" src="x1.png" width="830"/>
</div>
<figure class="ltx_figure ltx_align_center" id="S0.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text" id="S0.F1.2.1" style="font-size:90%;">VideoSAM produces open-world segmentation on videos with consistent object granularity. One color indicates one object.</span></figcaption>
</figure>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup class="ltx_sup" id="footnotex1.1">∗</sup> Equal Contribution, <sup class="ltx_sup" id="footnotex1.2">†</sup> Corresponding authors.</span></span></span><span class="ltx_note ltx_role_footnotetext" id="footnotex2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup class="ltx_sup" id="footnotex2.1">1</sup> Academy for Engineering and Technology, Fudan University. <sup class="ltx_sup" id="footnotex2.2">2</sup> Amazon Web Services Shanghai AI Lab. <sup class="ltx_sup" id="footnotex2.3">3</sup> Institute of Science and Technology for Brain-inspired Intelligence, Fudan University.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">INTRODUCTION</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Video segmentation is crucial in the development of robotics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib2" title="">2</a>]</cite> and autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib4" title="">4</a>]</cite>, presenting unparalleled challenges in open-world scenarios. The ability to accurately segment and associate anything across video frames is foundational for understanding dynamic environments, essential for safe and efficient navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib5" title="">5</a>]</cite> and manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib6" title="">6</a>]</cite>.
The Segment Anything Model (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib7" title="">7</a>]</cite>, has shown remarkable success in open-world image segmentation. Its ability to segment any object in an image without specific prior knowledge highlights its potential for addressing open-world challenges. However, applying SAM to videos introduces many challenges, especially in robotics where real-world environments require continuous perception over time.
Recent efforts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib10" title="">10</a>]</cite> seek to integrate SAM with existing propagation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib13" title="">13</a>]</cite>. While useful, these approaches lack end-to-end optimization and depend on human-defined inductive biases or rules that are less effective in complex scenarios.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this paper, we identify two major challenges in extending SAM to video scenarios, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S1.F2" title="Figure 2 ‣ I INTRODUCTION ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>.
1) <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">SAM embedding limitations</span>:
SAM mask embeddings, although effective for segmentation, struggle with associating objects across video frames. This is because SAM is highly specialized for segmentation, relying predominantly on high-level features from the MAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib14" title="">14</a>]</cite>-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib15" title="">15</a>]</cite> backbone, which limits its capacity to assess visual similarities between frames
2) <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">Granularity inconsistency across frames</span>: SAM often produces inconsistent segmentation granularity across frames, even prompted at the same object level. This inconsistency disrupts continuous object tracking and hinders the model’s ability to provide stable and reliable segmentation in videos.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To tackle these challenges, we propose VideoSAM, an end-to-end framework designed to consistently track and segment objects across frames while maintaining coherent granularity.
First, we integrate an agglomerated backbone, named RADIO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib16" title="">16</a>]</cite>, to overcome SAM embedding’s limitations in association. RADIO can perform well for both segmentation and visual similarity assessment, laying the foundation for a tracking system based on similarity metrics. Then, we introduce Cycle-ack Pairs Propagation and its corresponding memory mechanism in our system, designed to ensure high-confidence tracking of objects across frames, maintaining accurate and stable object identities throughout the tracking process.
Furthermore, to resolve the inconsistency in granularity, we introduce the AR-SAM Decode with autoregressive object prompt mechanism, adapted from SAM mask decoder. This object prompt is designed to carry forward object-level information across frames, ensuring stable and coherent segmentation. Meanwhile, the corresponding cycle-consistent training encourages consistent object granularity throughout the sequence, further enhancing robustness in dynamic scenes.
These not only achieve segmenting and tracking object with the consistent granularity across video frames in open-world scenario but also make the model end-to-end differentiable, enabling data-driven optimization to address tracking issues, compared to existing approaches relying on human-defined inductive biases that are less effective for complex scenarios. We validate the open-world video segmentation capabilities of the VideoSAM framework through extensive experiments on the UVO  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib17" title="">17</a>]</cite> and BURST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib18" title="">18</a>]</cite> benchmarks. Additionally, we apply VideoSAM to real-world robotic videos from RoboTAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib19" title="">19</a>]</cite>, further evaluating its general applicability across diverse environments.
We summarize our contributions as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce VideoSAM, an end-to-end framework that extends SAM to open-world video segmentation, specifically addressing the challenges of object association and consistent granularity across frames.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We integrate additional visual embeddings and introduce Cycle-ack Pairs Propagation with a memory mechanism to achieve efficient, robost, and stable object tracking across frames.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We present the AR-SAM decoder, incorporating the autoregressive object prompt within the SAM mask decoder. Combined with the cycle-sequential training strategy, it effectively addresses the issue of inconsistent segmentation granularity by differentiable optimization across frames.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We demonstrate the general applicability of VideoSAM through extensive experiments on open-world benchmarks, also validating its potential in real-world robotic applications.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S1.F2.g1" src="x2.png" width="797"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text" id="S1.F2.2.1" style="font-size:90%;">Limitations of the Segment Anything Model (SAM) in extending to the open-world video segmentation task. (a) We conduct oracle experiments using ground-truth masks for each frame to associate objects across frames, utilizing DINOv2/SAM embeddings pooled from the masks on the OVIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib20" title="">20</a>]</cite>. We also perform linear probing experiments on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib21" title="">21</a>]</cite>. These experiments demonstrate that while SAM embeddings are powerful for static image segmentation, they lack association and semantic information. (b) SAM exhibits inconsistent granularity when detecting objects across different frames, e.g., woman and cat.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Works</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Open-world segmentation</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Open-world segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib22" title="">22</a>]</cite>, a cutting-edge area in computer vision, challenges the norm by aiming to identify and delineate objects beyond pre-defined classes, crucial for applications like autonomous driving and robotic manipulation. It builds on advancements in open-world learning, such as incremental learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib23" title="">23</a>]</cite>, few-shot learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib24" title="">24</a>]</cite>, and meta-learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib2" title="">2</a>]</cite>, to tackle catastrophic forgetting and adaptively incorporate new object classes. Adaptations of semantic segmentation models for open-world settings focus on recognizing and categorizing ”unknown” entities through self-supervised learning and anomaly detection.
Recently, the Segment Anything model (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib7" title="">7</a>]</cite> has demonstrated strong zero-shot performance on image-level segmentation. The capability is gained through a data-engine that has a broader coverage of objects on scale and diversity. This effort has provide possibilities to more perception tasks. However, feature representation from SAM is proven not strong enough to support tasks requires strong semantic and association capabilities.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Object tracking</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Object tracking is a video-level task has vast application in video analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib27" title="">27</a>]</cite>, robotics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib29" title="">29</a>]</cite> and autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib30" title="">30</a>]</cite>. Usually, there are two streams of tracking pipeline: propagation-based and detection-based. Propagation-based methods propagate object mask from one frame to the next utilizing position, motion, similarity signals. Typical propagation-based tracking methods include XMem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib12" title="">12</a>]</cite>, AOT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib31" title="">31</a>]</cite>, DeAOT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib11" title="">11</a>]</cite>, etc. Detection-based tracking first run object detector on each frame, then associate object in different frames using feature similarity. Typical detection-based tracking approaches include MOTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib32" title="">32</a>]</cite>, TransTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib33" title="">33</a>]</cite>, OC-MOT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib27" title="">27</a>]</cite>, etc. Occlusion is a common challenge for tracking. Memory is proven to be a useful architecture to mitigate this issue. XMem provides a memory solution for propagation-based tracking. MeMOT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib34" title="">34</a>]</cite> provides that for detection-based tracking. OC-MOT provides the self-supervised way to build tracking memory.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Existing tracking-anything model</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Though SAM is an image-level model, several efforts started to integrate SAM with existing tracking modules in a plug-and-play way. TAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib10" title="">10</a>]</cite> integrates SAM with the propagation model X-mem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib12" title="">12</a>]</cite>. SAM-Track <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib8" title="">8</a>]</cite> integrates SAM with DeAOT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib11" title="">11</a>]</cite>. The model DEVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib35" title="">35</a>]</cite> didn’t use SAM in their paper submission but could be easily integrated. SAM-PT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib9" title="">9</a>]</cite> integrates SAM with a point-tracker, such as Cotracker <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib13" title="">13</a>]</cite>, propagating position prompts into the next frames. However, these methods lack end-to-end optimization and rely on human-defined inductive biases or rules that are less effective in complex scenarios. And SAM2 is a concurrent work with a focus on interactive segmentation but will suffer from high computational and memory costs in automatic video segmentation scenario, due to its per-object correspondence and memory design.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methods</span>
</h2>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="304" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overview of VideoSAM. It simultaneously employs DINOv2 and SAM embeddings from an efficient agglomerated encoder to handle object association and segmentation, respectively. Cycle-ack Pair Propagation is introduced to robustly associate objects across frames. AR-SAM Decoder, adapted from the mask decoder of SAM with temporal autoregressive object prompts, is used to maintain consistent segmentation granularity across video frames.</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce the architecture and components of the proposed VideoSAM framework, detailing how it extends SAM to the video domain for automatic open-world segmentation. We first provide an overview of SAM to establish the foundation for VideoSAM, followed by an in-depth discussion of the robust object association and granularity consistent segmentation we developed to address the key challenges of object association and granularity consistency across video frames.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Preliminaries of SAM</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.2">The Segment Anything Model (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib7" title="">7</a>]</cite> is a recent vision foundation model achieving state-of-the-art open-set image segmentation. It has showcased robust generation to produce high-quality masks from prompts including points, boxes, and masks. SAM consists of an image encoder, a flexible prompt encoder, and a lightweight mask decoder. The image encoder is a Vision Transformer (ViT) that transforms the high-resolution images to an image embedding of <math alttext="h\times w\times c" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">h</mi><mo id="S3.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">w</mi><mo id="S3.SS1.p1.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.4" xref="S3.SS1.p1.1.m1.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ℎ</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑤</ci><ci id="S3.SS1.p1.1.m1.1.1.4.cmml" xref="S3.SS1.p1.1.m1.1.1.4">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">h\times w\times c</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_h × italic_w × italic_c</annotation></semantics></math> spatial size. The prompt encoder projects prompts into <math alttext="c" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_c</annotation></semantics></math>-dimensional tokens as the input of the decoder. Besides prompt tokens, SAM decoder also takes three mask tokens as input, each indicating the mask granularity from subpart, part to whole object, and an extra IoU token indicating the mask reliability. The mask decoder then integrates these tokens with image embeddings to generate masks corresponding to three levels of granularity.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">However, in the context of detection-based tracking, we identify two issues of SAM: 1) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">Limited tracking capability of SAM image embeddings.</span> SAM image encoder are trained on large-scale SA-1B dataset but with segmentation loss only, which attributes to the fact that SAM image embeddings mainly preserve the localization signals for segmentation, while losing the crucial visual information such as semantics and appearances that are necessary for video correspondence learning. In our experiments as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S1.F2" title="Figure 2 ‣ I INTRODUCTION ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>(a), SAM image embeddings have demonstrated poor performance over the classification task under linear probe, and detection-based tracking task under mask feature pooling.
2) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">Granularity inconsistency of SAM masks.</span> Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S1.F2" title="Figure 2 ‣ I INTRODUCTION ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>(b) illustrates the granularity inconsistency issue of SAM across frames. SAM’s mask tokens are unable to maintain a specific segmentation granularity, making it challenging to achieve consistent object tracking, such as “always segmenting the complete person,” even when only using the object-level granularity.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">VideoSAM Framework</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To address the challenges of extending SAM from static images to videos, we propose VideoSAM, an end-to-end framework designed for open-world video segmentation.
VideoSAM extends SAM by introducing components for continuous and robust object tracking while maintaining consistent granularity across video frames. Specifically, it associates objects via tracking position prompts through the proposed Cycle-ack Pairs Propagation and segments them using the AR-SAM Decoder with a temporal autoregressive object prompt, as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S3.F3" title="Figure 3 ‣ III Methods ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>. In this framework, we fully exploit and preserve the open-world capabilities of the foundation model at each step, leveraging DINOv2 all-purpose embeddings for effective object association and SAM embeddings and decoder for zero-shot segmentation.
This framework preserves SAM’s strong segmentation ability while overcoming the two significant issues mentioned above. Additionally, instead of following tracking-by-detection paradigm, it avoids the inefficiency and uncorrectable detection granularity inconsistencies caused by generating frame-by-frame proposal masks in SAM everything mode.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Robust Object Association</span>
</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">Agglomerated Encoder.</span>
We replace the SAM encoder with an agglomerated encoder, RADIO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib16" title="">16</a>]</cite>, distilled from multiple vision foundation models including SAM, DINOv2, and CLIP, to provide strong features that have both powerful open-world segmentation capability (as in SAM), and rich visual information for feature association (as in DINOv2).
This design allows VideoSAM to fully exploit and the zero-shot segmentation and fine-grained matching ability of visual foundation model efficiently.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.8"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.8.1">Cycle-ack Pairs Propagation.</span>
VideoSAM associates objects across frames by tracking position prompts for the mask decoder. Specifically, given the image patch embeddings <math alttext="P\in\mathbb{R}^{H_{r}\times W_{r}\times d}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">P</mi><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml"><msub id="S3.SS3.p2.1.m1.1.1.3.3.2" xref="S3.SS3.p2.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.3.2.2" xref="S3.SS3.p2.1.m1.1.1.3.3.2.2.cmml">H</mi><mi id="S3.SS3.p2.1.m1.1.1.3.3.2.3" xref="S3.SS3.p2.1.m1.1.1.3.3.2.3.cmml">r</mi></msub><mo id="S3.SS3.p2.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.SS3.p2.1.m1.1.1.3.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.3.3.2" xref="S3.SS3.p2.1.m1.1.1.3.3.3.2.cmml">W</mi><mi id="S3.SS3.p2.1.m1.1.1.3.3.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.3.3.cmml">r</mi></msub><mo id="S3.SS3.p2.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS3.p2.1.m1.1.1.3.3.4" xref="S3.SS3.p2.1.m1.1.1.3.3.4.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><in id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></in><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑃</ci><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3"><times id="S3.SS3.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.1"></times><apply id="S3.SS3.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.2.2">𝐻</ci><ci id="S3.SS3.p2.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.2.3">𝑟</ci></apply><apply id="S3.SS3.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.3.2">𝑊</ci><ci id="S3.SS3.p2.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.3.3">𝑟</ci></apply><ci id="S3.SS3.p2.1.m1.1.1.3.3.4.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">P\in\mathbb{R}^{H_{r}\times W_{r}\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_P ∈ blackboard_R start_POSTSUPERSCRIPT italic_H start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT × italic_W start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> and object masks <math alttext="M\in\mathbb{O}^{N\times H_{r}\times W_{r}}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">M</mi><mo id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">𝕆</mi><mrow id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.3.2" xref="S3.SS3.p2.2.m2.1.1.3.3.2.cmml">N</mi><mo id="S3.SS3.p2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.2.m2.1.1.3.3.1.cmml">×</mo><msub id="S3.SS3.p2.2.m2.1.1.3.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.3.3.2" xref="S3.SS3.p2.2.m2.1.1.3.3.3.2.cmml">H</mi><mi id="S3.SS3.p2.2.m2.1.1.3.3.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.3.3.cmml">r</mi></msub><mo id="S3.SS3.p2.2.m2.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.2.m2.1.1.3.3.1.cmml">×</mo><msub id="S3.SS3.p2.2.m2.1.1.3.3.4" xref="S3.SS3.p2.2.m2.1.1.3.3.4.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.3.4.2" xref="S3.SS3.p2.2.m2.1.1.3.3.4.2.cmml">W</mi><mi id="S3.SS3.p2.2.m2.1.1.3.3.4.3" xref="S3.SS3.p2.2.m2.1.1.3.3.4.3.cmml">r</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><in id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"></in><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝑀</ci><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">𝕆</ci><apply id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3"><times id="S3.SS3.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.1"></times><ci id="S3.SS3.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.2">𝑁</ci><apply id="S3.SS3.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.3.2">𝐻</ci><ci id="S3.SS3.p2.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.3.3">𝑟</ci></apply><apply id="S3.SS3.p2.2.m2.1.1.3.3.4.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.3.3.4.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.4">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.3.3.4.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.4.2">𝑊</ci><ci id="S3.SS3.p2.2.m2.1.1.3.3.4.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.4.3">𝑟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">M\in\mathbb{O}^{N\times H_{r}\times W_{r}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_M ∈ blackboard_O start_POSTSUPERSCRIPT italic_N × italic_H start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT × italic_W start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> from the reference frame, along with the image patch embeddings from the current frame <math alttext="Q\in\mathbb{R}^{H_{t}\times W_{t}\times d}" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">Q</mi><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.2" xref="S3.SS3.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p2.3.m3.1.1.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.cmml"><msub id="S3.SS3.p2.3.m3.1.1.3.3.2" xref="S3.SS3.p2.3.m3.1.1.3.3.2.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.3.2.2" xref="S3.SS3.p2.3.m3.1.1.3.3.2.2.cmml">H</mi><mi id="S3.SS3.p2.3.m3.1.1.3.3.2.3" xref="S3.SS3.p2.3.m3.1.1.3.3.2.3.cmml">t</mi></msub><mo id="S3.SS3.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.SS3.p2.3.m3.1.1.3.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.3.3.2" xref="S3.SS3.p2.3.m3.1.1.3.3.3.2.cmml">W</mi><mi id="S3.SS3.p2.3.m3.1.1.3.3.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.cmml">t</mi></msub><mo id="S3.SS3.p2.3.m3.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS3.p2.3.m3.1.1.3.3.4" xref="S3.SS3.p2.3.m3.1.1.3.3.4.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><in id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></in><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">𝑄</ci><apply id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS3.p2.3.m3.1.1.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3"><times id="S3.SS3.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.1"></times><apply id="S3.SS3.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.3.3.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.3.3.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.2.2">𝐻</ci><ci id="S3.SS3.p2.3.m3.1.1.3.3.2.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.2.3">𝑡</ci></apply><apply id="S3.SS3.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.2">𝑊</ci><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3">𝑡</ci></apply><ci id="S3.SS3.p2.3.m3.1.1.3.3.4.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">Q\in\mathbb{R}^{H_{t}\times W_{t}\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_Q ∈ blackboard_R start_POSTSUPERSCRIPT italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT × italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, we aim to find the corresponding position for each object in the current frame to serve as the position prompt.
This process ensures that the segmentation masks are predicted in the order corresponding to their associated position prompts. Unlike the tracking-by-detection paradigm, this method eliminates inefficient frame-by-frame proposals and the need for complex post-processing.
We define a pair of patches from the reference and current frames, <math alttext="\mathrm{CP}\{p_{i},q_{j}\}" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.2"><semantics id="S3.SS3.p2.4.m4.2a"><mrow id="S3.SS3.p2.4.m4.2.2" xref="S3.SS3.p2.4.m4.2.2.cmml"><mi id="S3.SS3.p2.4.m4.2.2.4" xref="S3.SS3.p2.4.m4.2.2.4.cmml">CP</mi><mo id="S3.SS3.p2.4.m4.2.2.3" xref="S3.SS3.p2.4.m4.2.2.3.cmml">⁢</mo><mrow id="S3.SS3.p2.4.m4.2.2.2.2" xref="S3.SS3.p2.4.m4.2.2.2.3.cmml"><mo id="S3.SS3.p2.4.m4.2.2.2.2.3" stretchy="false" xref="S3.SS3.p2.4.m4.2.2.2.3.cmml">{</mo><msub id="S3.SS3.p2.4.m4.1.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.1.1.1.2" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml">p</mi><mi id="S3.SS3.p2.4.m4.1.1.1.1.1.3" xref="S3.SS3.p2.4.m4.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.p2.4.m4.2.2.2.2.4" xref="S3.SS3.p2.4.m4.2.2.2.3.cmml">,</mo><msub id="S3.SS3.p2.4.m4.2.2.2.2.2" xref="S3.SS3.p2.4.m4.2.2.2.2.2.cmml"><mi id="S3.SS3.p2.4.m4.2.2.2.2.2.2" xref="S3.SS3.p2.4.m4.2.2.2.2.2.2.cmml">q</mi><mi id="S3.SS3.p2.4.m4.2.2.2.2.2.3" xref="S3.SS3.p2.4.m4.2.2.2.2.2.3.cmml">j</mi></msub><mo id="S3.SS3.p2.4.m4.2.2.2.2.5" stretchy="false" xref="S3.SS3.p2.4.m4.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.2b"><apply id="S3.SS3.p2.4.m4.2.2.cmml" xref="S3.SS3.p2.4.m4.2.2"><times id="S3.SS3.p2.4.m4.2.2.3.cmml" xref="S3.SS3.p2.4.m4.2.2.3"></times><ci id="S3.SS3.p2.4.m4.2.2.4.cmml" xref="S3.SS3.p2.4.m4.2.2.4">CP</ci><set id="S3.SS3.p2.4.m4.2.2.2.3.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2"><apply id="S3.SS3.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2">𝑝</ci><ci id="S3.SS3.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS3.p2.4.m4.2.2.2.2.2.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.2.2">𝑞</ci><ci id="S3.SS3.p2.4.m4.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.2.3">𝑗</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.2c">\mathrm{CP}\{p_{i},q_{j}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.2d">roman_CP { italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }</annotation></semantics></math>, as the Cycle-ack Pair if, for a patch <math alttext="p_{i}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><mrow id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><msub id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml"><mi id="S3.SS3.p2.5.m5.1.1.2.2" xref="S3.SS3.p2.5.m5.1.1.2.2.cmml">p</mi><mi id="S3.SS3.p2.5.m5.1.1.2.3" xref="S3.SS3.p2.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS3.p2.5.m5.1.1.1" xref="S3.SS3.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml"><mi id="S3.SS3.p2.5.m5.1.1.3.2" xref="S3.SS3.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS3.p2.5.m5.1.1.3.3" xref="S3.SS3.p2.5.m5.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><in id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1.1"></in><apply id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.2.1.cmml" xref="S3.SS3.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2.2">𝑝</ci><ci id="S3.SS3.p2.5.m5.1.1.2.3.cmml" xref="S3.SS3.p2.5.m5.1.1.2.3">𝑖</ci></apply><apply id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.3.1.cmml" xref="S3.SS3.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.3.2.cmml" xref="S3.SS3.p2.5.m5.1.1.3.2">ℝ</ci><ci id="S3.SS3.p2.5.m5.1.1.3.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">p_{i}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> in the reference frame, its most similar patch in the current frame is <math alttext="q_{j}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.1"><semantics id="S3.SS3.p2.6.m6.1a"><mrow id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><msub id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2.2" xref="S3.SS3.p2.6.m6.1.1.2.2.cmml">q</mi><mi id="S3.SS3.p2.6.m6.1.1.2.3" xref="S3.SS3.p2.6.m6.1.1.2.3.cmml">j</mi></msub><mo id="S3.SS3.p2.6.m6.1.1.1" xref="S3.SS3.p2.6.m6.1.1.1.cmml">∈</mo><msup id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml"><mi id="S3.SS3.p2.6.m6.1.1.3.2" xref="S3.SS3.p2.6.m6.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS3.p2.6.m6.1.1.3.3" xref="S3.SS3.p2.6.m6.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><in id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1.1"></in><apply id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.2.1.cmml" xref="S3.SS3.p2.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2.2">𝑞</ci><ci id="S3.SS3.p2.6.m6.1.1.2.3.cmml" xref="S3.SS3.p2.6.m6.1.1.2.3">𝑗</ci></apply><apply id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.3.1.cmml" xref="S3.SS3.p2.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.3.2.cmml" xref="S3.SS3.p2.6.m6.1.1.3.2">ℝ</ci><ci id="S3.SS3.p2.6.m6.1.1.3.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">q_{j}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.1d">italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, and vice versa, the most relevant patch for <math alttext="q_{j}" class="ltx_Math" display="inline" id="S3.SS3.p2.7.m7.1"><semantics id="S3.SS3.p2.7.m7.1a"><msub id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml"><mi id="S3.SS3.p2.7.m7.1.1.2" xref="S3.SS3.p2.7.m7.1.1.2.cmml">q</mi><mi id="S3.SS3.p2.7.m7.1.1.3" xref="S3.SS3.p2.7.m7.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><apply id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.p2.7.m7.1.1.2.cmml" xref="S3.SS3.p2.7.m7.1.1.2">𝑞</ci><ci id="S3.SS3.p2.7.m7.1.1.3.cmml" xref="S3.SS3.p2.7.m7.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">q_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.7.m7.1d">italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> in the reference frame is <math alttext="p_{i}" class="ltx_Math" display="inline" id="S3.SS3.p2.8.m8.1"><semantics id="S3.SS3.p2.8.m8.1a"><msub id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml"><mi id="S3.SS3.p2.8.m8.1.1.2" xref="S3.SS3.p2.8.m8.1.1.2.cmml">p</mi><mi id="S3.SS3.p2.8.m8.1.1.3" xref="S3.SS3.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.1b"><apply id="S3.SS3.p2.8.m8.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.8.m8.1.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS3.p2.8.m8.1.1.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2">𝑝</ci><ci id="S3.SS3.p2.8.m8.1.1.3.cmml" xref="S3.SS3.p2.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.8.m8.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Formally, the condition for a Cycle-ack Pair is:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{CP}\{p_{i},q_{j}\}=\mathrm{True}~{}\text{if}~{}q_{j}=q_{j}^{*}~{}\text%
{and}~{}p_{i}=p_{i}^{*}," class="ltx_Math" display="block" id="S3.Ex1.m1.1"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.4" xref="S3.Ex1.m1.1.1.1.1.2.4.cmml">CP</mi><mo id="S3.Ex1.m1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S3.Ex1.m1.1.1.1.1.2.2.2" xref="S3.Ex1.m1.1.1.1.1.2.2.3.cmml"><mo id="S3.Ex1.m1.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.2.2.3.cmml">{</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml">p</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.Ex1.m1.1.1.1.1.2.2.2.4" xref="S3.Ex1.m1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.Ex1.m1.1.1.1.1.2.2.2.2" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.cmml">q</mi><mi id="S3.Ex1.m1.1.1.1.1.2.2.2.2.3" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.3.cmml">j</mi></msub><mo id="S3.Ex1.m1.1.1.1.1.2.2.2.5" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.2.2.3.cmml">}</mo></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.1.4" xref="S3.Ex1.m1.1.1.1.1.4.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.1.1.5" xref="S3.Ex1.m1.1.1.1.1.5.cmml"><mi id="S3.Ex1.m1.1.1.1.1.5.2" xref="S3.Ex1.m1.1.1.1.1.5.2.cmml">True</mi><mo id="S3.Ex1.m1.1.1.1.1.5.1" lspace="0.330em" xref="S3.Ex1.m1.1.1.1.1.5.1.cmml">⁢</mo><mtext id="S3.Ex1.m1.1.1.1.1.5.3" xref="S3.Ex1.m1.1.1.1.1.5.3a.cmml">if</mtext><mo id="S3.Ex1.m1.1.1.1.1.5.1a" lspace="0.330em" xref="S3.Ex1.m1.1.1.1.1.5.1.cmml">⁢</mo><msub id="S3.Ex1.m1.1.1.1.1.5.4" xref="S3.Ex1.m1.1.1.1.1.5.4.cmml"><mi id="S3.Ex1.m1.1.1.1.1.5.4.2" xref="S3.Ex1.m1.1.1.1.1.5.4.2.cmml">q</mi><mi id="S3.Ex1.m1.1.1.1.1.5.4.3" xref="S3.Ex1.m1.1.1.1.1.5.4.3.cmml">j</mi></msub></mrow><mo id="S3.Ex1.m1.1.1.1.1.6" xref="S3.Ex1.m1.1.1.1.1.6.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.1.1.7" xref="S3.Ex1.m1.1.1.1.1.7.cmml"><msubsup id="S3.Ex1.m1.1.1.1.1.7.2" xref="S3.Ex1.m1.1.1.1.1.7.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.7.2.2.2" xref="S3.Ex1.m1.1.1.1.1.7.2.2.2.cmml">q</mi><mi id="S3.Ex1.m1.1.1.1.1.7.2.2.3" xref="S3.Ex1.m1.1.1.1.1.7.2.2.3.cmml">j</mi><mo id="S3.Ex1.m1.1.1.1.1.7.2.3" xref="S3.Ex1.m1.1.1.1.1.7.2.3.cmml">∗</mo></msubsup><mo id="S3.Ex1.m1.1.1.1.1.7.1" xref="S3.Ex1.m1.1.1.1.1.7.1.cmml">⁢</mo><mtext id="S3.Ex1.m1.1.1.1.1.7.3" xref="S3.Ex1.m1.1.1.1.1.7.3a.cmml">and</mtext><mo id="S3.Ex1.m1.1.1.1.1.7.1a" lspace="0.330em" xref="S3.Ex1.m1.1.1.1.1.7.1.cmml">⁢</mo><msub id="S3.Ex1.m1.1.1.1.1.7.4" xref="S3.Ex1.m1.1.1.1.1.7.4.cmml"><mi id="S3.Ex1.m1.1.1.1.1.7.4.2" xref="S3.Ex1.m1.1.1.1.1.7.4.2.cmml">p</mi><mi id="S3.Ex1.m1.1.1.1.1.7.4.3" xref="S3.Ex1.m1.1.1.1.1.7.4.3.cmml">i</mi></msub></mrow><mo id="S3.Ex1.m1.1.1.1.1.8" xref="S3.Ex1.m1.1.1.1.1.8.cmml">=</mo><msubsup id="S3.Ex1.m1.1.1.1.1.9" xref="S3.Ex1.m1.1.1.1.1.9.cmml"><mi id="S3.Ex1.m1.1.1.1.1.9.2.2" xref="S3.Ex1.m1.1.1.1.1.9.2.2.cmml">p</mi><mi id="S3.Ex1.m1.1.1.1.1.9.2.3" xref="S3.Ex1.m1.1.1.1.1.9.2.3.cmml">i</mi><mo id="S3.Ex1.m1.1.1.1.1.9.3" xref="S3.Ex1.m1.1.1.1.1.9.3.cmml">∗</mo></msubsup></mrow><mo id="S3.Ex1.m1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><and id="S3.Ex1.m1.1.1.1.1a.cmml" xref="S3.Ex1.m1.1.1.1"></and><apply id="S3.Ex1.m1.1.1.1.1b.cmml" xref="S3.Ex1.m1.1.1.1"><eq id="S3.Ex1.m1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.1.1.4"></eq><apply id="S3.Ex1.m1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2"><times id="S3.Ex1.m1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3"></times><ci id="S3.Ex1.m1.1.1.1.1.2.4.cmml" xref="S3.Ex1.m1.1.1.1.1.2.4">CP</ci><set id="S3.Ex1.m1.1.1.1.1.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2"><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2">𝑝</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2">𝑞</ci><ci id="S3.Ex1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.3">𝑗</ci></apply></set></apply><apply id="S3.Ex1.m1.1.1.1.1.5.cmml" xref="S3.Ex1.m1.1.1.1.1.5"><times id="S3.Ex1.m1.1.1.1.1.5.1.cmml" xref="S3.Ex1.m1.1.1.1.1.5.1"></times><ci id="S3.Ex1.m1.1.1.1.1.5.2.cmml" xref="S3.Ex1.m1.1.1.1.1.5.2">True</ci><ci id="S3.Ex1.m1.1.1.1.1.5.3a.cmml" xref="S3.Ex1.m1.1.1.1.1.5.3"><mtext id="S3.Ex1.m1.1.1.1.1.5.3.cmml" xref="S3.Ex1.m1.1.1.1.1.5.3">if</mtext></ci><apply id="S3.Ex1.m1.1.1.1.1.5.4.cmml" xref="S3.Ex1.m1.1.1.1.1.5.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.5.4.1.cmml" xref="S3.Ex1.m1.1.1.1.1.5.4">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.5.4.2.cmml" xref="S3.Ex1.m1.1.1.1.1.5.4.2">𝑞</ci><ci id="S3.Ex1.m1.1.1.1.1.5.4.3.cmml" xref="S3.Ex1.m1.1.1.1.1.5.4.3">𝑗</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1.1.1c.cmml" xref="S3.Ex1.m1.1.1.1"><eq id="S3.Ex1.m1.1.1.1.1.6.cmml" xref="S3.Ex1.m1.1.1.1.1.6"></eq><share href="https://arxiv.org/html/2410.08781v1#S3.Ex1.m1.1.1.1.1.5.cmml" id="S3.Ex1.m1.1.1.1.1d.cmml" xref="S3.Ex1.m1.1.1.1"></share><apply id="S3.Ex1.m1.1.1.1.1.7.cmml" xref="S3.Ex1.m1.1.1.1.1.7"><times id="S3.Ex1.m1.1.1.1.1.7.1.cmml" xref="S3.Ex1.m1.1.1.1.1.7.1"></times><apply id="S3.Ex1.m1.1.1.1.1.7.2.cmml" xref="S3.Ex1.m1.1.1.1.1.7.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.7.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.7.2">superscript</csymbol><apply id="S3.Ex1.m1.1.1.1.1.7.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.7.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.7.2.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.7.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.7.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.7.2.2.2">𝑞</ci><ci id="S3.Ex1.m1.1.1.1.1.7.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.7.2.2.3">𝑗</ci></apply><times id="S3.Ex1.m1.1.1.1.1.7.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.7.2.3"></times></apply><ci id="S3.Ex1.m1.1.1.1.1.7.3a.cmml" xref="S3.Ex1.m1.1.1.1.1.7.3"><mtext id="S3.Ex1.m1.1.1.1.1.7.3.cmml" xref="S3.Ex1.m1.1.1.1.1.7.3">and</mtext></ci><apply id="S3.Ex1.m1.1.1.1.1.7.4.cmml" xref="S3.Ex1.m1.1.1.1.1.7.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.7.4.1.cmml" xref="S3.Ex1.m1.1.1.1.1.7.4">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.7.4.2.cmml" xref="S3.Ex1.m1.1.1.1.1.7.4.2">𝑝</ci><ci id="S3.Ex1.m1.1.1.1.1.7.4.3.cmml" xref="S3.Ex1.m1.1.1.1.1.7.4.3">𝑖</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1.1.1e.cmml" xref="S3.Ex1.m1.1.1.1"><eq id="S3.Ex1.m1.1.1.1.1.8.cmml" xref="S3.Ex1.m1.1.1.1.1.8"></eq><share href="https://arxiv.org/html/2410.08781v1#S3.Ex1.m1.1.1.1.1.7.cmml" id="S3.Ex1.m1.1.1.1.1f.cmml" xref="S3.Ex1.m1.1.1.1"></share><apply id="S3.Ex1.m1.1.1.1.1.9.cmml" xref="S3.Ex1.m1.1.1.1.1.9"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.9.1.cmml" xref="S3.Ex1.m1.1.1.1.1.9">superscript</csymbol><apply id="S3.Ex1.m1.1.1.1.1.9.2.cmml" xref="S3.Ex1.m1.1.1.1.1.9"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.9.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.9">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.9.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.9.2.2">𝑝</ci><ci id="S3.Ex1.m1.1.1.1.1.9.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.9.2.3">𝑖</ci></apply><times id="S3.Ex1.m1.1.1.1.1.9.3.cmml" xref="S3.Ex1.m1.1.1.1.1.9.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\mathrm{CP}\{p_{i},q_{j}\}=\mathrm{True}~{}\text{if}~{}q_{j}=q_{j}^{*}~{}\text%
{and}~{}p_{i}=p_{i}^{*},</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.1d">roman_CP { italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } = roman_True if italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT and italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="q_{j}^{*}=\arg\max_{q_{j}\in Q}\mathrm{cos}(p_{i},q_{j}),~{}~{}p_{i}^{*}=\arg%
\max_{p_{i}\in P}\mathrm{cos}(q_{j},p_{i})." class="ltx_Math" display="block" id="S3.Ex2.m1.1"><semantics id="S3.Ex2.m1.1a"><mrow id="S3.Ex2.m1.1.1.1"><mrow id="S3.Ex2.m1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.3.cmml"><mrow id="S3.Ex2.m1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.cmml"><msubsup id="S3.Ex2.m1.1.1.1.1.1.1.4" xref="S3.Ex2.m1.1.1.1.1.1.1.4.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.1.4.2.2" xref="S3.Ex2.m1.1.1.1.1.1.1.4.2.2.cmml">q</mi><mi id="S3.Ex2.m1.1.1.1.1.1.1.4.2.3" xref="S3.Ex2.m1.1.1.1.1.1.1.4.2.3.cmml">j</mi><mo id="S3.Ex2.m1.1.1.1.1.1.1.4.3" xref="S3.Ex2.m1.1.1.1.1.1.1.4.3.cmml">∗</mo></msubsup><mo id="S3.Ex2.m1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.1.3.cmml">=</mo><mrow id="S3.Ex2.m1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.1.2.cmml"><mrow id="S3.Ex2.m1.1.1.1.1.1.1.2.4" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.1.2.4.1" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.1.cmml">arg</mi><mo id="S3.Ex2.m1.1.1.1.1.1.1.2.4a" lspace="0.167em" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.cmml">⁡</mo><mrow id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.cmml"><munder id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.2" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.2.cmml">max</mi><mrow id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.cmml"><msub id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.2" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.2.cmml">q</mi><mi id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.3" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.3.cmml">j</mi></msub><mo id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.1" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.1.cmml">∈</mo><mi id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.3" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.3.cmml">Q</mi></mrow></munder><mo id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2a" lspace="0.167em" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.cmml">⁡</mo><mi id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.2" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.2.cmml">cos</mi></mrow></mrow><mo id="S3.Ex2.m1.1.1.1.1.1.1.2.3" xref="S3.Ex2.m1.1.1.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.3.cmml"><mo id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">p</mi><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.4" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">q</mi><mi id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.3.cmml">j</mi></msub><mo id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.5" stretchy="false" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex2.m1.1.1.1.1.2.3" rspace="0.827em" xref="S3.Ex2.m1.1.1.1.1.3a.cmml">,</mo><mrow id="S3.Ex2.m1.1.1.1.1.2.2" xref="S3.Ex2.m1.1.1.1.1.2.2.cmml"><msubsup id="S3.Ex2.m1.1.1.1.1.2.2.4" xref="S3.Ex2.m1.1.1.1.1.2.2.4.cmml"><mi id="S3.Ex2.m1.1.1.1.1.2.2.4.2.2" xref="S3.Ex2.m1.1.1.1.1.2.2.4.2.2.cmml">p</mi><mi id="S3.Ex2.m1.1.1.1.1.2.2.4.2.3" xref="S3.Ex2.m1.1.1.1.1.2.2.4.2.3.cmml">i</mi><mo id="S3.Ex2.m1.1.1.1.1.2.2.4.3" xref="S3.Ex2.m1.1.1.1.1.2.2.4.3.cmml">∗</mo></msubsup><mo id="S3.Ex2.m1.1.1.1.1.2.2.3" xref="S3.Ex2.m1.1.1.1.1.2.2.3.cmml">=</mo><mrow id="S3.Ex2.m1.1.1.1.1.2.2.2" xref="S3.Ex2.m1.1.1.1.1.2.2.2.cmml"><mrow id="S3.Ex2.m1.1.1.1.1.2.2.2.4" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.cmml"><mi id="S3.Ex2.m1.1.1.1.1.2.2.2.4.1" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.1.cmml">arg</mi><mo id="S3.Ex2.m1.1.1.1.1.2.2.2.4a" lspace="0.167em" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.cmml">⁡</mo><mrow id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.cmml"><munder id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.cmml"><mi id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.2" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.2.cmml">max</mi><mrow id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.cmml"><msub id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.cmml"><mi id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.2" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.2.cmml">p</mi><mi id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.3" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.1" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.1.cmml">∈</mo><mi id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.3" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.3.cmml">P</mi></mrow></munder><mo id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2a" lspace="0.167em" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.cmml">⁡</mo><mi id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.2" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.2.cmml">cos</mi></mrow></mrow><mo id="S3.Ex2.m1.1.1.1.1.2.2.2.3" xref="S3.Ex2.m1.1.1.1.1.2.2.2.3.cmml">⁢</mo><mrow id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.3.cmml"><mo id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.3" stretchy="false" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.3.cmml">(</mo><msub id="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.2.cmml">q</mi><mi id="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.4" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.3.cmml">,</mo><msub id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.cmml"><mi id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.2" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.2.cmml">p</mi><mi id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.3" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.5" stretchy="false" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex2.m1.1.1.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.1b"><apply id="S3.Ex2.m1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.3a.cmml" xref="S3.Ex2.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S3.Ex2.m1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1"><eq id="S3.Ex2.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.3"></eq><apply id="S3.Ex2.m1.1.1.1.1.1.1.4.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.4.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.4">superscript</csymbol><apply id="S3.Ex2.m1.1.1.1.1.1.1.4.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.4.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.1.1.4.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.4.2.2">𝑞</ci><ci id="S3.Ex2.m1.1.1.1.1.1.1.4.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.4.2.3">𝑗</ci></apply><times id="S3.Ex2.m1.1.1.1.1.1.1.4.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.4.3"></times></apply><apply id="S3.Ex2.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2"><times id="S3.Ex2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.3"></times><apply id="S3.Ex2.m1.1.1.1.1.1.1.2.4.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4"><arg id="S3.Ex2.m1.1.1.1.1.1.1.2.4.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.1"></arg><apply id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2"><apply id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1">subscript</csymbol><max id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.2"></max><apply id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3"><in id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.1"></in><apply id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.2">𝑞</ci><ci id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.2.3">𝑗</ci></apply><ci id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.1.3.3">𝑄</ci></apply></apply><ci id="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.4.2.2">cos</ci></apply></apply><interval closure="open" id="S3.Ex2.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.2"><apply id="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.2">𝑝</ci><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2">𝑞</ci><ci id="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.2.2.2.2.3">𝑗</ci></apply></interval></apply></apply><apply id="S3.Ex2.m1.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2"><eq id="S3.Ex2.m1.1.1.1.1.2.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.3"></eq><apply id="S3.Ex2.m1.1.1.1.1.2.2.4.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.4"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.2.2.4.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.4">superscript</csymbol><apply id="S3.Ex2.m1.1.1.1.1.2.2.4.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.4"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.2.2.4.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.4">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.2.2.4.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.4.2.2">𝑝</ci><ci id="S3.Ex2.m1.1.1.1.1.2.2.4.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.4.2.3">𝑖</ci></apply><times id="S3.Ex2.m1.1.1.1.1.2.2.4.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.4.3"></times></apply><apply id="S3.Ex2.m1.1.1.1.1.2.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2"><times id="S3.Ex2.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.3"></times><apply id="S3.Ex2.m1.1.1.1.1.2.2.2.4.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4"><arg id="S3.Ex2.m1.1.1.1.1.2.2.2.4.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.1"></arg><apply id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2"><apply id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1">subscript</csymbol><max id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.2"></max><apply id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3"><in id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.1"></in><apply id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.2">𝑝</ci><ci id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.2.3">𝑖</ci></apply><ci id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.1.3.3">𝑃</ci></apply></apply><ci id="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.4.2.2">cos</ci></apply></apply><interval closure="open" id="S3.Ex2.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.2"><apply id="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.2">𝑞</ci><ci id="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.1.1.1.1.3">𝑗</ci></apply><apply id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.2">𝑝</ci><ci id="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.1c">q_{j}^{*}=\arg\max_{q_{j}\in Q}\mathrm{cos}(p_{i},q_{j}),~{}~{}p_{i}^{*}=\arg%
\max_{p_{i}\in P}\mathrm{cos}(q_{j},p_{i}).</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.1d">italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ italic_Q end_POSTSUBSCRIPT roman_cos ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_P end_POSTSUBSCRIPT roman_cos ( italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p2.11"><math alttext="\mathrm{cos}(\cdot,\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p2.9.m1.2"><semantics id="S3.SS3.p2.9.m1.2a"><mrow id="S3.SS3.p2.9.m1.2.3" xref="S3.SS3.p2.9.m1.2.3.cmml"><mi id="S3.SS3.p2.9.m1.2.3.2" xref="S3.SS3.p2.9.m1.2.3.2.cmml">cos</mi><mo id="S3.SS3.p2.9.m1.2.3.1" xref="S3.SS3.p2.9.m1.2.3.1.cmml">⁢</mo><mrow id="S3.SS3.p2.9.m1.2.3.3.2" xref="S3.SS3.p2.9.m1.2.3.3.1.cmml"><mo id="S3.SS3.p2.9.m1.2.3.3.2.1" stretchy="false" xref="S3.SS3.p2.9.m1.2.3.3.1.cmml">(</mo><mo id="S3.SS3.p2.9.m1.1.1" lspace="0em" rspace="0em" xref="S3.SS3.p2.9.m1.1.1.cmml">⋅</mo><mo id="S3.SS3.p2.9.m1.2.3.3.2.2" rspace="0em" xref="S3.SS3.p2.9.m1.2.3.3.1.cmml">,</mo><mo id="S3.SS3.p2.9.m1.2.2" lspace="0em" rspace="0em" xref="S3.SS3.p2.9.m1.2.2.cmml">⋅</mo><mo id="S3.SS3.p2.9.m1.2.3.3.2.3" stretchy="false" xref="S3.SS3.p2.9.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m1.2b"><apply id="S3.SS3.p2.9.m1.2.3.cmml" xref="S3.SS3.p2.9.m1.2.3"><times id="S3.SS3.p2.9.m1.2.3.1.cmml" xref="S3.SS3.p2.9.m1.2.3.1"></times><ci id="S3.SS3.p2.9.m1.2.3.2.cmml" xref="S3.SS3.p2.9.m1.2.3.2">cos</ci><interval closure="open" id="S3.SS3.p2.9.m1.2.3.3.1.cmml" xref="S3.SS3.p2.9.m1.2.3.3.2"><ci id="S3.SS3.p2.9.m1.1.1.cmml" xref="S3.SS3.p2.9.m1.1.1">⋅</ci><ci id="S3.SS3.p2.9.m1.2.2.cmml" xref="S3.SS3.p2.9.m1.2.2">⋅</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m1.2c">\mathrm{cos}(\cdot,\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.9.m1.2d">roman_cos ( ⋅ , ⋅ )</annotation></semantics></math> denotes the cosine similarity, which ranges from <math alttext="[-1,1]" class="ltx_Math" display="inline" id="S3.SS3.p2.10.m2.2"><semantics id="S3.SS3.p2.10.m2.2a"><mrow id="S3.SS3.p2.10.m2.2.2.1" xref="S3.SS3.p2.10.m2.2.2.2.cmml"><mo id="S3.SS3.p2.10.m2.2.2.1.2" stretchy="false" xref="S3.SS3.p2.10.m2.2.2.2.cmml">[</mo><mrow id="S3.SS3.p2.10.m2.2.2.1.1" xref="S3.SS3.p2.10.m2.2.2.1.1.cmml"><mo id="S3.SS3.p2.10.m2.2.2.1.1a" xref="S3.SS3.p2.10.m2.2.2.1.1.cmml">−</mo><mn id="S3.SS3.p2.10.m2.2.2.1.1.2" xref="S3.SS3.p2.10.m2.2.2.1.1.2.cmml">1</mn></mrow><mo id="S3.SS3.p2.10.m2.2.2.1.3" xref="S3.SS3.p2.10.m2.2.2.2.cmml">,</mo><mn id="S3.SS3.p2.10.m2.1.1" xref="S3.SS3.p2.10.m2.1.1.cmml">1</mn><mo id="S3.SS3.p2.10.m2.2.2.1.4" stretchy="false" xref="S3.SS3.p2.10.m2.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.10.m2.2b"><interval closure="closed" id="S3.SS3.p2.10.m2.2.2.2.cmml" xref="S3.SS3.p2.10.m2.2.2.1"><apply id="S3.SS3.p2.10.m2.2.2.1.1.cmml" xref="S3.SS3.p2.10.m2.2.2.1.1"><minus id="S3.SS3.p2.10.m2.2.2.1.1.1.cmml" xref="S3.SS3.p2.10.m2.2.2.1.1"></minus><cn id="S3.SS3.p2.10.m2.2.2.1.1.2.cmml" type="integer" xref="S3.SS3.p2.10.m2.2.2.1.1.2">1</cn></apply><cn id="S3.SS3.p2.10.m2.1.1.cmml" type="integer" xref="S3.SS3.p2.10.m2.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.10.m2.2c">[-1,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.10.m2.2d">[ - 1 , 1 ]</annotation></semantics></math>. Cycle-ack pairs establish a highly reliable matching relationship, which can then be used to generate accurate position prompts for object masks in the current frame. For each object, as long as we identify the Cycle-ack Pairs from the patches in its mask, we can track the object position in the current frame and encode it as a point prompt for segmentation.
To efficiently identify cycle-ack pairs across all objects, we first compute a global similarity matrix between the reference and current frame features, <math alttext="S=\mathrm{cos}(P,Q)\in\mathbb{R}^{H_{r}W_{r}\times H_{t}W_{t}}" class="ltx_Math" display="inline" id="S3.SS3.p2.11.m3.2"><semantics id="S3.SS3.p2.11.m3.2a"><mrow id="S3.SS3.p2.11.m3.2.3" xref="S3.SS3.p2.11.m3.2.3.cmml"><mi id="S3.SS3.p2.11.m3.2.3.2" xref="S3.SS3.p2.11.m3.2.3.2.cmml">S</mi><mo id="S3.SS3.p2.11.m3.2.3.3" xref="S3.SS3.p2.11.m3.2.3.3.cmml">=</mo><mrow id="S3.SS3.p2.11.m3.2.3.4" xref="S3.SS3.p2.11.m3.2.3.4.cmml"><mi id="S3.SS3.p2.11.m3.2.3.4.2" xref="S3.SS3.p2.11.m3.2.3.4.2.cmml">cos</mi><mo id="S3.SS3.p2.11.m3.2.3.4.1" xref="S3.SS3.p2.11.m3.2.3.4.1.cmml">⁢</mo><mrow id="S3.SS3.p2.11.m3.2.3.4.3.2" xref="S3.SS3.p2.11.m3.2.3.4.3.1.cmml"><mo id="S3.SS3.p2.11.m3.2.3.4.3.2.1" stretchy="false" xref="S3.SS3.p2.11.m3.2.3.4.3.1.cmml">(</mo><mi id="S3.SS3.p2.11.m3.1.1" xref="S3.SS3.p2.11.m3.1.1.cmml">P</mi><mo id="S3.SS3.p2.11.m3.2.3.4.3.2.2" xref="S3.SS3.p2.11.m3.2.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p2.11.m3.2.2" xref="S3.SS3.p2.11.m3.2.2.cmml">Q</mi><mo id="S3.SS3.p2.11.m3.2.3.4.3.2.3" stretchy="false" xref="S3.SS3.p2.11.m3.2.3.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS3.p2.11.m3.2.3.5" xref="S3.SS3.p2.11.m3.2.3.5.cmml">∈</mo><msup id="S3.SS3.p2.11.m3.2.3.6" xref="S3.SS3.p2.11.m3.2.3.6.cmml"><mi id="S3.SS3.p2.11.m3.2.3.6.2" xref="S3.SS3.p2.11.m3.2.3.6.2.cmml">ℝ</mi><mrow id="S3.SS3.p2.11.m3.2.3.6.3" xref="S3.SS3.p2.11.m3.2.3.6.3.cmml"><mrow id="S3.SS3.p2.11.m3.2.3.6.3.2" xref="S3.SS3.p2.11.m3.2.3.6.3.2.cmml"><mrow id="S3.SS3.p2.11.m3.2.3.6.3.2.2" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.cmml"><msub id="S3.SS3.p2.11.m3.2.3.6.3.2.2.2" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.cmml"><mi id="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.2" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.2.cmml">H</mi><mi id="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.3" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.3.cmml">r</mi></msub><mo id="S3.SS3.p2.11.m3.2.3.6.3.2.2.1" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.1.cmml">⁢</mo><msub id="S3.SS3.p2.11.m3.2.3.6.3.2.2.3" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.cmml"><mi id="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.2" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.2.cmml">W</mi><mi id="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.3" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.3.cmml">r</mi></msub></mrow><mo id="S3.SS3.p2.11.m3.2.3.6.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.11.m3.2.3.6.3.2.1.cmml">×</mo><msub id="S3.SS3.p2.11.m3.2.3.6.3.2.3" xref="S3.SS3.p2.11.m3.2.3.6.3.2.3.cmml"><mi id="S3.SS3.p2.11.m3.2.3.6.3.2.3.2" xref="S3.SS3.p2.11.m3.2.3.6.3.2.3.2.cmml">H</mi><mi id="S3.SS3.p2.11.m3.2.3.6.3.2.3.3" xref="S3.SS3.p2.11.m3.2.3.6.3.2.3.3.cmml">t</mi></msub></mrow><mo id="S3.SS3.p2.11.m3.2.3.6.3.1" xref="S3.SS3.p2.11.m3.2.3.6.3.1.cmml">⁢</mo><msub id="S3.SS3.p2.11.m3.2.3.6.3.3" xref="S3.SS3.p2.11.m3.2.3.6.3.3.cmml"><mi id="S3.SS3.p2.11.m3.2.3.6.3.3.2" xref="S3.SS3.p2.11.m3.2.3.6.3.3.2.cmml">W</mi><mi id="S3.SS3.p2.11.m3.2.3.6.3.3.3" xref="S3.SS3.p2.11.m3.2.3.6.3.3.3.cmml">t</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.11.m3.2b"><apply id="S3.SS3.p2.11.m3.2.3.cmml" xref="S3.SS3.p2.11.m3.2.3"><and id="S3.SS3.p2.11.m3.2.3a.cmml" xref="S3.SS3.p2.11.m3.2.3"></and><apply id="S3.SS3.p2.11.m3.2.3b.cmml" xref="S3.SS3.p2.11.m3.2.3"><eq id="S3.SS3.p2.11.m3.2.3.3.cmml" xref="S3.SS3.p2.11.m3.2.3.3"></eq><ci id="S3.SS3.p2.11.m3.2.3.2.cmml" xref="S3.SS3.p2.11.m3.2.3.2">𝑆</ci><apply id="S3.SS3.p2.11.m3.2.3.4.cmml" xref="S3.SS3.p2.11.m3.2.3.4"><times id="S3.SS3.p2.11.m3.2.3.4.1.cmml" xref="S3.SS3.p2.11.m3.2.3.4.1"></times><ci id="S3.SS3.p2.11.m3.2.3.4.2.cmml" xref="S3.SS3.p2.11.m3.2.3.4.2">cos</ci><interval closure="open" id="S3.SS3.p2.11.m3.2.3.4.3.1.cmml" xref="S3.SS3.p2.11.m3.2.3.4.3.2"><ci id="S3.SS3.p2.11.m3.1.1.cmml" xref="S3.SS3.p2.11.m3.1.1">𝑃</ci><ci id="S3.SS3.p2.11.m3.2.2.cmml" xref="S3.SS3.p2.11.m3.2.2">𝑄</ci></interval></apply></apply><apply id="S3.SS3.p2.11.m3.2.3c.cmml" xref="S3.SS3.p2.11.m3.2.3"><in id="S3.SS3.p2.11.m3.2.3.5.cmml" xref="S3.SS3.p2.11.m3.2.3.5"></in><share href="https://arxiv.org/html/2410.08781v1#S3.SS3.p2.11.m3.2.3.4.cmml" id="S3.SS3.p2.11.m3.2.3d.cmml" xref="S3.SS3.p2.11.m3.2.3"></share><apply id="S3.SS3.p2.11.m3.2.3.6.cmml" xref="S3.SS3.p2.11.m3.2.3.6"><csymbol cd="ambiguous" id="S3.SS3.p2.11.m3.2.3.6.1.cmml" xref="S3.SS3.p2.11.m3.2.3.6">superscript</csymbol><ci id="S3.SS3.p2.11.m3.2.3.6.2.cmml" xref="S3.SS3.p2.11.m3.2.3.6.2">ℝ</ci><apply id="S3.SS3.p2.11.m3.2.3.6.3.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3"><times id="S3.SS3.p2.11.m3.2.3.6.3.1.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.1"></times><apply id="S3.SS3.p2.11.m3.2.3.6.3.2.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2"><times id="S3.SS3.p2.11.m3.2.3.6.3.2.1.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.1"></times><apply id="S3.SS3.p2.11.m3.2.3.6.3.2.2.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2"><times id="S3.SS3.p2.11.m3.2.3.6.3.2.2.1.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.1"></times><apply id="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.1.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.2.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.2">𝐻</ci><ci id="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.3.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.2.3">𝑟</ci></apply><apply id="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.3"><csymbol cd="ambiguous" id="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.1.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.3">subscript</csymbol><ci id="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.2.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.2">𝑊</ci><ci id="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.3.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.2.3.3">𝑟</ci></apply></apply><apply id="S3.SS3.p2.11.m3.2.3.6.3.2.3.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p2.11.m3.2.3.6.3.2.3.1.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.3">subscript</csymbol><ci id="S3.SS3.p2.11.m3.2.3.6.3.2.3.2.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.3.2">𝐻</ci><ci id="S3.SS3.p2.11.m3.2.3.6.3.2.3.3.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.2.3.3">𝑡</ci></apply></apply><apply id="S3.SS3.p2.11.m3.2.3.6.3.3.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.11.m3.2.3.6.3.3.1.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.3">subscript</csymbol><ci id="S3.SS3.p2.11.m3.2.3.6.3.3.2.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.3.2">𝑊</ci><ci id="S3.SS3.p2.11.m3.2.3.6.3.3.3.cmml" xref="S3.SS3.p2.11.m3.2.3.6.3.3.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.11.m3.2c">S=\mathrm{cos}(P,Q)\in\mathbb{R}^{H_{r}W_{r}\times H_{t}W_{t}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.11.m3.2d">italic_S = roman_cos ( italic_P , italic_Q ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_H start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT × italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>. We then identify all matching pairs through matrix operations:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\arg\max(S,~{}\mathrm{d}=H_{r}W_{r})=\arg\max(S,~{}\mathrm{d}=H_{t}W_{t})," class="ltx_Math" display="block" id="S3.Ex3.m1.5"><semantics id="S3.Ex3.m1.5a"><mrow id="S3.Ex3.m1.5.5.1" xref="S3.Ex3.m1.5.5.1.1.cmml"><mrow id="S3.Ex3.m1.5.5.1.1" xref="S3.Ex3.m1.5.5.1.1.cmml"><mrow id="S3.Ex3.m1.5.5.1.1.1" xref="S3.Ex3.m1.5.5.1.1.1.cmml"><mi id="S3.Ex3.m1.5.5.1.1.1.2" xref="S3.Ex3.m1.5.5.1.1.1.2.cmml">arg</mi><mo id="S3.Ex3.m1.5.5.1.1.1a" lspace="0.167em" xref="S3.Ex3.m1.5.5.1.1.1.cmml">⁡</mo><mrow id="S3.Ex3.m1.5.5.1.1.1.1.1" xref="S3.Ex3.m1.5.5.1.1.1.1.2.cmml"><mi id="S3.Ex3.m1.1.1" xref="S3.Ex3.m1.1.1.cmml">max</mi><mo id="S3.Ex3.m1.5.5.1.1.1.1.1a" xref="S3.Ex3.m1.5.5.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.Ex3.m1.5.5.1.1.1.1.1.1" xref="S3.Ex3.m1.5.5.1.1.1.1.2.cmml"><mo id="S3.Ex3.m1.5.5.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex3.m1.5.5.1.1.1.1.2.cmml">(</mo><mi id="S3.Ex3.m1.2.2" xref="S3.Ex3.m1.2.2.cmml">S</mi><mo id="S3.Ex3.m1.5.5.1.1.1.1.1.1.3" rspace="0.497em" xref="S3.Ex3.m1.5.5.1.1.1.1.2.cmml">,</mo><mrow id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.2" mathvariant="normal" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.2.cmml">d</mi><mo id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.cmml"><msub id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.2" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.2.cmml">H</mi><mi id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.3" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.3.cmml">r</mi></msub><mo id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.1" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><msub id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.2" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.2.cmml">W</mi><mi id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.3" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.3.cmml">r</mi></msub></mrow></mrow><mo id="S3.Ex3.m1.5.5.1.1.1.1.1.1.4" stretchy="false" xref="S3.Ex3.m1.5.5.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex3.m1.5.5.1.1.3" xref="S3.Ex3.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.Ex3.m1.5.5.1.1.2" xref="S3.Ex3.m1.5.5.1.1.2.cmml"><mi id="S3.Ex3.m1.5.5.1.1.2.2" xref="S3.Ex3.m1.5.5.1.1.2.2.cmml">arg</mi><mo id="S3.Ex3.m1.5.5.1.1.2a" lspace="0.167em" xref="S3.Ex3.m1.5.5.1.1.2.cmml">⁡</mo><mrow id="S3.Ex3.m1.5.5.1.1.2.1.1" xref="S3.Ex3.m1.5.5.1.1.2.1.2.cmml"><mi id="S3.Ex3.m1.3.3" xref="S3.Ex3.m1.3.3.cmml">max</mi><mo id="S3.Ex3.m1.5.5.1.1.2.1.1a" xref="S3.Ex3.m1.5.5.1.1.2.1.2.cmml">⁡</mo><mrow id="S3.Ex3.m1.5.5.1.1.2.1.1.1" xref="S3.Ex3.m1.5.5.1.1.2.1.2.cmml"><mo id="S3.Ex3.m1.5.5.1.1.2.1.1.1.2" stretchy="false" xref="S3.Ex3.m1.5.5.1.1.2.1.2.cmml">(</mo><mi id="S3.Ex3.m1.4.4" xref="S3.Ex3.m1.4.4.cmml">S</mi><mo id="S3.Ex3.m1.5.5.1.1.2.1.1.1.3" rspace="0.497em" xref="S3.Ex3.m1.5.5.1.1.2.1.2.cmml">,</mo><mrow id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.cmml"><mi id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.2" mathvariant="normal" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.2.cmml">d</mi><mo id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.1" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.cmml"><msub id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.cmml"><mi id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.2" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.2.cmml">H</mi><mi id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.3" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.3.cmml">t</mi></msub><mo id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.1" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.1.cmml">⁢</mo><msub id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.cmml"><mi id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.2" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.2.cmml">W</mi><mi id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.3" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.3.cmml">t</mi></msub></mrow></mrow><mo id="S3.Ex3.m1.5.5.1.1.2.1.1.1.4" stretchy="false" xref="S3.Ex3.m1.5.5.1.1.2.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex3.m1.5.5.1.2" xref="S3.Ex3.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.5b"><apply id="S3.Ex3.m1.5.5.1.1.cmml" xref="S3.Ex3.m1.5.5.1"><eq id="S3.Ex3.m1.5.5.1.1.3.cmml" xref="S3.Ex3.m1.5.5.1.1.3"></eq><apply id="S3.Ex3.m1.5.5.1.1.1.cmml" xref="S3.Ex3.m1.5.5.1.1.1"><arg id="S3.Ex3.m1.5.5.1.1.1.2.cmml" xref="S3.Ex3.m1.5.5.1.1.1.2"></arg><apply id="S3.Ex3.m1.5.5.1.1.1.1.2.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1"><max id="S3.Ex3.m1.1.1.cmml" xref="S3.Ex3.m1.1.1"></max><ci id="S3.Ex3.m1.2.2.cmml" xref="S3.Ex3.m1.2.2">𝑆</ci><apply id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1"><eq id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.1"></eq><ci id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.2">d</ci><apply id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3"><times id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.1"></times><apply id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.2">𝐻</ci><ci id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.2.3">𝑟</ci></apply><apply id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.2">𝑊</ci><ci id="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.Ex3.m1.5.5.1.1.1.1.1.1.1.3.3.3">𝑟</ci></apply></apply></apply></apply></apply><apply id="S3.Ex3.m1.5.5.1.1.2.cmml" xref="S3.Ex3.m1.5.5.1.1.2"><arg id="S3.Ex3.m1.5.5.1.1.2.2.cmml" xref="S3.Ex3.m1.5.5.1.1.2.2"></arg><apply id="S3.Ex3.m1.5.5.1.1.2.1.2.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1"><max id="S3.Ex3.m1.3.3.cmml" xref="S3.Ex3.m1.3.3"></max><ci id="S3.Ex3.m1.4.4.cmml" xref="S3.Ex3.m1.4.4">𝑆</ci><apply id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1"><eq id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.1.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.1"></eq><ci id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.2.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.2">d</ci><apply id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3"><times id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.1.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.1"></times><apply id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.1.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2">subscript</csymbol><ci id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.2.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.2">𝐻</ci><ci id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.3.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.2.3">𝑡</ci></apply><apply id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.1.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3">subscript</csymbol><ci id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.2.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.2">𝑊</ci><ci id="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.3.cmml" xref="S3.Ex3.m1.5.5.1.1.2.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.5c">\arg\max(S,~{}\mathrm{d}=H_{r}W_{r})=\arg\max(S,~{}\mathrm{d}=H_{t}W_{t}),</annotation><annotation encoding="application/x-llamapun" id="S3.Ex3.m1.5d">roman_arg roman_max ( italic_S , roman_d = italic_H start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) = roman_arg roman_max ( italic_S , roman_d = italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p2.16">where <math alttext="\arg\max(S,~{}\mathrm{d}=H_{r}W_{r})" class="ltx_Math" display="inline" id="S3.SS3.p2.12.m1.3"><semantics id="S3.SS3.p2.12.m1.3a"><mrow id="S3.SS3.p2.12.m1.3.3" xref="S3.SS3.p2.12.m1.3.3.cmml"><mi id="S3.SS3.p2.12.m1.3.3.2" xref="S3.SS3.p2.12.m1.3.3.2.cmml">arg</mi><mo id="S3.SS3.p2.12.m1.3.3a" lspace="0.167em" xref="S3.SS3.p2.12.m1.3.3.cmml">⁡</mo><mrow id="S3.SS3.p2.12.m1.3.3.1.1" xref="S3.SS3.p2.12.m1.3.3.1.2.cmml"><mi id="S3.SS3.p2.12.m1.1.1" xref="S3.SS3.p2.12.m1.1.1.cmml">max</mi><mo id="S3.SS3.p2.12.m1.3.3.1.1a" xref="S3.SS3.p2.12.m1.3.3.1.2.cmml">⁡</mo><mrow id="S3.SS3.p2.12.m1.3.3.1.1.1" xref="S3.SS3.p2.12.m1.3.3.1.2.cmml"><mo id="S3.SS3.p2.12.m1.3.3.1.1.1.2" stretchy="false" xref="S3.SS3.p2.12.m1.3.3.1.2.cmml">(</mo><mi id="S3.SS3.p2.12.m1.2.2" xref="S3.SS3.p2.12.m1.2.2.cmml">S</mi><mo id="S3.SS3.p2.12.m1.3.3.1.1.1.3" rspace="0.497em" xref="S3.SS3.p2.12.m1.3.3.1.2.cmml">,</mo><mrow id="S3.SS3.p2.12.m1.3.3.1.1.1.1" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.cmml"><mi id="S3.SS3.p2.12.m1.3.3.1.1.1.1.2" mathvariant="normal" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.2.cmml">d</mi><mo id="S3.SS3.p2.12.m1.3.3.1.1.1.1.1" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.1.cmml">=</mo><mrow id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.cmml"><msub id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.cmml"><mi id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.2" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.2.cmml">H</mi><mi id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.3" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.3.cmml">r</mi></msub><mo id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.1" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.1.cmml">⁢</mo><msub id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.cmml"><mi id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.2" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.2.cmml">W</mi><mi id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.3" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.3.cmml">r</mi></msub></mrow></mrow><mo id="S3.SS3.p2.12.m1.3.3.1.1.1.4" stretchy="false" xref="S3.SS3.p2.12.m1.3.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.12.m1.3b"><apply id="S3.SS3.p2.12.m1.3.3.cmml" xref="S3.SS3.p2.12.m1.3.3"><arg id="S3.SS3.p2.12.m1.3.3.2.cmml" xref="S3.SS3.p2.12.m1.3.3.2"></arg><apply id="S3.SS3.p2.12.m1.3.3.1.2.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1"><max id="S3.SS3.p2.12.m1.1.1.cmml" xref="S3.SS3.p2.12.m1.1.1"></max><ci id="S3.SS3.p2.12.m1.2.2.cmml" xref="S3.SS3.p2.12.m1.2.2">𝑆</ci><apply id="S3.SS3.p2.12.m1.3.3.1.1.1.1.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1"><eq id="S3.SS3.p2.12.m1.3.3.1.1.1.1.1.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.1"></eq><ci id="S3.SS3.p2.12.m1.3.3.1.1.1.1.2.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.2">d</ci><apply id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3"><times id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.1"></times><apply id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.1.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2">subscript</csymbol><ci id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.2.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.2">𝐻</ci><ci id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.3.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.2.3">𝑟</ci></apply><apply id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.1.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3">subscript</csymbol><ci id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.2.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.2">𝑊</ci><ci id="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.3.cmml" xref="S3.SS3.p2.12.m1.3.3.1.1.1.1.3.3.3">𝑟</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.12.m1.3c">\arg\max(S,~{}\mathrm{d}=H_{r}W_{r})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.12.m1.3d">roman_arg roman_max ( italic_S , roman_d = italic_H start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT )</annotation></semantics></math> refers to the matrix argmax operation along the <math alttext="H_{r}W_{r}" class="ltx_Math" display="inline" id="S3.SS3.p2.13.m2.1"><semantics id="S3.SS3.p2.13.m2.1a"><mrow id="S3.SS3.p2.13.m2.1.1" xref="S3.SS3.p2.13.m2.1.1.cmml"><msub id="S3.SS3.p2.13.m2.1.1.2" xref="S3.SS3.p2.13.m2.1.1.2.cmml"><mi id="S3.SS3.p2.13.m2.1.1.2.2" xref="S3.SS3.p2.13.m2.1.1.2.2.cmml">H</mi><mi id="S3.SS3.p2.13.m2.1.1.2.3" xref="S3.SS3.p2.13.m2.1.1.2.3.cmml">r</mi></msub><mo id="S3.SS3.p2.13.m2.1.1.1" xref="S3.SS3.p2.13.m2.1.1.1.cmml">⁢</mo><msub id="S3.SS3.p2.13.m2.1.1.3" xref="S3.SS3.p2.13.m2.1.1.3.cmml"><mi id="S3.SS3.p2.13.m2.1.1.3.2" xref="S3.SS3.p2.13.m2.1.1.3.2.cmml">W</mi><mi id="S3.SS3.p2.13.m2.1.1.3.3" xref="S3.SS3.p2.13.m2.1.1.3.3.cmml">r</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.13.m2.1b"><apply id="S3.SS3.p2.13.m2.1.1.cmml" xref="S3.SS3.p2.13.m2.1.1"><times id="S3.SS3.p2.13.m2.1.1.1.cmml" xref="S3.SS3.p2.13.m2.1.1.1"></times><apply id="S3.SS3.p2.13.m2.1.1.2.cmml" xref="S3.SS3.p2.13.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.13.m2.1.1.2.1.cmml" xref="S3.SS3.p2.13.m2.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.13.m2.1.1.2.2.cmml" xref="S3.SS3.p2.13.m2.1.1.2.2">𝐻</ci><ci id="S3.SS3.p2.13.m2.1.1.2.3.cmml" xref="S3.SS3.p2.13.m2.1.1.2.3">𝑟</ci></apply><apply id="S3.SS3.p2.13.m2.1.1.3.cmml" xref="S3.SS3.p2.13.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.13.m2.1.1.3.1.cmml" xref="S3.SS3.p2.13.m2.1.1.3">subscript</csymbol><ci id="S3.SS3.p2.13.m2.1.1.3.2.cmml" xref="S3.SS3.p2.13.m2.1.1.3.2">𝑊</ci><ci id="S3.SS3.p2.13.m2.1.1.3.3.cmml" xref="S3.SS3.p2.13.m2.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.13.m2.1c">H_{r}W_{r}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.13.m2.1d">italic_H start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> dimension, yielding indices <math alttext="\in\mathbb{N}^{H_{t}W_{t}}" class="ltx_Math" display="inline" id="S3.SS3.p2.14.m3.1"><semantics id="S3.SS3.p2.14.m3.1a"><mrow id="S3.SS3.p2.14.m3.1.1" xref="S3.SS3.p2.14.m3.1.1.cmml"><mi id="S3.SS3.p2.14.m3.1.1.2" xref="S3.SS3.p2.14.m3.1.1.2.cmml"></mi><mo id="S3.SS3.p2.14.m3.1.1.1" xref="S3.SS3.p2.14.m3.1.1.1.cmml">∈</mo><msup id="S3.SS3.p2.14.m3.1.1.3" xref="S3.SS3.p2.14.m3.1.1.3.cmml"><mi id="S3.SS3.p2.14.m3.1.1.3.2" xref="S3.SS3.p2.14.m3.1.1.3.2.cmml">ℕ</mi><mrow id="S3.SS3.p2.14.m3.1.1.3.3" xref="S3.SS3.p2.14.m3.1.1.3.3.cmml"><msub id="S3.SS3.p2.14.m3.1.1.3.3.2" xref="S3.SS3.p2.14.m3.1.1.3.3.2.cmml"><mi id="S3.SS3.p2.14.m3.1.1.3.3.2.2" xref="S3.SS3.p2.14.m3.1.1.3.3.2.2.cmml">H</mi><mi id="S3.SS3.p2.14.m3.1.1.3.3.2.3" xref="S3.SS3.p2.14.m3.1.1.3.3.2.3.cmml">t</mi></msub><mo id="S3.SS3.p2.14.m3.1.1.3.3.1" xref="S3.SS3.p2.14.m3.1.1.3.3.1.cmml">⁢</mo><msub id="S3.SS3.p2.14.m3.1.1.3.3.3" xref="S3.SS3.p2.14.m3.1.1.3.3.3.cmml"><mi id="S3.SS3.p2.14.m3.1.1.3.3.3.2" xref="S3.SS3.p2.14.m3.1.1.3.3.3.2.cmml">W</mi><mi id="S3.SS3.p2.14.m3.1.1.3.3.3.3" xref="S3.SS3.p2.14.m3.1.1.3.3.3.3.cmml">t</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.14.m3.1b"><apply id="S3.SS3.p2.14.m3.1.1.cmml" xref="S3.SS3.p2.14.m3.1.1"><in id="S3.SS3.p2.14.m3.1.1.1.cmml" xref="S3.SS3.p2.14.m3.1.1.1"></in><csymbol cd="latexml" id="S3.SS3.p2.14.m3.1.1.2.cmml" xref="S3.SS3.p2.14.m3.1.1.2">absent</csymbol><apply id="S3.SS3.p2.14.m3.1.1.3.cmml" xref="S3.SS3.p2.14.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.14.m3.1.1.3.1.cmml" xref="S3.SS3.p2.14.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.14.m3.1.1.3.2.cmml" xref="S3.SS3.p2.14.m3.1.1.3.2">ℕ</ci><apply id="S3.SS3.p2.14.m3.1.1.3.3.cmml" xref="S3.SS3.p2.14.m3.1.1.3.3"><times id="S3.SS3.p2.14.m3.1.1.3.3.1.cmml" xref="S3.SS3.p2.14.m3.1.1.3.3.1"></times><apply id="S3.SS3.p2.14.m3.1.1.3.3.2.cmml" xref="S3.SS3.p2.14.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.14.m3.1.1.3.3.2.1.cmml" xref="S3.SS3.p2.14.m3.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p2.14.m3.1.1.3.3.2.2.cmml" xref="S3.SS3.p2.14.m3.1.1.3.3.2.2">𝐻</ci><ci id="S3.SS3.p2.14.m3.1.1.3.3.2.3.cmml" xref="S3.SS3.p2.14.m3.1.1.3.3.2.3">𝑡</ci></apply><apply id="S3.SS3.p2.14.m3.1.1.3.3.3.cmml" xref="S3.SS3.p2.14.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.14.m3.1.1.3.3.3.1.cmml" xref="S3.SS3.p2.14.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.p2.14.m3.1.1.3.3.3.2.cmml" xref="S3.SS3.p2.14.m3.1.1.3.3.3.2">𝑊</ci><ci id="S3.SS3.p2.14.m3.1.1.3.3.3.3.cmml" xref="S3.SS3.p2.14.m3.1.1.3.3.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.14.m3.1c">\in\mathbb{N}^{H_{t}W_{t}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.14.m3.1d">∈ blackboard_N start_POSTSUPERSCRIPT italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>.
Next, we verify whether the position of <math alttext="q" class="ltx_Math" display="inline" id="S3.SS3.p2.15.m4.1"><semantics id="S3.SS3.p2.15.m4.1a"><mi id="S3.SS3.p2.15.m4.1.1" xref="S3.SS3.p2.15.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.15.m4.1b"><ci id="S3.SS3.p2.15.m4.1.1.cmml" xref="S3.SS3.p2.15.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.15.m4.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.15.m4.1d">italic_q</annotation></semantics></math> falls within the object mask in the reference frame, thus obtaining a set of cycle-ack points for each object in the current frame. For each point set, we select <math alttext="K" class="ltx_Math" display="inline" id="S3.SS3.p2.16.m5.1"><semantics id="S3.SS3.p2.16.m5.1a"><mi id="S3.SS3.p2.16.m5.1.1" xref="S3.SS3.p2.16.m5.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.16.m5.1b"><ci id="S3.SS3.p2.16.m5.1.1.cmml" xref="S3.SS3.p2.16.m5.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.16.m5.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.16.m5.1d">italic_K</annotation></semantics></math> points to encode the position prompt. The selection process involves calculating the centrality measure for each point, which is defined as the mean distance to all other points in the same set. After this, a top-k minimum selection is performed to choose the most central points.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">Memory Mechanism.</span>
To enable long-term object tracking, especially when the object undergoes significant deformation, we extend the reference of cycle-ack pairs propagation by introducing the CPs memory. The memory starts with the patch embeddings and object masks from the initial frame. As the video progresses, after each frame object is tracked and segmented, we add the patch embeddings and corresponding object mask (represented as one-hot vectors) of CPs into the memory, provided the point is indeed within the predicted mask.
If the memory reaches its maximum capacity of <math alttext="M" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_M</annotation></semantics></math> patches, some patches need to be discarded. To manage this, we track the utilization frequency of each CP, which reflects how often each patch has been used as a reference in forming CPs. Patches from the initial frame are preserved since they are the most reliable, and patches from the most recent frame are retained because they likely contain clues that are highly relevant to the current frame. For the remaining patches, we discard those with the lowest CPs utilization rate to make space for new patches, ensuring the memory does not exceed its maximum capacity.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Granularity Consistent Segmentation</span>
</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p1.1.1">AR-SAM Decoder with Object Prompt.</span>
To address the issue of granularity inconsistency difficult tracking, we introduce AR-SAM Decoder with a temporal autoregressive object prompt. This object prompt is designed to carry forward object-level information from one frame to the next, ensuring that objects are segmented with consistent granularity across the entire video.
Specifically, in mask decoder of VideoSAM, in addition to the position prompt, used to indicate object positions, and the learnable embeddings of mask tokens for object segmentation, we introduce an object prompt.
In addition, we insert an extra learned mask token into the set of prompt embeddings. This token is used at the decoder’s output to decode the mask with the specified granularity. A new self-attention layer is also added between all these prompt tokens, positioned between the original two-way attention block in the mask decoder.
The autoregressive object prompt mechanism effectively acts as a video-level granularity-consistent prompt, informing the mask decoder about the expected granularity of objects as they appear in successive frames.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">Cycle-sequential Training.</span>
To training the AR-SAM decoder and further encourage consistent granularity segmentation across frames, we introduce a cycle-sequential training strategy.
In cycle-sequential training, the network is trained on sequences of consecutive frames during each single iteration.
When segmenting the first frame, the model uses annotated or sampled points and ground-truth masks for training. For subsequent frames, it relies on the predictions from previous frames, utilizing both the maintaining memory and the AR-token to tracking object and maintain consistency.
Moreover, at the end of each sequence, the last frame is cycled back to the first frame. This requires that the AR-token produced by the last frame also generates a mask with the same granularity as the original frame, ensuring consistent segmentation throughout the sequence.
This training regime encourages the model to produce similar segmentations when tracking objects forward and backward through video sequences.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiment</span>
</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Video segmentation performance on UVO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib17" title="">17</a>]</cite>. Higher is better. Best performing method is bolded.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.1.1.1.1" style="padding-left:3.6pt;padding-right:3.6pt;">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2" style="padding-left:3.6pt;padding-right:3.6pt;">TAM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib10" title="">10</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3" style="padding-left:3.6pt;padding-right:3.6pt;">SAM-PT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib9" title="">9</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4" style="padding-left:3.6pt;padding-right:3.6pt;">SAM-PD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib36" title="">36</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5" style="padding-left:3.6pt;padding-right:3.6pt;">MASA-B<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib37" title="">37</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.6" style="padding-left:3.6pt;padding-right:3.6pt;">MASA-H<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib37" title="">37</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.7" style="padding-left:3.6pt;padding-right:3.6pt;">DEVA-SAM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib35" title="">35</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.8" style="padding-left:3.6pt;padding-right:3.6pt;">DEVA-SAM-o<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib35" title="">35</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.9" style="padding-left:3.6pt;padding-right:3.6pt;">OVTrackor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib38" title="">38</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.10" style="padding-left:3.6pt;padding-right:3.6pt;">VideoSAM</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.2.1.1" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text" id="S4.T1.1.2.1.1.1" style="font-size:70%;color:#808080;">Source</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.1.2" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text" id="S4.T1.1.2.1.2.1" style="font-size:70%;color:#808080;">arXiv’2023</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.1.3" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text" id="S4.T1.1.2.1.3.1" style="font-size:70%;color:#808080;">arXiv’2023</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.1.4" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text" id="S4.T1.1.2.1.4.1" style="font-size:70%;color:#808080;">arXiv’2024</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.1.5" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text" id="S4.T1.1.2.1.5.1" style="font-size:70%;color:#808080;">CVPR’2024</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.1.6" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text" id="S4.T1.1.2.1.6.1" style="font-size:70%;color:#808080;">CVPR’2024</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.1.7" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text" id="S4.T1.1.2.1.7.1" style="font-size:70%;color:#808080;">ICCV’2023</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.1.8" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text" id="S4.T1.1.2.1.8.1" style="font-size:70%;color:#808080;">ICCV’2023</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.1.9" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text" id="S4.T1.1.2.1.9.1" style="font-size:70%;color:#808080;">ICRA’2024</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.1.10" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text" id="S4.T1.1.2.1.10.1" style="font-size:70%;color:#808080;">Ours</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.3.2.1" style="padding-left:3.6pt;padding-right:3.6pt;">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.2" style="padding-left:3.6pt;padding-right:3.6pt;">1.7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.3" style="padding-left:3.6pt;padding-right:3.6pt;">6.8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.4" style="padding-left:3.6pt;padding-right:3.6pt;">6.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.5" style="padding-left:3.6pt;padding-right:3.6pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.6" style="padding-left:3.6pt;padding-right:3.6pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.7" style="padding-left:3.6pt;padding-right:3.6pt;">3.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.8" style="padding-left:3.6pt;padding-right:3.6pt;">8.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.9" style="padding-left:3.6pt;padding-right:3.6pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.3.2.10" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.10.1">9.7</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.1.4.3.1" style="padding-left:3.6pt;padding-right:3.6pt;">AR100</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.2" style="padding-left:3.6pt;padding-right:3.6pt;">24.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.3" style="padding-left:3.6pt;padding-right:3.6pt;">28.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.4" style="padding-left:3.6pt;padding-right:3.6pt;">24.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.5" style="padding-left:3.6pt;padding-right:3.6pt;">28.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.6" style="padding-left:3.6pt;padding-right:3.6pt;">28.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.7" style="padding-left:3.6pt;padding-right:3.6pt;">18.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.8" style="padding-left:3.6pt;padding-right:3.6pt;">22.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.9" style="padding-left:3.6pt;padding-right:3.6pt;">28.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.10" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.10.1">29.8</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="135" id="S4.F4.g1" src="x4.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative results of VideoSAM compared with the SAM baseline and DEVA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib35" title="">35</a>]</cite> on UVO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib17" title="">17</a>]</cite> and BURST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib18" title="">18</a>]</cite> datasets. The baseline prompts SAM with points propagated by feature similarity. In overall, VideoSAM reliably tracks objects and generates object masks with consistent granularity.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Implementation Details</span>
</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Training.</span> The encoder of VideoSAM is pretrained from RADIO, and frozen during training. The mask decoder is pretrained from SAM and fine-tuned via LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib39" title="">39</a>]</cite>. The parameters of the newly introduced learnable embeddings and layers for object prompts and the autoregressive mechanism are trained from scratch.
For training, we collected over 5,000 videos with segmentation annotations from video instance segmentation datasets, including YouTubeVIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib40" title="">40</a>]</cite>, UVO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib17" title="">17</a>]</cite>, and BURST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib18" title="">18</a>]</cite>. We trained the model for 120k iterations using 8 A10 GPUs, starting with an initial learning rate of 0.001, which was halved at [15k, 30k, 50k, 70k, 90k] iterations. Following SAM, we utilize focal loss and dice loss for mask segmentation, and mean squared error loss for IoU prediction in the mask decoder.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Inference.</span>
VideoSAM uses grid point traversal to detect objects in the first frame, similar to the “everything/automatic” mode in SAM. For the first frame, the object prompt is initially a learnable embedding, and the point prompts come from a 32x32 grid. We generate N confident object masks after filtering by the predicted IoU, mask stability scores, and NMS.
For subsequent frames, the position prompts are generated by Cycle-ack Pairs Propagation, and the object prompt is transformed from the previous frame autoregressive token.
For the object-in logic, we perform re-detection every few frames, as the same in DEVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib35" title="">35</a>]</cite>.
New objects are identified by matching their masks with the propagated object masks via IoU, and later added to the tracklets. For the object-out logic, we firstly pool N object tokens from the predicted object masks and N tracklet tokens from the historical masks. If the most similar counterpart of an object is not the matched by its own tracklet, we consider that object as “out” for that frame. If an object is out for 10 consecutive frames, it is considered to be permanently out of the video.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Benchmarks</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">UVO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib17" title="">17</a>]</cite> is a comprehensive benchmark for open-world, class-agnostic object segmentation in videos, featuring exhaustive annotations. On average, the UVO dataset provides 13.52 annotated instances per video. In contrast, the close-set video instance segmentation benchmark, YouTubeVIS, contains only 1.68 objects per video. This makes UVO a more suitable dataset for evaluating open-world capabilities.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">BURST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib18" title="">18</a>]</cite> is an open-vocabulary video segmentation benchmark whose sequences are longer (even thousands of frames) compared to UVO.
In this paper, we evaluate methods on BURST videos in a low-frame-rate scenario (1 FPS), where the tracking becomes more difficult due to the dramatic appearance change. T
Since BURST does not provide annations for all the objects shown in the video, we only evaluate the annotated objects that persist throughout the whole video.
</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">RoboTAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib19" title="">19</a>]</cite> is a point-tracking benchmark for robotic scenarios, providing point annotations but no segmentation mask annotations. We conducted qualitative experiments using robotic operation videos from RoboTAP to preliminarily validate the applicability of the proposed model in real-world robotic vision systems, such as in robot manipulation.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib17" title="">17</a>]</cite>, we adopt two evaluation metrics: video-level Average Precision (AP) and Average Recall (AR) in the class-agnostic manner, which are commonly used across various datasets and evaluation settings.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Evaluation</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.T1" title="TABLE I ‣ IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">I</span></a> and Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.T2" title="TABLE II ‣ IV-C Evaluation ‣ IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">II</span></a> quantitatively compare the open-world video segmentation performance of different methods on the UVO and BUSRT benchmarks, respectively. As we can observe, VideoSAM achieves the state-of-the-art performance across almost all metrics, showing its superiority on consistent object tracking and segmentation in both high-frame-rate and low-frame-rate scenarios.
DEVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib35" title="">35</a>]</cite>, the previous method with the high AP,
integrates SAM with a mask propagation module, XMem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib12" title="">12</a>]</cite>. However, it greatly suffers from the granularity issue from SAM, even using the object-level granularity for inference. DEVA-SAM uses SAM to automatically detect objects at all three levels of granularity, following the default settings of the original SAM. In contrast, DEVA-SAM-o exclusively uses object-level granularity for detection.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.F4" title="Figure 4 ‣ IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> provides a qualitative comparison between VideoSAM and two other methods on UVO and BURST. The baseline uses SAM (with object-level granularity) and follows the position prompt propagation paradigm (by taking the argmax of feature similarity, rather than our cycle-ack pairs propagation) to track objects.
As shown, VideoSAM consistently tracks and segments objects more stably, while DEVA’s pixel-level mask propagation model occasionally makes localized errors. Compared to the SAM baseline, VideoSAM maintains consistent granularity across frames as expected. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.F5" title="Figure 5 ‣ IV-C Evaluation ‣ IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>, further shows results of VideoSAM on the robotic grasping videos at 3FPS from RoboTAP. Without any finetuning, VideoSAM still produce reliable panoptic segmentation in real-world robotic vision systems.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Video segmentation performance on BURST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib18" title="">18</a>]</cite>. Higher is better. Best performing method is bolded.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.1" style="padding-left:2.8pt;padding-right:2.8pt;">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.2" style="padding-left:2.8pt;padding-right:2.8pt;">Source</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3" style="padding-left:2.8pt;padding-right:2.8pt;">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4" style="padding-left:2.8pt;padding-right:2.8pt;">AP50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.5" style="padding-left:2.8pt;padding-right:2.8pt;">AP75</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.6" style="padding-left:2.8pt;padding-right:2.8pt;">AR100</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.7" style="padding-left:2.8pt;padding-right:2.8pt;">AR10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.8" style="padding-left:2.8pt;padding-right:2.8pt;">AR1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1" style="padding-left:2.8pt;padding-right:2.8pt;">M2F+STCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib18" title="">18</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.2.1.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S4.T2.1.2.1.2.1" style="font-size:70%;color:#808080;">WACV’23</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3" style="padding-left:2.8pt;padding-right:2.8pt;">4.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4" style="padding-left:2.8pt;padding-right:2.8pt;">8.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.5" style="padding-left:2.8pt;padding-right:2.8pt;">3.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.6" style="padding-left:2.8pt;padding-right:2.8pt;">47.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.7" style="padding-left:2.8pt;padding-right:2.8pt;">41.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.8" style="padding-left:2.8pt;padding-right:2.8pt;">14.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.3.2.1" style="padding-left:2.8pt;padding-right:2.8pt;">DEVA-M2F <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib35" title="">35</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.3.2.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S4.T2.1.3.2.2.1" style="font-size:70%;color:#808080;">ICCV’23</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3" style="padding-left:2.8pt;padding-right:2.8pt;">4.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.4" style="padding-left:2.8pt;padding-right:2.8pt;">6.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.5" style="padding-left:2.8pt;padding-right:2.8pt;">4.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.6" style="padding-left:2.8pt;padding-right:2.8pt;">53.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.7" style="padding-left:2.8pt;padding-right:2.8pt;">40.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.8" style="padding-left:2.8pt;padding-right:2.8pt;">12.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.4.3.1" style="padding-left:2.8pt;padding-right:2.8pt;">DEVA-Entity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib35" title="">35</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.4.3.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S4.T2.1.4.3.2.1" style="font-size:70%;color:#808080;">ICCV’23</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.3" style="padding-left:2.8pt;padding-right:2.8pt;">6.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.4" style="padding-left:2.8pt;padding-right:2.8pt;">8.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.5" style="padding-left:2.8pt;padding-right:2.8pt;">6.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.6" style="padding-left:2.8pt;padding-right:2.8pt;">52.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.7" style="padding-left:2.8pt;padding-right:2.8pt;">41.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.8" style="padding-left:2.8pt;padding-right:2.8pt;">14.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.5.4.1" style="padding-left:2.8pt;padding-right:2.8pt;">DEVA-SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib35" title="">35</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.5.4.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S4.T2.1.5.4.2.1" style="font-size:70%;color:#808080;">ICCV’23</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.3" style="padding-left:2.8pt;padding-right:2.8pt;">4.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.4" style="padding-left:2.8pt;padding-right:2.8pt;">8.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.5" style="padding-left:2.8pt;padding-right:2.8pt;">3.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.6" style="padding-left:2.8pt;padding-right:2.8pt;">37.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.7" style="padding-left:2.8pt;padding-right:2.8pt;">27.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.8" style="padding-left:2.8pt;padding-right:2.8pt;">6.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.6.5.1" style="padding-left:2.8pt;padding-right:2.8pt;">DEVA-SAM-o <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib35" title="">35</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.6.5.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S4.T2.1.6.5.2.1" style="font-size:70%;color:#808080;">ICCV’23</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.3" style="padding-left:2.8pt;padding-right:2.8pt;">7.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.4" style="padding-left:2.8pt;padding-right:2.8pt;">11.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.5" style="padding-left:2.8pt;padding-right:2.8pt;">8.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.6.1">54.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.7" style="padding-left:2.8pt;padding-right:2.8pt;">43.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.8" style="padding-left:2.8pt;padding-right:2.8pt;">8.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.1.7.6.1" style="padding-left:2.8pt;padding-right:2.8pt;">VideoSAM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.1.7.6.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S4.T2.1.7.6.2.1" style="font-size:70%;color:#808080;">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.6.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.6.3.1">10.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.6.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.6.4.1">14.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.6.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.6.5.1">11.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.6.6" style="padding-left:2.8pt;padding-right:2.8pt;">53.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.6.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.6.7.1">45.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.6.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.6.8.1">18.1</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="234" id="S4.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Qualitative performance on the RoboTAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#bib.bib19" title="">19</a>]</cite> dataset.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Ablation study on tracking related components.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T3.1.1.1.1.1">RADIO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.2" rowspan="2"><span class="ltx_text" id="S4.T3.1.1.1.2.1">CPs</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.3" rowspan="2"><span class="ltx_text" id="S4.T3.1.1.1.3.1">Mem</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.1.1.1.4">UVO</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.1.1.1.5">BURST</th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.1">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T3.1.2.2.2">AR100</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.3">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.4">AR100</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.3.1.1">✓</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.1.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.4">12.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.1.5">26.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.6">10.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.7">53.5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.1.4.2.1">×</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.2.3">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.4">9.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.2.5">28.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.6">10.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.7">47.5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.1.5.3.1">✓</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.2">×</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.3.3">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.4">9.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.3.5">26.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.6">8.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.7">41.7</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.1.6.4.1">✓</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.6.4.3">×</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.4">9.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.6.4.5">17.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.6">11.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.7">38.8</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T3.1.7.5.1">×</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.7.5.2">×</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.7.5.3">×</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.7.5.4">7.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.7.5.5">15.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.7.5.6">7.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.7.5.7">29.3</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Ablation study on the points prompt number.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.1.1.1.1">K</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.2">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.3">2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.4">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.5">4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.6">5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T4.1.2.1.1">mAP</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.1.2.1.2">12.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.1.2.1.3">10.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.1.2.1.4">10.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.1.2.1.5">9.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.1.2.1.6">9.9</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="501" id="S4.F6.g1" src="x6.png" width="798"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Effect of Cycle-pairs Propagation. Red and green points represent propagated CPs, with red arrows highlighting key results. Best viewed when zoomed in.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Ablation study of granularity related components.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T5.1.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T5.1.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2" id="S4.T5.1.1.1.3">UVO</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2" id="S4.T5.1.1.1.4">BURST</th>
</tr>
<tr class="ltx_tr" id="S4.T5.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T5.1.2.2.1">Obj-Prompt</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T5.1.2.2.2">Cycle Train</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T5.1.2.2.3">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T5.1.2.2.4">AR100</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T5.1.2.2.5">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.1.2.2.6">AR100</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.3.1.1">✓</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.3.1.2">✓</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.3.1.3">12.1</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.3.1.4">26.6</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.3.1.5">10.7</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.3.1.6">53.5</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.1.4.2.1">✓</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.4.2.2">×</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.1.4.2.3">11.9</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.4.2.4">26.5</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.1.4.2.5">11.0</th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.2.6">49.4</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T5.1.5.3.1">×</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.1.5.3.2">×</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T5.1.5.3.3">9.2</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.1.5.3.4">17.1</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T5.1.5.3.5">10.9</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.5.3.6">38.8</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Ablation Study</span>
</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.1">Object Association.</span>
We extensively evaluate key designs of the position-prompt for robust object association and demonstrate their effectiveness. As shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.T3" title="TABLE III ‣ IV-C Evaluation ‣ IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">III</span></a>, the first row presents the default design of the proposed VideoSAM. The row-2 shows that RADIO features improve propagation compared to SAM features. The row-3 demonstrates the significant improvement of Cycle-Pairs over simply using the point with the highest similarity from the correlation map. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.F6" title="Figure 6 ‣ IV-C Evaluation ‣ IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the comparison between max correlation and CPs, where CPs offer more reliable propagation due to their strict bidirectional confirmation mechanism. Central CPs further refine this by identifying the most reliable central point. Since our mask decoder with object prompts effectively maintains object granularity, a single point is sufficient as the position prompt to accurately locate the object, as shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.T4" title="TABLE IV ‣ IV-C Evaluation ‣ IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">IV</span></a>. The row-4 highlights how continuously updating memory through CPs significantly enhances tracking performance. The final row presents the baseline described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.SS3" title="IV-C Evaluation ‣ IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Granularity Consistency.</span>
As shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.T5" title="TABLE V ‣ IV-C Evaluation ‣ IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">V</span></a>, the object prompt effectively addresses SAM’s granularity inconsistency issue, while cycle-sequential training further enhances the mask decoder’s ability to maintain granularity with the object prompt. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08781v1#S4.F4" title="Figure 4 ‣ IV Experiment ‣ VideoSAM: Open-World Video Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>, the second frame of the video on the left shows that VideoSAM preserves the complete granularity of the person segmentation, whereas both the SAM baseline and DEVA exhibit part-errors or over-segmentation. In the video on the right, both SAM and DEVA encounter over-segmentation or under-segmentation issues for the person and the large box, while VideoSAM resolves these problems through the use of object prompts.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we addressed two key challenges in extending SAM to the open-world video segmentation task by proposing VideoSAM, an end-to-end framework. The introduction of Cycle-ack Pairs Propagation with a memory mechanism enables efficient, robust, and stable object tracking across frames. Additionally, the AR-SAM decoder, which incorporates an autoregressive object prompt within the mask decoder, combined with a cycle-sequential training strategy, effectively resolves the issue of inconsistent segmentation granularity through differentiable optimization across frames. Extensive experiments on open-world benchmarks demonstrate the general applicability of VideoSAM, highlighting its potential for real-world robotic applications.
All code will be available to support the research community. In the future, we plan to explore more complex scenarios involving more frequent object entry and exit as well as camera transitions. Additionally, we will investigate how VideoSAM can be applied to robotic vision systems to improve navigation or manipulation performance.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. V. Hurtado and A. Valada, “Semantic scene segmentation for robotics,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Deep learning for robot perception and cognition</em>.   Elsevier, 2022, pp. 279–311.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Z. Zhao, Y. Jin, B. Lu, C.-F. Ng, Q. Dou, Y.-H. Liu, and P.-A. Heng, “One to many: Adaptive instrument segmentation via meta learning and dynamic online adaptation in robotic surgical video,” in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">2021 IEEE International Conference on Robotics and Automation (ICRA)</em>, 2021, pp. 13 553–13 559.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. Siam, A. Kendall, and M. Jagersand, “Video class agnostic segmentation benchmark for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 2825–2834.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K. Muhammad, T. Hussain, H. Ullah, J. Del Ser, M. Rezaei, N. Kumar, M. Hijji, P. Bellavista, and V. H. C. de Albuquerque, “Vision-based semantic segmentation in scene understanding for autonomous driving: Recent achievements, challenges, and outlooks,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 23, no. 12, pp. 22 694–22 715, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Rashed, A. El Sallab, S. Yogamani, and M. ElHelw, “Motion and depth augmented semantic segmentation for autonomous navigation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em>, 2019, pp. 0–0.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B. Lu, B. Li, W. Chen, Y. Jin, Z. Zhao, Q. Dou, P.-A. Heng, and Y. Liu, “Toward image-guided automated suture grasping under complex environments: A learning-enabled and optimization-based holistic framework,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">IEEE Transactions on Automation Science and Engineering</em>, vol. 19, no. 4, pp. 3794–3808, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et al.</em>, “Segment anything,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">arXiv preprint arXiv:2304.02643</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. Cheng, L. Li, Y. Xu, X. Li, Z. Yang, W. Wang, and Y. Yang, “Segment and track anything,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2305.06558</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
F. Rajič, L. Ke, Y.-W. Tai, C.-K. Tang, M. Danelljan, and F. Yu, “Segment anything meets point tracking,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2307.01197</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Yang, M. Gao, Z. Li, S. Gao, F. Wang, and F. Zheng, “Track anything: Segment anything meets videos,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2304.11968</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Yang and Y. Yang, “Decoupling features in hierarchical propagation for video object segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 36 324–36 336, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H. K. Cheng and A. G. Schwing, “Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">European Conference on Computer Vision</em>.   Springer, 2022, pp. 640–658.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
N. Karaev, I. Rocco, B. Graham, N. Neverova, A. Vedaldi, and C. Rupprecht, “Cotracker: It is better to track together,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2307.07635</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable vision learners,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 16 000–16 009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">et al.</em>, “An image is worth 16x16 words: Transformers for image recognition at scale,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Ranzinger, G. Heinrich, J. Kautz, and P. Molchanov, “Am-radio: Agglomerative model–reduce all domains into one,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2312.06709</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W. Wang, M. Feiszli, H. Wang, and D. Tran, “Unidentified video objects: A benchmark for dense, open-world segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 10 776–10 785.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Athar, J. Luiten, P. Voigtlaender, T. Khurana, A. Dave, B. Leibe, and D. Ramanan, “Burst: A benchmark for unifying object recognition, segmentation and tracking in video,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, 2023, pp. 1674–1683.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. Vecerik, C. Doersch, Y. Yang, T. Davchev, Y. Aytar, G. Zhou, R. Hadsell, L. Agapito, and J. Scholz, “Robotap: Tracking arbitrary points for few-shot visual imitation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 5397–5403.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Qi, Y. Gao, Y. Hu, X. Wang, X. Liu, X. Bai, S. Belongie, A. Yuille, P. H. Torr, and S. Bai, “Occluded video instance segmentation: A benchmark,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">International Journal of Computer Vision</em>, vol. 130, no. 8, pp. 2022–2039, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">2009 IEEE conference on computer vision and pattern recognition</em>.   Ieee, 2009, pp. 248–255.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
L. Qi, J. Kuen, Y. Wang, J. Gu, H. Zhao, P. Torr, Z. Lin, and J. Jia, “Open world entity segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 45, no. 7, pp. 8743–8756, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, and Y. Fu, “Large scale incremental learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 374–382.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Parnami and M. Lee, “Learning from few examples: A summary of approaches to few-shot learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2203.04291</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
K. Hsu, S. Levine, and C. Finn, “Unsupervised learning via meta-learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:1810.02334</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
X. Chen, H. Peng, D. Wang, H. Lu, and H. Hu, “Seqtrack: Sequence to sequence learning for visual object tracking,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2023, pp. 14 572–14 581.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Z. Zhao, J. Wang, M. Horn, Y. Ding, T. He, Z. Bai, D. Zietlow, C.-J. Simon-Gabriel, B. Shuai, Z. Tu, T. Brox, B. Schiele, Y. Fu, F. Locatello, Z. Zhang, and T. Xiao, “Object-centric multiple object tracking,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, October 2023, pp. 16 601–16 611.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Z. Zhao, Y. Jin, and P. Heng, “Trasetr: Track-to-segment transformer with contrastive query for instance-level instrument segmentation in robotic surgery,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">2022 International Conference on Robotics and Automation (ICRA)</em>, 2022, pp. 11 186–11 193.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Gad, T. Basmaji, M. Yaghi, H. Alheeh, M. Alkhedher, and M. Ghazal, “Multiple object tracking in robotic applications: Trends and challenges,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Applied Sciences</em>, vol. 12, no. 19, p. 9408, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
C. Luo, X. Yang, and A. Yuille, “Exploring simple 3d multi-object tracking for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 10 488–10 497.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Z. Yang, Y. Wei, and Y. Yang, “Associating objects with transformers for video object segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 2491–2502, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
F. Zeng, B. Dong, Y. Zhang, T. Wang, X. Zhang, and Y. Wei, “Motr: End-to-end multiple-object tracking with transformer,” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">European Conference on Computer Vision</em>.   Springer, 2022, pp. 659–675.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
P. Sun, J. Cao, Y. Jiang, R. Zhang, E. Xie, Z. Yuan, C. Wang, and P. Luo, “Transtrack: Multiple object tracking with transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2012.15460</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J. Cai, M. Xu, W. Li, Y. Xiong, W. Xia, Z. Tu, and S. Soatto, “Memot: Multi-object tracking with memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 8090–8100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
H. K. Cheng, S. W. Oh, B. Price, A. Schwing, and J.-Y. Lee, “Tracking anything with decoupled video segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 1316–1326.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T. Zhou, W. Luo, Q. Ye, Z. Shi, and J. Chen, “Sam-pd: How far can sam take us in tracking and segmenting anything in videos by prompt denoising,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2403.04194</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
S. Li, L. Ke, M. Danelljan, L. Piccinelli, M. Segu, L. Van Gool, and F. Yu, “Matching anything by segmenting anything,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024, pp. 18 963–18 973.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
W.-H. Chu, A. W. Harley, P. Tokmakov, A. Dave, L. Guibas, and K. Fragkiadaki, “Zero-shot open-vocabulary tracking with large pre-trained models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 4916–4923.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
L. Yang, Y. Fan, and N. Xu, “Video instance segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 5188–5197.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct 11 12:50:39 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
