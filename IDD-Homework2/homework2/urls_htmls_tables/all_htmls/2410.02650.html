<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Undesirable Memorization in Large Language Models: A Survey</title>
<!--Generated on Thu Oct  3 16:31:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Memorization,  Large Language Models,  Privacy in LLMs
" lang="en" name="keywords"/>
<base href="/html/2410.02650v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S1" title="In Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2" title="In Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Spectrum of Memorization</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS1" title="In II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Intentionality</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS1.SSS0.Px1" title="In II-A Intentionality ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Unintended memorization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS1.SSS0.Px2" title="In II-A Intentionality ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Deliberate memorization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS2" title="In II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Severity of memorization</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS2.SSS0.Px1" title="In II-B Severity of memorization ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Perfect memorization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS2.SSS0.Px2" title="In II-B Severity of memorization ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Verbatim memorization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS2.SSS0.Px3" title="In II-B Severity of memorization ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Approximate memorization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS3" title="In II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Retrievability</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS3.SSS0.Px1" title="In II-C Retrievability ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Extractable memorization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS3.SSS0.Px2" title="In II-C Retrievability ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Discoverable memorization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS3.SSS0.Px3" title="In II-C Retrievability ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Comparing discoverable and extractable Memorization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS4" title="In II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Abstraction</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS4.SSS0.Px1" title="In II-D Abstraction ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Factual memorization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS4.SSS0.Px2" title="In II-D Abstraction ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Conceptual memorization</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S3" title="In Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Measuring memorization</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S3.SS1" title="In III Measuring memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Exposure metric</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S3.SS2" title="In III Measuring memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Inference attacks</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S3.SS2.SSS0.Px1" title="In III-B Inference attacks ‣ III Measuring memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Membership Inference Attacks (MIA)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S3.SS2.SSS0.Px2" title="In III-B Inference attacks ‣ III Measuring memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Extraction Attacks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S3.SS3" title="In III Measuring memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Counterfactual memorization</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4" title="In Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Influences and dynamics of memorization</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS1" title="In IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Model capacity</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS2" title="In IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Training data characteristics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS3" title="In IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Input and prompting strategies</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS4" title="In IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Tokenization</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS5" title="In IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_italic">Sampling methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS6" title="In IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span> </span><span class="ltx_text ltx_font_italic">Fine-tuning and transfer learning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS7" title="In IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-G</span> </span><span class="ltx_text ltx_font_italic">Training process dynamics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS8" title="In IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-H</span> </span><span class="ltx_text ltx_font_italic">Forgetting mechanisms</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS8.SSS0.Px1" title="In IV-H Forgetting mechanisms ‣ IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Catastrophic forgetting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS8.SSS0.Px2" title="In IV-H Forgetting mechanisms ‣ IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Intentional forgetting techniques</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S5" title="In Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Memorization in specific model architectures</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S5.SS1" title="In V Memorization in specific model architectures ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Masked language models (MLM)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S5.SS2" title="In V Memorization in specific model architectures ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Aligned language models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S5.SS3" title="In V Memorization in specific model architectures ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Retrieval augmented generation (RAG)</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S6" title="In Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Mitigating memorization: strategies and techniques</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S6.SS1" title="In VI Mitigating memorization: strategies and techniques ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Data de-duplication</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S6.SS2" title="In VI Mitigating memorization: strategies and techniques ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">MemFree decoding</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S6.SS3" title="In VI Mitigating memorization: strategies and techniques ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Differential privacy</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S6.SS4" title="In VI Mitigating memorization: strategies and techniques ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-D</span> </span><span class="ltx_text ltx_font_italic">Unlearning methods</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S7" title="In Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Future directions and open challenges</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S7.SS1" title="In VII Future directions and open challenges ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-A</span> </span><span class="ltx_text ltx_font_italic">Balancing performance with privacy in LLMs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S7.SS2" title="In VII Future directions and open challenges ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-B</span> </span><span class="ltx_text ltx_font_italic">Reducing factual memorization in favor of conceptual memorization</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S7.SS3" title="In VII Future directions and open challenges ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-C</span> </span><span class="ltx_text ltx_font_italic">The boundary of memorization and understanding</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S7.SS4" title="In VII Future directions and open challenges ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-D</span> </span><span class="ltx_text ltx_font_italic">Memorization in specific contexts</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S7.SS4.SSS0.Px1" title="In VII-D Memorization in specific contexts ‣ VII Future directions and open challenges ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Conversational Agents</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S7.SS4.SSS0.Px2" title="In VII-D Memorization in specific contexts ‣ VII Future directions and open challenges ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Retrieval-Augmented Generation (RAG)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S7.SS4.SSS0.Px3" title="In VII-D Memorization in specific contexts ‣ VII Future directions and open challenges ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Multilingual Large Language Models (xLM)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S7.SS4.SSS0.Px4" title="In VII-D Memorization in specific contexts ‣ VII Future directions and open challenges ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Diffusion Language Models (DLM)</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S8" title="In Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Undesirable Memorization in Large Language Models: A Survey
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ali Satvaty
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">University of Groningen</span>
<br class="ltx_break"/>Groningen, the Netherlands 
<br class="ltx_break"/>a.satvaty@rug.nl

</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Suzan Verberne
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.1.id1">Leiden University</span>
<br class="ltx_break"/>Leiden, the Netherlands 
<br class="ltx_break"/>s.verberne@liacs.leidenuniv.nl

</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fatih Turkmen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.1.id1">University of Groningen</span>
<br class="ltx_break"/>Groningen, the Netherlands 
<br class="ltx_break"/>f.turkmen@rug.nl

</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">While recent research increasingly showcases the remarkable capabilities of Large Language Models (LLMs), it’s vital to confront their hidden pitfalls. Among these challenges, the issue of memorization stands out, posing significant ethical and legal risks. In this paper, we presents a Systematization of Knowledge (SoK) on the topic of memorization in LLMs. Memorization is the effect that a model tends to store and reproduce phrases or passages from the training data and has been shown to be the fundamental issue to various privacy and security attacks against LLMs.</p>
<p class="ltx_p" id="id5.id2">We begin by providing an overview of the literature on the memorization, exploring it across five key dimensions: intentionality, degree, retrievability, abstraction, and transparency. Next, we discuss the metrics and methods used to measure memorization, followed by an analysis of the factors that contribute to memorization phenomenon. We then examine how memorization manifests itself in specific model architectures and explore strategies for mitigating these effects. We conclude our overview by identifying potential research topics for the near future: to develop methods for balancing performance and privacy in LLMs, and the analysis of memorization in specific contexts, including conversational agents, retrieval-augmented generation, multilingual language models, and diffusion language models.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Memorization, Large Language Models, Privacy in LLMs

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, large language models (LLMs) have demonstrated remarkable advancements, driven by the scaling of model parameters, large amounts of data, or training paradigms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib1" title=""><span class="ltx_text" style="color:#000000;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib2" title=""><span class="ltx_text" style="color:#000000;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib3" title=""><span class="ltx_text" style="color:#000000;">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib4" title=""><span class="ltx_text" style="color:#000000;">4</span></a>]</cite>. State-of-the-art models have exhibited capabilities across a broad spectrum of Natural Language Processing (NLP) tasks, consistently pushing the benchmark records in areas such as text generation, code synthesis, machine translation, question answering, and summarization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib5" title=""><span class="ltx_text" style="color:#000000;">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib6" title=""><span class="ltx_text" style="color:#000000;">6</span></a>]</cite>. These models use self-supervised learning on massive datasets, enabling them to perform competitively or even surpass human-level performance in specific tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib7" title=""><span class="ltx_text" style="color:#000000;">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib8" title=""><span class="ltx_text" style="color:#000000;">8</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite these impressive advancements of LLMs, researchers have pointed out the problematic features of these models, including hallucination <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib9" title=""><span class="ltx_text" style="color:#000000;">9</span></a>]</cite>, bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib10" title=""><span class="ltx_text" style="color:#000000;">10</span></a>]</cite>, and privacy and security <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib11" title=""><span class="ltx_text" style="color:#000000;">11</span></a>]</cite>. In the context of data privacy, memorization is one of the most significant issues. Memorization in LLM refers to the model’s tendency to store and reproduce exact phrases or passages from the training data rather than generating novel or generalized outputs. While memorization can be advantageous in knowledge-intensive benchmarks, such as factual recall tasks or domain-specific question answering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib12" title=""><span class="ltx_text" style="color:#000000;">12</span></a>]</cite>, it introduces significant ethical and legal challenges: First, models may inadvertently reveal sensitive or private information included in their training data, posing significant privacy and security risks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib13" title=""><span class="ltx_text" style="color:#000000;">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib14" title=""><span class="ltx_text" style="color:#000000;">14</span></a>]</cite>. Second, the ability of LLMs to repeat verbatim copyrighted or proprietary text from their training data raises issues related to intellectual property infringement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib15" title=""><span class="ltx_text" style="color:#000000;">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib16" title=""><span class="ltx_text" style="color:#000000;">16</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">These challenges motivate the need to further explore memorization in LLMs to effectively tackle its associated challenges. We provide an overview of aspects related to memorization, emphasizing the need for further exploration of this topic, thereby balancing model performance with the risks of privacy breaches and ethical concerns.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Before recent advances in LLMs, memorization was explored extensively as a topic in machine learning and deep learning. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Usynin et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib17" title=""><span class="ltx_text" style="color:#000000;">17</span></a>]</cite> explore memorization in machine learning. They propose a framework to quantify the influence of individual data samples and detect memorization in various learning settings. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Wei et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib18" title=""><span class="ltx_text" style="color:#000000;">18</span></a>]</cite> provide a systematic framework for understanding memorization in deep neural networks, discussing LLM memorization as a type of deep neural network. Survey papers on the privacy and safety of LLMs often address memorization as a core phenomenon, framing it as both a privacy issue and a foundational factor that supports other challenges they explore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib19" title=""><span class="ltx_text" style="color:#000000;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib20" title=""><span class="ltx_text" style="color:#000000;">20</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Hartmann et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib21" title=""><span class="ltx_text" style="color:#000000;">21</span></a>]</cite> provide an overview of memorization in general-purpose LLMs. They aggregate memorization-related topics from the copyright, privacy, security, and model performance perspectives. In our survey, we not only add more recent work, but we specifically focus on memorization as an undesirable phenomenon, with the aspects intentionality, severity of memorization, retrievability, and transparency of memorization. In addition, we provide an extensive and concrete research agenda for the near future. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S1.T1" title="TABLE I ‣ I Introduction ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">I</span></a> provides a comparison of our scope to the scopes of previous surveys.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S1.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.1.1">
<span class="ltx_p" id="S1.T1.1.1.1.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.1.1.1.1">Paper</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S1.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.2.1">
<span class="ltx_p" id="S1.T1.1.1.1.2.1.1" style="width:142.3pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.2.1.1.1">Focus</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S1.T1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.3.1">
<span class="ltx_p" id="S1.T1.1.1.1.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.3.1.1.1">Differences With our Study</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.1.1">
<span class="ltx_p" id="S1.T1.1.2.1.1.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Usynin et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib17" title=""><span class="ltx_text" style="color:#000000;">17</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.2.1">
<span class="ltx_p" id="S1.T1.1.2.1.2.1.1" style="width:142.3pt;">Memorization in general machine learning</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.3.1">
<span class="ltx_p" id="S1.T1.1.2.1.3.1.1" style="width:199.2pt;">We focus on LLMs</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.1.1">
<span class="ltx_p" id="S1.T1.1.3.2.1.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Wei et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib18" title=""><span class="ltx_text" style="color:#000000;">18</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.2.1">
<span class="ltx_p" id="S1.T1.1.3.2.2.1.1" style="width:142.3pt;">Memorization in deep neural networks</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.3.1">
<span class="ltx_p" id="S1.T1.1.3.2.3.1.1" style="width:199.2pt;">We focus on LLMs</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.1.1">
<span class="ltx_p" id="S1.T1.1.4.3.1.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Neel and Chang</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib19" title=""><span class="ltx_text" style="color:#000000;">19</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.2.1">
<span class="ltx_p" id="S1.T1.1.4.3.2.1.1" style="width:142.3pt;">Privacy Problems in LLMs</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.3.1">
<span class="ltx_p" id="S1.T1.1.4.3.3.1.1" style="width:199.2pt;">Memorization is treated as an instance of a privacy related problems rather than being the focus.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.1.1">
<span class="ltx_p" id="S1.T1.1.5.4.1.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Smith et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib20" title=""><span class="ltx_text" style="color:#000000;">20</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.2.1">
<span class="ltx_p" id="S1.T1.1.5.4.2.1.1" style="width:142.3pt;">Privacy Problems in LLMs</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.3.1">
<span class="ltx_p" id="S1.T1.1.5.4.3.1.1" style="width:199.2pt;">Memorization is treated as an instance of a privacy related problems rather than being the focus.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S1.T1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.1.1">
<span class="ltx_p" id="S1.T1.1.6.5.1.1.1" style="width:85.4pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Hartmann et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib21" title=""><span class="ltx_text" style="color:#000000;">21</span></a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S1.T1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.2.1">
<span class="ltx_p" id="S1.T1.1.6.5.2.1.1" style="width:142.3pt;">Memorization in general-purpose LLMs</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S1.T1.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.3.1">
<span class="ltx_p" id="S1.T1.1.6.5.3.1.1" style="width:199.2pt;">We focus on unintended and undesirable memorization in LLMs, and propose concrete directions for future work.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>List of existing surveys vs our work</figcaption>
</figure>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In the following sections of this paper, we provide a comprehensive exploration of memorization in LLMs, structured into six main sections. We begin with the <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Spectrum of Memorization</span>[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2" title="II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">II</span></a>], where we examine the concept from multiple perspectives including intentionality, retrievability, severity, abstraction, and transparency, offering a deep understanding of how memorization occurs. Next, in <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">Measuring Memorization</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S3" title="III Measuring memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">III</span></a>], we review various methodologies that have been used in previous studies to quantify and assess memorization within LLMs. We then explore the <span class="ltx_text ltx_font_bold" id="S1.p5.1.3">Influences and Dynamics of Memorization</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S3" title="III Measuring memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">III</span></a>], identifying key factors and conditions that contribute to this phenomenon. Following this, in section [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S5" title="V Memorization in specific model architectures ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">V</span></a>] we analyze how memorization appears in specific LLM architectures. Next, in the <span class="ltx_text ltx_font_bold" id="S1.p5.1.4">Mitigating Memorization</span> section [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S6" title="VI Mitigating memorization: strategies and techniques ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">VI</span></a>], we discuss strategies and techniques employed to minimize or control memorization in LLMs, addressing concerns related to privacy and generalization. Finally, we conclude with <span class="ltx_text ltx_font_bold" id="S1.p5.1.5">Future Directions and Open Challenges</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S7" title="VII Future directions and open challenges ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">VII</span></a>], outlining potential areas for further research and unresolved questions, before summarizing our findings in the <span class="ltx_text ltx_font_bold" id="S1.p5.1.6">Conclusion</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S8" title="VIII Conclusion ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">VIII</span></a>]. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a> shows a visual overview of our systemization scope.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="485" id="S1.F1.g1" src="x1.png" width="418"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Graphic summary of our systematization of knowledge</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Spectrum of Memorization</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Intentionality</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Although memorization of factual information can be helpful for the model the model perform more accurately on benchmark tasks such as question answering, other data might be retained without any clear purpose, and this might cause issues with privacy and copyright. This raises questions about whether certain instances of memorization is deliberate (by design) or incidental and undesirable. Understanding the difference between deliberate memorization and unintended retention is crucial for both improving model design and addressing issues like privacy and data security <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib22" title=""><span class="ltx_text" style="color:#000000;">22</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib23" title=""><span class="ltx_text" style="color:#000000;">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib24" title=""><span class="ltx_text" style="color:#000000;">24</span></a>]</cite>.</p>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Unintended memorization</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib25" title=""><span class="ltx_text" style="color:#000000;">25</span></a>]</cite> explores the concept of unintended memorization in neural networks, identifying it as a form of memorization that occurs independently of overfitting and often early in the training process, before the learning has fully converged. To understand and measure this phenomenon, they introduced the “exposure metric” as part of a testing methodology (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S3.SS1" title="III-A Exposure metric ‣ III Measuring memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>). It is important to note that the language modeling loss function itself is chosen in a way to be optimal when the language model’s output are recreations of training data, however, for the unintended memorization, this happens even when the loss function still has some distance to its optimal value. Unintended memorization could lead to various issues including:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Privacy risks: LLMs might inadvertently memorize and potentially reveal sensitive personal information present in the training data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib16" title=""><span class="ltx_text" style="color:#000000;">16</span></a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Security vulnerabilities: Unintended memorization of confidential information like API keys or passwords could lead to security breaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib25" title=""><span class="ltx_text" style="color:#000000;">25</span></a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Copyright violation: Memorization of copyrighted material may lead to legal challenges, especially if the model can reproduce substantial portions of protected works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib16" title=""><span class="ltx_text" style="color:#000000;">16</span></a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Deliberate memorization</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">Deliberate memorization, in contrast, refers to the intentional design of LLMs to retain specific types of information from their training data. This is often a desirable feature, as it allows models to store and utilize vast amounts of knowledge.
Key aspects of deliberate memorization include:</p>
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1">Knowledge retention: LLMs are often deliberately designed to memorize facts, concepts, and general knowledge to enhance their performance in tasks like question answering and information retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib26" title=""><span class="ltx_text" style="color:#000000;">26</span></a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1">Language generation: Deliberate memorization of linguistic patterns, grammar rules, and vocabulary is crucial for the model’s ability to generate coherent and contextually appropriate text.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1">Alignment goals: Deliberate memorization can be utilized in the AI alignment phase to inject desired behaviors and values into the model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib27" title=""><span class="ltx_text" style="color:#000000;">27</span></a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p2.1">As <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Ranaldi et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib28" title=""><span class="ltx_text" style="color:#000000;">28</span></a>]</cite> show in their experiments, deliberate memorization is generally beneficial for model performance, however, it also raises important considerations:</p>
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1">Bias and fairness: The choice of what information to deliberately memorize can introduce or amplify biases in the model output.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1">Transparency and auditing: Understanding what has been deliberately memorized is crucial for model interpretability and auditing purposes.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Severity of memorization</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">While memorization in LLMs is widely discussed, there is no universally accepted definition. The definition of memorization can vary based on factors such as the level of detail required, the context in which the information is recalled, and the specific task or application at hand. This section discusses different severity levels of recall that are explored in the literature evolving around memorization in LLMs, including perfect memorization, verbatim memorization, and approximate memorization.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Perfect memorization</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">Perfect memorization refers to a scenario where a model retains and can reproduce all training data exactly. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Kandpal et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib29" title=""><span class="ltx_text" style="color:#000000;">29</span></a>]</cite> define Perfect Memorization as follows:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinitionx1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmdefinitionx1.1.1.1">Definition</span></span><span class="ltx_text ltx_font_bold" id="Thmdefinitionx1.2.2"> </span>(<span class="ltx_text ltx_font_bold" id="Thmdefinitionx1.3.3">Perfect Memorization</span>)<span class="ltx_text ltx_font_bold" id="Thmdefinitionx1.4.4">.</span>
</h6>
<div class="ltx_para" id="Thmdefinitionx1.p1">
<p class="ltx_p" id="Thmdefinitionx1.p1.1"><span class="ltx_text ltx_font_italic" id="Thmdefinitionx1.p1.1.1">A model is said to exhibit perfect memorization behavior if it only assigns non-zero probability to the samples it has been trained on. Sampling from a perfect memorization model is identical to sampling from the training data.</span></p>
</div>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p2.1">This form of memorization acts as an imaginary language model and is used as a baseline for the experiments.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Verbatim memorization</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">Verbatim memorization is a more specific form of memorization that involves the exact reproduction of strings from the training data. It captures instances where the model outputs a training sample without any alterations. This type of memorization is straightforward to identify and is often the primary focus in discussions about memorization in language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib30" title=""><span class="ltx_text" style="color:#000000;">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib13" title=""><span class="ltx_text" style="color:#000000;">13</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p2.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib13" title=""><span class="ltx_text" style="color:#000000;">13</span></a>]</cite> define verbatim memorization under the “Eidetic Memorization” terminology as follows:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinitionx2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmdefinitionx2.1.1.1">Definition</span></span><span class="ltx_text ltx_font_bold" id="Thmdefinitionx2.2.2"> </span>(<span class="ltx_text ltx_font_bold" id="Thmdefinitionx2.3.3">Verbatim memorization</span>)<span class="ltx_text ltx_font_bold" id="Thmdefinitionx2.4.4">.</span>
</h6>
<div class="ltx_para" id="Thmdefinitionx2.p1">
<p class="ltx_p" id="Thmdefinitionx2.p1.3"><span class="ltx_text ltx_font_italic" id="Thmdefinitionx2.p1.3.3">a string <math alttext="s" class="ltx_Math" display="inline" id="Thmdefinitionx2.p1.1.1.m1.1"><semantics id="Thmdefinitionx2.p1.1.1.m1.1a"><mi id="Thmdefinitionx2.p1.1.1.m1.1.1" xref="Thmdefinitionx2.p1.1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx2.p1.1.1.m1.1b"><ci id="Thmdefinitionx2.p1.1.1.m1.1.1.cmml" xref="Thmdefinitionx2.p1.1.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx2.p1.1.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx2.p1.1.1.m1.1d">italic_s</annotation></semantics></math> from the training set is defined as verbatim memorized if there exists a prompt <math alttext="p" class="ltx_Math" display="inline" id="Thmdefinitionx2.p1.2.2.m2.1"><semantics id="Thmdefinitionx2.p1.2.2.m2.1a"><mi id="Thmdefinitionx2.p1.2.2.m2.1.1" xref="Thmdefinitionx2.p1.2.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx2.p1.2.2.m2.1b"><ci id="Thmdefinitionx2.p1.2.2.m2.1.1.cmml" xref="Thmdefinitionx2.p1.2.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx2.p1.2.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx2.p1.2.2.m2.1d">italic_p</annotation></semantics></math> so that <math alttext="LM(p)=s" class="ltx_Math" display="inline" id="Thmdefinitionx2.p1.3.3.m3.1"><semantics id="Thmdefinitionx2.p1.3.3.m3.1a"><mrow id="Thmdefinitionx2.p1.3.3.m3.1.2" xref="Thmdefinitionx2.p1.3.3.m3.1.2.cmml"><mrow id="Thmdefinitionx2.p1.3.3.m3.1.2.2" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.cmml"><mi id="Thmdefinitionx2.p1.3.3.m3.1.2.2.2" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.2.cmml">L</mi><mo id="Thmdefinitionx2.p1.3.3.m3.1.2.2.1" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.1.cmml">⁢</mo><mi id="Thmdefinitionx2.p1.3.3.m3.1.2.2.3" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.3.cmml">M</mi><mo id="Thmdefinitionx2.p1.3.3.m3.1.2.2.1a" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.1.cmml">⁢</mo><mrow id="Thmdefinitionx2.p1.3.3.m3.1.2.2.4.2" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.cmml"><mo id="Thmdefinitionx2.p1.3.3.m3.1.2.2.4.2.1" stretchy="false" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.cmml">(</mo><mi id="Thmdefinitionx2.p1.3.3.m3.1.1" xref="Thmdefinitionx2.p1.3.3.m3.1.1.cmml">p</mi><mo id="Thmdefinitionx2.p1.3.3.m3.1.2.2.4.2.2" stretchy="false" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.cmml">)</mo></mrow></mrow><mo id="Thmdefinitionx2.p1.3.3.m3.1.2.1" xref="Thmdefinitionx2.p1.3.3.m3.1.2.1.cmml">=</mo><mi id="Thmdefinitionx2.p1.3.3.m3.1.2.3" xref="Thmdefinitionx2.p1.3.3.m3.1.2.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx2.p1.3.3.m3.1b"><apply id="Thmdefinitionx2.p1.3.3.m3.1.2.cmml" xref="Thmdefinitionx2.p1.3.3.m3.1.2"><eq id="Thmdefinitionx2.p1.3.3.m3.1.2.1.cmml" xref="Thmdefinitionx2.p1.3.3.m3.1.2.1"></eq><apply id="Thmdefinitionx2.p1.3.3.m3.1.2.2.cmml" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2"><times id="Thmdefinitionx2.p1.3.3.m3.1.2.2.1.cmml" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.1"></times><ci id="Thmdefinitionx2.p1.3.3.m3.1.2.2.2.cmml" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.2">𝐿</ci><ci id="Thmdefinitionx2.p1.3.3.m3.1.2.2.3.cmml" xref="Thmdefinitionx2.p1.3.3.m3.1.2.2.3">𝑀</ci><ci id="Thmdefinitionx2.p1.3.3.m3.1.1.cmml" xref="Thmdefinitionx2.p1.3.3.m3.1.1">𝑝</ci></apply><ci id="Thmdefinitionx2.p1.3.3.m3.1.2.3.cmml" xref="Thmdefinitionx2.p1.3.3.m3.1.2.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx2.p1.3.3.m3.1c">LM(p)=s</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx2.p1.3.3.m3.1d">italic_L italic_M ( italic_p ) = italic_s</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p3.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Tirumala et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib31" title=""><span class="ltx_text" style="color:#000000;">31</span></a>]</cite> consider another version of this as “Exact memorization”:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinitionx3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmdefinitionx3.1.1.1">Definition</span></span><span class="ltx_text ltx_font_bold" id="Thmdefinitionx3.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmdefinitionx3.p1">
<p class="ltx_p" id="Thmdefinitionx3.p1.2"><span class="ltx_text ltx_font_italic" id="Thmdefinitionx3.p1.2.2">[<span class="ltx_text ltx_font_bold" id="Thmdefinitionx3.p1.2.2.1">Exact memorization</span>] a context <math alttext="c=(p,s)" class="ltx_Math" display="inline" id="Thmdefinitionx3.p1.1.1.m1.2"><semantics id="Thmdefinitionx3.p1.1.1.m1.2a"><mrow id="Thmdefinitionx3.p1.1.1.m1.2.3" xref="Thmdefinitionx3.p1.1.1.m1.2.3.cmml"><mi id="Thmdefinitionx3.p1.1.1.m1.2.3.2" xref="Thmdefinitionx3.p1.1.1.m1.2.3.2.cmml">c</mi><mo id="Thmdefinitionx3.p1.1.1.m1.2.3.1" xref="Thmdefinitionx3.p1.1.1.m1.2.3.1.cmml">=</mo><mrow id="Thmdefinitionx3.p1.1.1.m1.2.3.3.2" xref="Thmdefinitionx3.p1.1.1.m1.2.3.3.1.cmml"><mo id="Thmdefinitionx3.p1.1.1.m1.2.3.3.2.1" stretchy="false" xref="Thmdefinitionx3.p1.1.1.m1.2.3.3.1.cmml">(</mo><mi id="Thmdefinitionx3.p1.1.1.m1.1.1" xref="Thmdefinitionx3.p1.1.1.m1.1.1.cmml">p</mi><mo id="Thmdefinitionx3.p1.1.1.m1.2.3.3.2.2" xref="Thmdefinitionx3.p1.1.1.m1.2.3.3.1.cmml">,</mo><mi id="Thmdefinitionx3.p1.1.1.m1.2.2" xref="Thmdefinitionx3.p1.1.1.m1.2.2.cmml">s</mi><mo id="Thmdefinitionx3.p1.1.1.m1.2.3.3.2.3" stretchy="false" xref="Thmdefinitionx3.p1.1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx3.p1.1.1.m1.2b"><apply id="Thmdefinitionx3.p1.1.1.m1.2.3.cmml" xref="Thmdefinitionx3.p1.1.1.m1.2.3"><eq id="Thmdefinitionx3.p1.1.1.m1.2.3.1.cmml" xref="Thmdefinitionx3.p1.1.1.m1.2.3.1"></eq><ci id="Thmdefinitionx3.p1.1.1.m1.2.3.2.cmml" xref="Thmdefinitionx3.p1.1.1.m1.2.3.2">𝑐</ci><interval closure="open" id="Thmdefinitionx3.p1.1.1.m1.2.3.3.1.cmml" xref="Thmdefinitionx3.p1.1.1.m1.2.3.3.2"><ci id="Thmdefinitionx3.p1.1.1.m1.1.1.cmml" xref="Thmdefinitionx3.p1.1.1.m1.1.1">𝑝</ci><ci id="Thmdefinitionx3.p1.1.1.m1.2.2.cmml" xref="Thmdefinitionx3.p1.1.1.m1.2.2">𝑠</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx3.p1.1.1.m1.2c">c=(p,s)</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx3.p1.1.1.m1.2d">italic_c = ( italic_p , italic_s )</annotation></semantics></math> from the training set is being memorized if <math alttext="Argmax(LM(p))=s" class="ltx_Math" display="inline" id="Thmdefinitionx3.p1.2.2.m2.2"><semantics id="Thmdefinitionx3.p1.2.2.m2.2a"><mrow id="Thmdefinitionx3.p1.2.2.m2.2.2" xref="Thmdefinitionx3.p1.2.2.m2.2.2.cmml"><mrow id="Thmdefinitionx3.p1.2.2.m2.2.2.1" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.cmml"><mi id="Thmdefinitionx3.p1.2.2.m2.2.2.1.3" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.3.cmml">A</mi><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.2" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.2.cmml">⁢</mo><mi id="Thmdefinitionx3.p1.2.2.m2.2.2.1.4" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.4.cmml">r</mi><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.2a" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.2.cmml">⁢</mo><mi id="Thmdefinitionx3.p1.2.2.m2.2.2.1.5" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.5.cmml">g</mi><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.2b" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.2.cmml">⁢</mo><mi id="Thmdefinitionx3.p1.2.2.m2.2.2.1.6" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.6.cmml">m</mi><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.2c" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.2.cmml">⁢</mo><mi id="Thmdefinitionx3.p1.2.2.m2.2.2.1.7" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.7.cmml">a</mi><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.2d" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.2.cmml">⁢</mo><mi id="Thmdefinitionx3.p1.2.2.m2.2.2.1.8" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.8.cmml">x</mi><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.2e" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.2.cmml">⁢</mo><mrow id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.cmml"><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.2" stretchy="false" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.cmml">(</mo><mrow id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.cmml"><mi id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.2" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.2.cmml">L</mi><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.1" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.1.cmml">⁢</mo><mi id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.3" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.3.cmml">M</mi><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.1a" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.1.cmml">⁢</mo><mrow id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.4.2" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.cmml"><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.4.2.1" stretchy="false" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.cmml">(</mo><mi id="Thmdefinitionx3.p1.2.2.m2.1.1" xref="Thmdefinitionx3.p1.2.2.m2.1.1.cmml">p</mi><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.4.2.2" stretchy="false" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.3" stretchy="false" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Thmdefinitionx3.p1.2.2.m2.2.2.2" xref="Thmdefinitionx3.p1.2.2.m2.2.2.2.cmml">=</mo><mi id="Thmdefinitionx3.p1.2.2.m2.2.2.3" xref="Thmdefinitionx3.p1.2.2.m2.2.2.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx3.p1.2.2.m2.2b"><apply id="Thmdefinitionx3.p1.2.2.m2.2.2.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2"><eq id="Thmdefinitionx3.p1.2.2.m2.2.2.2.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.2"></eq><apply id="Thmdefinitionx3.p1.2.2.m2.2.2.1.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1"><times id="Thmdefinitionx3.p1.2.2.m2.2.2.1.2.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.2"></times><ci id="Thmdefinitionx3.p1.2.2.m2.2.2.1.3.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.3">𝐴</ci><ci id="Thmdefinitionx3.p1.2.2.m2.2.2.1.4.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.4">𝑟</ci><ci id="Thmdefinitionx3.p1.2.2.m2.2.2.1.5.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.5">𝑔</ci><ci id="Thmdefinitionx3.p1.2.2.m2.2.2.1.6.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.6">𝑚</ci><ci id="Thmdefinitionx3.p1.2.2.m2.2.2.1.7.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.7">𝑎</ci><ci id="Thmdefinitionx3.p1.2.2.m2.2.2.1.8.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.8">𝑥</ci><apply id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1"><times id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.1.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.1"></times><ci id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.2.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.2">𝐿</ci><ci id="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.3.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.1.1.1.1.3">𝑀</ci><ci id="Thmdefinitionx3.p1.2.2.m2.1.1.cmml" xref="Thmdefinitionx3.p1.2.2.m2.1.1">𝑝</ci></apply></apply><ci id="Thmdefinitionx3.p1.2.2.m2.2.2.3.cmml" xref="Thmdefinitionx3.p1.2.2.m2.2.2.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx3.p1.2.2.m2.2c">Argmax(LM(p))=s</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx3.p1.2.2.m2.2d">italic_A italic_r italic_g italic_m italic_a italic_x ( italic_L italic_M ( italic_p ) ) = italic_s</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p4">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p4.1">This definition is more strict and does not capture the memorization under different forms of sampling methods, rather it only considers the greedy sampling [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS5" title="IV-E Sampling methods ‣ IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span></span></a>].</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p5">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p5.2">It should be noted that these definitions do not impose any restrictions on the length of prompt <math alttext="p" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p5.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p5.1.m1.1a"><mi id="S2.SS2.SSS0.Px2.p5.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p5.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p5.1.m1.1b"><ci id="S2.SS2.SSS0.Px2.p5.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p5.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p5.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p5.1.m1.1d">italic_p</annotation></semantics></math> or the generation <math alttext="s" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p5.2.m2.1"><semantics id="S2.SS2.SSS0.Px2.p5.2.m2.1a"><mi id="S2.SS2.SSS0.Px2.p5.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p5.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p5.2.m2.1b"><ci id="S2.SS2.SSS0.Px2.p5.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p5.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p5.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p5.2.m2.1d">italic_s</annotation></semantics></math>. However, if the generated text is too short, it may not be appropriate to classify it as memorization. Typically, a certain number of tokens is considered when assessing memorization; for example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib23" title=""><span class="ltx_text" style="color:#000000;">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib31" title=""><span class="ltx_text" style="color:#000000;">31</span></a>]</cite> use a 50-token output restriction.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Approximate memorization</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p1.1">Approximate memorization extends beyond verbatim memorization to include instances where the output is similar but not identical to the training data. Verbatim memorization does not capture the subtler forms of memorization, as it is too confined. For example, if two sentences differ by a minor detail, such as punctuation, a misspelled word, or a stylistic variation (e.g., American English vs. British English), these instances would fall outside the strict boundaries of verbatim memorization. However, human judges would likely consider these variations as memorized examples.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p2.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Ippolito et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib32" title=""><span class="ltx_text" style="color:#000000;">32</span></a>]</cite> define “Approximate memorization” as follows:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinitionx4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmdefinitionx4.1.1.1">Definition</span></span><span class="ltx_text ltx_font_bold" id="Thmdefinitionx4.2.2"> </span>(<span class="ltx_text ltx_font_bold" id="Thmdefinitionx4.3.3">Approximate memorization</span>)<span class="ltx_text ltx_font_bold" id="Thmdefinitionx4.4.4">.</span>
</h6>
<div class="ltx_para" id="Thmdefinitionx4.p1">
<p class="ltx_p" id="Thmdefinitionx4.p1.4"><span class="ltx_text ltx_font_italic" id="Thmdefinitionx4.p1.4.4">a suffix <math alttext="s" class="ltx_Math" display="inline" id="Thmdefinitionx4.p1.1.1.m1.1"><semantics id="Thmdefinitionx4.p1.1.1.m1.1a"><mi id="Thmdefinitionx4.p1.1.1.m1.1.1" xref="Thmdefinitionx4.p1.1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx4.p1.1.1.m1.1b"><ci id="Thmdefinitionx4.p1.1.1.m1.1.1.cmml" xref="Thmdefinitionx4.p1.1.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx4.p1.1.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx4.p1.1.1.m1.1d">italic_s</annotation></semantics></math> for prefix <math alttext="p" class="ltx_Math" display="inline" id="Thmdefinitionx4.p1.2.2.m2.1"><semantics id="Thmdefinitionx4.p1.2.2.m2.1a"><mi id="Thmdefinitionx4.p1.2.2.m2.1.1" xref="Thmdefinitionx4.p1.2.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx4.p1.2.2.m2.1b"><ci id="Thmdefinitionx4.p1.2.2.m2.1.1.cmml" xref="Thmdefinitionx4.p1.2.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx4.p1.2.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx4.p1.2.2.m2.1d">italic_p</annotation></semantics></math> is labeled as approximately memorized if for generation <math alttext="g=LM(p)" class="ltx_Math" display="inline" id="Thmdefinitionx4.p1.3.3.m3.1"><semantics id="Thmdefinitionx4.p1.3.3.m3.1a"><mrow id="Thmdefinitionx4.p1.3.3.m3.1.2" xref="Thmdefinitionx4.p1.3.3.m3.1.2.cmml"><mi id="Thmdefinitionx4.p1.3.3.m3.1.2.2" xref="Thmdefinitionx4.p1.3.3.m3.1.2.2.cmml">g</mi><mo id="Thmdefinitionx4.p1.3.3.m3.1.2.1" xref="Thmdefinitionx4.p1.3.3.m3.1.2.1.cmml">=</mo><mrow id="Thmdefinitionx4.p1.3.3.m3.1.2.3" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.cmml"><mi id="Thmdefinitionx4.p1.3.3.m3.1.2.3.2" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.2.cmml">L</mi><mo id="Thmdefinitionx4.p1.3.3.m3.1.2.3.1" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.1.cmml">⁢</mo><mi id="Thmdefinitionx4.p1.3.3.m3.1.2.3.3" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.3.cmml">M</mi><mo id="Thmdefinitionx4.p1.3.3.m3.1.2.3.1a" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.1.cmml">⁢</mo><mrow id="Thmdefinitionx4.p1.3.3.m3.1.2.3.4.2" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.cmml"><mo id="Thmdefinitionx4.p1.3.3.m3.1.2.3.4.2.1" stretchy="false" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.cmml">(</mo><mi id="Thmdefinitionx4.p1.3.3.m3.1.1" xref="Thmdefinitionx4.p1.3.3.m3.1.1.cmml">p</mi><mo id="Thmdefinitionx4.p1.3.3.m3.1.2.3.4.2.2" stretchy="false" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx4.p1.3.3.m3.1b"><apply id="Thmdefinitionx4.p1.3.3.m3.1.2.cmml" xref="Thmdefinitionx4.p1.3.3.m3.1.2"><eq id="Thmdefinitionx4.p1.3.3.m3.1.2.1.cmml" xref="Thmdefinitionx4.p1.3.3.m3.1.2.1"></eq><ci id="Thmdefinitionx4.p1.3.3.m3.1.2.2.cmml" xref="Thmdefinitionx4.p1.3.3.m3.1.2.2">𝑔</ci><apply id="Thmdefinitionx4.p1.3.3.m3.1.2.3.cmml" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3"><times id="Thmdefinitionx4.p1.3.3.m3.1.2.3.1.cmml" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.1"></times><ci id="Thmdefinitionx4.p1.3.3.m3.1.2.3.2.cmml" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.2">𝐿</ci><ci id="Thmdefinitionx4.p1.3.3.m3.1.2.3.3.cmml" xref="Thmdefinitionx4.p1.3.3.m3.1.2.3.3">𝑀</ci><ci id="Thmdefinitionx4.p1.3.3.m3.1.1.cmml" xref="Thmdefinitionx4.p1.3.3.m3.1.1">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx4.p1.3.3.m3.1c">g=LM(p)</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx4.p1.3.3.m3.1d">italic_g = italic_L italic_M ( italic_p )</annotation></semantics></math>, <math alttext="BLEU(g,s)&gt;0.75" class="ltx_Math" display="inline" id="Thmdefinitionx4.p1.4.4.m4.2"><semantics id="Thmdefinitionx4.p1.4.4.m4.2a"><mrow id="Thmdefinitionx4.p1.4.4.m4.2.3" xref="Thmdefinitionx4.p1.4.4.m4.2.3.cmml"><mrow id="Thmdefinitionx4.p1.4.4.m4.2.3.2" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.cmml"><mi id="Thmdefinitionx4.p1.4.4.m4.2.3.2.2" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.2.cmml">B</mi><mo id="Thmdefinitionx4.p1.4.4.m4.2.3.2.1" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.1.cmml">⁢</mo><mi id="Thmdefinitionx4.p1.4.4.m4.2.3.2.3" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.3.cmml">L</mi><mo id="Thmdefinitionx4.p1.4.4.m4.2.3.2.1a" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.1.cmml">⁢</mo><mi id="Thmdefinitionx4.p1.4.4.m4.2.3.2.4" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.4.cmml">E</mi><mo id="Thmdefinitionx4.p1.4.4.m4.2.3.2.1b" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.1.cmml">⁢</mo><mi id="Thmdefinitionx4.p1.4.4.m4.2.3.2.5" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.5.cmml">U</mi><mo id="Thmdefinitionx4.p1.4.4.m4.2.3.2.1c" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.1.cmml">⁢</mo><mrow id="Thmdefinitionx4.p1.4.4.m4.2.3.2.6.2" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.6.1.cmml"><mo id="Thmdefinitionx4.p1.4.4.m4.2.3.2.6.2.1" stretchy="false" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.6.1.cmml">(</mo><mi id="Thmdefinitionx4.p1.4.4.m4.1.1" xref="Thmdefinitionx4.p1.4.4.m4.1.1.cmml">g</mi><mo id="Thmdefinitionx4.p1.4.4.m4.2.3.2.6.2.2" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.6.1.cmml">,</mo><mi id="Thmdefinitionx4.p1.4.4.m4.2.2" xref="Thmdefinitionx4.p1.4.4.m4.2.2.cmml">s</mi><mo id="Thmdefinitionx4.p1.4.4.m4.2.3.2.6.2.3" stretchy="false" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.6.1.cmml">)</mo></mrow></mrow><mo id="Thmdefinitionx4.p1.4.4.m4.2.3.1" xref="Thmdefinitionx4.p1.4.4.m4.2.3.1.cmml">&gt;</mo><mn id="Thmdefinitionx4.p1.4.4.m4.2.3.3" xref="Thmdefinitionx4.p1.4.4.m4.2.3.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx4.p1.4.4.m4.2b"><apply id="Thmdefinitionx4.p1.4.4.m4.2.3.cmml" xref="Thmdefinitionx4.p1.4.4.m4.2.3"><gt id="Thmdefinitionx4.p1.4.4.m4.2.3.1.cmml" xref="Thmdefinitionx4.p1.4.4.m4.2.3.1"></gt><apply id="Thmdefinitionx4.p1.4.4.m4.2.3.2.cmml" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2"><times id="Thmdefinitionx4.p1.4.4.m4.2.3.2.1.cmml" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.1"></times><ci id="Thmdefinitionx4.p1.4.4.m4.2.3.2.2.cmml" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.2">𝐵</ci><ci id="Thmdefinitionx4.p1.4.4.m4.2.3.2.3.cmml" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.3">𝐿</ci><ci id="Thmdefinitionx4.p1.4.4.m4.2.3.2.4.cmml" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.4">𝐸</ci><ci id="Thmdefinitionx4.p1.4.4.m4.2.3.2.5.cmml" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.5">𝑈</ci><interval closure="open" id="Thmdefinitionx4.p1.4.4.m4.2.3.2.6.1.cmml" xref="Thmdefinitionx4.p1.4.4.m4.2.3.2.6.2"><ci id="Thmdefinitionx4.p1.4.4.m4.1.1.cmml" xref="Thmdefinitionx4.p1.4.4.m4.1.1">𝑔</ci><ci id="Thmdefinitionx4.p1.4.4.m4.2.2.cmml" xref="Thmdefinitionx4.p1.4.4.m4.2.2">𝑠</ci></interval></apply><cn id="Thmdefinitionx4.p1.4.4.m4.2.3.3.cmml" type="float" xref="Thmdefinitionx4.p1.4.4.m4.2.3.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx4.p1.4.4.m4.2c">BLEU(g,s)&gt;0.75</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx4.p1.4.4.m4.2d">italic_B italic_L italic_E italic_U ( italic_g , italic_s ) &gt; 0.75</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p3">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p3.1">They discuss that this threshold was chosen based on qualitative analyses of the samples. Examples of this are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.F2" title="Figure 2 ‣ Approximate memorization ‣ II-B Severity of memorization ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>. By adopting this definition of memorization, they show that the measurement of memorization can increase by a factor of two. However, they also note that this definition can lead to both false positives and false negatives when compared to human judgment, indicating a potential direction for future investigations.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="273" id="S2.F2.g1" src="extracted/5898740/images/blue-similarity.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>BLEU similarity score of more than 0.75 (from <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Ippolito et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib32" title=""><span class="ltx_text" style="color:#000000;">32</span></a>]</cite>)</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Retrievability</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The common way to analyze memorization in LLMs is through sampling. This involves selecting tokens probabilistically from the model’s predicted distribution at each step, to see if a sequence of probabilistically generated tokens can be traced back to the training data. Since the LLM’s input and output domains are both discrete, it is important to note that it might not be possible to extract all of the memorized content through given a finite number generation trials through sampling.
Based on how we can make an LLM output the training data we can classify memorization into <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">extractable</span> and <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.2">discoverable</span>.</p>
</div>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Extractable memorization</h4>
<div class="ltx_para" id="S2.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px1.p1.1">Extractable memorization refers to the ability to retrieve specific information from a model’s training data without direct access to that data. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib23" title=""><span class="ltx_text" style="color:#000000;">23</span></a>]</cite> defines “Extractable memorization” as follows:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinitionx5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmdefinitionx5.1.1.1">Definition</span></span><span class="ltx_text ltx_font_bold" id="Thmdefinitionx5.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmdefinitionx5.p1">
<p class="ltx_p" id="Thmdefinitionx5.p1.7"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Thmdefinitionx5.p1.7.7">(Extractable Memorization)<span class="ltx_text ltx_font_medium" id="Thmdefinitionx5.p1.7.7.7"> Given a model with a generation routine <math alttext="Gen" class="ltx_Math" display="inline" id="Thmdefinitionx5.p1.1.1.1.m1.1"><semantics id="Thmdefinitionx5.p1.1.1.1.m1.1a"><mrow id="Thmdefinitionx5.p1.1.1.1.m1.1.1" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1.cmml"><mi id="Thmdefinitionx5.p1.1.1.1.m1.1.1.2" mathvariant="normal" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1.2.cmml">G</mi><mo id="Thmdefinitionx5.p1.1.1.1.m1.1.1.1" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="Thmdefinitionx5.p1.1.1.1.m1.1.1.3" mathvariant="normal" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1.3.cmml">e</mi><mo id="Thmdefinitionx5.p1.1.1.1.m1.1.1.1a" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="Thmdefinitionx5.p1.1.1.1.m1.1.1.4" mathvariant="normal" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1.4.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx5.p1.1.1.1.m1.1b"><apply id="Thmdefinitionx5.p1.1.1.1.m1.1.1.cmml" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1"><times id="Thmdefinitionx5.p1.1.1.1.m1.1.1.1.cmml" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1.1"></times><ci id="Thmdefinitionx5.p1.1.1.1.m1.1.1.2.cmml" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1.2">G</ci><ci id="Thmdefinitionx5.p1.1.1.1.m1.1.1.3.cmml" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1.3">e</ci><ci id="Thmdefinitionx5.p1.1.1.1.m1.1.1.4.cmml" xref="Thmdefinitionx5.p1.1.1.1.m1.1.1.4">n</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx5.p1.1.1.1.m1.1c">Gen</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx5.p1.1.1.1.m1.1d">italic_G italic_e italic_n</annotation></semantics></math>, an example <math alttext="s" class="ltx_Math" display="inline" id="Thmdefinitionx5.p1.2.2.2.m2.1"><semantics id="Thmdefinitionx5.p1.2.2.2.m2.1a"><mi id="Thmdefinitionx5.p1.2.2.2.m2.1.1" mathvariant="normal" xref="Thmdefinitionx5.p1.2.2.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx5.p1.2.2.2.m2.1b"><ci id="Thmdefinitionx5.p1.2.2.2.m2.1.1.cmml" xref="Thmdefinitionx5.p1.2.2.2.m2.1.1">s</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx5.p1.2.2.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx5.p1.2.2.2.m2.1d">italic_s</annotation></semantics></math> from the training set <math alttext="S" class="ltx_Math" display="inline" id="Thmdefinitionx5.p1.3.3.3.m3.1"><semantics id="Thmdefinitionx5.p1.3.3.3.m3.1a"><mi id="Thmdefinitionx5.p1.3.3.3.m3.1.1" mathvariant="normal" xref="Thmdefinitionx5.p1.3.3.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx5.p1.3.3.3.m3.1b"><ci id="Thmdefinitionx5.p1.3.3.3.m3.1.1.cmml" xref="Thmdefinitionx5.p1.3.3.3.m3.1.1">S</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx5.p1.3.3.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx5.p1.3.3.3.m3.1d">italic_S</annotation></semantics></math> is extractably memorized if an adversary (without access to <math alttext="S" class="ltx_Math" display="inline" id="Thmdefinitionx5.p1.4.4.4.m4.1"><semantics id="Thmdefinitionx5.p1.4.4.4.m4.1a"><mi id="Thmdefinitionx5.p1.4.4.4.m4.1.1" mathvariant="normal" xref="Thmdefinitionx5.p1.4.4.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx5.p1.4.4.4.m4.1b"><ci id="Thmdefinitionx5.p1.4.4.4.m4.1.1.cmml" xref="Thmdefinitionx5.p1.4.4.4.m4.1.1">S</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx5.p1.4.4.4.m4.1c">S</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx5.p1.4.4.4.m4.1d">italic_S</annotation></semantics></math>) can construct a prompt <math alttext="p" class="ltx_Math" display="inline" id="Thmdefinitionx5.p1.5.5.5.m5.1"><semantics id="Thmdefinitionx5.p1.5.5.5.m5.1a"><mi id="Thmdefinitionx5.p1.5.5.5.m5.1.1" mathvariant="normal" xref="Thmdefinitionx5.p1.5.5.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx5.p1.5.5.5.m5.1b"><ci id="Thmdefinitionx5.p1.5.5.5.m5.1.1.cmml" xref="Thmdefinitionx5.p1.5.5.5.m5.1.1">p</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx5.p1.5.5.5.m5.1c">p</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx5.p1.5.5.5.m5.1d">italic_p</annotation></semantics></math> that makes the model produce <math alttext="s" class="ltx_Math" display="inline" id="Thmdefinitionx5.p1.6.6.6.m6.1"><semantics id="Thmdefinitionx5.p1.6.6.6.m6.1a"><mi id="Thmdefinitionx5.p1.6.6.6.m6.1.1" mathvariant="normal" xref="Thmdefinitionx5.p1.6.6.6.m6.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx5.p1.6.6.6.m6.1b"><ci id="Thmdefinitionx5.p1.6.6.6.m6.1.1.cmml" xref="Thmdefinitionx5.p1.6.6.6.m6.1.1">s</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx5.p1.6.6.6.m6.1c">s</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx5.p1.6.6.6.m6.1d">italic_s</annotation></semantics></math> (that is, <math alttext="Gen(p)=s" class="ltx_Math" display="inline" id="Thmdefinitionx5.p1.7.7.7.m7.1"><semantics id="Thmdefinitionx5.p1.7.7.7.m7.1a"><mrow id="Thmdefinitionx5.p1.7.7.7.m7.1.2" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.cmml"><mrow id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.cmml"><mi id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.2" mathvariant="normal" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.2.cmml">G</mi><mo id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.1" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.1.cmml">⁢</mo><mi id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.3" mathvariant="normal" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.3.cmml">e</mi><mo id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.1a" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.1.cmml">⁢</mo><mi id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.4" mathvariant="normal" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.4.cmml">n</mi><mo id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.1b" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.1.cmml">⁢</mo><mrow id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.5.2" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.cmml"><mo id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.5.2.1" stretchy="false" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.cmml">(</mo><mi id="Thmdefinitionx5.p1.7.7.7.m7.1.1" mathvariant="normal" xref="Thmdefinitionx5.p1.7.7.7.m7.1.1.cmml">p</mi><mo id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.5.2.2" stretchy="false" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.cmml">)</mo></mrow></mrow><mo id="Thmdefinitionx5.p1.7.7.7.m7.1.2.1" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.1.cmml">=</mo><mi id="Thmdefinitionx5.p1.7.7.7.m7.1.2.3" mathvariant="normal" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx5.p1.7.7.7.m7.1b"><apply id="Thmdefinitionx5.p1.7.7.7.m7.1.2.cmml" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2"><eq id="Thmdefinitionx5.p1.7.7.7.m7.1.2.1.cmml" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.1"></eq><apply id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.cmml" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2"><times id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.1.cmml" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.1"></times><ci id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.2.cmml" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.2">G</ci><ci id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.3.cmml" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.3">e</ci><ci id="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.4.cmml" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.2.4">n</ci><ci id="Thmdefinitionx5.p1.7.7.7.m7.1.1.cmml" xref="Thmdefinitionx5.p1.7.7.7.m7.1.1">p</ci></apply><ci id="Thmdefinitionx5.p1.7.7.7.m7.1.2.3.cmml" xref="Thmdefinitionx5.p1.7.7.7.m7.1.2.3">s</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx5.p1.7.7.7.m7.1c">Gen(p)=s</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx5.p1.7.7.7.m7.1d">italic_G italic_e italic_n ( italic_p ) = italic_s</annotation></semantics></math>) .</span></span></p>
</div>
</div>
<div class="ltx_para" id="S2.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS3.SSS0.Px1.p2.1">A more specific form of this is <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS0.Px1.p2.1.1">k-extractable memorization</span>:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinitionx6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmdefinitionx6.1.1.1">Definition</span></span><span class="ltx_text ltx_font_bold" id="Thmdefinitionx6.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmdefinitionx6.p1">
<p class="ltx_p" id="Thmdefinitionx6.p1.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Thmdefinitionx6.p1.2.2">(K-extractable Memorization)<span class="ltx_text ltx_font_medium" id="Thmdefinitionx6.p1.2.2.2"> A string <math alttext="s" class="ltx_Math" display="inline" id="Thmdefinitionx6.p1.1.1.1.m1.1"><semantics id="Thmdefinitionx6.p1.1.1.1.m1.1a"><mi id="Thmdefinitionx6.p1.1.1.1.m1.1.1" mathvariant="normal" xref="Thmdefinitionx6.p1.1.1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx6.p1.1.1.1.m1.1b"><ci id="Thmdefinitionx6.p1.1.1.1.m1.1.1.cmml" xref="Thmdefinitionx6.p1.1.1.1.m1.1.1">s</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx6.p1.1.1.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx6.p1.1.1.1.m1.1d">italic_s</annotation></semantics></math> is said to be k-extractable if it (a) exists in the training data, and (b) is generated by the language model by prompting with <math alttext="k" class="ltx_Math" display="inline" id="Thmdefinitionx6.p1.2.2.2.m2.1"><semantics id="Thmdefinitionx6.p1.2.2.2.m2.1a"><mi id="Thmdefinitionx6.p1.2.2.2.m2.1.1" mathvariant="normal" xref="Thmdefinitionx6.p1.2.2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx6.p1.2.2.2.m2.1b"><ci id="Thmdefinitionx6.p1.2.2.2.m2.1.1.cmml" xref="Thmdefinitionx6.p1.2.2.2.m2.1.1">k</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx6.p1.2.2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx6.p1.2.2.2.m2.1d">italic_k</annotation></semantics></math> prior tokens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib33" title=""><span class="ltx_text" style="color:#000000;">33</span></a>]</cite>.</span></span></p>
</div>
</div>
<div class="ltx_para" id="S2.SS3.SSS0.Px1.p3">
<p class="ltx_p" id="S2.SS3.SSS0.Px1.p3.1">Analyzing extractable memorization usually involves two main challenges: Designing prompts that best elicit memorization in a model and verifying if the model output is indeed from the training data.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS0.Px1.p4">
<p class="ltx_p" id="S2.SS3.SSS0.Px1.p4.1">Research in this area has employed various strategies. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib13" title=""><span class="ltx_text" style="color:#000000;">13</span></a>]</cite> recover training examples from GPT-2 by prompting it with short strings from the public Internet and manually verifying the outputs via Google search. This method confirmed the memorization of about 0.00001% of GPT-2’s training data due to the labor-intensive verification process. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Nasr et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib34" title=""><span class="ltx_text" style="color:#000000;">34</span></a>]</cite> conduct extensive analysis on Pythia, RedPajama, and GPT-Neo models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib35" title=""><span class="ltx_text" style="color:#000000;">35</span></a>]</cite>. They query these models with millions of 5-token blocks from Wikipedia and count for unique 50-grams that the model generates whether they exist in a combined dataset of the models’ training data. Their method was more successful, showing that 0.1% to 1% of the models’ outputs are memorized, with a strong correlation between model size and memorization abilities.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Discoverable memorization</h4>
<div class="ltx_para" id="S2.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px2.p1.1">Discoverable memorization measures the extent to which models can reproduce their training data when explicitly prompted with data from their training set. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Nasr et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib34" title=""><span class="ltx_text" style="color:#000000;">34</span></a>]</cite> suggests the following definition for “Discoverable memorization”:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinitionx7">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmdefinitionx7.1.1.1">Definition</span></span><span class="ltx_text ltx_font_bold" id="Thmdefinitionx7.2.2">.</span>
</h6>
<div class="ltx_para" id="Thmdefinitionx7.p1">
<p class="ltx_p" id="Thmdefinitionx7.p1.5"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Thmdefinitionx7.p1.5.5">(Discoverable memorization)<span class="ltx_text ltx_font_medium" id="Thmdefinitionx7.p1.5.5.5"> For a model with generation routine <math alttext="Gen" class="ltx_Math" display="inline" id="Thmdefinitionx7.p1.1.1.1.m1.1"><semantics id="Thmdefinitionx7.p1.1.1.1.m1.1a"><mrow id="Thmdefinitionx7.p1.1.1.1.m1.1.1" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1.cmml"><mi id="Thmdefinitionx7.p1.1.1.1.m1.1.1.2" mathvariant="normal" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1.2.cmml">G</mi><mo id="Thmdefinitionx7.p1.1.1.1.m1.1.1.1" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="Thmdefinitionx7.p1.1.1.1.m1.1.1.3" mathvariant="normal" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1.3.cmml">e</mi><mo id="Thmdefinitionx7.p1.1.1.1.m1.1.1.1a" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="Thmdefinitionx7.p1.1.1.1.m1.1.1.4" mathvariant="normal" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1.4.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx7.p1.1.1.1.m1.1b"><apply id="Thmdefinitionx7.p1.1.1.1.m1.1.1.cmml" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1"><times id="Thmdefinitionx7.p1.1.1.1.m1.1.1.1.cmml" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1.1"></times><ci id="Thmdefinitionx7.p1.1.1.1.m1.1.1.2.cmml" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1.2">G</ci><ci id="Thmdefinitionx7.p1.1.1.1.m1.1.1.3.cmml" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1.3">e</ci><ci id="Thmdefinitionx7.p1.1.1.1.m1.1.1.4.cmml" xref="Thmdefinitionx7.p1.1.1.1.m1.1.1.4">n</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx7.p1.1.1.1.m1.1c">Gen</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx7.p1.1.1.1.m1.1d">italic_G italic_e italic_n</annotation></semantics></math> and an example <math alttext="[p||s]" class="ltx_math_unparsed" display="inline" id="Thmdefinitionx7.p1.2.2.2.m2.1"><semantics id="Thmdefinitionx7.p1.2.2.2.m2.1a"><mrow id="Thmdefinitionx7.p1.2.2.2.m2.1b"><mo id="Thmdefinitionx7.p1.2.2.2.m2.1.1" stretchy="false">[</mo><mi id="Thmdefinitionx7.p1.2.2.2.m2.1.2" mathvariant="normal">p</mi><mo fence="false" id="Thmdefinitionx7.p1.2.2.2.m2.1.3" rspace="0.167em" stretchy="false">|</mo><mo fence="false" id="Thmdefinitionx7.p1.2.2.2.m2.1.4" rspace="0.167em" stretchy="false">|</mo><mi id="Thmdefinitionx7.p1.2.2.2.m2.1.5" mathvariant="normal">s</mi><mo id="Thmdefinitionx7.p1.2.2.2.m2.1.6" stretchy="false">]</mo></mrow><annotation encoding="application/x-tex" id="Thmdefinitionx7.p1.2.2.2.m2.1c">[p||s]</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx7.p1.2.2.2.m2.1d">[ italic_p | | italic_s ]</annotation></semantics></math> from the training set <math alttext="S" class="ltx_Math" display="inline" id="Thmdefinitionx7.p1.3.3.3.m3.1"><semantics id="Thmdefinitionx7.p1.3.3.3.m3.1a"><mi id="Thmdefinitionx7.p1.3.3.3.m3.1.1" mathvariant="normal" xref="Thmdefinitionx7.p1.3.3.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx7.p1.3.3.3.m3.1b"><ci id="Thmdefinitionx7.p1.3.3.3.m3.1.1.cmml" xref="Thmdefinitionx7.p1.3.3.3.m3.1.1">S</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx7.p1.3.3.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx7.p1.3.3.3.m3.1d">italic_S</annotation></semantics></math>, we say that <math alttext="s" class="ltx_Math" display="inline" id="Thmdefinitionx7.p1.4.4.4.m4.1"><semantics id="Thmdefinitionx7.p1.4.4.4.m4.1a"><mi id="Thmdefinitionx7.p1.4.4.4.m4.1.1" mathvariant="normal" xref="Thmdefinitionx7.p1.4.4.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx7.p1.4.4.4.m4.1b"><ci id="Thmdefinitionx7.p1.4.4.4.m4.1.1.cmml" xref="Thmdefinitionx7.p1.4.4.4.m4.1.1">s</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx7.p1.4.4.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx7.p1.4.4.4.m4.1d">italic_s</annotation></semantics></math> is discoverably memorized if <math alttext="Gen(p)=s" class="ltx_Math" display="inline" id="Thmdefinitionx7.p1.5.5.5.m5.1"><semantics id="Thmdefinitionx7.p1.5.5.5.m5.1a"><mrow id="Thmdefinitionx7.p1.5.5.5.m5.1.2" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.cmml"><mrow id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.cmml"><mi id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.2" mathvariant="normal" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.2.cmml">G</mi><mo id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.1" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.1.cmml">⁢</mo><mi id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.3" mathvariant="normal" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.3.cmml">e</mi><mo id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.1a" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.1.cmml">⁢</mo><mi id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.4" mathvariant="normal" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.4.cmml">n</mi><mo id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.1b" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.1.cmml">⁢</mo><mrow id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.5.2" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.cmml"><mo id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.5.2.1" stretchy="false" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.cmml">(</mo><mi id="Thmdefinitionx7.p1.5.5.5.m5.1.1" mathvariant="normal" xref="Thmdefinitionx7.p1.5.5.5.m5.1.1.cmml">p</mi><mo id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.5.2.2" stretchy="false" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.cmml">)</mo></mrow></mrow><mo id="Thmdefinitionx7.p1.5.5.5.m5.1.2.1" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.1.cmml">=</mo><mi id="Thmdefinitionx7.p1.5.5.5.m5.1.2.3" mathvariant="normal" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx7.p1.5.5.5.m5.1b"><apply id="Thmdefinitionx7.p1.5.5.5.m5.1.2.cmml" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2"><eq id="Thmdefinitionx7.p1.5.5.5.m5.1.2.1.cmml" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.1"></eq><apply id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.cmml" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2"><times id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.1.cmml" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.1"></times><ci id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.2.cmml" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.2">G</ci><ci id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.3.cmml" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.3">e</ci><ci id="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.4.cmml" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.2.4">n</ci><ci id="Thmdefinitionx7.p1.5.5.5.m5.1.1.cmml" xref="Thmdefinitionx7.p1.5.5.5.m5.1.1">p</ci></apply><ci id="Thmdefinitionx7.p1.5.5.5.m5.1.2.3.cmml" xref="Thmdefinitionx7.p1.5.5.5.m5.1.2.3">s</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx7.p1.5.5.5.m5.1c">Gen(p)=s</annotation><annotation encoding="application/x-llamapun" id="Thmdefinitionx7.p1.5.5.5.m5.1d">italic_G italic_e italic_n ( italic_p ) = italic_s</annotation></semantics></math>.</span></span></p>
</div>
</div>
<div class="ltx_para" id="S2.SS3.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS3.SSS0.Px2.p2.1">Discoverable memorization provides an upper bound for data extraction, compared to extractable memorization’s lower bound. Ideally, discoverable memorization requires querying the model with its entire training set, which is computationally intractable. Also, noteworthy is that discoverable memorization differs from extractable memorization in that the prompt <math alttext="p" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p2.1.m1.1"><semantics id="S2.SS3.SSS0.Px2.p2.1.m1.1a"><mi id="S2.SS3.SSS0.Px2.p2.1.m1.1.1" xref="S2.SS3.SSS0.Px2.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px2.p2.1.m1.1b"><ci id="S2.SS3.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS0.Px2.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px2.p2.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS0.Px2.p2.1.m1.1d">italic_p</annotation></semantics></math> is known to be from the training set.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS3.SSS0.Px2.p3.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib23" title=""><span class="ltx_text" style="color:#000000;">23</span></a>]</cite> investigate the upper bounds of data extraction in GPT-Neo models through discoverable memorization. They find that (a) LLMs discoverably memorize roughly 1% of their training datasets; (b) There is a log-linear correlation between data extraction and model size, repetition of data, and prefix context length. Other studies on different models (PaLM, MADLAD-400) corroborate the 1% memorization rate when prompting with about 50 tokens of context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib36" title=""><span class="ltx_text" style="color:#000000;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib37" title=""><span class="ltx_text" style="color:#000000;">37</span></a>]</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Comparing discoverable and extractable Memorization</h4>
<div class="ltx_para" id="S2.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px3.p1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Nasr et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib34" title=""><span class="ltx_text" style="color:#000000;">34</span></a>]</cite> compare their extractable memorization results with the discoverable memorizations from <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> [<a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib23" title=""><span class="ltx_text" style="color:#000000;">23</span></a>]</cite> for the GPT-Neo 6B parameter model. This comparison revealed that (1) some sequences are both discoverably and extractably memorized; (2) some sequences are discoverably memorized but not extractably memorized, and vice versa; (3) the overlap between these two types of memorization provides insights into the model’s information retention and retrieval mechanisms. This comparison highlights the complementary nature of these two approaches in understanding a model’s memorization capabilities and the retrievability of information from its training data.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Abstraction</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Abstraction in the context of memorization refers to the level of specificity at which information is retained and reproduced by language models. This spectrum ranges from highly specific, fact-based memorization to more generalized concept-level retention. This is related to the concept of verbatim memorization in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S2.SS2" title="II-B Severity of memorization ‣ II Spectrum of Memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>, but abstraction focuses more on information (semantics of facts) than on literal word use. In terms of abstraction, memorization can be categorized into two main types: factual memorization and conceptual memorization. These categories represent different levels of information granularity and have different implications for model performance, privacy concerns, and potential applications.</p>
</div>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Factual memorization</h4>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px1.p1.1"><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.1" style="color:#000000;">Factual memorization in language models involves accurately retaining and reproducing specific pieces of information from training data, often relying on context cues to recall details such as names, dates, and numbers or the relations between such entities. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Petroni et al.</span> <span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib12" title=""><span class="ltx_text" style="color:#000000;">12</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.3.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Jiang et al.</span> <span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib38" title=""><span class="ltx_text" style="color:#000000;">38</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.3.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">AlKhamissi et al.</span> <span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib39" title=""><span class="ltx_text" style="color:#000000;">39</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.3.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Luo et al.</span> <span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib40" title=""><span class="ltx_text" style="color:#000000;">40</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.4" style="color:#000000;"> investigate the measurement of factual knowledge in LLMs by evaluating their comprehension of facts from knowledge bases using ranking metrics. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Pezeshkpour</span> <span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib41" title=""><span class="ltx_text" style="color:#000000;">41</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.7" style="color:#000000;"> proposes two approaches of explicit and implicit instillation of factual knowledge in LLMs while proposing to use KL-divergence as a metric for measuring factual memory. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Yu et al.</span> <span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.8.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib42" title=""><span class="ltx_text" style="color:#000000;">42</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.9.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.10" style="color:#000000;"> show that the frequency of facts in the pretraining data significantly influences a language model’s behavior. They find that models are more likely to generate memorized capital cities for frequently mentioned countries, while being less responsive to in-context information. Additionally, they demonstrate that larger models tend to favor memorized answers over in-context details, even when the memorized facts are less frequent. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Kang and Choi</span> <span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.11.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib43" title=""><span class="ltx_text" style="color:#000000;">43</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.12.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.13" style="color:#000000;"> also show that factual knowledge probing accuracy in large language models is linked to subject-object co-occurrence. They reveal that LLMs often prioritize frequently co-occurring word pairs, which can override correct answers, particularly when those answers are infrequent. Their findings highlight the need to address co-occurrence bias in future model development.</span></p>
</div>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS4.SSS0.Px1.p2.1"><span class="ltx_text" id="S2.SS4.SSS0.Px1.p2.1.1" style="color:#000000;">While factual memorization could be beneficial for the performance of LLMs in tasks namely question answering, it could potentially lead to various issues such as undesired bias and privacy and copyright violations.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Conceptual memorization</h4>
<div class="ltx_para" id="S2.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px2.p1.1"><span class="ltx_text" id="S2.SS4.SSS0.Px2.p1.1.1" style="color:#000000;">Conceptual memorization involves the model’s ability to internalize and generalize broader ideas, patterns, and concepts from its training data </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS4.SSS0.Px2.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib44" title=""><span class="ltx_text" style="color:#000000;">44</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib45" title=""><span class="ltx_text" style="color:#000000;">45</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib46" title=""><span class="ltx_text" style="color:#000000;">46</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib4" title=""><span class="ltx_text" style="color:#000000;">4</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px2.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S2.SS4.SSS0.Px2.p1.1.4" style="color:#000000;">. That is, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Lee et al.</span> <span class="ltx_text" id="S2.SS4.SSS0.Px2.p1.1.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib47" title=""><span class="ltx_text" style="color:#000000;">47</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px2.p1.1.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S2.SS4.SSS0.Px2.p1.1.7" style="color:#000000;"> explore the </span><span class="ltx_text ltx_font_italic" id="S2.SS4.SSS0.Px2.p1.1.8" style="color:#000000;">idea plagiarism</span><span class="ltx_text" id="S2.SS4.SSS0.Px2.p1.1.9" style="color:#000000;"> of GPT-2 model and empirically show that language models go beyond merely generating verbatim text from their training data; rather they also rephrase sentences and imitate ideas from different sources.</span></p>
</div>
<div class="ltx_para" id="S2.SS4.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS4.SSS0.Px2.p2.1"><span class="ltx_text" id="S2.SS4.SSS0.Px2.p2.1.1" style="color:#000000;">Compared to factual memorization, conceptual memorization generally poses a lower risk of exposing specific, sensitive information. At the same time, it is more difficult to be detected and it may lead to perpetuation of biases present in the training data at a more systemic level.</span></p>
</div>
<div class="ltx_para" id="S2.SS4.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS4.SSS0.Px2.p3.1"><span class="ltx_text" id="S2.SS4.SSS0.Px2.p3.1.1" style="color:#000000;">The boundary between conceptual memorization and the understanding of the model (generalization) is subtle and still debated.</span></p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Measuring memorization</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text" id="S3.p1.1.1" style="color:#000000;">Measuring memorization in large language models is a complex task, primarily because we cannot easily compute the probability of all the samples present in the training data. To obtain an accurate measure of memorization in language models, we would ideally need to compare the model’s output probabilities for every possible sequence against the training data. However, given the vast space of possible sequences, this is computationally intractable. Therefore, researchers have developed various methods to approximate and quantify memorization.</span></p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Exposure metric</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text" id="S3.SS1.p1.1.1" style="color:#000000;">The Exposure Metric, introduced by </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S3.SS1.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib30" title=""><span class="ltx_text" style="color:#000000;">30</span></a><span class="ltx_text" id="S3.SS1.p1.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S3.SS1.p1.1.4" style="color:#000000;">, provides a quantitative measure of how much a model has memorized specific sequences from its training data. This metric is particularly useful for assessing the memorization of rare or unique information.</span></p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text" id="S3.SS1.p2.1.1" style="color:#000000;">The exposure metric has been widely adopted in memorization studies.
To practically apply the exposure metric, researchers often use the “canary extraction test” </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS1.p2.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib30" title=""><span class="ltx_text" style="color:#000000;">30</span></a><span class="ltx_text" id="S3.SS1.p2.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S3.SS1.p2.1.4" style="color:#000000;">. This involves inserting known ‘canary’ sequences into the training data and then measuring their exposure in the trained model.</span></p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Helali et al.</span> <span class="ltx_text" id="S3.SS1.p3.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib48" title=""><span class="ltx_text" style="color:#000000;">48</span></a><span class="ltx_text" id="S3.SS1.p3.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S3.SS1.p3.1.3" style="color:#000000;"> propose the so-called “d-exposure” metric as a measure of memorization for discriminative tasks such as text classification, sinse in those situations we don’t have access to the perplexity of a given text.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Inference attacks</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text" id="S3.SS2.p1.1.1" style="color:#000000;">Inference attacks are another approach to measuring memorization, focusing on the model’s ability to reveal information about its training data. These attacks typically perform under the assumption that when a model displays high confidence in its outputs, those outputs are likely to be from the training data, which means the model has memorized them. These attacks can be categorized into two main types:</span></p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Membership Inference Attacks (MIA)</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1"><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.1.1" style="color:#000000;">These attacks aim to determine whether a specific data point was part of the model’s training set. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Shokri et al.</span> <span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib49" title=""><span class="ltx_text" style="color:#000000;">49</span></a><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.1.4" style="color:#000000;"> introduced this concept for machine learning models, and it has since been adapted for language models. While the MIA approach in the context of machine learning uses a “shadow learning” paradigm, this is not feasible for LLMs, since they are much heavier in size. Instead, methods for MIA on LLMs try to extract the certainty of the LLMs on the given text and use it as sign of membership in the training data, which is usually done by computing the perplexity metric on the sequence </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.1.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib50" title=""><span class="ltx_text" style="color:#000000;">50</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib51" title=""><span class="ltx_text" style="color:#000000;">51</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib52" title=""><span class="ltx_text" style="color:#000000;">52</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib53" title=""><span class="ltx_text" style="color:#000000;">53</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib54" title=""><span class="ltx_text" style="color:#000000;">54</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib55" title=""><span class="ltx_text" style="color:#000000;">55</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib56" title=""><span class="ltx_text" style="color:#000000;">56</span></a><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.1.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.1.7" style="color:#000000;">.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Extraction Attacks</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1"><span class="ltx_text" id="S3.SS2.SSS0.Px2.p1.1.1" style="color:#000000;">These attacks attempt to extract specific pieces of information from the model that were present in its training data. Different works have demonstrated the feasibility of such attacks on language models, showing that private information could be extracted through prompting with different strategies </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.SSS0.Px2.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib57" title=""><span class="ltx_text" style="color:#000000;">57</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib34" title=""><span class="ltx_text" style="color:#000000;">34</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib58" title=""><span class="ltx_text" style="color:#000000;">58</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib25" title=""><span class="ltx_text" style="color:#000000;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib59" title=""><span class="ltx_text" style="color:#000000;">59</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib56" title=""><span class="ltx_text" style="color:#000000;">56</span></a><span class="ltx_text" id="S3.SS2.SSS0.Px2.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S3.SS2.SSS0.Px2.p1.1.4" style="color:#000000;">.</span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.1"><span class="ltx_text" id="S3.SS2.SSS0.Px2.p2.1.1" style="color:#000000;">The effectiveness of inference attacks can serve as a proxy measure for memorization. Models that are more susceptible to these attacks are generally considered to have higher levels of memorization.</span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p3.1"><span class="ltx_text" id="S3.SS2.SSS0.Px2.p3.1.1" style="color:#000000;">In conclusion, while measuring memorization in language models remains a challenging task, techniques like the Exposure Metric and Inference Attacks provide valuable insights into the memorization behavior of these models. These methods allow researchers to approximate the extent of memorization and assess potential privacy risks associated with large language models.</span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Counterfactual memorization</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2"><span class="ltx_text" id="S3.SS3.p1.2.1" style="color:#000000;">Previous work analyzed the memorization of large language models on sensitive information (e.g. phone numbers) in the training data </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS3.p1.2.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib13" title=""><span class="ltx_text" style="color:#000000;">13</span></a><span class="ltx_text" id="S3.SS3.p1.2.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S3.SS3.p1.2.4" style="color:#000000;"> or synthetically injected ‘canaries’ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS3.p1.2.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib30" title=""><span class="ltx_text" style="color:#000000;">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib60" title=""><span class="ltx_text" style="color:#000000;">60</span></a><span class="ltx_text" id="S3.SS3.p1.2.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S3.SS3.p1.2.7" style="color:#000000;">. However, not all the memorized texts are equally interesting. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Zhang et al.</span> <span class="ltx_text" id="S3.SS3.p1.2.8.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib61" title=""><span class="ltx_text" style="color:#000000;">61</span></a><span class="ltx_text" id="S3.SS3.p1.2.9.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S3.SS3.p1.2.10" style="color:#000000;"> dig into a new kind of memorization which is counterfactual memorization. The idea is to see how the presence or absence of a sample of the dataset, affects the performance of the model on the same sample. This definition of memorization has similarities with the definition of differential privacy. Differential privacy as mitigation strategy will be discussed in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S6.SS3" style="color:#000000;" title="VI-C Differential privacy ‣ VI Mitigating memorization: strategies and techniques ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-C</span></span></a><span class="ltx_text" id="S3.SS3.p1.2.11" style="color:#000000;">.
In their experiments, the authors create different subsets of a bigger dataset and then they fine-tune the LM on each of these. Then they consider an item </span><math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" mathcolor="#000000" xref="S3.SS3.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_x</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.2.12" style="color:#000000;"> from the datasets and based on the presence or absence of </span><math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" mathcolor="#000000" xref="S3.SS3.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_x</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.2.13" style="color:#000000;"> in the subsets, they divide the subsets into two groups: IN and OUT. Then they test and report the performance on the IN and OUT group of models by averaging. Their experiments on 400 trained models show the counterfactual memorized data, are generally unconventional text such as all-caps, structured formats (i.e. tables or bullet lists), and multilingual texts. In conclusion, counterfactual memorization provides a novel perspective on understanding what language models retain from their training data.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Influences and dynamics of memorization</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text" id="S4.p1.1.1" style="color:#000000;">Understanding the factors that influence memorization in large language models is crucial for developing more efficient, secure, and privacy-preserving systems. This section explores various aspects that affect memorization, from model architecture to training processes.</span></p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Model capacity</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text" id="S4.SS1.p1.1.1" style="color:#000000;">The first significant factor influencing memorization is model size. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S4.SS1.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib23" title=""><span class="ltx_text" style="color:#000000;">23</span></a><span class="ltx_text" id="S4.SS1.p1.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS1.p1.1.4" style="color:#000000;"> and </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Tirumala et al.</span> <span class="ltx_text" id="S4.SS1.p1.1.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib31" title=""><span class="ltx_text" style="color:#000000;">31</span></a><span class="ltx_text" id="S4.SS1.p1.1.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS1.p1.1.7" style="color:#000000;"> have both demonstrated that larger models are more prone to memorization and do so more rapidly. This trend persists across different architectures and datasets. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S4.SS1.p1.1.8.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib23" title=""><span class="ltx_text" style="color:#000000;">23</span></a><span class="ltx_text" id="S4.SS1.p1.1.9.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS1.p1.1.10" style="color:#000000;"> find that the relationship between model size and memorization grows consistently on a log-linear scale, with larger models memorizing a greater portion of the data.</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Tirumala et al.</span> <span class="ltx_text" id="S4.SS1.p2.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib31" title=""><span class="ltx_text" style="color:#000000;">31</span></a><span class="ltx_text" id="S4.SS1.p2.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS1.p2.1.3" style="color:#000000;"> further highlight that larger models not only memorize more but do so faster in the training process. Interestingly, while memorization increases with model size, it does not necessarily correlate with improved performance. This was shown by </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S4.SS1.p2.1.4.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib23" title=""><span class="ltx_text" style="color:#000000;">23</span></a><span class="ltx_text" id="S4.SS1.p2.1.5.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS1.p2.1.6" style="color:#000000;"> by comparison of models with similar capacities but differing performance levels because of their architectures. Overall, the findings suggest that the ability of LLMs to memorize is strongly linked to their size, potentially due to the high capacity of these models to store detailed information from training data.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Training data characteristics</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text" id="S4.SS2.p1.1.1" style="color:#000000;">The nature of the training data heavily influences memorization. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Lee et al.</span> <span class="ltx_text" id="S4.SS2.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib62" title=""><span class="ltx_text" style="color:#000000;">62</span></a><span class="ltx_text" id="S4.SS2.p1.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.4" style="color:#000000;"> develop tools to deduplicate training data and show that models trained on deduplicated data would produce memorized text ten times less frequently. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Kandpal et al.</span> <span class="ltx_text" id="S4.SS2.p1.1.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib29" title=""><span class="ltx_text" style="color:#000000;">29</span></a><span class="ltx_text" id="S4.SS2.p1.1.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.7" style="color:#000000;"> show that a sequence appearing 10 times in the training data is, on average, generated approximately 1000 times more frequently than a sequence that appears only once. A study done by </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Tirumala et al.</span> <span class="ltx_text" id="S4.SS2.p1.1.8.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib31" title=""><span class="ltx_text" style="color:#000000;">31</span></a><span class="ltx_text" id="S4.SS2.p1.1.9.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.10" style="color:#000000;"> on memorization of different parts of speech, reveals that nouns and numbers are memorized significantly faster than other parts of speech, likely because they serve as unique identifiers for specific samples.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Input and prompting strategies</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S4.SS3.p1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib13" title=""><span class="ltx_text" style="color:#000000;">13</span></a><span class="ltx_text" id="S4.SS3.p1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">McCoy et al.</span> <span class="ltx_text" id="S4.SS3.p1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib63" title=""><span class="ltx_text" style="color:#000000;">63</span></a><span class="ltx_text" id="S4.SS3.p1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Kandpal et al.</span> <span class="ltx_text" id="S4.SS3.p1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib29" title=""><span class="ltx_text" style="color:#000000;">29</span></a><span class="ltx_text" id="S4.SS3.p1.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS3.p1.1.3" style="color:#000000;"> show that longer prompts increase the likelihood of triggering memorized sequences, making it easier for language models to regurgitate training data. Moreover, methods like prefix tuning </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p1.1.4.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib64" title=""><span class="ltx_text" style="color:#000000;">64</span></a><span class="ltx_text" id="S4.SS3.p1.1.5.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS3.p1.1.6" style="color:#000000;"> and prompt engineering have been employed to maximize memorization. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Ozdayi et al.</span> <span class="ltx_text" id="S4.SS3.p1.1.7.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib65" title=""><span class="ltx_text" style="color:#000000;">65</span></a><span class="ltx_text" id="S4.SS3.p1.1.8.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS3.p1.1.9" style="color:#000000;"> introduce a novel approach using prompt tuning to control memorized content extraction rates in LLMs. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Weller et al.</span> <span class="ltx_text" id="S4.SS3.p1.1.10.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib66" title=""><span class="ltx_text" style="color:#000000;">66</span></a><span class="ltx_text" id="S4.SS3.p1.1.11.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS3.p1.1.12" style="color:#000000;"> propose ‘according-to’ prompting, a technique that directs LLMs to ground responses in previously observed text.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Tokenization</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Kharitonov et al.</span> <span class="ltx_text" id="S4.SS4.p1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib67" title=""><span class="ltx_text" style="color:#000000;">67</span></a><span class="ltx_text" id="S4.SS4.p1.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS4.p1.1.3" style="color:#000000;"> explore the impact of the tokenizer on memorization. They experiment with the size of the subword vocabulary learned through Byte-Pair Encoding (BPE) and demonstrate that increasing the subword vocabulary significantly affects the model’s ability and inclination to memorize training data. Furthermore, models with larger vocabularies are more likely to reproduce training data when given specific prompts. The authors suggest that this effect stems from the reduction in sequence lengths as BPE vocabulary size increases.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.5.1.1">IV-E</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS5.6.2">Sampling methods</span>
</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.2"><span class="ltx_text" id="S4.SS5.p1.2.1" style="color:#000000;">In their experiments, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S4.SS5.p1.2.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib13" title=""><span class="ltx_text" style="color:#000000;">13</span></a><span class="ltx_text" id="S4.SS5.p1.2.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS5.p1.2.4" style="color:#000000;"> initially opt for greedy sampling to maximize the regeneration of training data. One limitation is that this sampling scheme generates low diversity outputs; thus, they also experiment with the decaying temperature and Top-n sampling methods, the latter of which shows to be more successful. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Yu et al.</span> <span class="ltx_text" id="S4.SS5.p1.2.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib68" title=""><span class="ltx_text" style="color:#000000;">68</span></a><span class="ltx_text" id="S4.SS5.p1.2.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS5.p1.2.7" style="color:#000000;"> experiment with different sampling schemes including Decaying temperature, Top-n, Nucleus-</span><math alttext="\eta" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mi id="S4.SS5.p1.1.m1.1.1" mathcolor="#000000" xref="S4.SS5.p1.1.m1.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><ci id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">italic_η</annotation></semantics></math><span class="ltx_text" id="S4.SS5.p1.2.8" style="color:#000000;"> and Typical-</span><math alttext="\phi" class="ltx_Math" display="inline" id="S4.SS5.p1.2.m2.1"><semantics id="S4.SS5.p1.2.m2.1a"><mi id="S4.SS5.p1.2.m2.1.1" mathcolor="#000000" xref="S4.SS5.p1.2.m2.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><ci id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.2.m2.1d">italic_ϕ</annotation></semantics></math><span class="ltx_text" id="S4.SS5.p1.2.9" style="color:#000000;"> samplings </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS5.p1.2.10.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib69" title=""><span class="ltx_text" style="color:#000000;">69</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib70" title=""><span class="ltx_text" style="color:#000000;">70</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib71" title=""><span class="ltx_text" style="color:#000000;">71</span></a><span class="ltx_text" id="S4.SS5.p1.2.11.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS5.p1.2.12" style="color:#000000;"> and use an auto-tuning method on these to find the optimal sampling method that yields to the maximization of training data reproduction.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS6.5.1.1">IV-F</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS6.6.2">Fine-tuning and transfer learning</span>
</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Mireshghallah et al.</span> <span class="ltx_text" id="S4.SS6.p1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib72" title=""><span class="ltx_text" style="color:#000000;">72</span></a><span class="ltx_text" id="S4.SS6.p1.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS6.p1.1.3" style="color:#000000;"> evaluate how different fine-tuning methods — full model, model head, and adapter fine-tuning — vary in terms of memorization and vulnerability to privacy attacks. Their research, using membership inference and extraction attacks, finds that head fine-tuning is most susceptible to attacks, whereas adapter fine-tuning is less prone. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Zeng et al.</span> <span class="ltx_text" id="S4.SS6.p1.1.4.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib73" title=""><span class="ltx_text" style="color:#000000;">73</span></a><span class="ltx_text" id="S4.SS6.p1.1.5.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS6.p1.1.6" style="color:#000000;"> conduct a comprehensive analysis of fine-tuning T5 models </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS6.p1.1.7.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib74" title=""><span class="ltx_text" style="color:#000000;">74</span></a><span class="ltx_text" id="S4.SS6.p1.1.8.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS6.p1.1.9" style="color:#000000;"> across various tasks, including summarization, dialogue, question answering, and machine translation, finding that fine-tuned memorization varies significantly depending on the task. Additionally, they identify a strong link between attention score distributions and memorization, and propose that multi-task fine-tuning can mitigate memorization risks more effectively than single-task fine-tuning.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS7.5.1.1">IV-G</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS7.6.2">Training process dynamics</span>
</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Zhang et al.</span> <span class="ltx_text" id="S4.SS7.p1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib61" title=""><span class="ltx_text" style="color:#000000;">61</span></a><span class="ltx_text" id="S4.SS7.p1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Kandpal et al.</span> <span class="ltx_text" id="S4.SS7.p1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib29" title=""><span class="ltx_text" style="color:#000000;">29</span></a><span class="ltx_text" id="S4.SS7.p1.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS7.p1.1.3" style="color:#000000;"> show that memorization grows consistently with the number of training epochs, which makes sense, as more epochs push the model to potential overfitting. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Jagielski et al.</span> <span class="ltx_text" id="S4.SS7.p1.1.4.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib75" title=""><span class="ltx_text" style="color:#000000;">75</span></a><span class="ltx_text" id="S4.SS7.p1.1.5.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS7.p1.1.6" style="color:#000000;"> demonstrate that examples seen during the earlier stages of training are less prone to memorization and rather they are forgotten over time. These findings indicate that memorization increases with more training, while early-seen examples being more likely to be forgotten.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS8">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS8.5.1.1">IV-H</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS8.6.2">Forgetting mechanisms</span>
</h3>
<div class="ltx_para" id="S4.SS8.p1">
<p class="ltx_p" id="S4.SS8.p1.1"><span class="ltx_text" id="S4.SS8.p1.1.1" style="color:#000000;">In machine learning, forgetting mechanisms are the processes through which models lose or discard previously learned information </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS8.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib76" title=""><span class="ltx_text" style="color:#000000;">76</span></a><span class="ltx_text" id="S4.SS8.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.p1.1.4" style="color:#000000;">. These mechanisms can occur unintentionally as part of the natural training dynamics or be purposefully induced to meet specific objectives, such as improving model generalization or addressing privacy concerns </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S4.SS8.p1.1.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib77" title=""><span class="ltx_text" style="color:#000000;">77</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib78" title=""><span class="ltx_text" style="color:#000000;">78</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib79" title=""><span class="ltx_text" style="color:#000000;">79</span></a><span class="ltx_text" id="S4.SS8.p1.1.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.p1.1.7" style="color:#000000;">. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Blanco-Justicia et al.</span> <span class="ltx_text" id="S4.SS8.p1.1.8.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib80" title=""><span class="ltx_text" style="color:#000000;">80</span></a><span class="ltx_text" id="S4.SS8.p1.1.9.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.p1.1.10" style="color:#000000;"> provide a recent, detailed overview of forgetting in LLMs.</span></p>
</div>
<section class="ltx_paragraph" id="S4.SS8.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Catastrophic forgetting</h4>
<div class="ltx_para" id="S4.SS8.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS8.SSS0.Px1.p1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Kirkpatrick et al.</span> <span class="ltx_text" id="S4.SS8.SSS0.Px1.p1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib81" title=""><span class="ltx_text" style="color:#000000;">81</span></a><span class="ltx_text" id="S4.SS8.SSS0.Px1.p1.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.SSS0.Px1.p1.1.3" style="color:#000000;"> initially introduced catastrophic forgetting in the context of neural networks and continual learning. They propose a method to protect important model weights to retain knowledge. This approach has been effective in maintaining performance on older tasks, even after long periods of non-use.
</span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Tirumala et al.</span> <span class="ltx_text" id="S4.SS8.SSS0.Px1.p1.1.4.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib31" title=""><span class="ltx_text" style="color:#000000;">31</span></a><span class="ltx_text" id="S4.SS8.SSS0.Px1.p1.1.5.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.SSS0.Px1.p1.1.6" style="color:#000000;"> observe the forgetting mechanisms of a special batch through the learning process and show that it follows an exponentially degradation, reaching to a constant value baseline. They show that the mentioned baseline scales with the model size.
</span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Jagielski et al.</span> <span class="ltx_text" id="S4.SS8.SSS0.Px1.p1.1.7.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib75" title=""><span class="ltx_text" style="color:#000000;">75</span></a><span class="ltx_text" id="S4.SS8.SSS0.Px1.p1.1.8.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.SSS0.Px1.p1.1.9" style="color:#000000;"> address the dual phenomena of memorization and forgetting in LLMs through stronger privacy attacks and several strategies for measuring the worst-case forgetting of the training examples. The study introduces a method to measure to what extent models forget specific training data, highlighting that standard image, speech, and language models do indeed forget examples over time, though non-convex models might retain data indefinitely in the worst case. The findings suggest that examples from early training phases, such as those used in pre-training large models, might enjoy privacy benefits but could disadvantage examples encountered later in training.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS8.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Intentional forgetting techniques</h4>
<div class="ltx_para" id="S4.SS8.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS8.SSS0.Px2.p1.1"><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.1" style="color:#000000;">As memorization could lead to privacy risks and copyright issues, intentional forgetting could be necessary in some situations. Methods like knowledge unlearning aim to selectively remove specific information from trained models without retraining it from scratch. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Bourtoule et al.</span> <span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib82" title=""><span class="ltx_text" style="color:#000000;">82</span></a><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.4" style="color:#000000;"> introduce the SISA framework for efficient machine unlearning, which divides the training data into shards (shards partition the data into disjoint segments) and trains sub-models that can be easily retrained if data needs to be removed. For LLMs specifically, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Chen and Yang</span> <span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib83" title=""><span class="ltx_text" style="color:#000000;">83</span></a><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.7" style="color:#000000;"> introduce lightweight unlearning layers into transformers, allowing for selective data removal without full model retraining. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Pawelczyk et al.</span> <span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.8.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib84" title=""><span class="ltx_text" style="color:#000000;">84</span></a><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.9.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.10" style="color:#000000;"> introduce “In-Context Unlearning”, which involves providing specific training instances with flipped labels and additional correctly labeled instances as inputs during inference, effectively removing the targeted information without updating model parameters. Additionally, knowledge unlearning techniques have been categorized into parameter optimization, parameter merging, and in-context learning, each offering unique advantages in efficiently removing harmful or undesirable knowledge from LLMs </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.11.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib85" title=""><span class="ltx_text" style="color:#000000;">85</span></a><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.12.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.13" style="color:#000000;">. These methods not only enhance privacy and security but also ensure that the overall performance of the models remains intact, making them scalable and practical for real-world applications </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.14.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib86" title=""><span class="ltx_text" style="color:#000000;">86</span></a><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.15.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S4.SS8.SSS0.Px2.p1.1.16" style="color:#000000;">.</span></p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1">
<span class="ltx_text" id="S4.T2.1.1.1.1.1" style="color:#000000;">Factor from Section </span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4" style="color:#000000;" title="IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">IV</span></a>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.2"><span class="ltx_text" id="S4.T2.1.1.1.2.1" style="color:#000000;">Key findings</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.3"><span class="ltx_text" id="S4.T2.1.1.1.3.1" style="color:#000000;">References</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.1.1.1">
<span class="ltx_p" id="S4.T2.1.2.1.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.1.2.1.1.1.1.1" style="color:#000000;">Model capacity</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.1.2.1">
<span class="ltx_p" id="S4.T2.1.2.1.2.1.1" style="width:199.2pt;"><span class="ltx_text" id="S4.T2.1.2.1.2.1.1.1" style="color:#000000;">Larger models memorize more</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.1.3.1">
<span class="ltx_p" id="S4.T2.1.2.1.3.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S4.T2.1.2.1.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib23" title=""><span class="ltx_text" style="color:#000000;">23</span></a><span class="ltx_text" id="S4.T2.1.2.1.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Tirumala et al.</span> <span class="ltx_text" id="S4.T2.1.2.1.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib31" title=""><span class="ltx_text" style="color:#000000;">31</span></a><span class="ltx_text" id="S4.T2.1.2.1.3.1.1.2.2.2.1" style="color:#000000;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.1.1">
<span class="ltx_p" id="S4.T2.1.3.2.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.1.3.2.1.1.1.1" style="color:#000000;">Training data</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.2.1">
<span class="ltx_p" id="S4.T2.1.3.2.2.1.1" style="width:199.2pt;"><span class="ltx_text" id="S4.T2.1.3.2.2.1.1.1" style="color:#000000;">Duplicated data amplifies memorization</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.3.1">
<span class="ltx_p" id="S4.T2.1.3.2.3.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Lee et al.</span> <span class="ltx_text" id="S4.T2.1.3.2.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib62" title=""><span class="ltx_text" style="color:#000000;">62</span></a><span class="ltx_text" id="S4.T2.1.3.2.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Kandpal et al.</span> <span class="ltx_text" id="S4.T2.1.3.2.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib29" title=""><span class="ltx_text" style="color:#000000;">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib29" title=""><span class="ltx_text" style="color:#000000;">29</span></a><span class="ltx_text" id="S4.T2.1.3.2.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Tirumala et al.</span> <span class="ltx_text" id="S4.T2.1.3.2.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib31" title=""><span class="ltx_text" style="color:#000000;">31</span></a><span class="ltx_text" id="S4.T2.1.3.2.3.1.1.2.2.2.1" style="color:#000000;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.1.1">
<span class="ltx_p" id="S4.T2.1.4.3.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.1.4.3.1.1.1.1" style="color:#000000;">Input and prompting</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.2.1">
<span class="ltx_p" id="S4.T2.1.4.3.2.1.1" style="width:199.2pt;"><span class="ltx_text" id="S4.T2.1.4.3.2.1.1.1" style="color:#000000;">Longer prompts and prompt tuning can facilitate recall of the memorized suffix.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.3.1">
<span class="ltx_p" id="S4.T2.1.4.3.3.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S4.T2.1.4.3.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib13" title=""><span class="ltx_text" style="color:#000000;">13</span></a><span class="ltx_text" id="S4.T2.1.4.3.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">McCoy et al.</span> <span class="ltx_text" id="S4.T2.1.4.3.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib63" title=""><span class="ltx_text" style="color:#000000;">63</span></a><span class="ltx_text" id="S4.T2.1.4.3.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Kandpal et al.</span> <span class="ltx_text" id="S4.T2.1.4.3.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib29" title=""><span class="ltx_text" style="color:#000000;">29</span></a><span class="ltx_text" id="S4.T2.1.4.3.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Li and Liang</span> <span class="ltx_text" id="S4.T2.1.4.3.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib64" title=""><span class="ltx_text" style="color:#000000;">64</span></a><span class="ltx_text" id="S4.T2.1.4.3.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Ozdayi et al.</span> <span class="ltx_text" id="S4.T2.1.4.3.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib65" title=""><span class="ltx_text" style="color:#000000;">65</span></a><span class="ltx_text" id="S4.T2.1.4.3.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Weller et al.</span> <span class="ltx_text" id="S4.T2.1.4.3.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib66" title=""><span class="ltx_text" style="color:#000000;">66</span></a><span class="ltx_text" id="S4.T2.1.4.3.3.1.1.2.2.2.1" style="color:#000000;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.1.1">
<span class="ltx_p" id="S4.T2.1.5.4.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.1.5.4.1.1.1.1" style="color:#000000;">Tokenization</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.2.1">
<span class="ltx_p" id="S4.T2.1.5.4.2.1.1" style="width:199.2pt;"><span class="ltx_text" id="S4.T2.1.5.4.2.1.1.1" style="color:#000000;">Bigger tokenizer vocabulary leads to more memorization</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.3.1">
<span class="ltx_p" id="S4.T2.1.5.4.3.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Kharitonov et al.</span> <span class="ltx_text" id="S4.T2.1.5.4.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib67" title=""><span class="ltx_text" style="color:#000000;">67</span></a><span class="ltx_text" id="S4.T2.1.5.4.3.1.1.2.2.2.1" style="color:#000000;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.5.1.1">
<span class="ltx_p" id="S4.T2.1.6.5.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.1.6.5.1.1.1.1" style="color:#000000;">Sampling methods</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.5.2.1">
<span class="ltx_p" id="S4.T2.1.6.5.2.1.1" style="width:199.2pt;"><span class="ltx_text" id="S4.T2.1.6.5.2.1.1.1" style="color:#000000;">While greedy sampling can pinpoint extremely memorized samples, top-n sampling is the most effective method to retrieve more memorized items.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.5.3.1">
<span class="ltx_p" id="S4.T2.1.6.5.3.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S4.T2.1.6.5.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib13" title=""><span class="ltx_text" style="color:#000000;">13</span></a><span class="ltx_text" id="S4.T2.1.6.5.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Yu et al.</span> <span class="ltx_text" id="S4.T2.1.6.5.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib68" title=""><span class="ltx_text" style="color:#000000;">68</span></a><span class="ltx_text" id="S4.T2.1.6.5.3.1.1.2.2.2.1" style="color:#000000;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.6.1.1">
<span class="ltx_p" id="S4.T2.1.7.6.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.1.7.6.1.1.1.1" style="color:#000000;">Fine-tuning</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.6.2.1">
<span class="ltx_p" id="S4.T2.1.7.6.2.1.1" style="width:199.2pt;"><span class="ltx_text" id="S4.T2.1.7.6.2.1.1.1" style="color:#000000;">The amount of memorization after fine-tuning significantly varies depending on the task.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.6.3.1">
<span class="ltx_p" id="S4.T2.1.7.6.3.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Mireshghallah et al.</span> <span class="ltx_text" id="S4.T2.1.7.6.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib87" title=""><span class="ltx_text" style="color:#000000;">87</span></a><span class="ltx_text" id="S4.T2.1.7.6.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Zeng et al.</span> <span class="ltx_text" id="S4.T2.1.7.6.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib73" title=""><span class="ltx_text" style="color:#000000;">73</span></a><span class="ltx_text" id="S4.T2.1.7.6.3.1.1.2.2.2.1" style="color:#000000;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.8.7.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.7.1.1">
<span class="ltx_p" id="S4.T2.1.8.7.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.1.8.7.1.1.1.1" style="color:#000000;">Training process</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.7.2.1">
<span class="ltx_p" id="S4.T2.1.8.7.2.1.1" style="width:199.2pt;"><span class="ltx_text" id="S4.T2.1.8.7.2.1.1.1" style="color:#000000;">Earlier phases of training are less prone to memorization</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.1.8.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.8.7.3.1">
<span class="ltx_p" id="S4.T2.1.8.7.3.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Zhang et al.</span> <span class="ltx_text" id="S4.T2.1.8.7.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib61" title=""><span class="ltx_text" style="color:#000000;">61</span></a><span class="ltx_text" id="S4.T2.1.8.7.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Kandpal et al.</span> <span class="ltx_text" id="S4.T2.1.8.7.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib29" title=""><span class="ltx_text" style="color:#000000;">29</span></a><span class="ltx_text" id="S4.T2.1.8.7.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Jagielski et al.</span> <span class="ltx_text" id="S4.T2.1.8.7.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib75" title=""><span class="ltx_text" style="color:#000000;">75</span></a><span class="ltx_text" id="S4.T2.1.8.7.3.1.1.2.2.2.1" style="color:#000000;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S4.T2.1.9.8.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.8.1.1">
<span class="ltx_p" id="S4.T2.1.9.8.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.1.9.8.1.1.1.1" style="color:#000000;">Forgetting mechanisms</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S4.T2.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.8.2.1">
<span class="ltx_p" id="S4.T2.1.9.8.2.1.1" style="width:199.2pt;"><span class="ltx_text" id="S4.T2.1.9.8.2.1.1.1" style="color:#000000;">Forgetting follows an exponentially decaying curve</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S4.T2.1.9.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.9.8.3.1">
<span class="ltx_p" id="S4.T2.1.9.8.3.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Tirumala et al.</span> <span class="ltx_text" id="S4.T2.1.9.8.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib31" title=""><span class="ltx_text" style="color:#000000;">31</span></a><span class="ltx_text" id="S4.T2.1.9.8.3.1.1.2.2.2.1" style="color:#000000;">]</span>, <span class="ltx_text" style="color:#000000;">Jagielski et al.</span> <span class="ltx_text" id="S4.T2.1.9.8.3.1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib75" title=""><span class="ltx_text" style="color:#000000;">75</span></a><span class="ltx_text" id="S4.T2.1.9.8.3.1.1.2.2.2.1" style="color:#000000;">]</span></cite></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Influences and dynamics of memorization and their key findings (discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4" title="IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">IV</span></a>)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS8.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS8.SSS0.Px2.p2.1"><span class="ltx_text" id="S4.SS8.SSS0.Px2.p2.1.1" style="color:#000000;">In conclusion, memorization in LLMs is influenced by a complex interplay of factors ranging from model architecture to training dynamics. Understanding these influences is crucial for developing more robust, privacy-preserving, and ethically sound language models. An overview of the findings for each of the factors discussed in this subsections is provided in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.T2" style="color:#000000;" title="TABLE II ‣ Intentional forgetting techniques ‣ IV-H Forgetting mechanisms ‣ IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">II</span></a><span class="ltx_text" id="S4.SS8.SSS0.Px2.p2.1.2" style="color:#000000;">.</span></p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Memorization in specific model architectures</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text" id="S5.p1.1.1" style="color:#000000;">While the target of most of the papers discussed in this survey is auto-regressive large language models, in this section we will overview the memorization in other types of language models, as it would help us gain a deeper understanding of this phenomena.</span></p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Masked language models (MLM)</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text" id="S5.SS1.p1.1.1" style="color:#000000;">Early studies investigating privacy leakage through the memorization of masked language models (MLMs) yielded limited success. For instance, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Vakili and Dalianis</span> <span class="ltx_text" id="S5.SS1.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib88" title=""><span class="ltx_text" style="color:#000000;">88</span></a><span class="ltx_text" id="S5.SS1.p1.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS1.p1.1.4" style="color:#000000;"> attempt to extract patients’ medical conditions from BERT models trained on clinical data, concluding that the risk of privacy leakage in BERT-based models is minimal. Similarly, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Lehman et al.</span> <span class="ltx_text" id="S5.SS1.p1.1.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib89" title=""><span class="ltx_text" style="color:#000000;">89</span></a><span class="ltx_text" id="S5.SS1.p1.1.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS1.p1.1.7" style="color:#000000;"> conduct attacks on BERT models trained on the MIMIC-III corpus, reaching the same conclusion that stronger attacks may be necessary to trigger memorization in MLMs. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Jagannatha et al.</span> <span class="ltx_text" id="S5.SS1.p1.1.8.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib90" title=""><span class="ltx_text" style="color:#000000;">90</span></a><span class="ltx_text" id="S5.SS1.p1.1.9.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS1.p1.1.10" style="color:#000000;"> employ membership inference attacks on clinical MLMs, demonstrating that these models are far less vulnerable to such attacks compared to auto-regressive (generative) models. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S5.SS1.p1.1.11.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib23" title=""><span class="ltx_text" style="color:#000000;">23</span></a><span class="ltx_text" id="S5.SS1.p1.1.12.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS1.p1.1.13" style="color:#000000;"> extend their previous work on auto-regressive language models by conducting experiments on the T5 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.SS1.p1.1.14.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib74" title=""><span class="ltx_text" style="color:#000000;">74</span></a><span class="ltx_text" id="S5.SS1.p1.1.15.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS1.p1.1.16" style="color:#000000;"> model family, trained on the C4 corpus. They demonstrated that scaling laws still apply, but the degree of memorization in MLMs remains significantly lower compared to similarly sized auto-regressive models.</span></p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Mireshghallah et al.</span> <span class="ltx_text" id="S5.SS1.p2.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib87" title=""><span class="ltx_text" style="color:#000000;">87</span></a><span class="ltx_text" id="S5.SS1.p2.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS1.p2.1.3" style="color:#000000;"> introduce “likelihood ratio membership inference attacks” and show that privacy risks in MLMs, while smaller, can still be significant if the attacker uses more advanced methods.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Aligned language models</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text" id="S5.SS2.p1.1.1" style="color:#000000;">In studies exploring memorization in aligned language models, experiments typically use either targeted prompting or random prompting approaches. However, these methods face two key challenges that limit their effectiveness.</span></p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text" id="S5.SS2.p2.1.1" style="color:#000000;">First, the role tag which is prepended to both input queries and model responses disrupts traditional attack methods, such as the prefix prompting methods. Second, alignment training further complicates extraction attempts. Even if attackers could bypass the role tag, the model’s alignment training, designed to ensure ethical responses, reduces the likelihood of disclosing sensitive or inappropriate information.</span></p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text" id="S5.SS2.p3.1.1" style="color:#000000;">To address these challenges, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Nasr et al.</span> <span class="ltx_text" id="S5.SS2.p3.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib34" title=""><span class="ltx_text" style="color:#000000;">34</span></a><span class="ltx_text" id="S5.SS2.p3.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS2.p3.1.4" style="color:#000000;"> introduce the “Divergence Attack.” This method asks the model to endlessly repeat a specific word, causing it to break from its alignment training and revert to its original language modeling objectives. In some cases, this led to the verbatim reproduction of training data. Notably, this only occurs when single-token words were repeated, as multi-token prompts led to the model continuing the repetition without reproducing training data.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Retrieval augmented generation (RAG)</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><span class="ltx_text" id="S5.SS3.p1.1.1" style="color:#000000;">Retrieval-augmented language models generate text distributions referencing both the parameters of the underlying language model and the information retrieved from a document index.
</span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Huang et al.</span> <span class="ltx_text" id="S5.SS3.p1.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib91" title=""><span class="ltx_text" style="color:#000000;">91</span></a><span class="ltx_text" id="S5.SS3.p1.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS3.p1.1.4" style="color:#000000;"> analyze privacy issues in retrieval-based language models. They use the kNN-LM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.SS3.p1.1.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib92" title=""><span class="ltx_text" style="color:#000000;">92</span></a><span class="ltx_text" id="S5.SS3.p1.1.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS3.p1.1.7" style="color:#000000;"> as their target model through API access. They investigate a scenario in which a model creator has a private, domain-specific data-store that improves model performance on domain-specific tasks, but may also contain sensitive information that should not be revealed. They analyze the access to the text completion and perplexity calculation APIs of the model. Their findings show that kNN-LMs are more prone to disclosing private data from their private data-store compared to parametric models.
</span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Zeng et al.</span> <span class="ltx_text" id="S5.SS3.p1.1.8.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib93" title=""><span class="ltx_text" style="color:#000000;">93</span></a><span class="ltx_text" id="S5.SS3.p1.1.9.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS3.p1.1.10" style="color:#000000;"> conduct targeted and prefix attacks on Retrieval-Augmented Generation (RAG) frameworks to examine how their memorization behaviors differ from those of standard LLMs. The results showed that incorporating retrieval mechanisms significantly lowers the model’s memorization capacity, reducing the success rate of training data extraction attacks in both targeted and prefix attack scenarios. This suggests that the use of retrieval-based data in RAG models makes them more resilient to such attacks compared to standalone LLMs, as the external retrieval component limits the model’s reliance on internal memorization.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Mitigating memorization: strategies and techniques</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text" id="S6.p1.1.1" style="color:#000000;">As discussed in earlier sections, memorization is influenced by a range of factors and it is sometimes necessary for the learning process </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib94" title=""><span class="ltx_text" style="color:#000000;">94</span></a><span class="ltx_text" id="S6.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.p1.1.4" style="color:#000000;">. However, in scenarios where memorization could lead to privacy concerns or security vulnerabilities, some methods could be employed to mitigate its impact. In these cases, specific strategies are utilized to limit the retention of sensitive information, ensuring that potential risks related to data exposure or misuse are minimized.</span></p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Data de-duplication</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Lee et al.</span> <span class="ltx_text" id="S6.SS1.p1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib62" title=""><span class="ltx_text" style="color:#000000;">62</span></a><span class="ltx_text" id="S6.SS1.p1.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS1.p1.1.3" style="color:#000000;"> run exact matching and approximate matching (MiniHash) de-duplication algorithms on the C4, RealNews, LM1B, and Wiki40B datasets and show that these datasets contain up to 13.6% near duplicates and up to 19.4% exact duplicates; showing that de-duplication can reduce training cost in the first place. Measuring the amount of duplicates shared between train and validation splits of these datasets, they show that there is a possibility of validation data leakage which causes the over-estimation of the model accuracy or may lead to biases when considering hyperparameters tuning.</span></p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1"><span class="ltx_text" id="S6.SS1.p2.1.1" style="color:#000000;">To investigate the impact of data de-duplication on language model’s memorization, they train a 1.5B parameter GPT-2 model from scratch on three different settings: C4-Original, C4-NearDup, and C4-ExactSubstr, each for two epochs. Then the evaluate the memorization in no prompt and prompted settings and count the 50-token substrings generations that are presented in the training data as memorization. The no-prompt experiment generations show </span><math alttext="10\times" class="ltx_math_unparsed" display="inline" id="S6.SS1.p2.1.m1.1"><semantics id="S6.SS1.p2.1.m1.1a"><mrow id="S6.SS1.p2.1.m1.1b"><mn id="S6.SS1.p2.1.m1.1.1" mathcolor="#000000">10</mn><mo id="S6.SS1.p2.1.m1.1.2" lspace="0.222em" mathcolor="#000000">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">10\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.1.m1.1d">10 ×</annotation></semantics></math><span class="ltx_text" id="S6.SS1.p2.1.2" style="color:#000000;"> less memorization in de-duplicated trained models. On the other hand, in the prompted experiment, when the prompt comes from the duplicate examples, the model trained on C4-Original generates the true exact continuation over 40% of the time. The other two models also generate the ground truth more often when the prompt is sampled from the duplicate examples, suggesting that more harsh de-duplication algorithms are needed to prevent memorization.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">MemFree decoding</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Ippolito et al.</span> <span class="ltx_text" id="S6.SS2.p1.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib32" title=""><span class="ltx_text" style="color:#000000;">32</span></a><span class="ltx_text" id="S6.SS2.p1.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p1.1.3" style="color:#000000;"> try to prevent memorization using the novel MemFree Decoding method. Their approach proposes a new sampling strategy to prevent the emission of memorized samples. In the sampling phase, they keep track of the current n-gram (where ‘n‘ is a fixed parameter and should be chosen carefully; small n would prevent many common phrases to appear and big n may not prevent memorization) that is being generated and after generation of each token, they check the existence of the last n-gram in the training set. If this n-gram is a member of the training set, they ignore the newly generated token and sample another token, so as to obtain a non-memorized string. They inspect the GPT-Neo </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p1.1.4.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib95" title=""><span class="ltx_text" style="color:#000000;">95</span></a><span class="ltx_text" id="S6.SS2.p1.1.5.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p1.1.6" style="color:#000000;"> family of models from smallest size to the largest one, prompting with 50 token sequences from the training data and anticipating for a 50 token suffix match. They show that their method significantly reduces the similarity of the generations with respect to the training data. They also show that although the approximate memorization increases with the model size log-linearly, MemFree defence stays effective in keeping the similarity around 0.6 BLEU score even in the largest model size.</span></p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text" id="S6.SS2.p2.1.1" style="color:#000000;">While this verbatim memorization filtering reduces the average similarity score between the training data and the generations, qualitative analysis of the generations shows that the models opt to cheat in some sense. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Ippolito et al.</span> <span class="ltx_text" id="S6.SS2.p2.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib32" title=""><span class="ltx_text" style="color:#000000;">32</span></a><span class="ltx_text" id="S6.SS2.p2.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p2.1.4" style="color:#000000;"> inspect the production model GitHub Copilot </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p2.1.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib96" title=""><span class="ltx_text" style="color:#000000;">96</span></a><span class="ltx_text" id="S6.SS2.p2.1.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p2.1.7" style="color:#000000;"> and show that when it is prompted with the training data, it successfully avoids verbatim generation, however, a simple style transfer in the prompts easily bypasses the Copilots memorization defence. They repeat this experiment on the English language models such as PaLM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p2.1.8.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib36" title=""><span class="ltx_text" style="color:#000000;">36</span></a><span class="ltx_text" id="S6.SS2.p2.1.9.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p2.1.10" style="color:#000000;"> and show how this defense could be bypassed, concluding this defence is an incomplete defence.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.5.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.6.2">Differential privacy</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1"><span class="ltx_text" id="S6.SS3.p1.1.1" style="color:#000000;">Differential privacy (DP) is a technique that protects individual data points by adding noise, minimizing the impact of any single data point on the model’s output.
DP-SGD (Differentially Private Stochastic Gradient Descent) is an adaptation of the standard SGD algorithm, designed to fine-tune language models while maintaining privacy </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS3.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib97" title=""><span class="ltx_text" style="color:#000000;">97</span></a><span class="ltx_text" id="S6.SS3.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS3.p1.1.4" style="color:#000000;">. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S6.SS3.p1.1.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib30" title=""><span class="ltx_text" style="color:#000000;">30</span></a><span class="ltx_text" id="S6.SS3.p1.1.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS3.p1.1.7" style="color:#000000;"> demonstrate that by adjusting the privacy budget parameter </span><math alttext="\epsilon" class="ltx_Math" display="inline" id="S6.SS3.p1.1.m1.1"><semantics id="S6.SS3.p1.1.m1.1a"><mi id="S6.SS3.p1.1.m1.1.1" mathcolor="#000000" xref="S6.SS3.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><ci id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.1.m1.1d">italic_ϵ</annotation></semantics></math><span class="ltx_text" id="S6.SS3.p1.1.8" style="color:#000000;"> in DP-SGD training, the exposure of memorized data can be reduced to a level that makes it indistinguishable from any other data. However, this comes at the cost of reduced model utility and a slower training process.</span></p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1"><span class="ltx_text" id="S6.SS3.p2.1.1" style="color:#000000;">To address these utility issues, some studies propose selective differential privacy approaches </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS3.p2.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib98" title=""><span class="ltx_text" style="color:#000000;">98</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib99" title=""><span class="ltx_text" style="color:#000000;">99</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib100" title=""><span class="ltx_text" style="color:#000000;">100</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib101" title=""><span class="ltx_text" style="color:#000000;">101</span></a><span class="ltx_text" id="S6.SS3.p2.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS3.p2.1.4" style="color:#000000;">. For instance, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Kerrigan et al.</span> <span class="ltx_text" id="S6.SS3.p2.1.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib98" title=""><span class="ltx_text" style="color:#000000;">98</span></a><span class="ltx_text" id="S6.SS3.p2.1.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS3.p2.1.7" style="color:#000000;"> propose training a non-private base model on a public dataset and then fine-tuning it on a private dataset using DP-SGD. This approach aims to balance privacy with model performance.</span></p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Li et al.</span> <span class="ltx_text" id="S6.SS3.p3.1.1.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib99" title=""><span class="ltx_text" style="color:#000000;">99</span></a><span class="ltx_text" id="S6.SS3.p3.1.2.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS3.p3.1.3" style="color:#000000;"> show that with carefully chosen hyperparameters and downstream task objectives, fine-tuning pretrained language models with DP-SGD can yield strong performance on a variety of NLP tasks at privacy levels. Remarkably, some of their fine-tuned models even outperform non-private baselines and models trained under heuristic privacy approaches.</span></p>
</div>
<div class="ltx_para" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1"><span class="ltx_text" id="S6.SS3.p4.1.1" style="color:#000000;">While DP is effective for reducing memorization and preserving privacy, it is crucial to select hyperparameters wisely. Otherwise, the model may not withstand stronger privacy attacks, potentially compromising its effectiveness as shown in </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Lukas et al.</span> <span class="ltx_text" id="S6.SS3.p4.1.2.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib59" title=""><span class="ltx_text" style="color:#000000;">59</span></a><span class="ltx_text" id="S6.SS3.p4.1.3.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS3.p4.1.4" style="color:#000000;">.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS4.5.1.1">VI-D</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS4.6.2">Unlearning methods</span>
</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1"><span class="ltx_text" id="S6.SS4.p1.1.1" style="color:#000000;">As discussed in the intentional forgetting section [</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S4.SS8" style="color:#000000;" title="IV-H Forgetting mechanisms ‣ IV Influences and dynamics of memorization ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-H</span></span></a><span class="ltx_text" id="S6.SS4.p1.1.2" style="color:#000000;">], unlearning methods offer effective strategies to mitigate memorization by selectively removing specific information from trained models without the need for full retraining. However, a limitation of these unlearning methods is that they require prior knowledge of what the model has memorized in order to selectively remove it, which can be challenging to determine and may limit the effectiveness of these techniques.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Future directions and open challenges</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1"><span class="ltx_text" id="S7.p1.1.1" style="color:#000000;">Based on the discussion of the existing literature to date on memorization in LLMs, we make suggestions for research topics to be addressed in the near future.</span></p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS1.5.1.1">VII-A</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS1.6.2">Balancing performance with privacy in LLMs</span>
</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1"><span class="ltx_text" id="S7.SS1.p1.1.1" style="color:#000000;">As discussed earlier, memorization in LLMs can also have beneficial uses, such as improving factual recall and enhancing performance in specific tasks. The precondition is that privacy leakage and copyright issues have been taken care of. Methods like Differential Privacy (DP), that are designed to mitigate memorization risks and prevent sensitive data exposure, often come at the cost of reduced model performance, as they limit the model’s ability to retain useful information. To fully harness the benefits of memorization while safeguarding against privacy breaches, future research needs to focus on strategies that balance these competing goals. This includes developing techniques that protect sensitive data and intellectual property without significantly degrading the model’s accuracy and utility. Such efforts will be key to ensuring both ethical compliance and high-performance outcomes in LLMs.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS2.5.1.1">VII-B</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS2.6.2">Reducing factual memorization in favor of conceptual memorization</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1"><span class="ltx_text" id="S7.SS2.p1.1.1" style="color:#000000;">The interplay between factual and conceptual memorization is relevant in LLM development, because there is a trade-off between correctness of information, and undesirable recall of training data: While factual memorization can provide precise information recall, it also presents higher risks in terms of privacy and data protection. Conceptual memorization, on the other hand, contributes to the model’s ability to generalize and apply knowledge flexibly, but may be more challenging to audit and control. For example, when using LLMs for question answering, literal reproduction of facts is desired; this includes names and numbers (e.g. “Who was the first person to fly across the ocean and when did this take place?”). It is straightforward to evaluate LLMs for generating the correct facts. If we prefer conceptual memorization in LLMs, we should accept a higher rate of hallucinations, because the model more freely generates text based on its parametric knowledge.</span></p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1"><span class="ltx_text" id="S7.SS2.p2.1.1" style="color:#000000;">Future research in this area should therefore focus on developing techniques to balance these two types of memorization, enhancing the benefits of each while mitigating their respective risks. This could involve methods to selectively encourage conceptual memorization while limiting unnecessary factual memorization, particularly of sensitive information, depending on the type of task at hand.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS3.5.1.1">VII-C</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS3.6.2">The boundary of memorization and understanding</span>
</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1"><span class="ltx_text" id="S7.SS3.p1.1.1" style="color:#000000;">It is relatively straightforward to design experiments that demonstrate a model’s ability to generate fluent and human-like text, such as generating novel content or adapting to new contexts. However, verifying that these capabilities are not simply the result of memorization rather than abstraction over the information processed during training is much more challenging, as both can produce similar outcomes. Distinguishing between these requires carefully designed experiments, and further research is needed to clarify when a model is abstracting over learned concepts versus merely memorizing and reproducing them.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS4.5.1.1">VII-D</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS4.6.2">Memorization in specific contexts</span>
</h3>
<div class="ltx_para" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1"><span class="ltx_text" id="S7.SS4.p1.1.1" style="color:#000000;">Several application domains are currently understudied with respect to the effect of memorization. We identified four contexts specifically where more research is needed.</span></p>
</div>
<section class="ltx_paragraph" id="S7.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Conversational Agents</h4>
<div class="ltx_para" id="S7.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS4.SSS0.Px1.p1.1"><span class="ltx_text" id="S7.SS4.SSS0.Px1.p1.1.1" style="color:#000000;">LLMs that have been fine-tuned for conversations can be used as conversational agents (chatbots), e.g., to assist customers of online services </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS4.SSS0.Px1.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib102" title=""><span class="ltx_text" style="color:#000000;">102</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib103" title=""><span class="ltx_text" style="color:#000000;">103</span></a><span class="ltx_text" id="S7.SS4.SSS0.Px1.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.SS4.SSS0.Px1.p1.1.4" style="color:#000000;">. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Nasr et al.</span> <span class="ltx_text" id="S7.SS4.SSS0.Px1.p1.1.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib34" title=""><span class="ltx_text" style="color:#000000;">34</span></a><span class="ltx_text" id="S7.SS4.SSS0.Px1.p1.1.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.SS4.SSS0.Px1.p1.1.7" style="color:#000000;"> introduce their so-called “divergence attack” to extract memorized samples from conversation-aligned LLMs; however, as they also discuss, these attacks are not powerful enough to stimulate training data reproduction. This makes us conclude that attacking conversation-aligned language models requires more advanced methods. Since most of the production language models are only available in a conversational settings, addressing memorization in conversational agents and conducting novel attacking methods to alleviate training data extraction in these models would be a prominent research direction.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Retrieval-Augmented Generation (RAG)</h4>
<div class="ltx_para" id="S7.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S7.SS4.SSS0.Px2.p1.1"><span class="ltx_text" id="S7.SS4.SSS0.Px2.p1.1.1" style="color:#000000;">In RAG frameworks, LLMs are helped with a retrieval component that first selects the most relevant documents from a collection and feeds them to the prompt </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS4.SSS0.Px2.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib104" title=""><span class="ltx_text" style="color:#000000;">104</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib105" title=""><span class="ltx_text" style="color:#000000;">105</span></a><span class="ltx_text" id="S7.SS4.SSS0.Px2.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.SS4.SSS0.Px2.p1.1.4" style="color:#000000;">. The LLM then generates an answer based on the provided sources. This has advantages such as the reduction of hallucination </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS4.SSS0.Px2.p1.1.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib106" title=""><span class="ltx_text" style="color:#000000;">106</span></a><span class="ltx_text" id="S7.SS4.SSS0.Px2.p1.1.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.SS4.SSS0.Px2.p1.1.7" style="color:#000000;">, the transparency of sources, and the potential of generating answers related to proprietary or novel sources that were not in the LLM training data. Although very popular, RAG is not yet thoroughly analyzed regarding memorization. This is important, because a reduction of hallucination when using RAG in LLMs could also have an increase in memorization as a side effect: less hallucination means that the LLM stays closer to the original content. Recently, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Zeng et al.</span> <span class="ltx_text" id="S7.SS4.SSS0.Px2.p1.1.8.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib93" title=""><span class="ltx_text" style="color:#000000;">93</span></a><span class="ltx_text" id="S7.SS4.SSS0.Px2.p1.1.9.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.SS4.SSS0.Px2.p1.1.10" style="color:#000000;"> analyzed privacy aspects in RAG. They show that RAG systems are vulnerable of leaking private information. On the other hand, they also found that RAG can mitigate the reproduction of the LLM training data. A broader study focusing on memorization in RAG-based LLMs that compares different proposed retrieval architectures is an important future research direction.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Multilingual Large Language Models (xLM)</h4>
<div class="ltx_para" id="S7.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S7.SS4.SSS0.Px3.p1.1"><span class="ltx_text" id="S7.SS4.SSS0.Px3.p1.1.1" style="color:#000000;">xLMs are trained to interpret and generate text in multiple languages. Trained on extensive datasets that include various languages, these models develop language-agnostic representations </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS4.SSS0.Px3.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib107" title=""><span class="ltx_text" style="color:#000000;">107</span></a><span class="ltx_text" id="S7.SS4.SSS0.Px3.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.SS4.SSS0.Px3.p1.1.4" style="color:#000000;">. Despite their potential, xLMs often face challenges related to data scarcity for low-resource languages, leading to performance disparities </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS4.SSS0.Px3.p1.1.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib108" title=""><span class="ltx_text" style="color:#000000;">108</span></a><span class="ltx_text" id="S7.SS4.SSS0.Px3.p1.1.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.SS4.SSS0.Px3.p1.1.7" style="color:#000000;">. This necessitates ongoing research to enhance their efficacy and fairness across all languages. For future work, we propose to investigate whether a low-resource language setting is more prone to memorization by comparing to English scenarios. Addressing this research direction is essential not only for understanding the privacy implications associated with LLMs and data leakage in low-resource settings but also for ensuring AI safety in societies using LLMs with languages that have fewer resources.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS4.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Diffusion Language Models (DLM)</h4>
<div class="ltx_para" id="S7.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S7.SS4.SSS0.Px4.p1.1"><span class="ltx_text" id="S7.SS4.SSS0.Px4.p1.1.1" style="color:#000000;">Since DLMs show remarkable performance in the vision domain, researchers have started to adopt the diffusion models (DM) idea to the text domain and utilize their generative capabilities </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS4.SSS0.Px4.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib109" title=""><span class="ltx_text" style="color:#000000;">109</span></a><span class="ltx_text" id="S7.SS4.SSS0.Px4.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.SS4.SSS0.Px4.p1.1.4" style="color:#000000;">. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Carlini et al.</span> <span class="ltx_text" id="S7.SS4.SSS0.Px4.p1.1.5.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib110" title=""><span class="ltx_text" style="color:#000000;">110</span></a><span class="ltx_text" id="S7.SS4.SSS0.Px4.p1.1.6.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.SS4.SSS0.Px4.p1.1.7" style="color:#000000;"> have conducted data extraction analysis on the image diffusion models, showing that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="color:#000000;">Gu et al.</span> <span class="ltx_text" id="S7.SS4.SSS0.Px4.p1.1.8.1.1.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib111" title=""><span class="ltx_text" style="color:#000000;">111</span></a><span class="ltx_text" id="S7.SS4.SSS0.Px4.p1.1.9.2.2.1" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.SS4.SSS0.Px4.p1.1.10" style="color:#000000;"> also show that according to the training objective of the diffusion models, a memorization behavior is theoretically expected and then they quantify the impact of the influential factors on the memorization behaviors in DMs. However, research focusing on the memorization issues related to DLMs for text remains unexplored. Even though the idea of diffusion language models is the same as the vision domain, they are inherently different, because of the discrete nature of the text domain. Therefore an analysis of the general vision diffusion models may not be applicable to the diffusion language models, making an independent research on memorization against DLMs a prominent future direction.</span></p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Conclusion</span>
</h2>
<figure class="ltx_figure" id="S8.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="277" id="S8.F3.g1" src="extracted/5898740/images/papers_by_year.png" width="479"/>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The distribution by publication year of the papers discussed in this survey</figcaption>
</figure>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1"><span class="ltx_text" id="S8.p1.1.1" style="color:#000000;">In this paper, we systematically organized, summarized, and discussed the existing scientific work related to unintended and undesirable memorization in LLMs. Undesirable memorization might lead to privacy risks and other ethical consequences.
We found that there exists a large body of research on memorization in LLMs, given the young age of the technology: Transformer models were first developed in 2017 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S8.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib112" title=""><span class="ltx_text" style="color:#000000;">112</span></a><span class="ltx_text" id="S8.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S8.p1.1.4" style="color:#000000;">, and the first generative large language model with emergent abilities was released in 2022 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S8.p1.1.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib1" title=""><span class="ltx_text" style="color:#000000;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#bib.bib5" title=""><span class="ltx_text" style="color:#000000;">5</span></a><span class="ltx_text" id="S8.p1.1.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S8.p1.1.7" style="color:#000000;">. The majority of papers on the topic were therefore published in very recent years (see Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2410.02650v1#S8.F3" style="color:#000000;" title="Figure 3 ‣ VIII Conclusion ‣ Undesirable Memorization in Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S8.p1.1.8" style="color:#000000;">). This indicates that the field is working fast to analyze memorization and developing methods to mitigate undesirable memorization.</span></p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1"><span class="ltx_text" id="S8.p2.1.1" style="color:#000000;">Despite the fast-growing body of literature on the topic, we argue that there are important areas that require more attention in research in the coming years. We have pointed out four specific contexts in which memorization needs to be studied and, when needed and possible, mitigated: LLM-based conversational agents, retrieval-augmented generation, multilingual LLMs, and diffusion language models.
These areas all are of high importance, not only from the academic perspective, but maybe even more from the application and industry perspective. In particular conversational agents and retrieval-augmented generation are actively being developed in the commercial context. When applied to proprietary databases or in interaction with customer information, these applications are particularly vulnerable to privacy and security risks.
</span></p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section" style="color:#000000;">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1"><span class="ltx_text" id="Sx1.p1.1.1" style="color:#000000;">The main limitation of a SoK paper on a highly active research topic is the fast pace in which the field is moving. A survey paper written in 2024 risks to become outdated by 2026. Although we acknowledge this, we argue that the topic of memorization is too important to not create this systemization of knowledge – especially because of the added value that we created with our concrete suggestions for future directions.</span></p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section" style="color:#000000;">Acknowledgment</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1"><span class="ltx_text" id="Sx2.p1.1.1" style="color:#000000;">This publication is part of the project LESSEN</span><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://lessen-project.nl</span></span></span><span class="ltx_text" id="Sx2.p1.1.2" style="color:#000000;"> with project number NWA.1389.20.183 of the research program NWA ORC 2020/21 which is (partly) financed by the Dutch Research Council (NWO).</span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="color:#000000;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="color:#000000;">Brown et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="color:#000000;">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss,
G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter,
C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Language
models are few-shot learners,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.8.2" style="color:#000000;">Advances in Neural Information
Processing Systems</em><span class="ltx_text" id="bib.bib1.9.3" style="color:#000000;">, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
H. Lin, Eds., vol. 33.   Curran
Associates, Inc., 2020, pp. 1877–1901.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="color:#000000;">Touvron et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="color:#000000;">
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C.
Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu,
B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou,
H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S.
Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao,
X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton,
J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith,
R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan,
P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang,
A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, “Llama 2: Open
foundation and fine-tuned chat models,” 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.4.4.1" style="color:#000000;">OpenAI [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.6.1" style="color:#000000;">
OpenAI, “Chatgpt,” 2023. [Online]. Available: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com" style="color:#000000;" title="">https://chat.openai.com</a><span class="ltx_text" id="bib.bib3.7.2" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="color:#000000;">Ouyang et al. [2022a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="color:#000000;">
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,
S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller,
M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe,
“Training language models to follow instructions with human feedback,” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib4.8.2" style="color:#000000;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib4.9.3" style="color:#000000;">, S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35.   Curran Associates, Inc., 2022, pp.
27 730–27 744.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="color:#000000;">Wei et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="color:#000000;">
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama,
M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang,
J. Dean, and W. Fedus, “Emergent abilities of large language models,”
</span><em class="ltx_emph ltx_font_italic" id="bib.bib5.8.2" style="color:#000000;">Transactions on Machine Learning Research</em><span class="ltx_text" id="bib.bib5.9.3" style="color:#000000;">, 2022, survey Certification.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="color:#000000;">Xu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="color:#000000;">
F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn, “A systematic evaluation
of large language models of code,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.8.2" style="color:#000000;">Proceedings of the 6th ACM
SIGPLAN International Symposium on Machine Programming</em><span class="ltx_text" id="bib.bib6.9.3" style="color:#000000;">, ser. MAPS
2022.   New York, NY, USA: Association
for Computing Machinery, 2022, p. 1–10.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="color:#000000;">Guo et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="color:#000000;">
Z. Guo, R. Jin, C. Liu, Y. Huang, D. Shi, L. Yu, Y. Liu, J. Li, B. Xiong,
D. Xiong </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib7.9.3" style="color:#000000;">, “Evaluating large language models: A comprehensive
survey,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.10.4" style="color:#000000;">arXiv preprint arXiv:2310.19736</em><span class="ltx_text" id="bib.bib7.11.5" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="color:#000000;">Chang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="color:#000000;">
Y.-C. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang,
Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, “A survey
on evaluation of large language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.8.2" style="color:#000000;">ArXiv</em><span class="ltx_text" id="bib.bib8.9.3" style="color:#000000;">, vol. abs/2307.03109,
2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="color:#000000;">Huang et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="color:#000000;">
L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng,
B. Qin </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib9.9.3" style="color:#000000;">, “A survey on hallucination in large language models:
Principles, taxonomy, challenges, and open questions,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.10.4" style="color:#000000;">arXiv preprint
arXiv:2311.05232</em><span class="ltx_text" id="bib.bib9.11.5" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="color:#000000;">Tjuatja et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="color:#000000;">
L. Tjuatja, V. Chen, T. Wu, A. Talwalkwar, and G. Neubig, “Do llms exhibit
human-like response biases? a case study in survey design,”
</span><em class="ltx_emph ltx_font_italic" id="bib.bib10.8.2" style="color:#000000;">Transactions of the Association for Computational Linguistics</em><span class="ltx_text" id="bib.bib10.9.3" style="color:#000000;">,
vol. 12, pp. 1011–1026, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="color:#000000;">Yao et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="color:#000000;">
Y. Yao, J. Duan, K. Xu, Y. Cai, Z. Sun, and Y. Zhang, “A survey on large
language model (llm) security and privacy: The good, the bad, and the ugly,”
</span><em class="ltx_emph ltx_font_italic" id="bib.bib11.8.2" style="color:#000000;">High-Confidence Computing</em><span class="ltx_text" id="bib.bib11.9.3" style="color:#000000;">, p. 100211, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="color:#000000;">Petroni et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="color:#000000;">
F. Petroni, T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu, and
A. Miller, “Language models as knowledge bases?” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.8.2" style="color:#000000;">Proceedings of
the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP)</em><span class="ltx_text" id="bib.bib12.9.3" style="color:#000000;">, K. Inui, J. Jiang, V. Ng, and X. Wan, Eds.   Hong Kong, China: Association for Computational
Linguistics, Nov. 2019, pp. 2463–2473.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="color:#000000;">Carlini et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="color:#000000;">
N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee,
A. Roberts, T. Brown, D. Song, U. Erlingsson </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib13.9.3" style="color:#000000;">, “Extracting
training data from large language models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.4" style="color:#000000;">30th USENIX Security
Symposium (USENIX Security 21)</em><span class="ltx_text" id="bib.bib13.11.5" style="color:#000000;">, 2021, pp. 2633–2650.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="color:#000000;">Song et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="color:#000000;">
B. Song, M. Deng, S. Pokhrel, Q. Lan, R. Doss, and G. Li, “Digital privacy
under attack: Challenges and enablers,” 02 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="color:#000000;">Bender et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="color:#000000;">
E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, “On the dangers
of stochastic parrots: Can language models be too big?” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.8.2" style="color:#000000;">Proceedings
of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em><span class="ltx_text" id="bib.bib15.9.3" style="color:#000000;">,
ser. FAccT ’21.   New York, NY, USA:
Association for Computing Machinery, 2021, p. 610–623. [Online]. Available:
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3442188.3445922" style="color:#000000;" title="">https://doi.org/10.1145/3442188.3445922</a><span class="ltx_text" id="bib.bib15.10.4" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="color:#000000;">Henderson et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="color:#000000;">
P. Henderson, X. Li, D. Jurafsky, T. Hashimoto, M. A. Lemley, and P. Liang,
“Foundation models and fair use,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.8.2" style="color:#000000;">Journal of Machine Learning
Research</em><span class="ltx_text" id="bib.bib16.9.3" style="color:#000000;">, vol. 24, no. 400, pp. 1–79, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="color:#000000;">Usynin et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="color:#000000;">
D. Usynin, M. Knolle, and G. Kaissis, “Memorisation in machine learning: A
survey of results,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.8.2" style="color:#000000;">Transactions on Machine Learning Research</em><span class="ltx_text" id="bib.bib17.9.3" style="color:#000000;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="color:#000000;">Wei et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="color:#000000;">
J. Wei, Y. Zhang, L. Y. Zhang, M. Ding, C. Chen, K.-L. Ong, J. Zhang, and
Y. Xiang, “Memorization in deep learning: A survey,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.8.2" style="color:#000000;">arXiv preprint
arXiv:2406.03880</em><span class="ltx_text" id="bib.bib18.9.3" style="color:#000000;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.4.4.1" style="color:#000000;">Neel and Chang [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.6.1" style="color:#000000;">
S. Neel and P. Chang, “Privacy issues in large language models: A survey,”
</span><em class="ltx_emph ltx_font_italic" id="bib.bib19.7.2" style="color:#000000;">arXiv preprint arXiv:2312.06717</em><span class="ltx_text" id="bib.bib19.8.3" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="color:#000000;">Smith et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="color:#000000;">
V. Smith, A. S. Shamsabadi, C. Ashurst, and A. Weller, “Identifying and
mitigating privacy risks stemming from language models: A survey,”
</span><em class="ltx_emph ltx_font_italic" id="bib.bib20.8.2" style="color:#000000;">arXiv preprint arXiv:2310.01424</em><span class="ltx_text" id="bib.bib20.9.3" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="color:#000000;">Hartmann et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="color:#000000;">
V. Hartmann, A. Suri, V. Bindschaedler, D. Evans, S. Tople, and R. West, “Sok:
Memorization in general-purpose large language models,” 2023. [Online].
Available: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.18362" style="color:#000000;" title="">https://arxiv.org/abs/2310.18362</a><span class="ltx_text" id="bib.bib21.8.2" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="color:#000000;">Peris et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="color:#000000;">
C. Peris, C. Dupuy, J. Majmudar, R. Parikh, S. Smaili, R. Zemel, and R. Gupta,
“Privacy in the time of language models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.8.2" style="color:#000000;">Proceedings of the
Sixteenth ACM International Conference on Web Search and Data Mining</em><span class="ltx_text" id="bib.bib22.9.3" style="color:#000000;">, ser.
WSDM ’23.   New York, NY, USA:
Association for Computing Machinery, 2023, p. 1291–1292.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="color:#000000;">Carlini et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="color:#000000;">
N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang,
“Quantifying memorization across neural language models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.8.2" style="color:#000000;">The
Eleventh International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib23.9.3" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="color:#000000;">Huang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="color:#000000;">
W. R. Huang, S. Chien, O. Thakkar, and R. Mathews, “Detecting unintended
memorization in language-model-fused asr,” pp. 2808–2812, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="color:#000000;">Carlini et al. [2019a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="color:#000000;">
N. Carlini, C. Liu, U. Erlingsson, J. Kos, and D. Song, “The secret sharer:
Evaluating and testing unintended memorization in neural networks,” ser.
SEC’19.   USA: USENIX Association, 2019,
p. 267–284.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="color:#000000;">Chen et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="color:#000000;">
L. Chen, Y. Deng, Y. Bian, Z. Qin, B. Wu, T.-S. Chua, and K.-F. Wong, “Beyond
factuality: A comprehensive evaluation of large language models as knowledge
generators,” pp. 6325–6341, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="color:#000000;">Ouyang et al. [2022b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="color:#000000;">
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,
S. Agarwal, K. Slama, A. Ray </span><em class="ltx_emph ltx_font_italic" id="bib.bib27.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib27.9.3" style="color:#000000;">, “Training language models to
follow instructions with human feedback,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib27.10.4" style="color:#000000;">Advances in neural
information processing systems</em><span class="ltx_text" id="bib.bib27.11.5" style="color:#000000;">, vol. 35, pp. 27 730–27 744, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="color:#000000;">Ranaldi et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="color:#000000;">
L. Ranaldi, E. S. Ruzzetti, and F. M. Zanzotto, “PreCog: Exploring the
relation between memorization and performance in pre-trained language
models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.8.2" style="color:#000000;">Proceedings of the 14th International Conference on Recent
Advances in Natural Language Processing</em><span class="ltx_text" id="bib.bib28.9.3" style="color:#000000;">, R. Mitkov and G. Angelova,
Eds.   Varna, Bulgaria: INCOMA Ltd.,
Shoumen, Bulgaria, Sep. 2023, pp. 961–967.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="color:#000000;">Kandpal et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="color:#000000;">
N. Kandpal, E. Wallace, and C. Raffel, “Deduplicating training data mitigates
privacy risks in language models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib29.8.2" style="color:#000000;">International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em><span class="ltx_text" id="bib.bib29.9.3" style="color:#000000;">,
ser. Proceedings of Machine Learning Research, vol. 162.   PMLR, 2022, pp. 10 697–10 707.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="color:#000000;">Carlini et al. [2019b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="color:#000000;">
N. Carlini, C. Liu, U. Erlingsson, J. Kos, and D. Song, “The secret sharer:
Evaluating and testing unintended memorization in neural networks,” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib30.8.2" style="color:#000000;">Proceedings of the 28th USENIX Conference on Security Symposium</em><span class="ltx_text" id="bib.bib30.9.3" style="color:#000000;">, ser.
SEC’19.   USA: USENIX Association, 2019,
p. 267–284.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="color:#000000;">Tirumala et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="color:#000000;">
K. Tirumala, A. H. Markosyan, L. Zettlemoyer, and A. Aghajanyan, “Memorization
without overfitting: Analyzing the training dynamics of large language
models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib31.8.2" style="color:#000000;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib31.9.3" style="color:#000000;">, A. H.
Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="color:#000000;">Ippolito et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="color:#000000;">
D. Ippolito, F. Tramer, M. Nasr, C. Zhang, M. Jagielski, K. Lee,
C. Choquette Choo, and N. Carlini, “Preventing generation of verbatim
memorization in language models gives a false sense of privacy,” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib32.8.2" style="color:#000000;">Proceedings of the 16th International Natural Language Generation
Conference</em><span class="ltx_text" id="bib.bib32.9.3" style="color:#000000;">, C. M. Keet, H.-Y. Lee, and S. Zarrieß, Eds.   Prague, Czechia: Association for Computational
Linguistics, Sep. 2023, pp. 28–53.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="color:#000000;">Biderman et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="color:#000000;">
S. Biderman, U. S. Prashanth, L. Sutawika, H. Schoelkopf, Q. Anthony,
S. Purohit, and E. Raff, “Emergent and predictable memorization in large
language models,” 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="color:#000000;">Nasr et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="color:#000000;">
M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A.
Choquette-Choo, E. Wallace, F. Tramèr, and K. Lee, “Scalable extraction of
training data from (production) language models,” 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="color:#000000;">Biderman et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="color:#000000;">
S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien, E. Hallahan,
M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika,
and O. Van Der Wal, “Pythia: a suite for analyzing large language models
across training and scaling,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.8.2" style="color:#000000;">Proceedings of the 40th International
Conference on Machine Learning</em><span class="ltx_text" id="bib.bib35.9.3" style="color:#000000;">, ser. ICML’23.   JMLR.org, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="color:#000000;">Anil et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="color:#000000;">
R. Anil, A. M. Dai, O. Firat, M. Johnson </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib36.9.3" style="color:#000000;">, “Palm 2 technical
report,” 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="color:#000000;">Kudugunta et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="color:#000000;">
S. Kudugunta, I. Caswell, B. Zhang, X. Garcia, C. A. Choquette-Choo, K. Lee,
D. Xin, A. Kusupati, R. Stella, A. Bapna, and O. Firat, “Madlad-400: A
multilingual and document-level large audited dataset,” 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="color:#000000;">Jiang et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="color:#000000;">
Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know what language
models know?” </span><em class="ltx_emph ltx_font_italic" id="bib.bib38.8.2" style="color:#000000;">Transactions of the Association for Computational
Linguistics</em><span class="ltx_text" id="bib.bib38.9.3" style="color:#000000;">, vol. 8, pp. 423–438, 2020. [Online]. Available:
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.tacl-1.28" style="color:#000000;" title="">https://aclanthology.org/2020.tacl-1.28</a><span class="ltx_text" id="bib.bib38.10.4" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="color:#000000;">AlKhamissi et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="color:#000000;">
B. AlKhamissi, M. Li, A. Celikyilmaz, M. Diab, and M. Ghazvininejad, “A review
on language models as knowledge bases,” 2022. [Online]. Available:
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.06031" style="color:#000000;" title="">https://arxiv.org/abs/2204.06031</a><span class="ltx_text" id="bib.bib39.8.2" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.5.5.1" style="color:#000000;">Luo et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="color:#000000;">
L. Luo, T.-T. Vu, D. Q. Phung, and G. Haffari, “Systematic assessment of
factual knowledge in large language models,” pp. 13 272–13 286, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.4.4.1" style="color:#000000;">Pezeshkpour [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.6.1" style="color:#000000;">
P. Pezeshkpour, “Measuring and modifying factual knowledge in large language
models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib41.7.2" style="color:#000000;">2023 International Conference on Machine Learning and
Applications (ICMLA)</em><span class="ltx_text" id="bib.bib41.8.3" style="color:#000000;">, 2023, pp. 831–838.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="color:#000000;">Yu et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="color:#000000;">
Q. Yu, J. Merullo, and E. Pavlick, “Characterizing mechanisms for factual
recall in language models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib42.8.2" style="color:#000000;">Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing</em><span class="ltx_text" id="bib.bib42.9.3" style="color:#000000;">, H. Bouamor, J. Pino, and
K. Bali, Eds.   Singapore: Association
for Computational Linguistics, Dec. 2023, pp. 9924–9959. [Online].
Available: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.615" style="color:#000000;" title="">https://aclanthology.org/2023.emnlp-main.615</a><span class="ltx_text" id="bib.bib42.10.4" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.4.4.1" style="color:#000000;">Kang and Choi [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.6.1" style="color:#000000;">
C. Kang and J. Choi, “Impact of co-occurrence on factual knowledge of large
language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib43.7.2" style="color:#000000;">ArXiv</em><span class="ltx_text" id="bib.bib43.8.3" style="color:#000000;">, vol. abs/2310.08256, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="color:#000000;">Bubeck et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="color:#000000;">
S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar,
P. Lee, Y. T. Lee, Y. Li, S. Lundberg </span><em class="ltx_emph ltx_font_italic" id="bib.bib44.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib44.9.3" style="color:#000000;">, “Sparks of artificial
general intelligence: Early experiments with gpt-4,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib44.10.4" style="color:#000000;">arXiv preprint
arXiv:2303.12712</em><span class="ltx_text" id="bib.bib44.11.5" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="color:#000000;">Srivastava et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="color:#000000;">
A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid </span><em class="ltx_emph ltx_font_italic" id="bib.bib45.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib45.9.3" style="color:#000000;">,
“Beyond the imitation game: Quantifying and extrapolating the capabilities
of language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib45.10.4" style="color:#000000;">Transactions on Machine Learning Research</em><span class="ltx_text" id="bib.bib45.11.5" style="color:#000000;">, 2023.
[Online]. Available: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=uyTL5Bvosj" style="color:#000000;" title="">https://openreview.net/forum?id=uyTL5Bvosj</a><span class="ltx_text" id="bib.bib45.12.6" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.5.5.1" style="color:#000000;">Reif et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="color:#000000;">
E. Reif, D. Ippolito, A. Yuan, A. Coenen, C. Callison-Burch, and J. Wei, “A
recipe for arbitrary text style transfer with large language models,” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib46.8.2" style="color:#000000;">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers)</em><span class="ltx_text" id="bib.bib46.9.3" style="color:#000000;">, S. Muresan, P. Nakov,
and A. Villavicencio, Eds.   Dublin,
Ireland: Association for Computational Linguistics, May 2022, pp. 837–848.
[Online]. Available: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-short.94" style="color:#000000;" title="">https://aclanthology.org/2022.acl-short.94</a><span class="ltx_text" id="bib.bib46.10.4" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="color:#000000;">Lee et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="color:#000000;">
J. Lee, T. Le, J. Chen, and D. Lee, “Do language models plagiarize?” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib47.8.2" style="color:#000000;">Proceedings of the ACM Web Conference 2023</em><span class="ltx_text" id="bib.bib47.9.3" style="color:#000000;">, ser. WWW ’23.   New York, NY, USA: Association for Computing
Machinery, 2023, p. 3637–3647. [Online]. Available:
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3543507.3583199" style="color:#000000;" title="">https://doi.org/10.1145/3543507.3583199</a><span class="ltx_text" id="bib.bib47.10.4" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="color:#000000;">Helali et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="color:#000000;">
M. Helali, T. Kleinbauer, and D. Klakow, “Assessing unintended memorization in
neural discriminative sequence models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib48.8.2" style="color:#000000;">Text, Speech, and
Dialogue</em><span class="ltx_text" id="bib.bib48.9.3" style="color:#000000;">, P. Sojka, I. Kopeček, K. Pala, and A. Horák, Eds.   Cham: Springer International Publishing,
2020, pp. 265–272.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.5.5.1" style="color:#000000;">Shokri et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.7.1" style="color:#000000;">
R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference
attacks against machine learning models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib49.8.2" style="color:#000000;">2017 IEEE Symposium on
Security and Privacy (SP)</em><span class="ltx_text" id="bib.bib49.9.3" style="color:#000000;">, 2017, pp. 3–18.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.5.5.1" style="color:#000000;">Mattern et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.7.1" style="color:#000000;">
J. Mattern, F. Mireshghallah, Z. Jin, B. Schoelkopf, M. Sachan, and
T. Berg-Kirkpatrick, “Membership inference attacks against language models
via neighbourhood comparison,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib50.8.2" style="color:#000000;">Findings of the Association for
Computational Linguistics: ACL 2023</em><span class="ltx_text" id="bib.bib50.9.3" style="color:#000000;">, A. Rogers, J. Boyd-Graber, and
N. Okazaki, Eds.   Toronto, Canada:
Association for Computational Linguistics, Jul. 2023, pp. 11 330–11 343.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.5.5.1" style="color:#000000;">Shachor et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.7.1" style="color:#000000;">
S. Shachor, N. Razinkov, and A. Goldsteen, “Improved membership inference
attacks against language classification models,” 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.5.5.1" style="color:#000000;">Shejwalkar et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.7.1" style="color:#000000;">
V. Shejwalkar, H. A. Inan, A. Houmansadr, and R. Sim, “Membership inference
attacks against NLP classification models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib52.8.2" style="color:#000000;">NeurIPS 2021 Workshop
Privacy in Machine Learning</em><span class="ltx_text" id="bib.bib52.9.3" style="color:#000000;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.5.5.1" style="color:#000000;">Jagannatha et al. [2021a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.7.1" style="color:#000000;">
A. N. Jagannatha, B. P. S. Rawat, and H. Yu, “Membership inference attack
susceptibility of clinical language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib53.8.2" style="color:#000000;">ArXiv</em><span class="ltx_text" id="bib.bib53.9.3" style="color:#000000;">, vol.
abs/2104.08305, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.5.5.1" style="color:#000000;">Wang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.7.1" style="color:#000000;">
Y. Wang, N. Xu, S. Huang, K. Mahmood, D. Guo, C. Ding, W. Wen, and
S. Rajasekaran, “Analyzing and defending against membership inference
attacks in natural language processing classification,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib54.8.2" style="color:#000000;">2022 IEEE
International Conference on Big Data (Big Data)</em><span class="ltx_text" id="bib.bib54.9.3" style="color:#000000;">, 2022, pp. 5823–5832.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.4.4.1" style="color:#000000;">Song and Shmatikov [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.6.1" style="color:#000000;">
C. Song and V. Shmatikov, “Auditing data provenance in text-generation
models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib55.7.2" style="color:#000000;">Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery &amp; Data Mining</em><span class="ltx_text" id="bib.bib55.8.3" style="color:#000000;">, ser. KDD ’19.   New York, NY, USA: Association for Computing
Machinery, 2019, p. 196–206.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib56.5.5.1" style="color:#000000;">Wang et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.7.1" style="color:#000000;">
J. G. Wang, J. Wang, M. Li, and S. Neel, “Pandora’s white-box: Increased
training data leakage in open llms,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib56.8.2" style="color:#000000;">arXiv preprint arXiv:2402.17012</em><span class="ltx_text" id="bib.bib56.9.3" style="color:#000000;">,
2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib57.4.4.1" style="color:#000000;">Ishihara [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.6.1" style="color:#000000;">
S. Ishihara, “Training data extraction from pre-trained language models: A
survey,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib57.7.2" style="color:#000000;">Proceedings of the 3rd Workshop on Trustworthy Natural
Language Processing (TrustNLP 2023)</em><span class="ltx_text" id="bib.bib57.8.3" style="color:#000000;">, A. Ovalle, K.-W. Chang, N. Mehrabi,
Y. Pruksachatkun, A. Galystan, J. Dhamala, A. Verma, T. Cao, A. Kumar, and
R. Gupta, Eds.   Toronto, Canada:
Association for Computational Linguistics, Jul. 2023, pp. 260–275.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib58.5.5.1" style="color:#000000;">Parikh et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.7.1" style="color:#000000;">
R. Parikh, C. Dupuy, and R. Gupta, “Canary extraction in natural language
understanding models,” pp. 552–560, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib59.5.5.1" style="color:#000000;">Lukas et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.7.1" style="color:#000000;">
N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, and S. Zanella-Beguelin,
“Analyzing leakage of personally identifiable information in language
models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib59.8.2" style="color:#000000;">2023 IEEE Symposium on Security and Privacy (SP)</em><span class="ltx_text" id="bib.bib59.9.3" style="color:#000000;">.   Los Alamitos, CA, USA: IEEE Computer
Society, may 2023, pp. 346–363.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib60.5.5.1" style="color:#000000;">Henderson et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.7.1" style="color:#000000;">
P. Henderson, K. Sinha, N. Angelard-Gontier, N. R. Ke, G. Fried, R. Lowe, and
J. Pineau, “Ethical challenges in data-driven dialogue systems,” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib60.8.2" style="color:#000000;">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and
Society</em><span class="ltx_text" id="bib.bib60.9.3" style="color:#000000;">, ser. AIES ’18.   New York, NY,
USA: Association for Computing Machinery, 2018, p. 123–129.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib61.5.5.1" style="color:#000000;">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.7.1" style="color:#000000;">
C. Zhang, D. Ippolito, K. Lee, M. Jagielski, F. Tramer, and N. Carlini,
“Counterfactual memorization in neural language models,” 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib62.5.5.1" style="color:#000000;">Lee et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.7.1" style="color:#000000;">
K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and
N. Carlini, “Deduplicating training data makes language models better,” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib62.8.2" style="color:#000000;">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em><span class="ltx_text" id="bib.bib62.9.3" style="color:#000000;">, S. Muresan, P. Nakov, and
A. Villavicencio, Eds.   Dublin,
Ireland: Association for Computational Linguistics, May 2022, pp. 8424–8445.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib63.5.5.1" style="color:#000000;">McCoy et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.7.1" style="color:#000000;">
R. T. McCoy, P. Smolensky, T. Linzen, J. Gao, and A. Celikyilmaz, “How much do
language models copy from their training data? evaluating linguistic novelty
in text generation using RAVEN,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib63.8.2" style="color:#000000;">Transactions of the Association for
Computational Linguistics</em><span class="ltx_text" id="bib.bib63.9.3" style="color:#000000;">, vol. 11, pp. 652–670, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib64.4.4.1" style="color:#000000;">Li and Liang [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.6.1" style="color:#000000;">
X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for
generation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib64.7.2" style="color:#000000;">Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers)</em><span class="ltx_text" id="bib.bib64.8.3" style="color:#000000;">, C. Zong,
F. Xia, W. Li, and R. Navigli, Eds.   Online: Association for Computational Linguistics, Aug. 2021, pp. 4582–4597.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib65.5.5.1" style="color:#000000;">Ozdayi et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.7.1" style="color:#000000;">
M. Ozdayi, C. Peris, J. FitzGerald, C. Dupuy, J. Majmudar, H. Khan, R. Parikh,
and R. Gupta, “Controlling the extraction of memorized data from large
language models via prompt-tuning,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib65.8.2" style="color:#000000;">Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers)</em><span class="ltx_text" id="bib.bib65.9.3" style="color:#000000;">, A. Rogers, J. Boyd-Graber, and N. Okazaki, Eds.   Toronto, Canada: Association for Computational
Linguistics, Jul. 2023, pp. 1512–1521. [Online]. Available:
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.acl-short.129" style="color:#000000;" title="">https://aclanthology.org/2023.acl-short.129</a><span class="ltx_text" id="bib.bib65.10.4" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib66.5.5.1" style="color:#000000;">Weller et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.7.1" style="color:#000000;">
O. Weller, M. Marone, N. Weir, D. Lawrie, D. Khashabi, and B. Van Durme,
““according to . . . ”: Prompting language models improves quoting
from pre-training data,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib66.8.2" style="color:#000000;">Proceedings of the 18th Conference of the
European Chapter of the Association for Computational Linguistics (Volume 1:
Long Papers)</em><span class="ltx_text" id="bib.bib66.9.3" style="color:#000000;">, Y. Graham and M. Purver, Eds.   St. Julian’s, Malta: Association for Computational Linguistics,
Mar. 2024, pp. 2288–2301.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib67.5.5.1" style="color:#000000;">Kharitonov et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.7.1" style="color:#000000;">
E. Kharitonov, M. Baroni, and D. Hupkes, “How BPE affects memorization in
transformers,” 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib68.5.5.1" style="color:#000000;">Yu et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.7.1" style="color:#000000;">
W. Yu, T. Pang, Q. Liu, C. Du, B. Kang, Y. Huang, M. Lin, and S. Yan, “Bag of
tricks for training data extraction from language models,” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib68.8.2" style="color:#000000;">Proceedings of the 40th International Conference on Machine Learning</em><span class="ltx_text" id="bib.bib68.9.3" style="color:#000000;">,
ser. ICML’23.   JMLR.org, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib69.5.5.1" style="color:#000000;">Fan et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.7.1" style="color:#000000;">
A. Fan, M. Lewis, and Y. Dauphin, “Hierarchical neural story generation,”
</span><em class="ltx_emph ltx_font_italic" id="bib.bib69.8.2" style="color:#000000;">arXiv preprint arXiv:1805.04833</em><span class="ltx_text" id="bib.bib69.9.3" style="color:#000000;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib70.5.5.1" style="color:#000000;">Holtzman et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.7.1" style="color:#000000;">
A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, “The curious case of
neural text degeneration,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib70.8.2" style="color:#000000;">arXiv preprint arXiv:1904.09751</em><span class="ltx_text" id="bib.bib70.9.3" style="color:#000000;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib71.5.5.1" style="color:#000000;">Meister et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.7.1" style="color:#000000;">
C. Meister, T. Pimentel, G. Wiher, and R. Cotterell, “Locally typical
sampling,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib71.8.2" style="color:#000000;">Transactions of the Association for Computational
Linguistics</em><span class="ltx_text" id="bib.bib71.9.3" style="color:#000000;">, vol. 11, pp. 102–121, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib72.5.5.1" style="color:#000000;">Mireshghallah et al. [2022a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.7.1" style="color:#000000;">
F. Mireshghallah, A. Uniyal, T. Wang, D. Evans, and T. Berg-Kirkpatrick, “An
empirical analysis of memorization in fine-tuned autoregressive language
models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib72.8.2" style="color:#000000;">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em><span class="ltx_text" id="bib.bib72.9.3" style="color:#000000;">, Y. Goldberg, Z. Kozareva, and Y. Zhang,
Eds.   Abu Dhabi, United Arab Emirates:
Association for Computational Linguistics, Dec. 2022, pp. 1816–1826.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib73.5.5.1" style="color:#000000;">Zeng et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.7.1" style="color:#000000;">
S. Zeng, Y. Li, J. Ren, Y. Liu, H. Xu, P. He, Y. Xing, J. Tang, and D. Yin,
“Exploring memorization in fine-tuned language models,” 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib74.5.5.1" style="color:#000000;">Raffel et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.7.1" style="color:#000000;">
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a
unified text-to-text transformer,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib74.8.2" style="color:#000000;">Journal of machine learning
research</em><span class="ltx_text" id="bib.bib74.9.3" style="color:#000000;">, vol. 21, no. 140, pp. 1–67, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib75.5.5.1" style="color:#000000;">Jagielski et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.7.1" style="color:#000000;">
M. Jagielski, O. Thakkar, F. Tramer, D. Ippolito, K. Lee, N. Carlini,
E. Wallace, S. Song, A. G. Thakurta, N. Papernot, and C. Zhang, “Measuring
forgetting of memorized training examples,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib75.8.2" style="color:#000000;">The Eleventh
International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib75.9.3" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib76.5.5.1" style="color:#000000;">De Lange et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.7.1" style="color:#000000;">
M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis,
G. Slabaugh, and T. Tuytelaars, “A continual learning survey: Defying
forgetting in classification tasks,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib76.8.2" style="color:#000000;">IEEE transactions on pattern
analysis and machine intelligence</em><span class="ltx_text" id="bib.bib76.9.3" style="color:#000000;">, vol. 44, no. 7, pp. 3366–3385, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib77.5.5.1" style="color:#000000;">Timm et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.7.1" style="color:#000000;">
I. J. Timm, S. Staab, M. Siebers, C. Schon, U. Schmid, K. Sauerwald, L. Reuter,
M. Ragni, C. Niederée, H. Maus </span><em class="ltx_emph ltx_font_italic" id="bib.bib77.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib77.9.3" style="color:#000000;">, “Intentional forgetting in
artificial intelligence systems: Perspectives and challenges,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib77.10.4" style="color:#000000;">KI
2018: Advances in Artificial Intelligence: 41st German Conference on AI,
Berlin, Germany, September 24–28, 2018, Proceedings 41</em><span class="ltx_text" id="bib.bib77.11.5" style="color:#000000;">.   Springer, 2018, pp. 357–365.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib78.4.4.1" style="color:#000000;">Beierle and Timm [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.6.1" style="color:#000000;">
C. Beierle and I. J. Timm, “Intentional forgetting: An emerging field in ai
and beyond,” pp. 5–8, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib79.5.5.1" style="color:#000000;">Nguyen et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.7.1" style="color:#000000;">
T. T. Nguyen, T. T. Huynh, P. L. Nguyen, A. W.-C. Liew, H. Yin, and Q. V. H.
Nguyen, “A survey of machine unlearning,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib79.8.2" style="color:#000000;">arXiv preprint
arXiv:2209.02299</em><span class="ltx_text" id="bib.bib79.9.3" style="color:#000000;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib80.5.5.1" style="color:#000000;">Blanco-Justicia et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.7.1" style="color:#000000;">
A. Blanco-Justicia, N. Jebreel, B. Manzanares, D. Sánchez,
J. Domingo-Ferrer, G. Collell, and K. E. Tan, “Digital forgetting in large
language models: A survey of unlearning methods,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib80.8.2" style="color:#000000;">arXiv preprint
arXiv:2404.02062</em><span class="ltx_text" id="bib.bib80.9.3" style="color:#000000;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib81.5.5.1" style="color:#000000;">Kirkpatrick et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.7.1" style="color:#000000;">
J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A.
Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska </span><em class="ltx_emph ltx_font_italic" id="bib.bib81.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib81.9.3" style="color:#000000;">,
“Overcoming catastrophic forgetting in neural networks,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib81.10.4" style="color:#000000;">Proceedings
of the national academy of sciences</em><span class="ltx_text" id="bib.bib81.11.5" style="color:#000000;">, vol. 114, no. 13, pp. 3521–3526, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib82.5.5.1" style="color:#000000;">Bourtoule et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.7.1" style="color:#000000;">
L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers,
B. Zhang, D. Lie, and N. Papernot, “Machine unlearning,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib82.8.2" style="color:#000000;">2021 IEEE
Symposium on Security and Privacy (SP)</em><span class="ltx_text" id="bib.bib82.9.3" style="color:#000000;">.   IEEE, 2021, pp. 141–159.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib83.4.4.1" style="color:#000000;">Chen and Yang [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.6.1" style="color:#000000;">
J. Chen and D. Yang, “Unlearn what you want to forget: Efficient unlearning
for LLMs,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib83.7.2" style="color:#000000;">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em><span class="ltx_text" id="bib.bib83.8.3" style="color:#000000;">, H. Bouamor, J. Pino, and K. Bali,
Eds.   Singapore: Association for
Computational Linguistics, Dec. 2023, pp. 12 041–12 052.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib84.5.5.1" style="color:#000000;">Pawelczyk et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.7.1" style="color:#000000;">
M. Pawelczyk, S. Neel, and H. Lakkaraju, “In-context unlearning: Language
models as few shot unlearners,” 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib85.5.5.1" style="color:#000000;">Si et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.7.1" style="color:#000000;">
N. Si, H. Zhang, H. Chang, W. Zhang, D. Qu, and W. Zhang, “Knowledge
unlearning for llms: Tasks, methods, and challenges,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib85.8.2" style="color:#000000;">ArXiv</em><span class="ltx_text" id="bib.bib85.9.3" style="color:#000000;">, vol.
abs/2311.15766, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib86.5.5.1" style="color:#000000;">Yao et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.7.1" style="color:#000000;">
Y. Yao, X. Xu, and Y. Liu, “Large language model unlearning,” 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib87.5.5.1" style="color:#000000;">Mireshghallah et al. [2022b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.7.1" style="color:#000000;">
F. Mireshghallah, K. Goyal, A. Uniyal, T. Berg-Kirkpatrick, and R. Shokri,
“Quantifying privacy risks of masked language models using membership
inference attacks,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib87.8.2" style="color:#000000;">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing</em><span class="ltx_text" id="bib.bib87.9.3" style="color:#000000;">, Y. Goldberg, Z. Kozareva, and
Y. Zhang, Eds.   Abu Dhabi, United Arab
Emirates: Association for Computational Linguistics, Dec. 2022, pp.
8332–8347.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib88.4.4.1" style="color:#000000;">Vakili and Dalianis [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.6.1" style="color:#000000;">
T. Vakili and H. Dalianis, “Are clinical bert models privacy preserving? the
difficulty of extracting patient-condition associations,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib88.7.2" style="color:#000000;">AAAI 2021
Fall Symposium on Human Partnership with Medical AI: Design,
Operationalization, and Ethics (AAAI-HUMAN 2021), Virtual Event, November
4-6, 2021</em><span class="ltx_text" id="bib.bib88.8.3" style="color:#000000;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib89.5.5.1" style="color:#000000;">Lehman et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.7.1" style="color:#000000;">
E. Lehman, S. Jain, K. Pichotta, Y. Goldberg, and B. Wallace, “Does BERT
pretrained on clinical notes reveal sensitive data?” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib89.8.2" style="color:#000000;">Proceedings of
the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies</em><span class="ltx_text" id="bib.bib89.9.3" style="color:#000000;">, K. Toutanova,
A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard,
R. Cotterell, T. Chakraborty, and Y. Zhou, Eds.   Online: Association for Computational Linguistics, Jun. 2021, pp.
946–959.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib90.5.5.1" style="color:#000000;">Jagannatha et al. [2021b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.7.1" style="color:#000000;">
A. N. Jagannatha, B. P. S. Rawat, and H. Yu, “Membership inference attack
susceptibility of clinical language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib90.8.2" style="color:#000000;">ArXiv</em><span class="ltx_text" id="bib.bib90.9.3" style="color:#000000;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib91.5.5.1" style="color:#000000;">Huang et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.7.1" style="color:#000000;">
Y. Huang, S. Gupta, Z. Zhong, K. Li, and D. Chen, “Privacy implications of
retrieval-based language models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib91.8.2" style="color:#000000;">Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text" id="bib.bib91.9.3" style="color:#000000;">, H. Bouamor,
J. Pino, and K. Bali, Eds.   Singapore:
Association for Computational Linguistics, Dec. 2023, pp. 14 887–14 902.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib92.5.5.1" style="color:#000000;">Khandelwal et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.7.1" style="color:#000000;">
U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis,
“Generalization through memorization: Nearest neighbor language models,” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib92.8.2" style="color:#000000;">International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib92.9.3" style="color:#000000;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib93.5.5.1" style="color:#000000;">Zeng et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.7.1" style="color:#000000;">
S. Zeng, J. Zhang, P. He, Y. Xing, Y. Liu, H. Xu, J. Ren, S. Wang, D. Yin,
Y. Chang, and J. Tang, “The good and the bad: Exploring privacy issues in
retrieval-augmented generation (rag),” 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib94.4.4.1" style="color:#000000;">Feldman [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.6.1" style="color:#000000;">
V. Feldman, “Does learning require memorization? a short tale about a long
tail,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib94.7.2" style="color:#000000;">Proceedings of the 52nd Annual ACM SIGACT Symposium on
Theory of Computing</em><span class="ltx_text" id="bib.bib94.8.3" style="color:#000000;">, ser. STOC 2020.   New York, NY, USA: Association for Computing Machinery, 2020, p. 954–959.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib95.5.5.1" style="color:#000000;">Black et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.7.1" style="color:#000000;">
S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman, “Gpt-neo: Large scale
autoregressive language modeling with mesh-tensorflow,” 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib96.3.3.1" style="color:#000000;">[96]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.5.1" style="color:#000000;">
[Online]. Available: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/features/copilot" style="color:#000000;" title="">https://github.com/features/copilot</a><span class="ltx_text" id="bib.bib96.6.2" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib97.5.5.1" style="color:#000000;">Abadi et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.7.1" style="color:#000000;">
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang, “Deep learning with differential privacy,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib97.8.2" style="color:#000000;">Proceedings
of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em><span class="ltx_text" id="bib.bib97.9.3" style="color:#000000;">,
ser. CCS’16.   ACM, Oct. 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib98.5.5.1" style="color:#000000;">Kerrigan et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.7.1" style="color:#000000;">
G. Kerrigan, D. Slack, and J. Tuyls, “Differentially private language models
benefit from public pre-training,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib98.8.2" style="color:#000000;">Proceedings of the Second
Workshop on Privacy in NLP</em><span class="ltx_text" id="bib.bib98.9.3" style="color:#000000;">, O. Feyisetan, S. Ghanavati, S. Malmasi, and
P. Thaine, Eds.   Online: Association
for Computational Linguistics, Nov. 2020, pp. 39–45.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib99.5.5.1" style="color:#000000;">Li et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib99.7.1" style="color:#000000;">
X. Li, F. Tramer, P. Liang, and T. Hashimoto, “Large language models can be
strong differentially private learners,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib99.8.2" style="color:#000000;">International Conference
on Learning Representations</em><span class="ltx_text" id="bib.bib99.9.3" style="color:#000000;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib100.5.5.1" style="color:#000000;">Shi et al. [2022a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib100.7.1" style="color:#000000;">
W. Shi, R. Shea, S. Chen, C. Zhang, R. Jia, and Z. Yu, “Just fine-tune twice:
Selective differential privacy for large language models,” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib100.8.2" style="color:#000000;">Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing</em><span class="ltx_text" id="bib.bib100.9.3" style="color:#000000;">, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds.   Abu Dhabi, United Arab Emirates: Association for
Computational Linguistics, Dec. 2022, pp. 6327–6340. [Online]. Available:
</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-main.425" style="color:#000000;" title="">https://aclanthology.org/2022.emnlp-main.425</a><span class="ltx_text" id="bib.bib100.10.4" style="color:#000000;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib101.5.5.1" style="color:#000000;">Shi et al. [2022b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib101.7.1" style="color:#000000;">
W. Shi, A. Cui, E. Li, R. Jia, and Z. Yu, “Selective differential privacy for
language modeling,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib101.8.2" style="color:#000000;">Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies</em><span class="ltx_text" id="bib.bib101.9.3" style="color:#000000;">, M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz,
Eds.   Seattle, United States:
Association for Computational Linguistics, Jul. 2022, pp. 2848–2859.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib102.5.5.1" style="color:#000000;">Xi et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib102.7.1" style="color:#000000;">
Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin,
E. Zhou </span><em class="ltx_emph ltx_font_italic" id="bib.bib102.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib102.9.3" style="color:#000000;">, “The rise and potential of large language model based
agents: A survey,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib102.10.4" style="color:#000000;">arXiv preprint arXiv:2309.07864</em><span class="ltx_text" id="bib.bib102.11.5" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib103.5.5.1" style="color:#000000;">Deng et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib103.7.1" style="color:#000000;">
Y. Deng, W. Lei, M. Huang, and T.-S. Chua, “Rethinking conversational agents
in the era of llms: Proactivity, non-collaborativity, and beyond,” in
</span><em class="ltx_emph ltx_font_italic" id="bib.bib103.8.2" style="color:#000000;">Proceedings of the Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval in the Asia Pacific
Region</em><span class="ltx_text" id="bib.bib103.9.3" style="color:#000000;">, 2023, pp. 298–301.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib104.5.5.1" style="color:#000000;">Lewis et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib104.7.1" style="color:#000000;">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,
H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel </span><em class="ltx_emph ltx_font_italic" id="bib.bib104.8.2" style="color:#000000;">et al.</em><span class="ltx_text" id="bib.bib104.9.3" style="color:#000000;">,
“Retrieval-augmented generation for knowledge-intensive nlp tasks,”
</span><em class="ltx_emph ltx_font_italic" id="bib.bib104.10.4" style="color:#000000;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib104.11.5" style="color:#000000;">, vol. 33, pp.
9459–9474, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib105.5.5.1" style="color:#000000;">Gao et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib105.7.1" style="color:#000000;">
Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang,
“Retrieval-augmented generation for large language models: A survey,”
</span><em class="ltx_emph ltx_font_italic" id="bib.bib105.8.2" style="color:#000000;">arXiv preprint arXiv:2312.10997</em><span class="ltx_text" id="bib.bib105.9.3" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib106.5.5.1" style="color:#000000;">Shuster et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib106.7.1" style="color:#000000;">
K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, “Retrieval augmentation
reduces hallucination in conversation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib106.8.2" style="color:#000000;">Findings of the Association
for Computational Linguistics: EMNLP 2021</em><span class="ltx_text" id="bib.bib106.9.3" style="color:#000000;">, 2021, pp. 3784–3803.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib107.5.5.1" style="color:#000000;">Qin et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib107.7.1" style="color:#000000;">
L. Qin, Q. Chen, Y. Zhou, Z. Chen, Y. Li, L. Liao, M. Li, W. Che, and P. S. Yu,
“Multilingual large language model: A survey of resources, taxonomy and
frontiers,” 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib108.5.5.1" style="color:#000000;">Xu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib108.7.1" style="color:#000000;">
Y. Xu, L. Hu, J. Zhao, Z. Qiu, Y. Ye, and H. Gu, “A survey on multilingual
large language models: Corpora, alignment, and bias,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib108.8.2" style="color:#000000;">arXiv preprint
arXiv:2404.00929</em><span class="ltx_text" id="bib.bib108.9.3" style="color:#000000;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib109.5.5.1" style="color:#000000;">Zou et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib109.7.1" style="color:#000000;">
H. Zou, Z. M. Kim, and D. Kang, “A survey of diffusion models in natural
language processing,” 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib110.5.5.1" style="color:#000000;">Carlini et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib110.7.1" style="color:#000000;">
N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramer, B. Balle,
D. Ippolito, and E. Wallace, “Extracting training data from diffusion
models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib110.8.2" style="color:#000000;">32nd USENIX Security Symposium (USENIX Security 23)</em><span class="ltx_text" id="bib.bib110.9.3" style="color:#000000;">,
2023, pp. 5253–5270.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib111.5.5.1" style="color:#000000;">Gu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib111.7.1" style="color:#000000;">
X. Gu, C. Du, T. Pang, C. Li, M. Lin, and Y. Wang, “On memorization in
diffusion models,” 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib112.5.5.1" style="color:#000000;">Vaswani et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib112.7.1" style="color:#000000;">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u.
Kaiser, and I. Polosukhin, “Attention is all you need,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib112.8.2" style="color:#000000;">Advances
in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib112.9.3" style="color:#000000;">, I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.,
vol. 30.   Curran Associates, Inc.,
2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct  3 16:31:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
