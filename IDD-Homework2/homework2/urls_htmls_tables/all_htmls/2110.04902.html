<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.04902] Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing</title><meta property="og:description" content="Synthetic data is a powerful tool in training data hungry deep learning algorithms. However, to date, camera-based physiological sensing has not taken full advantage of these techniques. In this work, we leverage a hig…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.04902">

<!--Generated on Tue Mar 19 15:18:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Synthetic Data for Multi-Parameter
<br class="ltx_break">Camera-Based Physiological Sensing
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel McDuff<sup id="id9.9.id1" class="ltx_sup"><span id="id9.9.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Xin Liu<sup id="id10.10.id2" class="ltx_sup"><span id="id10.10.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, Javier Hernandez<sup id="id11.11.id3" class="ltx_sup"><span id="id11.11.id3.1" class="ltx_text ltx_font_italic">1</span></sup>, Erroll Wood<sup id="id12.12.id4" class="ltx_sup"><span id="id12.12.id4.1" class="ltx_text ltx_font_italic">3</span></sup> and Tadas Baltrusaitis<sup id="id13.13.id5" class="ltx_sup"><span id="id13.13.id5.1" class="ltx_text ltx_font_italic">3</span></sup>
</span><span class="ltx_author_notes"><sup id="id14.14.id1" class="ltx_sup"><span id="id14.14.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Daniel McDuff and Javier Hernandez are with Microsoft Research, Redmond, WA, USA
<span id="id15.15.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{damcduff, javierh}@microsoft.com</span>.<sup id="id16.16.id1" class="ltx_sup"><span id="id16.16.id1.1" class="ltx_text ltx_font_italic">2</span></sup>Xin Liu is with the University of Washington, Seattle, WA, USA
<span id="id17.17.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">xliu0@cs.washington.edu</span>.<sup id="id18.18.id1" class="ltx_sup"><span id="id18.18.id1.1" class="ltx_text ltx_font_italic">3</span></sup>Erroll Wood and Tadas Baltrusaitis are with Microsoft, Cambridge, UK
<span id="id19.19.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{erwood, tabaltru}@microsoft.com</span>.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.id1" class="ltx_p">Synthetic data is a powerful tool in training data hungry deep learning algorithms. However, to date, camera-based physiological sensing has not taken full advantage of these techniques. In this work, we leverage a high-fidelity synthetics pipeline for generating videos of faces with faithful blood flow and breathing patterns. We present systematic experiments showing how physiologically-grounded synthetic data can be used in training camera-based multi-parameter cardiopulmonary sensing. We provide empirical evidence that heart and breathing rate measurement accuracy increases with the number of synthetic avatars in the training set. Furthermore, training with avatars with darker skin types leads to better overall performance than training with avatars with lighter skin types. Finally, we discuss the opportunities that synthetics present in the domain of camera-based physiological sensing and limitations that need to be overcome.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2110.04902/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="308" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Screenshots of the 1,000 avatars we created for our analyses. Larger examples are shown below to highlight the visual realism of the avatars and the diversity in appearance, lighting and surroundings.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The application of computer vision in non-contact physiological measurement is an area of growing interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The opportunity to create ubiquitous health sensors out of webcams and smartphones is attractive as it would lower the barrier to measurement and allow for comfortable, convenient longitudinal sensing. Cameras offer the opportunity for capturing rich contextual information, through a myriad of different computer vision models (e.g., scene understanding, action and gesture recognition). Context is often tricky to derive from contact-based sensor data (e.g., wearables) and is necessary for appropriately interpreting physiological measurements.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Significant improvements in camera-based measurement accuracy have been achieved using supervised neural models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Training computer vision algorithms to recover physiological signals from video in this way requires synchronized video recordings and gold-standard measurements. Such data is time consuming and expensive to collect. Furthermore, these recordings contain personally identifiable biometric data. In many domains, such as object recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, body pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and gaze estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, face and gesture recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and scene understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, synthetic data has become a key tool for training models. Synthetic data pipelines enable systematic data generation with precisely synchronized labels and parameterized control of the dataset properties. A recent paper presented the first example of synthetics applied to the problem of remote imaging photoplethysmography (iPPG) measurement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However, otherwise there remains much to study in this domain.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Once a synthetics pipeline has been created, it can be used to generate data in a highly scalable fashion, but how much does additional data impact the performance of camera-based physiological sensing systems? Will algorithms benefit from learning from hundreds or thousands of synthetic avatars?
An attractive property of synthetics pipelines is that the data distribution can be controlled. For example, making it easier to sample uniformly from appearance characteristics such as skin type and gender which are known to differently impact the performance of computer vision models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. But does controlling the distribution of synthetic data actually lead to an improvement in how the resulting algorithms perform and help create less biased models? And what other insights can we gain from using synthetic data?</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we present a set of systematic experiments using synthetic data in the training of non-contact vision-based physiological measurement algorithms. First, we examine how the number of synthetically generated avatars in a training set impacts the performance of pulse rate and breathing rate measurement on two benchmark video datasets. Next, we investigate how the skin type distribution of these avatars impacts performance. To achieve this we leverage a state-of-the-art synthetics pipeline for creating video sequences of avatars with physically-grounded simulations of blood flow and breathing. We synthesize 1,000 high-fidelity videos of avatars in this way, to our knowledge the largest such synthetics dataset that exists.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To summarize the contributions of this work are to: 1) present the first multi-parameter camera-based physiological measurement results using training data created via a synthetics pipeline, 2) show that generalization of both pulse and breathing measurements from video improves with increased numbers of synthetic avatars in the training set, and 3) reveal that training on faces with darker skin types appears to improve model generalization across most skin type categories, compared to training on faces with lighter skin types.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">BACKGROUND</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Vision-Based Physiological Measurement</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Video-based physiological measurement is an established and growing interdisciplinary field of research. Over the past two decades <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> increasingly sophisticated methods have driven significant reductions in error. The field has benefited by grounding these models via a principled approach to modeling the optics of the skin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. However, the best performing algorithms are by and large supervised neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. These algorithms are “data hungry” and can be brittle if only trained on videos that do not reflect the diversity of real-world conditions (appearance, lighting, motion, etc.). Indeed, human appearance and physiology do contain large individual variability making generalization challenging in this domain. Models trained on one dataset and tested on another lead to substantially poorer performance than a model tested on data withheld from the same set as the training data.
Specific examples of this include over-fitting to the video codec and/or rate factor of the videos in the training set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and to the skin type of the subjects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Synthetic Data in Computer Vision</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">There is a long history of the use of synthetic data in training and evaluating computer vision systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Synthetics have been employed extensively in models for face and body analysis specifically <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. But the same is not true for camera-based physiological sensing.
One attractive feature of synthetic data generation using parameteric models is the ability to control the distribution of samples. These data can then be used to help address problematic biases that exist in models. For example, Kortylewaski et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> show that the damage of real-world dataset biases on facial recognition systems can be partially addressed by pre-training on synthetic data.
In this work we built on this prior work and examine specifically what synthetic data can offer camera-based physiological measurement. In particular, we focused on two questions: How much does increasing the number of subjects with different facial appearances improve cross-dataset generalization performance? And how is performance on subjects with different skin types impacted by the distribution of skin types in the synthetic training set. To avoid conflating the impact of real videos with synthetic data we train <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">only</em> on synthetics. Previous work shows that we can expect some additional benefit by combining synthetic and real data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> but that is not our focus here.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2110.04902/assets/figures/pieline.png" id="S2.F2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="169" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A combined illustration of our multi-parameter data synthesis pipeline and multi-task temporal shift convolutional attention network for camera-based physiological measurement. In this work the models were trained entirely on synthetic data and tested on real videos.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">SYNTHETIC DATASET</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Generating our synthetic dataset involved creating facial avatars that had simulations of facial blood flow and breathing motions. This section provides more details about the synthetics pipeline.
We synthesized a large corpora of avatars with unique combinations of facial appearance, head motion and expression, and environment (including ambient lighting configuration). These dimensions were selected based on some of the most significant generalization challenges that existing non-contact physiological models face (e.g., generalizing to different appearance, motion and illumination conditions).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Base Avatars</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Facial Appearance.</span> Our first goal was to assess how increasing the number of avatars impacted generalization performance. Therefore, we created 1,000 unique appearances which required randomly picking a skin material with a particular albedo texture. For approximately half of the appearances, we included some form of facial hair (beard and/or moustache).
In addition, we modified the skin color properties of the different faces. Fig. <a href="#S0.F1" title="Figure 1 ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows some examples.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Facial Motion</span>. We introduced rigid head motions by rotating the head about the vertical axis at angular velocities of 0, 10, 20, and 30 degrees/second. In addition, we synthesized videos with smiling, blinking, and mouth opening by using a collection of artist-created blend shapes.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Environment.</span> We randomly picked a static background and illumination on the face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> from a large existing collection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Fig. <a href="#S0.F1" title="Figure 1 ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows some examples.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Physiological Changes</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To simulate the facial blood flow, we leveraged photoplethysmographic signals from PhysioNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. In particular, we used the BIDMC PPG and Respiration Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> which includes 53 8-minute contact PPG and respiration recordings from the original MIMIC-II dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. We then added each of the signals to the avatars by modifying two main properties of a physically-based shading material<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.blender.org/</span></span></span>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Subsurface Skin Color:</span> We simulated skin tone changes by globally varying subsurface color across all skin pixels on the albedo map which is a texture map transferred from a high-quality 3D face scan.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Subsurface Scattering:</span> We manipulated the subsurface radius for the channels to capture the changes in subsurface scattering as the blood volume varies. In particular, we used an artist-created spatially-weighted scattering radius texture (see Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Synthetic Data in Computer Vision ‣ II BACKGROUND ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) which captures variations in the thickness of the skin across the face.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Breathing Motion:</span> We controlled both the torso and head motions to simulate motions of the body due to breathing. Specifically, the pitch of the chest was rotated subtly using the breathing input signal. The rotation of the head was dampened slightly to create greater variance in the appearance changes and so that the breathing motions were most dominant in the chest and shoulders.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">TESTING DATASETS</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We performed all our testing on videos of real people with gold-standard measurements for reference.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">AFRL</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>: Videos were recorded at 658x492 pixel resolution and 120 frames per second (fps). Twenty-five participants (17 males) were recruited to participate in the study. Gold-standard PPG measurements were captured from a fingertip sensor and breathing measurements were captured from a chest strap. These were recorded using a research-grade biopotential acquisition unit. Each participant was recorded six times for 5-minutes each. The angular velocity of the head motion was increased in each task and this process was repeated twice in front of two background screens.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">MMSE-HR</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>:
102 videos of 40 participants were recorded at 25 fps capturing 1040x1392 resolution images during spontaneous emotion elicitation experiments. The ground truth contact signal was measured via a Biopac2 MP150 system<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.biopac.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.biopac.com/</a></span></span></span> which provided pulse rate at 1000 fps and was updated after each heartbeat. These videos feature smaller but more spontaneous motions than those in the AFRL dataset. Gold-standard breathing measurements are not included in the MMSE-HR dataset.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2110.04902/assets/x2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="322" height="308" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Training loss over 12 epochs for models trained on [25, 50, 100, 200, 300, 400. 600. 800, 1000] avatars.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2110.04902/assets/x3.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="269" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Pulse rate error (beats/min) for models trained with [25, 50, 100, 200, 300, 400. 600. 800, 1000] randomly sampled avatars. Standard error bars shown for MAE and SNR plots.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2110.04902/assets/x4.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="142" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Breathing results for models trained with [25, 50, 100, 200, 300, 400. 600. 800, 1000] randomly sampled avatars. Standard error bars shown for MAE and SNR plots.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2110.04902/assets/x5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="380" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Pulse rate error (beats/min) for models trained with light skin type (blue) and dark skin type (red) avatars. Results are shown by Fitzpatrick skin type (II, III, IV, V) for subjects in the MMSE-HR dataset. </figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Model Training and Testing</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Our goal here was not to propose a new inference model, therefore we used an existing state-of-the-art neural architecture for evaluating how synthetic data can improve camera-based physiological sensing. This model, multi-task temporal shift convolutional attention network (MTTS-CAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, captures rich spatial and temporal relationships in the data and is therefore a good candidate for investigating the impact of the synthetic data. MTTS-CAN has a two-branch network, illustrated in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Synthetic Data in Computer Vision ‣ II BACKGROUND ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, comprising of a motion branch and an appearance branch. The motion branch efficiently models spatial-temporal features by shifting the frames along the temporal axis. The appearance branch provides an attention mechanism to help guide the motion representation to focus on spatial regions of interests (e.g., skin) containing physiological signals instead of others (e.g., hair).
The loss function during training was the average mean squared error of pulse and breathing waveform predictions compared to the ground truth waveforms. We trained using the first-derivative of the waveforms as prior work has shown that this is effective for predicting both PPG and breathing signals from video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Using our synthetics pipeline, we generated videos of 1,000 avatars with blood flow (PPG) and breathing signals. Fig. <a href="#S0.F1" title="Figure 1 ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows a frame from each of the avatar videos. The videos were six seconds in length and were created at a frame-rate of 30Hz, providing 180 frames per avatar and a total of 180,000 frames. This is still a relatively small number of frames compared to many video datasets, but arguably has much greater variability in terms of appearance. Synthesizing face videos is still relatively computationally expensive and time consuming. Creating these high fidelity avatar videos took approximately one month using three computers each with an Nvidia M40 GPU.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">For each model in our experiment, we trained the network for a fixed number (12) epochs. Fig. <a href="#S4.F3" title="Figure 3 ‣ IV TESTING DATASETS ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that although we vary the number of avatars in our experiments, the training loss consistently converges by this point.
To evaluate the trained models we tested on the AFRL and MMSE datasets. For the AFRL data we divide each video into 5 60-second non-overlapping windows. For the MMSE dataset we use the full video for each prediction. We use three commonly employed evaluation metrics for pulse and breathing: mean absolute error between the predicted rate and the ground truth, signal-to-noise ratio (SNR), and correlation between predicted rate and the ground truth.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">SYNTHETICS IN TRAINING</span>
</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.5.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.6.2" class="ltx_text ltx_font_italic">Number of Subjects in the Training Set</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">First, we tested models in which we trained networks with data from [25, 50, 100, 200, 300, 400, 600, 800, 1000] avatars. We randomly sample from the pool of avatars to create the subsets for training. Fig. <a href="#S4.F4" title="Figure 4 ‣ IV TESTING DATASETS ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the mean absolute pulse rate error, SNR and correlation for the AFRL and MMSE datasets (error bars show standard error). Fig. <a href="#S4.F5" title="Figure 5 ‣ IV TESTING DATASETS ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the mean absolute breathing rate error, SNR and correlation for the AFRL dataset (the MMSE dataset does not include breathing ground truth data). In both cases (pulse rate and breathing rate estimation) the testing mean absolute error dropped significantly for networks trained with 600 avatars compared to 25 avatars.
For pulse rate measurement we saw diminishing returns with more than 600 avatars and with breathing rate measurement we saw diminishing returns with more than 200 avatars. These numbers probably reflect the limits of the variance in samples that can be created with out current simulation. After generating 600 unique avatars any additional avatars we generated were likely to have some similarities with one or more of the avatars in the set, thus these additional training samples offered little additional new information. We plan to develop our synthetics pipeline further to address this bottleneck.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.5.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.6.2" class="ltx_text ltx_font_italic">Sim-to-Real Gap</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">From the results in Figs. <a href="#S4.F4" title="Figure 4 ‣ IV TESTING DATASETS ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S4.F5" title="Figure 5 ‣ IV TESTING DATASETS ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> it was apparent that even when training with 1000 avatars the heart rate and breathing rate measurement performance was still not comparable with the state-of-the-art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. We hypothesize that this is due to the “sim-to-real” gap. Training only on synthetic data has limitations because there remains a domain gap between the appearance of synthetic avatars and real videos. Combining our synthetic data with real videos will likely lead to improvements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> as would other methods for addressing “sim-to-real” generalization. Our goal here was to primarily to examine the impact of synthetic dataset properties and therefore, we leave these steps for future work.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.5.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.6.2" class="ltx_text ltx_font_italic">Diversity of Subjects in the Training Set</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Our synthetic data allowed us to examine empirically how the distribution of appearance of the avatars impacts generalization performance. Perhaps the most obvious appearance characteristic when measuring the blood volume pulse optically is skin type. Previous work has found systematic biases in the performance of non-contact photoplethymography measurement algorithms with skin type <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Specifically, performance is over signficantly poorer for darker skin types.
The albedo textures were sorted by skin tone. We then trained two models one with “lighter” skin tone avatars and another with “darker” skin tone avatars. Fig. <a href="#S4.F6" title="Figure 6 ‣ IV TESTING DATASETS ‣ Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the pulse rate mean absolute error (beats/min) for the two models on the MMSE-HR dataset by Fitzpatrick skin type <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. We expected to observe that the model trained on light skin type avatars would perform best on light skin type participants (groups II and III) and the model trained on darker skin type avatars would perform best on dark skin type participants (groups IV and V). However, we observed that the model trained on dark skin type avatars performed as well as, or better than the other model on all categories. This suggests that training on data with darker skin types leads to a more robust model, perhaps because the task is harder - forcing the model to learn better representations or a more robust attention mechanism. This is an interesting observation that warrants further investigation.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Our results highlight that synthetic data can be used to train multi-parameter camera-based physiological measurement algorithms. Our experiments have shown that increasing the diversity of appearance of avatars can positively impact generalization performance on real videos for both pulse and breathing measurement. A rich synthetics pipeline presents the possibility to exploit this further, by generating data with more varied facial expressions, body motions, and illumination conditions. In addition to leveraging these at training time, they could also be used for systematic testing.
However, it is clear that there is a gap between the performance that can be obtained when training only on our current simulation data.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS1.5.1.1" class="ltx_text">VII-A</span> </span><span id="S7.SS1.6.2" class="ltx_text ltx_font_italic">Opportunities for Synthetics</span>
</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Synthetics open up a number of promising directions for camera-based physiological measurement. As shown in this work, the controlled generation of different facial attributes and potential variations (e.g., motion, environment) allowed us to systematically study the effect of different factors and create more generalizable models. In addition, this methodology allowed us to gain more understanding about the impact of each of the variations in a standardized setting which could be used for prioritizing research questions such as “what is the source of variation that most heavily impacts the algorithm?.” The generation of synthetic data enabled us to quickly increase the volume of data. This is likely be even more advantageous for training even more data hungry models such as transformers. Finally, the proposed methodology offers the potential to improve other relevant physiological domains. For instance, this work considered data from healthy individuals to generate the physiological signals but future work may consider leveraging datasets with cardio-respiratory abnormalities that may be difficult to replicate during data collection from real people, especially in the context of camera-based physiological sensing.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS2.5.1.1" class="ltx_text">VII-B</span> </span><span id="S7.SS2.6.2" class="ltx_text ltx_font_italic">Limitations of Synthetics</span>
</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Despite the many benefits, it is important to note that the proposed approach has some limitations too. One of the biggest challenges is that the creation of hyper-realistic avatars like the ones considered in this work requires a large overhead both in terms of time and expense. Once the synthetics pipeline is created, generating the avatars themselves is computationally intensive. However, we expect these costs to be reduced in the future. Another limitation is that using synthetic data can only get us so far in terms of performance. Even though we leveraged state-of-the-art avatar generation, there is still some domain gap between real people and synthesized ones. Future efforts may leverage advances in generative networks like StyleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> to further increase their realism and bridge the “sim-to-real” gap.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We have shown, via empirical evidence that increasing the number of unique avatars in a synthetic dataset can lead to reductions in physiological parameter estimation. We hope that this evidence inspires more research using synthetic data for training camera-based physiological sensing algorithms and that in turn we are able to realize more of the potential for this technology.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D. McDuff, J. R. Estepp, A. M. Piasecki, and E. B. Blackford, “A survey of
remote optical photoplethysmographic imaging methods,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2015 37th
Annual International Conference of the IEEE Engineering in Medicine and
Biology Society</em>.   IEEE, 2015, pp.
6398–6404.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
W. Chen and D. McDuff, “Deepphys: Video-based physiological measurement using
convolutional attention networks,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.07888</em>,
2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
X. Liu, J. Fromm, S. Patel, and D. McDuff, “Multi-task temporal shift
attention networks for on-device contactless vitals measurement,”
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing Systems (NeurIPS)</em>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
randomization for transferring deep neural networks from simulation to the
real world,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2017 IEEE/RSJ international conference on intelligent
robots and systems (IROS)</em>.   IEEE,
2017, pp. 23–30.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore,
A. Kipman, and A. Blake, “Real-time human pose recognition in parts from
single depth images,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CVPR 2011</em>.   Ieee, 2011, pp. 1297–1304.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb,
“Learning from simulated and unsupervised images through adversarial
training,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision
and pattern recognition</em>, 2017, pp. 2107–2116.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Kortylewski, B. Egger, A. Schneider, T. Gerig, A. Morel-Forster, and
T. Vetter, “Analyzing and reducing the damage of dataset bias to face
recognition with synthetic data,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition Workshops</em>, 2019, pp.
0–0.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, and R. Cipolla,
“Understanding real world indoor scenes with synthetic data,” in
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2016, pp. 4077–4085.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D. McDuff, J. Hernandez, E. Wood, X. Liu, and T. Baltrusaitis, “Advancing
non-contact vital sign measurement using synthetic avatars,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2010.12949</em>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Buolamwini and T. Gebru, “Gender shades: Intersectional accuracy
disparities in commercial gender classification,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Conference on
fairness, accountability and transparency</em>.   PMLR, 2018, pp. 77–91.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T. Wu, V. Blazek, and H. J. Schmitt, “Photoplethysmography imaging: a new
noninvasive and noncontact method for mapping of the dermal perfusion
changes,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">EOS/SPIE European Biomedical Optics Week</em>.   International Society for Optics and Photonics, 2000,
pp. 62–70.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
V. Blazek, T. Wu, and D. Hoelscher, “Near-infrared ccd imaging: Possibilities
for noninvasive and contactless 2d mapping of dermal venous hemodynamics,”
in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Optical Diagnostics of Biological Fluids V</em>, vol. 3923.   International Society for Optics and Photonics,
2000, pp. 2–9.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. Takano and Y. Ohta, “Heart rate measurement based on a time-lapse image,”
<em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Medical engineering &amp; physics</em>, vol. 29, no. 8, pp. 853–857, 2007.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
W. Verkruysse, L. O. Svaasand, and J. S. Nelson, “Remote plethysmographic
imaging using ambient light,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Optics express</em>, vol. 16, no. 26, pp.
21 434–21 445, 2008.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M.-Z. Poh, D. McDuff, and R. W. Picard, “Non-contact, automated cardiac pulse
measurements using video imaging and blind source separation,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Optics
Express</em>, vol. 18, no. 10, pp. 10 762–10 774, 2010.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
G. de Haan and V. Jeanne, “Robust pulse rate from chrominance-based rppg,”
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Biomedical Engineering</em>, vol. 60, no. 10, pp.
2878–2886, 2013.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W. Wang, S. Stuijk, and G. de Haan, “Exploiting spatial redundancy of image
sensor for motion robust rppg.” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Biomedical
Engineering</em>, vol. 62, no. 2, pp. 415–425, 2015.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
X. Liu, Z. Jiang, J. Fromm, X. Xu, S. Patel, and D. McDuff, “Metaphys:
few-shot adaptation for non-contact physiological measurement,” in
<em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Health, Inference, and Learning</em>,
2021, pp. 154–163.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
W. Wang, A. Den Brinker, S. Stuijk, and G. De Haan, “Algorithmic
Principles of Remote-PPG,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Biomedical
Engineering</em>, vol. PP, no. 99, pp. 1–12, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Q. Zhan, W. Wang, and G. de Haan, “Analysis of cnn-based remote-ppg to
understand limitations and sensitivities,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Biomedical optics express</em>,
vol. 11, no. 3, pp. 1268–1283, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
E. M. Nowara, D. McDuff, and A. Veeraraghavan, “Systematic analysis of
video-based pulse measurement from compressed videos,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Biomedical
Optics Express</em>, vol. 12, no. 1, pp. 494–508, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
——, “A meta-analysis of the impact of skin tone and gender on non-contact
photoplethysmography measurements,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition Workshops</em>, 2020, pp.
284–285.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
V. Veeravasarapu, R. N. Hota, C. Rothkopf, and R. Visvanathan, “Model
validation for vision systems via graphics simulation,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1512.01401</em>, 2015.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
——, “Simulations for validation of vision systems,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1512.01030</em>, 2015.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
V. Veeravasarapu, C. Rothkopf, and V. Ramesh, “Model-driven simulations for
deep convolutional neural networks,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1605.09582</em>,
2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
D. Vazquez, A. M. Lopez, J. Marin, D. Ponsa, and D. Geronimo, “Virtual and
real world adaptation for pedestrian detection,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on
pattern analysis and machine intelligence</em>, vol. 36, no. 4, pp. 797–809,
2014.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
W. Qiu and A. Yuille, “Unrealcv: Connecting computer vision to unreal
engine,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2016, pp. 909–916.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D. McDuff, R. Cheng, and A. Kapoor, “Identifying bias in ai using
simulation,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.00471</em>, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
D. McDuff, S. Ma, Y. Song, and A. Kapoor, “Characterizing bias in classifiers
using generative models,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>, vol. 32, pp. 5403–5414, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
R. M. Haralick, “Performance characterization in computer vision,” in
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">BMVC92</em>.   Springer, 1992, pp.
1–8.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Kortylewski, B. Egger, A. Schneider, T. Gerig, A. Morel-Forster, and
T. Vetter, “Empirically analyzing the effect of dataset biases on deep face
recognition systems,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops</em>, 2018, pp. 2093–2102.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T. Baltrusaitis, E. Wood, V. Estellers, C. Hewitt, S. Dziadzio, M. Kowalski,
M. Johnson, T. J. Cashman, and J. Shotton, “A high fidelity synthetic face
framework for computer vision,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.08364</em>,
2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
P. Debevec, “Image-based lighting,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ACM SIGGRAPH 2006 Courses</em>,
2006, pp. 4–es.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
G. Zaal, “HDRI haven,” 2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov, R. G.
Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley, “Physiobank,
physiotoolkit, and physionet: components of a new research resource for
complex physiologic signals,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">circulation</em>, vol. 101, no. 23, pp.
e215–e220, 2000.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
M. A. Pimentel, A. E. Johnson, P. H. Charlton, D. Birrenkott, P. J. Watkinson,
L. Tarassenko, and D. A. Clifton, “Toward a robust estimation of respiratory
rate from pulse oximeters,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Biomedical
Engineering</em>, vol. 64, no. 8, pp. 1914–1923, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
M. Saeed, M. Villarroel, A. T. Reisner, G. Clifford, L.-W. Lehman, G. Moody,
T. Heldt, T. H. Kyaw, B. Moody, and R. G. Mark, “Multiparameter intelligent
monitoring in intensive care ii (mimic-ii): a public-access intensive care
unit database,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Critical care medicine</em>, vol. 39, no. 5, p. 952, 2011.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. R. Estepp, E. B. Blackford, and C. M. Meier, “Recovering pulse rate during
motion artifact with a multi-imager array for non-contact imaging
photoplethysmography,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Systems, Man and Cybernetics (SMC), 2014
IEEE International Conference on</em>.   IEEE, 2014, pp. 1462–1469.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Z. Zhang, J. M. Girard, Y. Wu, X. Zhang, P. Liu, U. Ciftci, S. Canavan,
M. Reale, A. Horowitz, H. Yang <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Multimodal spontaneous
emotion corpus for human behavior analysis,” in <em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition</em>, 2016, pp.
3438–3446.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
P. S. Addison, D. Jacquel, D. M. Foo, and U. R. Borg, “Video-based heart rate
monitoring across a range of skin pigmentations during an acute hypoxic
challenge,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Journal of clinical monitoring and computing</em>, vol. 32,
no. 5, pp. 871–880, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
T. B. Fitzpatrick, “The validity and practicality of sun-reactive skin types i
through vi,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Archives of dermatology</em>, vol. 124, no. 6, pp. 869–871,
1988.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for
generative adversarial networks,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Computer
Society Conference on Computer Vision and Pattern Recognition</em>, vol.
2019-June.   IEEE Computer Society, jun
2019, pp. 4396–4405.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.04901" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.04902" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2110.04902">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.04902" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.04903" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 15:18:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
