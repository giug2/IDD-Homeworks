<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2105.00831] Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains.</title><meta property="og:description" content="Large scale contextual representation models have significantly advanced NLP in recent years, understanding the semantics of text to a degree never seen before. However, they need to process large amounts of data to ac‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2105.00831">

<!--Generated on Wed Mar 13 10:52:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning
<span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">thanks: </span>¬†¬†This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Garcia Bernal<span id="footnote1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>¬†¬†¬†Lodovico Giaretta<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span>¬†¬†¬† ≈†ar≈´nas Girdzijauskas<span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span>¬†¬†¬†Magnus Sahlgren<span id="footnotex3" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">3</span></span></span></span>
<br class="ltx_break"><span id="footnotex4" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span>¬†¬†KTH Royal Institute of Technology 
<br class="ltx_break"><span id="footnotex5" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">3</span></span></span></span>¬†¬†RISE Research Institutes of Sweden 
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">{danigb,lodovico,sarunasg}@kth.se ¬†¬†¬†¬†¬†¬†¬†¬†¬† magnus.sahlgren@ri.se</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Large scale contextual representation models have significantly advanced NLP in recent years, understanding the semantics of text to a degree never seen before. However, they need to process large amounts of data to achieve high-quality results. Joining and accessing all these data from multiple sources can be extremely challenging due to privacy and regulatory reasons. Federated Learning can solve these limitations by training models in a distributed fashion, taking advantage of the hardware of the devices that generate the data. We show the viability of training NLP models, specifically Word2Vec, with the Federated Learning protocol. In particular, we focus on a scenario in which a small number of organizations each hold a relatively large corpus. The results show that neither the quality of the results nor the convergence time in Federated Word2Vec deteriorates as compared to centralised Word2Vec.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A central task in NLP is the generation of word embeddings to encode the meaning of words and their relationships in a vector space. This task is usually performed by a self-supervised Machine Learning (ML) model such as Word2Vec¬†<cite class="ltx_cite ltx_citemacro_cite">Mikolov et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2013</a>)</cite>, ELMo¬†<cite class="ltx_cite ltx_citemacro_cite">Peters et¬†al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> or BERT¬†<cite class="ltx_cite ltx_citemacro_cite">Devlin et¬†al. (<a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>, with access to a large corpus of documents as input. These representations can then be used to perform advanced analytics on textual data. The larger and more complete the corpus is, the more accurate the representations will be.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As such, it can be useful for multiple organizations to collaborate with each other, each providing access to their corpora, in order to obtain the best results. However, different organizations typically cannot easily share their data, as they have to protect the privacy of their users and the details of their internal operations, or might be bound by external laws preventing the sharing of the data. One way organizations could overcome these issues is by employing Federated Learning protocol¬†<cite class="ltx_cite ltx_citemacro_cite">McMahan et¬†al. (<a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite> to generate a global model without sharing any data. It is therefore fundamental to assess the performance, quality and privacy preservation characteristics of such approach.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Distributed vs data-private massively-distributed approaches</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Datacenter-scale ML algorithms work with vast amounts of data allowing the training of large models, exploiting multiple machines and multiple GPUs per machine. Currently, GPUs offer enough power to satisfy the needs of state of the art models. However, this exposes another important aspect for consideration - Data Privacy. In recent years, users and governments have started to be aware of this issue, publishing new and stricter regulations, such as the GDPR¬†<cite class="ltx_cite ltx_citemacro_cite">European Commission (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>. Companies started making efforts to shield themselves from any security leak that could happen in a centralised system. This created a need to move the research towards distributed architectures where the data is not gathered by a central entity.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">The fast development of smart devices, the growth of their computational power and of fast Internet connections, like 4G and 5G, enable new approaches that exploit them to train distributed models. This solution is currently not at the same scale of resources that a datacenter can offer yet, but the research and development of edge devices is making it feasible.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">For these reasons, researchers are exploring the possibilities of different massively-distributed training designs. These new designs should offer scalability, ensure data privacy and reduce large traffic of data over the network. The main massively-distributed approach to large-scale training that fulfills all these requirements is Federated Learning <cite class="ltx_cite ltx_citemacro_cite">McMahan et¬†al. (<a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Federated Learning in a small collaborative NLP scenario</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Federated learning is frequently applied in a commercial, global-scale setting as an on-device learning framework. In this common scenario, many small devices, like smartphones, collaborate and grant access to their data, typically small due to storage limitations, to train a higher-quality model. These models are usually NLP applications to make suggestions <cite class="ltx_cite ltx_citemacro_cite">Yang et¬†al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> or predict a user behaviour <cite class="ltx_cite ltx_citemacro_cite">Hard et¬†al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">However, Federated Learning could be applied in different contexts. This paper shifts the focus to address a new scenario where the users are a small number of large organizations with access to large corpora, which cannot be shared or centralised. These organizations are willing to cooperate in order to have access to a larger corpus with diverse topics and to overcome the very strict data privacy policies. A practical example could be a group of government agencies, each of which has only access to sensitive documents in a specific domain (e.g. taxes), which alone would not be sufficient for high-quality training.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Federated Word2Vec</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Federated Learning addresses the privacy concerns as the data is not shared between the organizations. It stays in the node of the owner and the information transferred through the network is only the gradient of the model. It also avoids the expensive transfer of the training data, replacing it with the repeated transfer of gradients, the total size of which is, generally, less than that of the dataset. And even if repeated gradient exchanges were to surpass the size of the dataset, their transfer would be spread on a long time and divided in smaller batches, so it would not delay the training as much as having to send a huge dataset before starting. The only common point that all the nodes share in this architecture is the existence of a central node which oversees the training process, directing the data transfers and merging the contributions of each node. Having the central node can facilitate the inclusion of additional safety measures to shield the training process from malicious attacks <cite class="ltx_cite ltx_citemacro_cite">Bhagoji et¬†al. (<a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.5" class="ltx_p">In Federated Word2Vec, each organization owns a private dataset, which means that words that appear in the corpus of one organization may not be present in the one of another. This an issue as the input vocabulary must be common to all local models, so that the gradients can be aggregated. Preserving the privacy of the content of the text is very important, so this paper will overcome the aforementioned issue through a strategy that consists of a common agreement of all the participants in a global vocabulary. All must agree on a fixed vocabulary size <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">N</annotation></semantics></math> and a minimum threshold of occurrences <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">T</annotation></semantics></math>. Each participant must provide a list of their top <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p2.3.m3.1a"><mi id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">N</annotation></semantics></math> words that appear in their respective texts surpassing <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.p2.4.m4.1a"><mi id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><ci id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">T</annotation></semantics></math> occurrences. The privacy is preserved as the organizations only share a set of isolated, unsorted words. However, the question arises of how to merge these sets. There are two operations that can be applied to produce the final vocabulary: intersection and union. We use, and recommend to use, the union operation. The final vocabulary is larger than the initial size <math id="S2.p2.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p2.5.m5.1a"><mi id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><ci id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">N</annotation></semantics></math>, but all organizations keep their relevant words. Although this approach requires more time to converge due to many words appearing only in certain datasets, the words meaning and knowledge return to the participants is enriched.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Once the participants receive the common agreed vocabulary, the training process can start following the FederatedSGD algorithm from <cite class="ltx_cite ltx_citemacro_cite">McMahan et¬†al. (<a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite>. The gradient is transferred from all the external nodes to the main node in each iteration. The average gradient is calculated and transferred back to perform the updating process.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data collection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We generated topic-related datasets collected from Wikipedia articles and organised in different sizes. Two Wikipedia dumps were downloaded to satisfy the aforementioned datasets characteristics: a partial dump with a compressed size of 180 MB to simulate small organizations with short corpora; and a 16 GB compressed file with all the text content published in Wikipedia. From the whole Wikipedia dump, the Wikipedia Extractor script <cite class="ltx_cite ltx_citemacro_cite">Baugh and Flor (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite> is used with a tag to filter categories and prepare 5 different datasets divided by topic. The chosen topics are: biology, history, finance, geography, and sports. Although the themes are quite specific, some articles can appear in more than one dataset because of the distribution of the Wikipedia tree of categories. So, if an article is tagged with the biology category, it is included in the dataset of biological content. In order to simulate a larger number of organizations, every topic is split between two organizations.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Setup</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">The simulation is performed sequentially on a single machine and with a single GPU, a Nvidia Quadro RTX 5000. This limits the possibility to study the influence of the network, that is thus not covered in this work. The hyperparameters were set to sensible values based on existing literature and should provide a good compromise of training speed and quality. We use a <span id="S3.SS2.p1.3.1" class="ltx_text ltx_font_bold">fixed batch size</span> of 2,048 samples. It is important to notice that one iteration of Federated Word2Vec processes as much data as <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">N</annotation></semantics></math> iterations of centralized Word2Vec, because each of the <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">N</annotation></semantics></math> nodes processes one batch in parallel during each iteration, in our study <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">N</annotation></semantics></math> is equal to 10 nodes. The <span id="S3.SS2.p1.3.2" class="ltx_text ltx_font_bold">embeddings size</span> is fixed to 200. The number of <span id="S3.SS2.p1.3.3" class="ltx_text ltx_font_bold">negative samples</span> per batch is 64, a small amount of negative samples, compared to the total batch size, but sufficient to achieve good results <cite class="ltx_cite ltx_citemacro_cite">Mikolov et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2013</a>)</cite>. The <span id="S3.SS2.p1.3.4" class="ltx_text ltx_font_bold">vocabulary size</span> is 200,000 unique words, with a minimum threshold of 10 occurrences.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Proving convergence of the model with small datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Figure <a href="#S4.F1" title="Figure 1 ‚Ä£ 4.1 Proving convergence of the model with small datasets ‚Ä£ 4 Results ‚Ä£ Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> compares the validation loss of Federated Word2Vec with that of a baseline, centralized implementation. In order to compare the two models when both have processed the same amount of data, Federated Word2Vec is stopped at epoch 70, which is iteration 500,000.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2105.00831/assets/base_together_scale.png" id="S4.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="221" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>On the left, validation loss per epoch in a full execution of Word2Vec. On the right, validation loss per epoch of the first 500.000 iterations of Federated Word2Vec. The red lanes represent the average of the validation loss calculated by aggregating all previous values from each epoch. Y-axis is in logarithmic scale.</figcaption>
</figure>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2105.00831/assets/x1.jpg" id="S4.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="227" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>On the left, validation loss per iteration in a full execution of Word2Vec. On the right, validation loss per iterarions in a full execution of Federated Word2Vec. The red lanes represent the average of the validation loss calculated by aggregating all previous values from each epoch. Y-axis is in logarithmic scale.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The loss of Word2Vec presents a value of <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="{\sim}10^{4}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml"></mi><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">‚àº</mo><msup id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><mn id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml">10</mn><mn id="S4.SS1.p2.1.m1.1.1.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml">4</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">absent</csymbol><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2">10</cn><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">{\sim}10^{4}</annotation></semantics></math>. It is stable, but with a small descending trend. On the other hand, although Federated Word2Vec does not reach the same loss (it is 10 times greater) its trend is clearly decreasing. To check if the trend continues, Figure <a href="#S4.F2" title="Figure 2 ‚Ä£ 4.1 Proving convergence of the model with small datasets ‚Ä£ 4 Results ‚Ä£ Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the validation loss in terms of iteration for the full execution, 2 million iterations. The loss keeps going down until 1 million iterations when it stabilises. Overall, the two models provide very similar results. Centralized Word2Vec has a faster initial convergence; however, this might be overcome by adapting the hyperparameters to the distributed setting, for example with learning rate scaling <cite class="ltx_cite ltx_citemacro_cite">Goyal et¬†al. (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Proving convergence of the model with large datasets</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The training of Federated Word2Vec with a large dataset presents improved results compared to the previous graphs. Figure <a href="#S4.F3" title="Figure 3 ‚Ä£ 4.2 Proving convergence of the model with large datasets ‚Ä£ 4 Results ‚Ä£ Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> freezes the training in the iteration 500,000 as it was done in Figure <a href="#S4.F1" title="Figure 1 ‚Ä£ 4.1 Proving convergence of the model with small datasets ‚Ä£ 4 Results ‚Ä£ Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The number of epochs is fewer than in the former experiment but the loss presents a clear downward trend with a steeper slope. In Figure <a href="#S4.F4" title="Figure 4 ‚Ä£ 4.2 Proving convergence of the model with large datasets ‚Ä£ 4 Results ‚Ä£ Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, where the execution continues until iteration 2 millions, the loss keeps decreasing reaching values of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="10^{3}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msup id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">10</mn><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">10</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">10^{3}</annotation></semantics></math>, something that did not happen in Figure <a href="#S4.F2" title="Figure 2 ‚Ä£ 4.1 Proving convergence of the model with small datasets ‚Ä£ 4 Results ‚Ä£ Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2105.00831/assets/x2.jpg" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="181" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Validation loss of the first 500.000 iterations of Federated Word2Vec with a larger dataset, divided by epoch. The red lanes represent the average of the validation loss calculated by aggregating all previous values from each epoch. Y-axis is in logarithmic scale.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2105.00831/assets/x3.jpg" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="180" height="109" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Validation loss of a full execution of Federated Word2Vec with a larger dataset, represented in blue. The red lanes represent the average of the validation loss calculated by aggregating all previous values from each epoch. Y-axis is in logarithmic scale.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Consequently, Federated Word2Vec seems to work better with larger datasets as it benefits from learning from multiple sources at the same time. The results show that Federated Word2Vec is not better, and might perform slightly worse, than Word2Vec under the same settings. However, it is proven that Federated Word2Vec has a similar convergence pattern to Word2Vec and easily scales to a large dataset.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>How categorised data influence the results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We then compare collaborative training with Federated Word2Vec to local training by a single organization, which only has access to the <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">finance</span> dataset. We analyse the organization of the words in the embedding space, using their cosine distance, by identifying the top-5 closest neighbours for a number of target words, as shown in Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.3 How categorised data influence the results ‚Ä£ 4 Results ‚Ä£ Federated Word2Vec: Leveraging Federated Learning to Encourage Collaborative Representation Learning This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 813162. The content of this paper reflects the views only of their author (s). The European Commission/ Research Executive Agency are not responsible for any use that may be made of the information it contains." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The most striking finding in this analysis is that clusters are populated with more meaningful words in Federated Word2Vec. This behaviour was expected for the target word <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">bacteria</span>, as it does not appear frequently in the <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">finance</span> dataset. However, the same situation happens with <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_italic">market</span>, presenting meaningless words as the closest neighbours in its community, while the execution of Federated Word2Vec shows more specific context words.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Moreover, <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_italic">market</span> is not an outlier. Most words that are relevant to the <span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_italic">finance</span> dataset present similar results. The resultant neighbourhood of the word <span id="S4.SS3.p3.1.3" class="ltx_text ltx_font_italic">money</span> trained with baseline Word2Vec on the financial dataset still presents generic words such as {<span id="S4.SS3.p3.1.4" class="ltx_text ltx_font_italic">stated, said, there</span>}. In contrast, the community generated during the federated training clearly gathers meaning from the finance topic.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">These results show the importance of having a full picture of the language to produce high-quality embeddings, even for domain-specific tasks. This, in turn, underscores the need for collaboration among organizations.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Word</span></td>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">W2V</span></td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Fed W2V</span></td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<td id="S4.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">Market</span></td>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">Top-5</span></td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.2.2.3.1" class="ltx_text ltx_font_bold">Dist</span></td>
<td id="S4.T1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.2.2.4.1" class="ltx_text ltx_font_bold">Top-5</span></td>
<td id="S4.T1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.2.2.5.1" class="ltx_text ltx_font_bold">Dist</span></td>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<td id="S4.T1.1.3.3.1" class="ltx_td ltx_border_t"></td>
<td id="S4.T1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t">this</td>
<td id="S4.T1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_t">0.023</td>
<td id="S4.T1.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t">markets</td>
<td id="S4.T1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_t">0.029</td>
</tr>
<tr id="S4.T1.1.4.4" class="ltx_tr">
<td id="S4.T1.1.4.4.1" class="ltx_td"></td>
<td id="S4.T1.1.4.4.2" class="ltx_td ltx_align_left">proposed</td>
<td id="S4.T1.1.4.4.3" class="ltx_td ltx_align_left">0.024</td>
<td id="S4.T1.1.4.4.4" class="ltx_td ltx_align_left">company</td>
<td id="S4.T1.1.4.4.5" class="ltx_td ltx_align_left">0.035</td>
</tr>
<tr id="S4.T1.1.5.5" class="ltx_tr">
<td id="S4.T1.1.5.5.1" class="ltx_td"></td>
<td id="S4.T1.1.5.5.2" class="ltx_td ltx_align_left">some</td>
<td id="S4.T1.1.5.5.3" class="ltx_td ltx_align_left">0.024</td>
<td id="S4.T1.1.5.5.4" class="ltx_td ltx_align_left">share</td>
<td id="S4.T1.1.5.5.5" class="ltx_td ltx_align_left">0.042</td>
</tr>
<tr id="S4.T1.1.6.6" class="ltx_tr">
<td id="S4.T1.1.6.6.1" class="ltx_td"></td>
<td id="S4.T1.1.6.6.2" class="ltx_td ltx_align_left">all</td>
<td id="S4.T1.1.6.6.3" class="ltx_td ltx_align_left">0.025</td>
<td id="S4.T1.1.6.6.4" class="ltx_td ltx_align_left">trading</td>
<td id="S4.T1.1.6.6.5" class="ltx_td ltx_align_left">0.048</td>
</tr>
<tr id="S4.T1.1.7.7" class="ltx_tr">
<td id="S4.T1.1.7.7.1" class="ltx_td"></td>
<td id="S4.T1.1.7.7.2" class="ltx_td ltx_align_left">other</td>
<td id="S4.T1.1.7.7.3" class="ltx_td ltx_align_left">0.025</td>
<td id="S4.T1.1.7.7.4" class="ltx_td ltx_align_left">assets</td>
<td id="S4.T1.1.7.7.5" class="ltx_td ltx_align_left">0.049</td>
</tr>
<tr id="S4.T1.1.8.8" class="ltx_tr">
<td id="S4.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.8.8.1.1" class="ltx_text ltx_font_bold">Bacteria</span></td>
<td id="S4.T1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.8.8.2.1" class="ltx_text ltx_font_bold">Top-5</span></td>
<td id="S4.T1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.8.8.3.1" class="ltx_text ltx_font_bold">Dist</span></td>
<td id="S4.T1.1.8.8.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.8.8.4.1" class="ltx_text ltx_font_bold">Top-5</span></td>
<td id="S4.T1.1.8.8.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.8.8.5.1" class="ltx_text ltx_font_bold">Dist</span></td>
</tr>
<tr id="S4.T1.1.9.9" class="ltx_tr">
<td id="S4.T1.1.9.9.1" class="ltx_td ltx_border_t"></td>
<td id="S4.T1.1.9.9.2" class="ltx_td ltx_align_left ltx_border_t">rare</td>
<td id="S4.T1.1.9.9.3" class="ltx_td ltx_align_left ltx_border_t">0.026</td>
<td id="S4.T1.1.9.9.4" class="ltx_td ltx_align_left ltx_border_t">organism</td>
<td id="S4.T1.1.9.9.5" class="ltx_td ltx_align_left ltx_border_t">0.070</td>
</tr>
<tr id="S4.T1.1.10.10" class="ltx_tr">
<td id="S4.T1.1.10.10.1" class="ltx_td"></td>
<td id="S4.T1.1.10.10.2" class="ltx_td ltx_align_left">animals</td>
<td id="S4.T1.1.10.10.3" class="ltx_td ltx_align_left">0.026</td>
<td id="S4.T1.1.10.10.4" class="ltx_td ltx_align_left">toxic</td>
<td id="S4.T1.1.10.10.5" class="ltx_td ltx_align_left">0.075</td>
</tr>
<tr id="S4.T1.1.11.11" class="ltx_tr">
<td id="S4.T1.1.11.11.1" class="ltx_td"></td>
<td id="S4.T1.1.11.11.2" class="ltx_td ltx_align_left">applied</td>
<td id="S4.T1.1.11.11.3" class="ltx_td ltx_align_left">0.026</td>
<td id="S4.T1.1.11.11.4" class="ltx_td ltx_align_left">tissue</td>
<td id="S4.T1.1.11.11.5" class="ltx_td ltx_align_left">0.077</td>
</tr>
<tr id="S4.T1.1.12.12" class="ltx_tr">
<td id="S4.T1.1.12.12.1" class="ltx_td"></td>
<td id="S4.T1.1.12.12.2" class="ltx_td ltx_align_left">result</td>
<td id="S4.T1.1.12.12.3" class="ltx_td ltx_align_left">0.027</td>
<td id="S4.T1.1.12.12.4" class="ltx_td ltx_align_left">cells</td>
<td id="S4.T1.1.12.12.5" class="ltx_td ltx_align_left">0.081</td>
</tr>
<tr id="S4.T1.1.13.13" class="ltx_tr">
<td id="S4.T1.1.13.13.1" class="ltx_td"></td>
<td id="S4.T1.1.13.13.2" class="ltx_td ltx_align_left">plants</td>
<td id="S4.T1.1.13.13.3" class="ltx_td ltx_align_left">0.027</td>
<td id="S4.T1.1.13.13.4" class="ltx_td ltx_align_left">humans</td>
<td id="S4.T1.1.13.13.5" class="ltx_td ltx_align_left">0.083</td>
</tr>
<tr id="S4.T1.1.14.14" class="ltx_tr">
<td id="S4.T1.1.14.14.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.14.14.1.1" class="ltx_text ltx_font_bold">Money</span></td>
<td id="S4.T1.1.14.14.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.14.14.2.1" class="ltx_text ltx_font_bold">Top-5</span></td>
<td id="S4.T1.1.14.14.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.14.14.3.1" class="ltx_text ltx_font_bold">Dist</span></td>
<td id="S4.T1.1.14.14.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.14.14.4.1" class="ltx_text ltx_font_bold">Top-5</span></td>
<td id="S4.T1.1.14.14.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.1.14.14.5.1" class="ltx_text ltx_font_bold">Dist</span></td>
</tr>
<tr id="S4.T1.1.15.15" class="ltx_tr">
<td id="S4.T1.1.15.15.1" class="ltx_td ltx_border_t"></td>
<td id="S4.T1.1.15.15.2" class="ltx_td ltx_align_left ltx_border_t">stated</td>
<td id="S4.T1.1.15.15.3" class="ltx_td ltx_align_left ltx_border_t">0.028</td>
<td id="S4.T1.1.15.15.4" class="ltx_td ltx_align_left ltx_border_t">paid</td>
<td id="S4.T1.1.15.15.5" class="ltx_td ltx_align_left ltx_border_t">0.045</td>
</tr>
<tr id="S4.T1.1.16.16" class="ltx_tr">
<td id="S4.T1.1.16.16.1" class="ltx_td"></td>
<td id="S4.T1.1.16.16.2" class="ltx_td ltx_align_left">said</td>
<td id="S4.T1.1.16.16.3" class="ltx_td ltx_align_left">0.028</td>
<td id="S4.T1.1.16.16.4" class="ltx_td ltx_align_left">offer</td>
<td id="S4.T1.1.16.16.5" class="ltx_td ltx_align_left">0.053</td>
</tr>
<tr id="S4.T1.1.17.17" class="ltx_tr">
<td id="S4.T1.1.17.17.1" class="ltx_td"></td>
<td id="S4.T1.1.17.17.2" class="ltx_td ltx_align_left">there</td>
<td id="S4.T1.1.17.17.3" class="ltx_td ltx_align_left">0.028</td>
<td id="S4.T1.1.17.17.4" class="ltx_td ltx_align_left">sell</td>
<td id="S4.T1.1.17.17.5" class="ltx_td ltx_align_left">0.062</td>
</tr>
<tr id="S4.T1.1.18.18" class="ltx_tr">
<td id="S4.T1.1.18.18.1" class="ltx_td"></td>
<td id="S4.T1.1.18.18.2" class="ltx_td ltx_align_left">take</td>
<td id="S4.T1.1.18.18.3" class="ltx_td ltx_align_left">0.029</td>
<td id="S4.T1.1.18.18.4" class="ltx_td ltx_align_left">cash</td>
<td id="S4.T1.1.18.18.5" class="ltx_td ltx_align_left">0.071</td>
</tr>
<tr id="S4.T1.1.19.19" class="ltx_tr">
<td id="S4.T1.1.19.19.1" class="ltx_td"></td>
<td id="S4.T1.1.19.19.2" class="ltx_td ltx_align_left">help</td>
<td id="S4.T1.1.19.19.3" class="ltx_td ltx_align_left">0.031</td>
<td id="S4.T1.1.19.19.4" class="ltx_td ltx_align_left">interest</td>
<td id="S4.T1.1.19.19.5" class="ltx_td ltx_align_left">0.073</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> Top-5 nearest neighbours of each central word, using the cosine distance in the training of W2V with finance dataset and Fed W2V with all 5 datasets.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The purpose of this paper was to implement and test the viability of a distributed, efficient, data-private approach that allows a small number of organizations, each owning a large private text corpus, to train global word representations. The results indicate the potential for applicability to real scenarios of collaborative training. The main contributions of this work are

<span id="S5.I1" class="ltx_inline-enumerate">
<span id="S5.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">1.</span> <span id="S5.I1.i1.1" class="ltx_text">the viability of training NLP models like Word2Vec under the Federated Learning protocol with convergence times, at least, at the same level of the widely tested Word2Vec;
</span></span>
<span id="S5.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">2.</span> <span id="S5.I1.i2.1" class="ltx_text">the importance for organizations to cooperate, as cooperation provides models that are not only globally good, but also locally better than locally-trained models; and
</span></span>
<span id="S5.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">3.</span> <span id="S5.I1.i3.1" class="ltx_text">the quality of vector representations is not affected by the size of the corpora.
</span></span>
</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baugh and Flor (2015)</span>
<span class="ltx_bibblock">
Wesley Baugh and Patrick Flor. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://medialab.di.unipi.it/wiki/Wikipedia_Extractor" title="" class="ltx_ref ltx_href">Medialab: Wikipedia extractor</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhagoji et¬†al. (2019)</span>
<span class="ltx_bibblock">
Arjun¬†Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.
2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v97/bhagoji19a.html" title="" class="ltx_ref ltx_href">Analyzing
federated learning through an adversarial lens</a>.

</span>
<span class="ltx_bibblock">volume¬†97 of <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages
634‚Äì643, Long Beach, California, USA. PMLR.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et¬†al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1810.04805" title="" class="ltx_ref ltx_href">Bert: Pre-training of deep
bidirectional transformers for language understanding</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">European Commission (2018)</span>
<span class="ltx_bibblock">
European Commission. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ec.europa.eu/info/law/law-topic/data-protection/eu-data-protection-rules_en" title="" class="ltx_ref ltx_href">2018 reform of eu data protection rules</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et¬†al. (2018)</span>
<span class="ltx_bibblock">
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski,
Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1706.02677v2" title="" class="ltx_ref ltx_href">Accurate, large minibatch
sgd: Training imagenet in 1 hour</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et¬†al. (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Fran√ßoise
Beaufays, Sean Augenstein, Hubert Eichner, Chlo√© Kiddon, and Daniel Ramage.
2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1811.03604" title="" class="ltx_ref ltx_href">Federated learning for
mobile keyboard prediction</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et¬†al. (2016)</span>
<span class="ltx_bibblock">
H.¬†Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise¬†Ag√ºera y¬†Arcas. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1602.05629" title="" class="ltx_ref ltx_href">Communication-efficient
learning of deep networks from decentralized data</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et¬†al. (2013)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, G.s Corrado, and Jeffrey Dean. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5555/2999792.2999959" title="" class="ltx_ref ltx_href">Distributed
representations of words and phrases and their compositionality</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters et¬†al. (2018)</span>
<span class="ltx_bibblock">
Matthew¬†E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
Kenton Lee, and Luke Zettlemoyer. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1802.05365" title="" class="ltx_ref ltx_href">Deep contextualized word
representations</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et¬†al. (2018)</span>
<span class="ltx_bibblock">
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Fran√ßoise Beaufays. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1812.02903" title="" class="ltx_ref ltx_href">Applied federated learning:
Improving google keyboard query suggestions</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2105.00829" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2105.00831" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2105.00831">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2105.00831" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2105.00832" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 10:52:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
