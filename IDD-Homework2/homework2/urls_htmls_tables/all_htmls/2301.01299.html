<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2301.01299] Recent Advances on Federated Learning: A Systematic Survey</title><meta property="og:description" content="Federated learning has emerged as an effective paradigm to achieve privacy-preserving collaborative learning among different parties. Compared to traditional centralized learning that requires collecting data from each…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Recent Advances on Federated Learning: A Systematic Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Recent Advances on Federated Learning: A Systematic Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2301.01299">

<!--Generated on Fri Mar  1 08:07:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Decentralized AI,  Federated Learning,  Neural Networks,  Survey">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Recent Advances on Federated Learning: A Systematic Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bingyan Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:bingyanliu@bupt.edu.cn">bingyanliu@bupt.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Beijing University of Posts and Telecommunications</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">China</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_postcode">100871</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nuoyan Lv
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:lvnuoyan@bupt.edu.cn">lvnuoyan@bupt.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">Beijing University of Posts and Telecommunications</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_country">China</span><span id="id8.4.id4" class="ltx_text ltx_affiliation_postcode">100871</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuanchun Guo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:gyc2001@bupt.edu.cn">gyc2001@bupt.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">Beijing University of Posts and Telecommunications</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id11.3.id3" class="ltx_text ltx_affiliation_country">China</span><span id="id12.4.id4" class="ltx_text ltx_affiliation_postcode">100871</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yawen Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:warmly0716@126.com">warmly0716@126.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Beijing University of Posts and Telecommunications</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_country">China</span><span id="id16.4.id4" class="ltx_text ltx_affiliation_postcode">100871</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id17.id1" class="ltx_p">Federated learning has emerged as an effective paradigm to achieve privacy-preserving collaborative learning among different parties. Compared to traditional centralized learning that requires collecting data from each party, in federated learning, only the locally trained models or computed gradients are exchanged, without exposing any data information. As a result, it is able to protect privacy to some extent. In recent years, federated learning has become more and more prevalent and there have been many surveys for summarizing related methods in this hot research topic. However, most of them focus on a specific perspective or lack the latest research progress. In this paper, we provide a systematic survey on federated learning, aiming to review the recent advanced federated methods and applications from different aspects. Specifically, this paper includes four major contributions. First, we present a new taxonomy of federated learning in terms of the pipeline and challenges in federated scenarios. Second, we summarize federated learning methods into several categories and briefly introduce the state-of-the-art methods under these categories. Third, we overview some prevalent federated learning frameworks and introduce their features. Finally, some potential deficiencies of current methods and several future directions are discussed.</p>
</div>
<div class="ltx_keywords">Decentralized AI, Federated Learning, Neural Networks, Survey
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the past few years, deep neural networks (DNNs) have received a lot of attention due to their remarkable performance on various tasks such as Computer Vision (CV) <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky
et al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2012</a>; Simonyan and
Zisserman, <a href="#bib.bib126" title="" class="ltx_ref">2014</a>; Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib92" title="" class="ltx_ref">2019b</a>, <a href="#bib.bib90" title="" class="ltx_ref">2021a</a>)</cite>, Natural Language Processing (NLP) <cite class="ltx_cite ltx_citemacro_citep">(Devlin
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2018</a>; Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2017</a>; Xu
et al<span class="ltx_text">.</span>, <a href="#bib.bib152" title="" class="ltx_ref">2021</a>)</cite>, Recommendation Systems (RS) <cite class="ltx_cite ltx_citemacro_citep">(Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2022</a>, <a href="#bib.bib18" title="" class="ltx_ref">2020b</a>, <a href="#bib.bib19" title="" class="ltx_ref">2020c</a>)</cite> and Data Mining (DM) <cite class="ltx_cite ltx_citemacro_citep">(Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2020</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2021c</a>; Shao
et al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2014</a>)</cite>. However, the superiority of DNNs depends on the support of big data, which is hard to access in a certain party considering the limitation of the storage space and the difficulty of data collection. Gathering data from different parties to a central server for training is a direct solution to the issue. Nevertheless, data in each party may be sensitive or include some user privacy information. For example, medical images in a hospital are prohibited from outsourcing due to their privacy property. Besides, policies such as General
Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_citep">(Albrecht, <a href="#bib.bib5" title="" class="ltx_ref">2016</a>)</cite> also highlight the importance of protecting privacy when sharing information among different organizations. Thus, how to aggregate the data knowledge from different parties while ensuring privacy is an important and practical problem in real-world scenarios.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated learning (FL) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2017</a>)</cite>, which enables multiple parties to collaboratively train a DNN with the help of a central server, can be regarded as an effective solution to the aforementioned problem. Different from the traditional centralized learning that needs to collect data from each party, in FL, data do not need to upload for a joint training. Instead, the local trained models are exchanged with a central server, which are used to aggregate the knowledge from all of the uploaded models and then distribute the global model to each party. As a result, each party is able to benefit from other parties, improving the model accuracy. In recent years, there have been
many applications based on FL in practice, such as loan status prediction, health situation assessment, and next-word prediction <cite class="ltx_cite ltx_citemacro_citep">(Hard et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2018</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2018</a>; Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We take Fig. <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> as an example to illustrate a typical FL pipeline. First, each hospital (party) trains the local model distributed from a central cloud. The training process is usually implemented based on SGD with local data and then generates corresponding local updates. Second, the local updates rather than local data are transferred to the cloud, where the updates are sampled in terms of some heuristic rules to ensure the overhead and some aggregation algorithms (e.g., FedAvg <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2017</a>)</cite>) are conducted to achieve effective knowledge integration. In this way, the cloud can get an improved new global model and distributes it to each hospital for further tuning. These steps may repeat several times until the healthcare service can be satisfied (e.g., the accuracy of the learned model is acceptable for practical deployment).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">There have been other surveys on FL over the past few years. For instance, Li <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2021b</a>)</cite> summarized related FL methods from the system perspective, where the authors provided the definition of federated learning systems and analyzed the system
components. Lim <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Lim et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2020</a>)</cite> focused on the FL application in mobile edge networks. Lyu <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2020b</a>)</cite> paid more attention to the security and privacy issues existed in current FL schemes. However, these surveys only review a specific aspect of federated learning, failing to give readers a comprehensive understanding on FL. Towards the general FL overviews, most of them are out of date and cannot catch the latest trend in FL research. For example, Yang <span id="S1.p4.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2019</a>)</cite> divided FL methods into three categories (i.e., horizontal federated learning, vertical federated learning and federated transfer learning) and described their features respectively. Kairouz <span id="S1.p4.1.5" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2021b</a>)</cite> gave a comprehensive introduction of federated learning theory and application. Notice that both of the surveys mainly cited papers published before 2020, which is impossible to track the latest research progress on FL considering the rapid development in this field. As shown in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we can clearly see that the number of accepted FL papers in top-tier conferences increases dramatically after 2020, which calls for a timely survey to summarize the advances in the FL community. Besides, the rapid update of FL frameworks also requires us to highlight their latest features.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2301.01299/assets/FL_pipeline.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1256" height="621" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>An example of the FL pipeline <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020b</a>)</cite>.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2301.01299/assets/x1.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The number of pulished FL papers in top-tie conference from 2019-2022.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we attempt to provide a systematic survey on federated learning, targeting at reviewing the recent advanced federated methods and applications from different aspects. Specifically, the key contributions of this survey are as follows: (1) we present a new taxonomy based on the federated learning pipeline and challenges, which includes four typical aspects: <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">aggregation optimization, heterogeneity, privacy protection, fairness</span>. We will give detailed explanation in the following sections. (2) we summarize different federated learning methods into the proposed categories and briefly describe the state-of-the-art methods under these categories. (3) we overview the latest federated learning frameworks and introduce their features. (4) we discuss some potential deficiencies of current methods and several future directions.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The remainder of this survey is structured as follows. In Section <a href="#S2" title="2. Preliminaries ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we first introduce preliminaries of federated learning. In Section <a href="#S3" title="3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we propose the taxonomy of federated learning according to different aspects, in which various federated learning approaches are discussed and categorized. Then, in Section <a href="#S4" title="4. Prevalent frameworks of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we introduce some prevalent frameworks to show the practical deployment of federated learning. Finally, Section <a href="#S5" title="5. Discussion ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Section <a href="#S6" title="6. Conclusion ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> discuss the future work and concludes this paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Preliminaries</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Problem formulation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p">In this section, we first introduce some notations and symbols used in this survey to formally define federated learning. In general, there are two ends participated in the round of federated learning: <span id="S2.SS1.p1.6.1" class="ltx_text ltx_font_italic">client end</span> and <span id="S2.SS1.p1.6.2" class="ltx_text ltx_font_italic">server end</span>. The client end holds a series of local private data <math id="S2.SS1.p1.1.m1.4" class="ltx_Math" alttext="\mathcal{D}=\{\mathcal{D}_{1},\mathcal{D}_{2},...,\mathcal{D}_{N}\}" display="inline"><semantics id="S2.SS1.p1.1.m1.4a"><mrow id="S2.SS1.p1.1.m1.4.4" xref="S2.SS1.p1.1.m1.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.4.4.5" xref="S2.SS1.p1.1.m1.4.4.5.cmml">𝒟</mi><mo id="S2.SS1.p1.1.m1.4.4.4" xref="S2.SS1.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S2.SS1.p1.1.m1.4.4.3.3" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS1.p1.1.m1.4.4.3.3.4" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S2.SS1.p1.1.m1.2.2.1.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.2.2.1.1.1.2" xref="S2.SS1.p1.1.m1.2.2.1.1.1.2.cmml">𝒟</mi><mn id="S2.SS1.p1.1.m1.2.2.1.1.1.3" xref="S2.SS1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p1.1.m1.4.4.3.3.5" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.1.m1.3.3.2.2.2" xref="S2.SS1.p1.1.m1.3.3.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.3.3.2.2.2.2" xref="S2.SS1.p1.1.m1.3.3.2.2.2.2.cmml">𝒟</mi><mn id="S2.SS1.p1.1.m1.3.3.2.2.2.3" xref="S2.SS1.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p1.1.m1.4.4.3.3.6" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">…</mi><mo id="S2.SS1.p1.1.m1.4.4.3.3.7" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.1.m1.4.4.3.3.3" xref="S2.SS1.p1.1.m1.4.4.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.4.4.3.3.3.2" xref="S2.SS1.p1.1.m1.4.4.3.3.3.2.cmml">𝒟</mi><mi id="S2.SS1.p1.1.m1.4.4.3.3.3.3" xref="S2.SS1.p1.1.m1.4.4.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="S2.SS1.p1.1.m1.4.4.3.3.8" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.4b"><apply id="S2.SS1.p1.1.m1.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4"><eq id="S2.SS1.p1.1.m1.4.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4.4"></eq><ci id="S2.SS1.p1.1.m1.4.4.5.cmml" xref="S2.SS1.p1.1.m1.4.4.5">𝒟</ci><set id="S2.SS1.p1.1.m1.4.4.3.4.cmml" xref="S2.SS1.p1.1.m1.4.4.3.3"><apply id="S2.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1.2">𝒟</ci><cn type="integer" id="S2.SS1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.2.2">𝒟</ci><cn type="integer" id="S2.SS1.p1.1.m1.3.3.2.2.2.3.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">…</ci><apply id="S2.SS1.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS1.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.SS1.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.SS1.p1.1.m1.4.4.3.3.3.2">𝒟</ci><ci id="S2.SS1.p1.1.m1.4.4.3.3.3.3.cmml" xref="S2.SS1.p1.1.m1.4.4.3.3.3.3">𝑁</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.4c">\mathcal{D}=\{\mathcal{D}_{1},\mathcal{D}_{2},...,\mathcal{D}_{N}\}</annotation></semantics></math>, which are then used to train the model in each client and generate local models <math id="S2.SS1.p1.2.m2.4" class="ltx_Math" alttext="\mathcal{M}=\{M_{1},M_{2},...,M_{N}\}" display="inline"><semantics id="S2.SS1.p1.2.m2.4a"><mrow id="S2.SS1.p1.2.m2.4.4" xref="S2.SS1.p1.2.m2.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.4.4.5" xref="S2.SS1.p1.2.m2.4.4.5.cmml">ℳ</mi><mo id="S2.SS1.p1.2.m2.4.4.4" xref="S2.SS1.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S2.SS1.p1.2.m2.4.4.3.3" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS1.p1.2.m2.4.4.3.3.4" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml">{</mo><msub id="S2.SS1.p1.2.m2.2.2.1.1.1" xref="S2.SS1.p1.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS1.p1.2.m2.2.2.1.1.1.2" xref="S2.SS1.p1.2.m2.2.2.1.1.1.2.cmml">M</mi><mn id="S2.SS1.p1.2.m2.2.2.1.1.1.3" xref="S2.SS1.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p1.2.m2.4.4.3.3.5" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.2.m2.3.3.2.2.2" xref="S2.SS1.p1.2.m2.3.3.2.2.2.cmml"><mi id="S2.SS1.p1.2.m2.3.3.2.2.2.2" xref="S2.SS1.p1.2.m2.3.3.2.2.2.2.cmml">M</mi><mn id="S2.SS1.p1.2.m2.3.3.2.2.2.3" xref="S2.SS1.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p1.2.m2.4.4.3.3.6" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">…</mi><mo id="S2.SS1.p1.2.m2.4.4.3.3.7" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.2.m2.4.4.3.3.3" xref="S2.SS1.p1.2.m2.4.4.3.3.3.cmml"><mi id="S2.SS1.p1.2.m2.4.4.3.3.3.2" xref="S2.SS1.p1.2.m2.4.4.3.3.3.2.cmml">M</mi><mi id="S2.SS1.p1.2.m2.4.4.3.3.3.3" xref="S2.SS1.p1.2.m2.4.4.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="S2.SS1.p1.2.m2.4.4.3.3.8" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.4b"><apply id="S2.SS1.p1.2.m2.4.4.cmml" xref="S2.SS1.p1.2.m2.4.4"><eq id="S2.SS1.p1.2.m2.4.4.4.cmml" xref="S2.SS1.p1.2.m2.4.4.4"></eq><ci id="S2.SS1.p1.2.m2.4.4.5.cmml" xref="S2.SS1.p1.2.m2.4.4.5">ℳ</ci><set id="S2.SS1.p1.2.m2.4.4.3.4.cmml" xref="S2.SS1.p1.2.m2.4.4.3.3"><apply id="S2.SS1.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS1.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.2.m2.2.2.1.1.1.2">𝑀</ci><cn type="integer" id="S2.SS1.p1.2.m2.2.2.1.1.1.3.cmml" xref="S2.SS1.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.p1.2.m2.3.3.2.2.2.cmml" xref="S2.SS1.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.3.3.2.2.2.1.cmml" xref="S2.SS1.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS1.p1.2.m2.3.3.2.2.2.2">𝑀</ci><cn type="integer" id="S2.SS1.p1.2.m2.3.3.2.2.2.3.cmml" xref="S2.SS1.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">…</ci><apply id="S2.SS1.p1.2.m2.4.4.3.3.3.cmml" xref="S2.SS1.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.4.4.3.3.3.1.cmml" xref="S2.SS1.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.2.m2.4.4.3.3.3.2.cmml" xref="S2.SS1.p1.2.m2.4.4.3.3.3.2">𝑀</ci><ci id="S2.SS1.p1.2.m2.4.4.3.3.3.3.cmml" xref="S2.SS1.p1.2.m2.4.4.3.3.3.3">𝑁</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.4c">\mathcal{M}=\{M_{1},M_{2},...,M_{N}\}</annotation></semantics></math>. Here <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">N</annotation></semantics></math> denotes the number of clients. After the local training process, the local models <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\mathcal{M}</annotation></semantics></math>, rather than the data <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">\mathcal{D}</annotation></semantics></math>, are uploaded to the server end, where aggregation algorithms are implemented to obtain a global model <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="M_{global}" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><msub id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml">M</mi><mrow id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml"><mi id="S2.SS1.p1.6.m6.1.1.3.2" xref="S2.SS1.p1.6.m6.1.1.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.1.1.3.1" xref="S2.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.6.m6.1.1.3.3" xref="S2.SS1.p1.6.m6.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.1.1.3.1a" xref="S2.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.6.m6.1.1.3.4" xref="S2.SS1.p1.6.m6.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.1.1.3.1b" xref="S2.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.6.m6.1.1.3.5" xref="S2.SS1.p1.6.m6.1.1.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.1.1.3.1c" xref="S2.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.6.m6.1.1.3.6" xref="S2.SS1.p1.6.m6.1.1.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.1.1.3.1d" xref="S2.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.6.m6.1.1.3.7" xref="S2.SS1.p1.6.m6.1.1.3.7.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2">𝑀</ci><apply id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3"><times id="S2.SS1.p1.6.m6.1.1.3.1.cmml" xref="S2.SS1.p1.6.m6.1.1.3.1"></times><ci id="S2.SS1.p1.6.m6.1.1.3.2.cmml" xref="S2.SS1.p1.6.m6.1.1.3.2">𝑔</ci><ci id="S2.SS1.p1.6.m6.1.1.3.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3">𝑙</ci><ci id="S2.SS1.p1.6.m6.1.1.3.4.cmml" xref="S2.SS1.p1.6.m6.1.1.3.4">𝑜</ci><ci id="S2.SS1.p1.6.m6.1.1.3.5.cmml" xref="S2.SS1.p1.6.m6.1.1.3.5">𝑏</ci><ci id="S2.SS1.p1.6.m6.1.1.3.6.cmml" xref="S2.SS1.p1.6.m6.1.1.3.6">𝑎</ci><ci id="S2.SS1.p1.6.m6.1.1.3.7.cmml" xref="S2.SS1.p1.6.m6.1.1.3.7">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">M_{global}</annotation></semantics></math>. The process can be defined as</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\displaystyle M_{global}=AGG(M_{1},M_{2},...,M_{N})," display="inline"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1" xref="S2.E1.m1.2.2.1.1.cmml"><msub id="S2.E1.m1.2.2.1.1.5" xref="S2.E1.m1.2.2.1.1.5.cmml"><mi id="S2.E1.m1.2.2.1.1.5.2" xref="S2.E1.m1.2.2.1.1.5.2.cmml">M</mi><mrow id="S2.E1.m1.2.2.1.1.5.3" xref="S2.E1.m1.2.2.1.1.5.3.cmml"><mi id="S2.E1.m1.2.2.1.1.5.3.2" xref="S2.E1.m1.2.2.1.1.5.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.5.3.1" xref="S2.E1.m1.2.2.1.1.5.3.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.5.3.3" xref="S2.E1.m1.2.2.1.1.5.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.5.3.1a" xref="S2.E1.m1.2.2.1.1.5.3.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.5.3.4" xref="S2.E1.m1.2.2.1.1.5.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.5.3.1b" xref="S2.E1.m1.2.2.1.1.5.3.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.5.3.5" xref="S2.E1.m1.2.2.1.1.5.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.5.3.1c" xref="S2.E1.m1.2.2.1.1.5.3.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.5.3.6" xref="S2.E1.m1.2.2.1.1.5.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.5.3.1d" xref="S2.E1.m1.2.2.1.1.5.3.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.5.3.7" xref="S2.E1.m1.2.2.1.1.5.3.7.cmml">l</mi></mrow></msub><mo id="S2.E1.m1.2.2.1.1.4" xref="S2.E1.m1.2.2.1.1.4.cmml">=</mo><mrow id="S2.E1.m1.2.2.1.1.3" xref="S2.E1.m1.2.2.1.1.3.cmml"><mi id="S2.E1.m1.2.2.1.1.3.5" xref="S2.E1.m1.2.2.1.1.3.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.3.4" xref="S2.E1.m1.2.2.1.1.3.4.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.3.6" xref="S2.E1.m1.2.2.1.1.3.6.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.3.4a" xref="S2.E1.m1.2.2.1.1.3.4.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.3.7" xref="S2.E1.m1.2.2.1.1.3.7.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.3.4b" xref="S2.E1.m1.2.2.1.1.3.4.cmml">​</mo><mrow id="S2.E1.m1.2.2.1.1.3.3.3" xref="S2.E1.m1.2.2.1.1.3.3.4.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.3.3.3.4" xref="S2.E1.m1.2.2.1.1.3.3.4.cmml">(</mo><msub id="S2.E1.m1.2.2.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.2.cmml">M</mi><mn id="S2.E1.m1.2.2.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.E1.m1.2.2.1.1.3.3.3.5" xref="S2.E1.m1.2.2.1.1.3.3.4.cmml">,</mo><msub id="S2.E1.m1.2.2.1.1.2.2.2.2" xref="S2.E1.m1.2.2.1.1.2.2.2.2.cmml"><mi id="S2.E1.m1.2.2.1.1.2.2.2.2.2" xref="S2.E1.m1.2.2.1.1.2.2.2.2.2.cmml">M</mi><mn id="S2.E1.m1.2.2.1.1.2.2.2.2.3" xref="S2.E1.m1.2.2.1.1.2.2.2.2.3.cmml">2</mn></msub><mo id="S2.E1.m1.2.2.1.1.3.3.3.6" xref="S2.E1.m1.2.2.1.1.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">…</mi><mo id="S2.E1.m1.2.2.1.1.3.3.3.7" xref="S2.E1.m1.2.2.1.1.3.3.4.cmml">,</mo><msub id="S2.E1.m1.2.2.1.1.3.3.3.3" xref="S2.E1.m1.2.2.1.1.3.3.3.3.cmml"><mi id="S2.E1.m1.2.2.1.1.3.3.3.3.2" xref="S2.E1.m1.2.2.1.1.3.3.3.3.2.cmml">M</mi><mi id="S2.E1.m1.2.2.1.1.3.3.3.3.3" xref="S2.E1.m1.2.2.1.1.3.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="S2.E1.m1.2.2.1.1.3.3.3.8" xref="S2.E1.m1.2.2.1.1.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.1.1.cmml" xref="S2.E1.m1.2.2.1"><eq id="S2.E1.m1.2.2.1.1.4.cmml" xref="S2.E1.m1.2.2.1.1.4"></eq><apply id="S2.E1.m1.2.2.1.1.5.cmml" xref="S2.E1.m1.2.2.1.1.5"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.5.1.cmml" xref="S2.E1.m1.2.2.1.1.5">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.5.2.cmml" xref="S2.E1.m1.2.2.1.1.5.2">𝑀</ci><apply id="S2.E1.m1.2.2.1.1.5.3.cmml" xref="S2.E1.m1.2.2.1.1.5.3"><times id="S2.E1.m1.2.2.1.1.5.3.1.cmml" xref="S2.E1.m1.2.2.1.1.5.3.1"></times><ci id="S2.E1.m1.2.2.1.1.5.3.2.cmml" xref="S2.E1.m1.2.2.1.1.5.3.2">𝑔</ci><ci id="S2.E1.m1.2.2.1.1.5.3.3.cmml" xref="S2.E1.m1.2.2.1.1.5.3.3">𝑙</ci><ci id="S2.E1.m1.2.2.1.1.5.3.4.cmml" xref="S2.E1.m1.2.2.1.1.5.3.4">𝑜</ci><ci id="S2.E1.m1.2.2.1.1.5.3.5.cmml" xref="S2.E1.m1.2.2.1.1.5.3.5">𝑏</ci><ci id="S2.E1.m1.2.2.1.1.5.3.6.cmml" xref="S2.E1.m1.2.2.1.1.5.3.6">𝑎</ci><ci id="S2.E1.m1.2.2.1.1.5.3.7.cmml" xref="S2.E1.m1.2.2.1.1.5.3.7">𝑙</ci></apply></apply><apply id="S2.E1.m1.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.3"><times id="S2.E1.m1.2.2.1.1.3.4.cmml" xref="S2.E1.m1.2.2.1.1.3.4"></times><ci id="S2.E1.m1.2.2.1.1.3.5.cmml" xref="S2.E1.m1.2.2.1.1.3.5">𝐴</ci><ci id="S2.E1.m1.2.2.1.1.3.6.cmml" xref="S2.E1.m1.2.2.1.1.3.6">𝐺</ci><ci id="S2.E1.m1.2.2.1.1.3.7.cmml" xref="S2.E1.m1.2.2.1.1.3.7">𝐺</ci><vector id="S2.E1.m1.2.2.1.1.3.3.4.cmml" xref="S2.E1.m1.2.2.1.1.3.3.3"><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.2">𝑀</ci><cn type="integer" id="S2.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.3">1</cn></apply><apply id="S2.E1.m1.2.2.1.1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.2.2">𝑀</ci><cn type="integer" id="S2.E1.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.2.3">2</cn></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">…</ci><apply id="S2.E1.m1.2.2.1.1.3.3.3.3.cmml" xref="S2.E1.m1.2.2.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.3.3.3.3.1.cmml" xref="S2.E1.m1.2.2.1.1.3.3.3.3">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.3.3.3.3.2.cmml" xref="S2.E1.m1.2.2.1.1.3.3.3.3.2">𝑀</ci><ci id="S2.E1.m1.2.2.1.1.3.3.3.3.3.cmml" xref="S2.E1.m1.2.2.1.1.3.3.3.3.3">𝑁</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\displaystyle M_{global}=AGG(M_{1},M_{2},...,M_{N}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.7" class="ltx_p">where <math id="S2.SS1.p1.7.m1.1" class="ltx_Math" alttext="AGG" display="inline"><semantics id="S2.SS1.p1.7.m1.1a"><mrow id="S2.SS1.p1.7.m1.1.1" xref="S2.SS1.p1.7.m1.1.1.cmml"><mi id="S2.SS1.p1.7.m1.1.1.2" xref="S2.SS1.p1.7.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.7.m1.1.1.1" xref="S2.SS1.p1.7.m1.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.7.m1.1.1.3" xref="S2.SS1.p1.7.m1.1.1.3.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.7.m1.1.1.1a" xref="S2.SS1.p1.7.m1.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.7.m1.1.1.4" xref="S2.SS1.p1.7.m1.1.1.4.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m1.1b"><apply id="S2.SS1.p1.7.m1.1.1.cmml" xref="S2.SS1.p1.7.m1.1.1"><times id="S2.SS1.p1.7.m1.1.1.1.cmml" xref="S2.SS1.p1.7.m1.1.1.1"></times><ci id="S2.SS1.p1.7.m1.1.1.2.cmml" xref="S2.SS1.p1.7.m1.1.1.2">𝐴</ci><ci id="S2.SS1.p1.7.m1.1.1.3.cmml" xref="S2.SS1.p1.7.m1.1.1.3">𝐺</ci><ci id="S2.SS1.p1.7.m1.1.1.4.cmml" xref="S2.SS1.p1.7.m1.1.1.4">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m1.1c">AGG</annotation></semantics></math> represents the aggregation algorithms. In this way, we finish one round of federated learning and distribute the global model to each client side for further local training. The concrete number of round is usually determined by the model performance (i.e., we stop the process until the model can achieve desirable accuracy). In addition, to provide a more rigorous privacy protection, each client may enforce some encryption techniques to the models before uploading them. Differential privacy (DP) <cite class="ltx_cite ltx_citemacro_citep">(Dwork, <a href="#bib.bib31" title="" class="ltx_ref">2008</a>)</cite> and homomorphic encryption (HE) <cite class="ltx_cite ltx_citemacro_citep">(Gentry, <a href="#bib.bib40" title="" class="ltx_ref">2009</a>)</cite> are widely used to conduct such protection.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.9" class="ltx_p">Based on the aforementioned statement, we can see that the performance of federated learning largely depends on the aggregation algorithm in the server end. Formally, the goal of federated learning is to optimize the following objective function</p>
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2.m1.4" class="ltx_Math" alttext="\displaystyle\min_{w}\ L(w),\ where\ L(w)=\sum_{i=1}^{N}f_{i}L_{i}(w)," display="inline"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1" xref="S2.E2.m1.4.4.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.2.2" xref="S2.E2.m1.4.4.1.1.2.3.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml"><munder id="S2.E2.m1.4.4.1.1.1.1.1.2.1" xref="S2.E2.m1.4.4.1.1.1.1.1.2.1.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.2.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.1.2.cmml">min</mi><mi id="S2.E2.m1.4.4.1.1.1.1.1.2.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.2.1.3.cmml">w</mi></munder><mo lspace="0.667em" id="S2.E2.m1.4.4.1.1.1.1.1.2a" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml">⁡</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2.cmml">L</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.1.1.3.2.1" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">w</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.1.1.3.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.667em" id="S2.E2.m1.4.4.1.1.2.2.3" xref="S2.E2.m1.4.4.1.1.2.3.cmml">,</mo><mrow id="S2.E2.m1.4.4.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.2.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.2.2.2.2" xref="S2.E2.m1.4.4.1.1.2.2.2.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2.2.2.1" xref="S2.E2.m1.4.4.1.1.2.2.2.1.cmml">​</mo><mi id="S2.E2.m1.4.4.1.1.2.2.2.3" xref="S2.E2.m1.4.4.1.1.2.2.2.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2.2.2.1a" xref="S2.E2.m1.4.4.1.1.2.2.2.1.cmml">​</mo><mi id="S2.E2.m1.4.4.1.1.2.2.2.4" xref="S2.E2.m1.4.4.1.1.2.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2.2.2.1b" xref="S2.E2.m1.4.4.1.1.2.2.2.1.cmml">​</mo><mi id="S2.E2.m1.4.4.1.1.2.2.2.5" xref="S2.E2.m1.4.4.1.1.2.2.2.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2.2.2.1c" xref="S2.E2.m1.4.4.1.1.2.2.2.1.cmml">​</mo><mi id="S2.E2.m1.4.4.1.1.2.2.2.6" xref="S2.E2.m1.4.4.1.1.2.2.2.6.cmml">e</mi><mo lspace="0.500em" rspace="0em" id="S2.E2.m1.4.4.1.1.2.2.2.1d" xref="S2.E2.m1.4.4.1.1.2.2.2.1.cmml">​</mo><mi id="S2.E2.m1.4.4.1.1.2.2.2.7" xref="S2.E2.m1.4.4.1.1.2.2.2.7.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2.2.2.1e" xref="S2.E2.m1.4.4.1.1.2.2.2.1.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.2.2.2.8.2" xref="S2.E2.m1.4.4.1.1.2.2.2.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.2.2.2.8.2.1" xref="S2.E2.m1.4.4.1.1.2.2.2.cmml">(</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">w</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.2.2.2.8.2.2" xref="S2.E2.m1.4.4.1.1.2.2.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.3" xref="S2.E2.m1.4.4.1.1.3.cmml">=</mo><mrow id="S2.E2.m1.4.4.1.1.4" xref="S2.E2.m1.4.4.1.1.4.cmml"><mstyle displaystyle="true" id="S2.E2.m1.4.4.1.1.4.1" xref="S2.E2.m1.4.4.1.1.4.1.cmml"><munderover id="S2.E2.m1.4.4.1.1.4.1a" xref="S2.E2.m1.4.4.1.1.4.1.cmml"><mo movablelimits="false" id="S2.E2.m1.4.4.1.1.4.1.2.2" xref="S2.E2.m1.4.4.1.1.4.1.2.2.cmml">∑</mo><mrow id="S2.E2.m1.4.4.1.1.4.1.2.3" xref="S2.E2.m1.4.4.1.1.4.1.2.3.cmml"><mi id="S2.E2.m1.4.4.1.1.4.1.2.3.2" xref="S2.E2.m1.4.4.1.1.4.1.2.3.2.cmml">i</mi><mo id="S2.E2.m1.4.4.1.1.4.1.2.3.1" xref="S2.E2.m1.4.4.1.1.4.1.2.3.1.cmml">=</mo><mn id="S2.E2.m1.4.4.1.1.4.1.2.3.3" xref="S2.E2.m1.4.4.1.1.4.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.4.4.1.1.4.1.3" xref="S2.E2.m1.4.4.1.1.4.1.3.cmml">N</mi></munderover></mstyle><mrow id="S2.E2.m1.4.4.1.1.4.2" xref="S2.E2.m1.4.4.1.1.4.2.cmml"><msub id="S2.E2.m1.4.4.1.1.4.2.2" xref="S2.E2.m1.4.4.1.1.4.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.4.2.2.2" xref="S2.E2.m1.4.4.1.1.4.2.2.2.cmml">f</mi><mi id="S2.E2.m1.4.4.1.1.4.2.2.3" xref="S2.E2.m1.4.4.1.1.4.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.4.2.1" xref="S2.E2.m1.4.4.1.1.4.2.1.cmml">​</mo><msub id="S2.E2.m1.4.4.1.1.4.2.3" xref="S2.E2.m1.4.4.1.1.4.2.3.cmml"><mi id="S2.E2.m1.4.4.1.1.4.2.3.2" xref="S2.E2.m1.4.4.1.1.4.2.3.2.cmml">L</mi><mi id="S2.E2.m1.4.4.1.1.4.2.3.3" xref="S2.E2.m1.4.4.1.1.4.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.4.2.1a" xref="S2.E2.m1.4.4.1.1.4.2.1.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.4.2.4.2" xref="S2.E2.m1.4.4.1.1.4.2.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.4.2.4.2.1" xref="S2.E2.m1.4.4.1.1.4.2.cmml">(</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">w</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.4.2.4.2.2" xref="S2.E2.m1.4.4.1.1.4.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E2.m1.4.4.1.2" xref="S2.E2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.1.1.cmml" xref="S2.E2.m1.4.4.1"><eq id="S2.E2.m1.4.4.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.3"></eq><list id="S2.E2.m1.4.4.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.2.2"><apply id="S2.E2.m1.4.4.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1"><times id="S2.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1"></times><apply id="S2.E2.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2"><apply id="S2.E2.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.2.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.1">subscript</csymbol><min id="S2.E2.m1.4.4.1.1.1.1.1.2.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.1.2"></min><ci id="S2.E2.m1.4.4.1.1.1.1.1.2.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.1.3">𝑤</ci></apply><ci id="S2.E2.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2">𝐿</ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑤</ci></apply><apply id="S2.E2.m1.4.4.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.2.2.2"><times id="S2.E2.m1.4.4.1.1.2.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.2.2.2.1"></times><ci id="S2.E2.m1.4.4.1.1.2.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.2.2.2.2">𝑤</ci><ci id="S2.E2.m1.4.4.1.1.2.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.2.2.2.3">ℎ</ci><ci id="S2.E2.m1.4.4.1.1.2.2.2.4.cmml" xref="S2.E2.m1.4.4.1.1.2.2.2.4">𝑒</ci><ci id="S2.E2.m1.4.4.1.1.2.2.2.5.cmml" xref="S2.E2.m1.4.4.1.1.2.2.2.5">𝑟</ci><ci id="S2.E2.m1.4.4.1.1.2.2.2.6.cmml" xref="S2.E2.m1.4.4.1.1.2.2.2.6">𝑒</ci><ci id="S2.E2.m1.4.4.1.1.2.2.2.7.cmml" xref="S2.E2.m1.4.4.1.1.2.2.2.7">𝐿</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝑤</ci></apply></list><apply id="S2.E2.m1.4.4.1.1.4.cmml" xref="S2.E2.m1.4.4.1.1.4"><apply id="S2.E2.m1.4.4.1.1.4.1.cmml" xref="S2.E2.m1.4.4.1.1.4.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.4.1.1.cmml" xref="S2.E2.m1.4.4.1.1.4.1">superscript</csymbol><apply id="S2.E2.m1.4.4.1.1.4.1.2.cmml" xref="S2.E2.m1.4.4.1.1.4.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.4.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.4.1">subscript</csymbol><sum id="S2.E2.m1.4.4.1.1.4.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.4.1.2.2"></sum><apply id="S2.E2.m1.4.4.1.1.4.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.4.1.2.3"><eq id="S2.E2.m1.4.4.1.1.4.1.2.3.1.cmml" xref="S2.E2.m1.4.4.1.1.4.1.2.3.1"></eq><ci id="S2.E2.m1.4.4.1.1.4.1.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.4.1.2.3.2">𝑖</ci><cn type="integer" id="S2.E2.m1.4.4.1.1.4.1.2.3.3.cmml" xref="S2.E2.m1.4.4.1.1.4.1.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.4.4.1.1.4.1.3.cmml" xref="S2.E2.m1.4.4.1.1.4.1.3">𝑁</ci></apply><apply id="S2.E2.m1.4.4.1.1.4.2.cmml" xref="S2.E2.m1.4.4.1.1.4.2"><times id="S2.E2.m1.4.4.1.1.4.2.1.cmml" xref="S2.E2.m1.4.4.1.1.4.2.1"></times><apply id="S2.E2.m1.4.4.1.1.4.2.2.cmml" xref="S2.E2.m1.4.4.1.1.4.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.4.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.4.2.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.4.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.4.2.2.2">𝑓</ci><ci id="S2.E2.m1.4.4.1.1.4.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.4.2.2.3">𝑖</ci></apply><apply id="S2.E2.m1.4.4.1.1.4.2.3.cmml" xref="S2.E2.m1.4.4.1.1.4.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.4.2.3.1.cmml" xref="S2.E2.m1.4.4.1.1.4.2.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.4.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.4.2.3.2">𝐿</ci><ci id="S2.E2.m1.4.4.1.1.4.2.3.3.cmml" xref="S2.E2.m1.4.4.1.1.4.2.3.3">𝑖</ci></apply><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">𝑤</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\displaystyle\min_{w}\ L(w),\ where\ L(w)=\sum_{i=1}^{N}f_{i}L_{i}(w),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.8" class="ltx_p">where <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">w</annotation></semantics></math> is the weights of DNNs, <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="L(w)" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.2" xref="S2.SS1.p2.2.m2.1.2.cmml"><mi id="S2.SS1.p2.2.m2.1.2.2" xref="S2.SS1.p2.2.m2.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.2.m2.1.2.1" xref="S2.SS1.p2.2.m2.1.2.1.cmml">​</mo><mrow id="S2.SS1.p2.2.m2.1.2.3.2" xref="S2.SS1.p2.2.m2.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.2.m2.1.2.3.2.1" xref="S2.SS1.p2.2.m2.1.2.cmml">(</mo><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">w</mi><mo stretchy="false" id="S2.SS1.p2.2.m2.1.2.3.2.2" xref="S2.SS1.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.2.cmml" xref="S2.SS1.p2.2.m2.1.2"><times id="S2.SS1.p2.2.m2.1.2.1.cmml" xref="S2.SS1.p2.2.m2.1.2.1"></times><ci id="S2.SS1.p2.2.m2.1.2.2.cmml" xref="S2.SS1.p2.2.m2.1.2.2">𝐿</ci><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">L(w)</annotation></semantics></math> is the global loss function and <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="L_{i}(w)" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mrow id="S2.SS1.p2.3.m3.1.2" xref="S2.SS1.p2.3.m3.1.2.cmml"><msub id="S2.SS1.p2.3.m3.1.2.2" xref="S2.SS1.p2.3.m3.1.2.2.cmml"><mi id="S2.SS1.p2.3.m3.1.2.2.2" xref="S2.SS1.p2.3.m3.1.2.2.2.cmml">L</mi><mi id="S2.SS1.p2.3.m3.1.2.2.3" xref="S2.SS1.p2.3.m3.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p2.3.m3.1.2.1" xref="S2.SS1.p2.3.m3.1.2.1.cmml">​</mo><mrow id="S2.SS1.p2.3.m3.1.2.3.2" xref="S2.SS1.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.3.m3.1.2.3.2.1" xref="S2.SS1.p2.3.m3.1.2.cmml">(</mo><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">w</mi><mo stretchy="false" id="S2.SS1.p2.3.m3.1.2.3.2.2" xref="S2.SS1.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.2.cmml" xref="S2.SS1.p2.3.m3.1.2"><times id="S2.SS1.p2.3.m3.1.2.1.cmml" xref="S2.SS1.p2.3.m3.1.2.1"></times><apply id="S2.SS1.p2.3.m3.1.2.2.cmml" xref="S2.SS1.p2.3.m3.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.2.2.1.cmml" xref="S2.SS1.p2.3.m3.1.2.2">subscript</csymbol><ci id="S2.SS1.p2.3.m3.1.2.2.2.cmml" xref="S2.SS1.p2.3.m3.1.2.2.2">𝐿</ci><ci id="S2.SS1.p2.3.m3.1.2.2.3.cmml" xref="S2.SS1.p2.3.m3.1.2.2.3">𝑖</ci></apply><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">L_{i}(w)</annotation></semantics></math> is the local loss function in the <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="i_{th}" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><msub id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><mi id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml">i</mi><mrow id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml"><mi id="S2.SS1.p2.4.m4.1.1.3.2" xref="S2.SS1.p2.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.4.m4.1.1.3.1" xref="S2.SS1.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p2.4.m4.1.1.3.3" xref="S2.SS1.p2.4.m4.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2">𝑖</ci><apply id="S2.SS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3"><times id="S2.SS1.p2.4.m4.1.1.3.1.cmml" xref="S2.SS1.p2.4.m4.1.1.3.1"></times><ci id="S2.SS1.p2.4.m4.1.1.3.2.cmml" xref="S2.SS1.p2.4.m4.1.1.3.2">𝑡</ci><ci id="S2.SS1.p2.4.m4.1.1.3.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">i_{th}</annotation></semantics></math> client. <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="f_{i}" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><msub id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml"><mi id="S2.SS1.p2.5.m5.1.1.2" xref="S2.SS1.p2.5.m5.1.1.2.cmml">f</mi><mi id="S2.SS1.p2.5.m5.1.1.3" xref="S2.SS1.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><apply id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.1.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p2.5.m5.1.1.2.cmml" xref="S2.SS1.p2.5.m5.1.1.2">𝑓</ci><ci id="S2.SS1.p2.5.m5.1.1.3.cmml" xref="S2.SS1.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">f_{i}</annotation></semantics></math> represents the importance of the <math id="S2.SS1.p2.6.m6.1" class="ltx_Math" alttext="i_{th}" display="inline"><semantics id="S2.SS1.p2.6.m6.1a"><msub id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml"><mi id="S2.SS1.p2.6.m6.1.1.2" xref="S2.SS1.p2.6.m6.1.1.2.cmml">i</mi><mrow id="S2.SS1.p2.6.m6.1.1.3" xref="S2.SS1.p2.6.m6.1.1.3.cmml"><mi id="S2.SS1.p2.6.m6.1.1.3.2" xref="S2.SS1.p2.6.m6.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.6.m6.1.1.3.1" xref="S2.SS1.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p2.6.m6.1.1.3.3" xref="S2.SS1.p2.6.m6.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><apply id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.6.m6.1.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p2.6.m6.1.1.2.cmml" xref="S2.SS1.p2.6.m6.1.1.2">𝑖</ci><apply id="S2.SS1.p2.6.m6.1.1.3.cmml" xref="S2.SS1.p2.6.m6.1.1.3"><times id="S2.SS1.p2.6.m6.1.1.3.1.cmml" xref="S2.SS1.p2.6.m6.1.1.3.1"></times><ci id="S2.SS1.p2.6.m6.1.1.3.2.cmml" xref="S2.SS1.p2.6.m6.1.1.3.2">𝑡</ci><ci id="S2.SS1.p2.6.m6.1.1.3.3.cmml" xref="S2.SS1.p2.6.m6.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">i_{th}</annotation></semantics></math> client and <math id="S2.SS1.p2.7.m7.1" class="ltx_Math" alttext="\sum_{k=1}^{N}=1" display="inline"><semantics id="S2.SS1.p2.7.m7.1a"><mrow id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml"><msubsup id="S2.SS1.p2.7.m7.1.1.2" xref="S2.SS1.p2.7.m7.1.1.2.cmml"><mo id="S2.SS1.p2.7.m7.1.1.2.2.2" xref="S2.SS1.p2.7.m7.1.1.2.2.2.cmml">∑</mo><mrow id="S2.SS1.p2.7.m7.1.1.2.2.3" xref="S2.SS1.p2.7.m7.1.1.2.2.3.cmml"><mi id="S2.SS1.p2.7.m7.1.1.2.2.3.2" xref="S2.SS1.p2.7.m7.1.1.2.2.3.2.cmml">k</mi><mo id="S2.SS1.p2.7.m7.1.1.2.2.3.1" xref="S2.SS1.p2.7.m7.1.1.2.2.3.1.cmml">=</mo><mn id="S2.SS1.p2.7.m7.1.1.2.2.3.3" xref="S2.SS1.p2.7.m7.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.SS1.p2.7.m7.1.1.2.3" xref="S2.SS1.p2.7.m7.1.1.2.3.cmml">N</mi></msubsup><mo lspace="0.278em" id="S2.SS1.p2.7.m7.1.1.1" xref="S2.SS1.p2.7.m7.1.1.1.cmml">=</mo><mn id="S2.SS1.p2.7.m7.1.1.3" xref="S2.SS1.p2.7.m7.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.1b"><apply id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1"><eq id="S2.SS1.p2.7.m7.1.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1.1"></eq><apply id="S2.SS1.p2.7.m7.1.1.2.cmml" xref="S2.SS1.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.7.m7.1.1.2.1.cmml" xref="S2.SS1.p2.7.m7.1.1.2">superscript</csymbol><apply id="S2.SS1.p2.7.m7.1.1.2.2.cmml" xref="S2.SS1.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.7.m7.1.1.2.2.1.cmml" xref="S2.SS1.p2.7.m7.1.1.2">subscript</csymbol><sum id="S2.SS1.p2.7.m7.1.1.2.2.2.cmml" xref="S2.SS1.p2.7.m7.1.1.2.2.2"></sum><apply id="S2.SS1.p2.7.m7.1.1.2.2.3.cmml" xref="S2.SS1.p2.7.m7.1.1.2.2.3"><eq id="S2.SS1.p2.7.m7.1.1.2.2.3.1.cmml" xref="S2.SS1.p2.7.m7.1.1.2.2.3.1"></eq><ci id="S2.SS1.p2.7.m7.1.1.2.2.3.2.cmml" xref="S2.SS1.p2.7.m7.1.1.2.2.3.2">𝑘</ci><cn type="integer" id="S2.SS1.p2.7.m7.1.1.2.2.3.3.cmml" xref="S2.SS1.p2.7.m7.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.SS1.p2.7.m7.1.1.2.3.cmml" xref="S2.SS1.p2.7.m7.1.1.2.3">𝑁</ci></apply><cn type="integer" id="S2.SS1.p2.7.m7.1.1.3.cmml" xref="S2.SS1.p2.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.1c">\sum_{k=1}^{N}=1</annotation></semantics></math>. In federated learning, the aggregation algorithm determines the value allocation for <math id="S2.SS1.p2.8.m8.1" class="ltx_Math" alttext="f_{i}" display="inline"><semantics id="S2.SS1.p2.8.m8.1a"><msub id="S2.SS1.p2.8.m8.1.1" xref="S2.SS1.p2.8.m8.1.1.cmml"><mi id="S2.SS1.p2.8.m8.1.1.2" xref="S2.SS1.p2.8.m8.1.1.2.cmml">f</mi><mi id="S2.SS1.p2.8.m8.1.1.3" xref="S2.SS1.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.8.m8.1b"><apply id="S2.SS1.p2.8.m8.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.8.m8.1.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p2.8.m8.1.1.2.cmml" xref="S2.SS1.p2.8.m8.1.1.2">𝑓</ci><ci id="S2.SS1.p2.8.m8.1.1.3.cmml" xref="S2.SS1.p2.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.8.m8.1c">f_{i}</annotation></semantics></math>. Many research papers that try to improve the accuracy performance of federated learning are focused on this aspect.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Key challenges</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Different from traditional centralized learning or distributed learning, federated learning faces the following key challenges:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Heterogeneity problem.</span>
In federated learning, the heterogeneity comes from three aspects:(1) Data heterogeneity. Considering that each participator collects data from its local end, the overall data distribution inevitably conforms to the non-independent identically distribution (non-iid) situation. For example, the same object image collected from different environments, or the same activity coming from different people, can lead to different data distributions, which will further affect the performance of federated aggregation <cite class="ltx_cite ltx_citemacro_citep">(Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib175" title="" class="ltx_ref">2018</a>)</cite>. (2) Model heterogeneity. In real-world scenarios, it is hard to limit the federated clients to use an identical model architecture. Instead, each client may prefer a distinctive model architecture for improved task performance. Therefore, how to aggregate these heterogeneous models is challenging in practical federated learning conditions. (3) System heterogeneity. Because of the variability in hardware, different parties may have different storage space, computation power, and communication capabilities. As a result, the server end needs to decide whether to wait for all parties to upload their models for better accuracy or remove stragglers (i.e., the parties with weak hardware performance) for accelerating the federation process.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Privacy leakage.</span>
The key idea of federated learning is to achieve collaborative learning in a privacy-preserving manner, which differs from the traditional paradigm that exchanges data or other sensitive information. Keeping data in the local end and transferring corresponding models is the original privacy protection design in federated learning. However, the parameters of the uploaded models may also be exploited by attackers to infer the user privacy information <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib181" title="" class="ltx_ref">2019</a>)</cite>. So we require more rigorous encryption or obfuscation methods to ensure privacy.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Unfairness.</span>
In traditional centralized learning or distributed learning, the unfairness problem does not exist since the participants belong to a same organization. However, the participants in federated learning come from various parties with different data resources. According to a previous work <cite class="ltx_cite ltx_citemacro_citep">(Dwork et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2012</a>)</cite>, if individuals with similar preferences and characteristics receive substantially different outcomes, then we say that the model violates individual fairness. Thus, it is necessary to generate federated models that go beyond average accuracy to further consider the fairness performance.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Approaches of federated learning</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first present a taxonomy of federated learning and allocate different federated approaches into different categories according to the taxonomy. Then for each category, we describe in detail how various methods achieve their goal.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2301.01299/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="968" height="528" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Our taxonomy of different federated learning methods.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Taxonomy</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this survey, we propose a new taxonomy to classify the existing federated learning methods (Fig. <a href="#S3.F3" title="Figure 3 ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
Our taxonomy is motivated by the pipeline and challenges in federated learning. As stated in the previous section, the key step in the federated learning pipeline is the aggregation algorithm and the key challenges come from three different aspects. Therefore, in our taxonomy, federated learning approaches can be summarized into four cases: aggregation optimization, heterogeneous federated learning, secure federated learning and fair federated learning.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Aggregation optimization.</span>
Considering that the number of participants in a federated learning system is usually large, it is essential to implement an effective aggregation optimization for outputting a better global model compared to the ones with local training. This survey investigates various aggregation methods such as FedAvg <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2017</a>; Nagalapatti and
Narayanam, <a href="#bib.bib110" title="" class="ltx_ref">2021</a>; Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib175" title="" class="ltx_ref">2018</a>)</cite>, FedMA <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2020b</a>)</cite> and FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2020c</a>)</cite>, with a focus on how to combine local models into an improved global model.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Heterogeneous federated learning.</span>
In real-world scenarios, federated clients may come from different environments or equip with various hardware, leading to the heterogeneity problem. In the following sections, we respectively explore how related research efforts address the issue of data heterogeneity, model heterogeneity and system heterogeneity. In particular, techniques such as meta-learning <cite class="ltx_cite ltx_citemacro_citep">(Fallah
et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2020</a>; Acar et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2021</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2019</a>; Khodak
et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2019</a>; Zheng
et al<span class="ltx_text">.</span>, <a href="#bib.bib177" title="" class="ltx_ref">2021</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>, multi-task learning <cite class="ltx_cite ltx_citemacro_citep">(Smith
et al<span class="ltx_text">.</span>, <a href="#bib.bib128" title="" class="ltx_ref">2017</a>; Vanhaesebrouck et al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2017</a>; Corinzia
et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2019</a>; Zantedeschi
et al<span class="ltx_text">.</span>, <a href="#bib.bib164" title="" class="ltx_ref">2020</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2021a</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2021a</a>; Marfoq et al<span class="ltx_text">.</span>, <a href="#bib.bib104" title="" class="ltx_ref">2021</a>; He et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2021</a>; Zhou
et al<span class="ltx_text">.</span>, <a href="#bib.bib178" title="" class="ltx_ref">2022</a>; Chen and Zhang, <a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>, transfer learning <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2019b</a>; Yu
et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2020</a>; Peterson
et al<span class="ltx_text">.</span>, <a href="#bib.bib117" title="" class="ltx_ref">2019</a>; Ozkara
et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2021</a>)</cite> and clustering <cite class="ltx_cite ltx_citemacro_citep">(Sattler
et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2020</a>; Ghosh
et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2020</a>, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib168" title="" class="ltx_ref">2020c</a>; Ruan and Joe-Wong, <a href="#bib.bib120" title="" class="ltx_ref">2022</a>; Lubana
et al<span class="ltx_text">.</span>, <a href="#bib.bib98" title="" class="ltx_ref">2022</a>)</cite> are incorporated to achieve our goal.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Secure federated learning.</span>
Although traditional federated learning has attempted to protect data privacy by only exchanging parameters of the local trained models, malicious attackers can still design some scheme to infer the properties of raw data. In our survey, we first summarize a series of attacks targeting federated learning, where we describe how backdoor attack <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Sun
et al<span class="ltx_text">.</span>, <a href="#bib.bib131" title="" class="ltx_ref">2019</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib142" title="" class="ltx_ref">2020a</a>; Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib151" title="" class="ltx_ref">2020</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib173" title="" class="ltx_ref">2022d</a>; Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib150" title="" class="ltx_ref">2021</a>; Ozdayi
et al<span class="ltx_text">.</span>, <a href="#bib.bib112" title="" class="ltx_ref">2021</a>)</cite>, gradients attack <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib181" title="" class="ltx_ref">2019</a>; Lam
et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2021</a>; Hitaj
et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2017</a>; Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib174" title="" class="ltx_ref">2020</a>; Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib157" title="" class="ltx_ref">2021</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2022b</a>; Zhu and Blaschko, <a href="#bib.bib180" title="" class="ltx_ref">2021</a>; Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2020</a>)</cite> and poison attack <cite class="ltx_cite ltx_citemacro_citep">(Bhagoji et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib130" title="" class="ltx_ref">2021</a>; Panda et al<span class="ltx_text">.</span>, <a href="#bib.bib115" title="" class="ltx_ref">2022</a>; Wu
et al<span class="ltx_text">.</span>, <a href="#bib.bib148" title="" class="ltx_ref">2022</a>)</cite> are applied to compromise federated learning. Then we introduce how to combine federated learning, differential privacy (DP) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a href="#bib.bib147" title="" class="ltx_ref">2020</a>; Geyer
et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2017</a>; McMahan
et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2018</a>; Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2021a</a>; Agarwal
et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>; Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib169" title="" class="ltx_ref">2022a</a>; Zheng
et al<span class="ltx_text">.</span>, <a href="#bib.bib177" title="" class="ltx_ref">2021</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2020b</a>; Girgis et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2021</a>)</cite>, homomorphic encryption (HE) <cite class="ltx_cite ltx_citemacro_citep">(Hardy et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2017</a>; Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib166" title="" class="ltx_ref">2020b</a>)</cite>, trusted execution environment (TEE) <cite class="ltx_cite ltx_citemacro_citep">(Mo et al<span class="ltx_text">.</span>, <a href="#bib.bib107" title="" class="ltx_ref">2021</a>, <a href="#bib.bib108" title="" class="ltx_ref">2020</a>)</cite> and other algorithms <cite class="ltx_cite ltx_citemacro_citep">(Baruch
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>; Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib150" title="" class="ltx_ref">2021</a>; Huang
et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2021b</a>; Tang
et al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2022</a>)</cite> to defend aforementioned attacks.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Fair federated learning.</span>
During federated learning, it is possible that the performance of the global model varies significantly across the devices, resulting in the fairness problem. This survey reviews literature about how to ensure fair federated learning, such as designing minimax optimization strategies <cite class="ltx_cite ltx_citemacro_citep">(Sharma
et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2022</a>; Tarzanagh et al<span class="ltx_text">.</span>, <a href="#bib.bib135" title="" class="ltx_ref">2022</a>)</cite> and sample reweighting approaches <cite class="ltx_cite ltx_citemacro_citep">(Zhao and Joshi, <a href="#bib.bib176" title="" class="ltx_ref">2022</a>; Enthoven and
Al-Ars, <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Aggregation optimization</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The goal of aggregation optimization is to improve the performance of the final global model, which is the core output in federated learning. There have been a large number of aggregation algorithms proposed to combine these local models to a better global one. In the following parts, we will describe in detail how different types of aggregation methods work.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Weight-level aggregation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.5" class="ltx_p">A typical and prevalent weight-level aggregation method called FedAvg <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2017</a>)</cite> is mostly adopted by developers. The key idea of FedAvg is to aggregate these local models in a coordinate-based weight averaging manner, which can be denoted as</p>
<table id="S6.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle W_{g}^{r}=\frac{1}{N}\sum_{k=1}^{N}w_{k}^{r}," display="inline"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msubsup id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.cmml">W</mi><mi id="S3.E3.m1.1.1.1.1.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">g</mi><mi id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml">r</mi></msubsup><mo id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><mstyle displaystyle="true" id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml"><mfrac id="S3.E3.m1.1.1.1.1.3.2a" xref="S3.E3.m1.1.1.1.1.3.2.cmml"><mn id="S3.E3.m1.1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.1.3.2.2.cmml">1</mn><mi id="S3.E3.m1.1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.1.3.2.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.3.1.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.3.3.cmml"><mstyle displaystyle="true" id="S3.E3.m1.1.1.1.1.3.3.1" xref="S3.E3.m1.1.1.1.1.3.3.1.cmml"><munderover id="S3.E3.m1.1.1.1.1.3.3.1a" xref="S3.E3.m1.1.1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S3.E3.m1.1.1.1.1.3.3.1.2.2" xref="S3.E3.m1.1.1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E3.m1.1.1.1.1.3.3.1.2.3" xref="S3.E3.m1.1.1.1.1.3.3.1.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.3.1.2.3.2" xref="S3.E3.m1.1.1.1.1.3.3.1.2.3.2.cmml">k</mi><mo id="S3.E3.m1.1.1.1.1.3.3.1.2.3.1" xref="S3.E3.m1.1.1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E3.m1.1.1.1.1.3.3.1.2.3.3" xref="S3.E3.m1.1.1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.1.1.1.1.3.3.1.3" xref="S3.E3.m1.1.1.1.1.3.3.1.3.cmml">N</mi></munderover></mstyle><msubsup id="S3.E3.m1.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.3.3.2.cmml"><mi id="S3.E3.m1.1.1.1.1.3.3.2.2.2" xref="S3.E3.m1.1.1.1.1.3.3.2.2.2.cmml">w</mi><mi id="S3.E3.m1.1.1.1.1.3.3.2.2.3" xref="S3.E3.m1.1.1.1.1.3.3.2.2.3.cmml">k</mi><mi id="S3.E3.m1.1.1.1.1.3.3.2.3" xref="S3.E3.m1.1.1.1.1.3.3.2.3.cmml">r</mi></msubsup></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"></eq><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2">𝑊</ci><ci id="S3.E3.m1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3">𝑔</ci></apply><ci id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3">𝑟</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><times id="S3.E3.m1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.1"></times><apply id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2"><divide id="S3.E3.m1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2"></divide><cn type="integer" id="S3.E3.m1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2">1</cn><ci id="S3.E3.m1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3">𝑁</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3"><apply id="S3.E3.m1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.1.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.3.3.1.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1">subscript</csymbol><sum id="S3.E3.m1.1.1.1.1.3.3.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.2.2"></sum><apply id="S3.E3.m1.1.1.1.1.3.3.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.2.3"><eq id="S3.E3.m1.1.1.1.1.3.3.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.2.3.1"></eq><ci id="S3.E3.m1.1.1.1.1.3.3.1.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.2.3.2">𝑘</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.3.3.1.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.1.1.1.1.3.3.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.3">𝑁</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.3.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2.2.2">𝑤</ci><ci id="S3.E3.m1.1.1.1.1.3.3.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2.2.3">𝑘</ci></apply><ci id="S3.E3.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2.3">𝑟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle W_{g}^{r}=\frac{1}{N}\sum_{k=1}^{N}w_{k}^{r},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p1.4" class="ltx_p">where N is the number of federated clients. <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="w_{k}" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><msub id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">w</mi><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">𝑤</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">w_{k}</annotation></semantics></math> denotes the weight parameters of the <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="k_{th}" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><msub id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml">k</mi><mrow id="S3.SS2.SSS1.p1.2.m2.1.1.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.2.m2.1.1.3.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2">𝑘</ci><apply id="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3"><times id="S3.SS2.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.2">𝑡</ci><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">k_{th}</annotation></semantics></math> client and <math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="W_{g}^{r}" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><msubsup id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.2.cmml">W</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.3.cmml">g</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml">r</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.2">𝑊</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.3">𝑔</ci></apply><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">W_{g}^{r}</annotation></semantics></math> is the final aggregated model at the <math id="S3.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="r_{th}" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><msub id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml">r</mi><mrow id="S3.SS2.SSS1.p1.4.m4.1.1.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.4.m4.1.1.3.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><apply id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2">𝑟</ci><apply id="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3"><times id="S3.SS2.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.1"></times><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.2">𝑡</ci><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">r_{th}</annotation></semantics></math> round. Researchers have shown the remarkable performance of FedAvg on a variety of public datasets (e.g., MNIST <cite class="ltx_cite ltx_citemacro_citep">(LeCun
et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">1998</a>)</cite> and CIFAR-10 <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky
et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2009</a>)</cite>) and provided some theoretical analyses to prove why FedAvg works well <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2020a</a>)</cite>.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.2" class="ltx_p">Despite being widely applied, FedAvg still suffers from the weight divergence problem <cite class="ltx_cite ltx_citemacro_citep">(Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib175" title="" class="ltx_ref">2018</a>)</cite>: the weight in the same coordinates (e.g., same layer or same filter) may have a large mismatching due to the highly skewed data distribution in each distinctive client/party. Therefore, directly averaging them will degrade the accuracy of the generated global model. To solve the issue, researchers leverage a particular DNN principle, <span id="S3.SS2.SSS1.p2.2.1" class="ltx_text ltx_font_italic">weight permutation invariance</span>, which has been mentioned and discussed by recent works <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2020b</a>; Yurochkin et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2018</a>, <a href="#bib.bib163" title="" class="ltx_ref">2019b</a>)</cite>. The key idea of this principle is that the weights in a DNN can be specially shuffled without incurring much accuracy drop. Concretely, suppose <math id="S3.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="l_{j}" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><msub id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p2.1.m1.1.1.2" xref="S3.SS2.SSS1.p2.1.m1.1.1.2.cmml">l</mi><mi id="S3.SS2.SSS1.p2.1.m1.1.1.3" xref="S3.SS2.SSS1.p2.1.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><apply id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1.2">𝑙</ci><ci id="S3.SS2.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">l_{j}</annotation></semantics></math> and <math id="S3.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="l_{j+1}" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.1a"><msub id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p2.2.m2.1.1.2" xref="S3.SS2.SSS1.p2.2.m2.1.1.2.cmml">l</mi><mrow id="S3.SS2.SSS1.p2.2.m2.1.1.3" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.2" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.2.cmml">j</mi><mo id="S3.SS2.SSS1.p2.2.m2.1.1.3.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">+</mo><mn id="S3.SS2.SSS1.p2.2.m2.1.1.3.3" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.1b"><apply id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.2">𝑙</ci><apply id="S3.SS2.SSS1.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3"><plus id="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1"></plus><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS2.SSS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.1c">l_{j+1}</annotation></semantics></math> are the weight of two continuous layers in a DNN model, where the output function can be denoted as</p>
<table id="S6.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle O_{j+1}=l_{j+1}l_{j}I," display="inline"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.2.2.cmml">O</mi><mrow id="S3.E4.m1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.2.3.2.cmml">j</mi><mo id="S3.E4.m1.1.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.1.2.3.1.cmml">+</mo><mn id="S3.E4.m1.1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><msub id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml"><mi id="S3.E4.m1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.1.1.3.2.2.cmml">l</mi><mrow id="S3.E4.m1.1.1.1.1.3.2.3" xref="S3.E4.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.2.3.2" xref="S3.E4.m1.1.1.1.1.3.2.3.2.cmml">j</mi><mo id="S3.E4.m1.1.1.1.1.3.2.3.1" xref="S3.E4.m1.1.1.1.1.3.2.3.1.cmml">+</mo><mn id="S3.E4.m1.1.1.1.1.3.2.3.3" xref="S3.E4.m1.1.1.1.1.3.2.3.3.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.3.1.cmml">​</mo><msub id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.2.cmml">l</mi><mi id="S3.E4.m1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.3.1a" xref="S3.E4.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.3.4" xref="S3.E4.m1.1.1.1.1.3.4.cmml">I</mi></mrow></mrow><mo id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"></eq><apply id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2">𝑂</ci><apply id="S3.E4.m1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.3"><plus id="S3.E4.m1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.3.1"></plus><ci id="S3.E4.m1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.3.2">𝑗</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><times id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.1"></times><apply id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2">𝑙</ci><apply id="S3.E4.m1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3"><plus id="S3.E4.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3.1"></plus><ci id="S3.E4.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3.2">𝑗</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3.3">1</cn></apply></apply><apply id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2">𝑙</ci><ci id="S3.E4.m1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3">𝑗</ci></apply><ci id="S3.E4.m1.1.1.1.1.3.4.cmml" xref="S3.E4.m1.1.1.1.1.3.4">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle O_{j+1}=l_{j+1}l_{j}I,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p2.6" class="ltx_p">where <math id="S3.SS2.SSS1.p2.3.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS2.SSS1.p2.3.m1.1a"><mi id="S3.SS2.SSS1.p2.3.m1.1.1" xref="S3.SS2.SSS1.p2.3.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m1.1b"><ci id="S3.SS2.SSS1.p2.3.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m1.1c">I</annotation></semantics></math> is the input and <math id="S3.SS2.SSS1.p2.4.m2.1" class="ltx_Math" alttext="O_{j+1}" display="inline"><semantics id="S3.SS2.SSS1.p2.4.m2.1a"><msub id="S3.SS2.SSS1.p2.4.m2.1.1" xref="S3.SS2.SSS1.p2.4.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p2.4.m2.1.1.2" xref="S3.SS2.SSS1.p2.4.m2.1.1.2.cmml">O</mi><mrow id="S3.SS2.SSS1.p2.4.m2.1.1.3" xref="S3.SS2.SSS1.p2.4.m2.1.1.3.cmml"><mi id="S3.SS2.SSS1.p2.4.m2.1.1.3.2" xref="S3.SS2.SSS1.p2.4.m2.1.1.3.2.cmml">j</mi><mo id="S3.SS2.SSS1.p2.4.m2.1.1.3.1" xref="S3.SS2.SSS1.p2.4.m2.1.1.3.1.cmml">+</mo><mn id="S3.SS2.SSS1.p2.4.m2.1.1.3.3" xref="S3.SS2.SSS1.p2.4.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.4.m2.1b"><apply id="S3.SS2.SSS1.p2.4.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.4.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.4.m2.1.1.2">𝑂</ci><apply id="S3.SS2.SSS1.p2.4.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p2.4.m2.1.1.3"><plus id="S3.SS2.SSS1.p2.4.m2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p2.4.m2.1.1.3.1"></plus><ci id="S3.SS2.SSS1.p2.4.m2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p2.4.m2.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS2.SSS1.p2.4.m2.1.1.3.3.cmml" xref="S3.SS2.SSS1.p2.4.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.4.m2.1c">O_{j+1}</annotation></semantics></math> is the output of the <math id="S3.SS2.SSS1.p2.5.m3.1" class="ltx_Math" alttext="{j+1}_{th}" display="inline"><semantics id="S3.SS2.SSS1.p2.5.m3.1a"><mrow id="S3.SS2.SSS1.p2.5.m3.1.1" xref="S3.SS2.SSS1.p2.5.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p2.5.m3.1.1.2" xref="S3.SS2.SSS1.p2.5.m3.1.1.2.cmml">j</mi><mo id="S3.SS2.SSS1.p2.5.m3.1.1.1" xref="S3.SS2.SSS1.p2.5.m3.1.1.1.cmml">+</mo><msub id="S3.SS2.SSS1.p2.5.m3.1.1.3" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.cmml"><mn id="S3.SS2.SSS1.p2.5.m3.1.1.3.2" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.2.cmml">1</mn><mrow id="S3.SS2.SSS1.p2.5.m3.1.1.3.3" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.3.cmml"><mi id="S3.SS2.SSS1.p2.5.m3.1.1.3.3.2" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.5.m3.1.1.3.3.1" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.5.m3.1.1.3.3.3" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.3.3.cmml">h</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.5.m3.1b"><apply id="S3.SS2.SSS1.p2.5.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.5.m3.1.1"><plus id="S3.SS2.SSS1.p2.5.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p2.5.m3.1.1.1"></plus><ci id="S3.SS2.SSS1.p2.5.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p2.5.m3.1.1.2">𝑗</ci><apply id="S3.SS2.SSS1.p2.5.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p2.5.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.5.m3.1.1.3.1.cmml" xref="S3.SS2.SSS1.p2.5.m3.1.1.3">subscript</csymbol><cn type="integer" id="S3.SS2.SSS1.p2.5.m3.1.1.3.2.cmml" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.2">1</cn><apply id="S3.SS2.SSS1.p2.5.m3.1.1.3.3.cmml" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.3"><times id="S3.SS2.SSS1.p2.5.m3.1.1.3.3.1.cmml" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.3.1"></times><ci id="S3.SS2.SSS1.p2.5.m3.1.1.3.3.2.cmml" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.3.2">𝑡</ci><ci id="S3.SS2.SSS1.p2.5.m3.1.1.3.3.3.cmml" xref="S3.SS2.SSS1.p2.5.m3.1.1.3.3.3">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.5.m3.1c">{j+1}_{th}</annotation></semantics></math> layer. Note that for each weight matrix <math id="S3.SS2.SSS1.p2.6.m4.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS2.SSS1.p2.6.m4.1a"><mi id="S3.SS2.SSS1.p2.6.m4.1.1" xref="S3.SS2.SSS1.p2.6.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.6.m4.1b"><ci id="S3.SS2.SSS1.p2.6.m4.1.1.cmml" xref="S3.SS2.SSS1.p2.6.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.6.m4.1c">l</annotation></semantics></math>, it can be further decomposed as follows</p>
<table id="S6.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle l=l\mathbf{1}=l\Pi\Pi^{T}," display="inline"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.cmml">l</mi><mo id="S3.E5.m1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.4" xref="S3.E5.m1.1.1.1.1.4.cmml"><mi id="S3.E5.m1.1.1.1.1.4.2" xref="S3.E5.m1.1.1.1.1.4.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.4.1" xref="S3.E5.m1.1.1.1.1.4.1.cmml">​</mo><mn id="S3.E5.m1.1.1.1.1.4.3" xref="S3.E5.m1.1.1.1.1.4.3.cmml">𝟏</mn></mrow><mo id="S3.E5.m1.1.1.1.1.5" xref="S3.E5.m1.1.1.1.1.5.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.6" xref="S3.E5.m1.1.1.1.1.6.cmml"><mi id="S3.E5.m1.1.1.1.1.6.2" xref="S3.E5.m1.1.1.1.1.6.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.6.1" xref="S3.E5.m1.1.1.1.1.6.1.cmml">​</mo><mi mathvariant="normal" id="S3.E5.m1.1.1.1.1.6.3" xref="S3.E5.m1.1.1.1.1.6.3.cmml">Π</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.6.1a" xref="S3.E5.m1.1.1.1.1.6.1.cmml">​</mo><msup id="S3.E5.m1.1.1.1.1.6.4" xref="S3.E5.m1.1.1.1.1.6.4.cmml"><mi mathvariant="normal" id="S3.E5.m1.1.1.1.1.6.4.2" xref="S3.E5.m1.1.1.1.1.6.4.2.cmml">Π</mi><mi id="S3.E5.m1.1.1.1.1.6.4.3" xref="S3.E5.m1.1.1.1.1.6.4.3.cmml">T</mi></msup></mrow></mrow><mo id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><and id="S3.E5.m1.1.1.1.1a.cmml" xref="S3.E5.m1.1.1.1"></and><apply id="S3.E5.m1.1.1.1.1b.cmml" xref="S3.E5.m1.1.1.1"><eq id="S3.E5.m1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.3"></eq><ci id="S3.E5.m1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2">𝑙</ci><apply id="S3.E5.m1.1.1.1.1.4.cmml" xref="S3.E5.m1.1.1.1.1.4"><times id="S3.E5.m1.1.1.1.1.4.1.cmml" xref="S3.E5.m1.1.1.1.1.4.1"></times><ci id="S3.E5.m1.1.1.1.1.4.2.cmml" xref="S3.E5.m1.1.1.1.1.4.2">𝑙</ci><cn type="integer" id="S3.E5.m1.1.1.1.1.4.3.cmml" xref="S3.E5.m1.1.1.1.1.4.3">1</cn></apply></apply><apply id="S3.E5.m1.1.1.1.1c.cmml" xref="S3.E5.m1.1.1.1"><eq id="S3.E5.m1.1.1.1.1.5.cmml" xref="S3.E5.m1.1.1.1.1.5"></eq><share href="#S3.E5.m1.1.1.1.1.4.cmml" id="S3.E5.m1.1.1.1.1d.cmml" xref="S3.E5.m1.1.1.1"></share><apply id="S3.E5.m1.1.1.1.1.6.cmml" xref="S3.E5.m1.1.1.1.1.6"><times id="S3.E5.m1.1.1.1.1.6.1.cmml" xref="S3.E5.m1.1.1.1.1.6.1"></times><ci id="S3.E5.m1.1.1.1.1.6.2.cmml" xref="S3.E5.m1.1.1.1.1.6.2">𝑙</ci><ci id="S3.E5.m1.1.1.1.1.6.3.cmml" xref="S3.E5.m1.1.1.1.1.6.3">Π</ci><apply id="S3.E5.m1.1.1.1.1.6.4.cmml" xref="S3.E5.m1.1.1.1.1.6.4"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.6.4.1.cmml" xref="S3.E5.m1.1.1.1.1.6.4">superscript</csymbol><ci id="S3.E5.m1.1.1.1.1.6.4.2.cmml" xref="S3.E5.m1.1.1.1.1.6.4.2">Π</ci><ci id="S3.E5.m1.1.1.1.1.6.4.3.cmml" xref="S3.E5.m1.1.1.1.1.6.4.3">𝑇</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle l=l\mathbf{1}=l\Pi\Pi^{T},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p2.7" class="ltx_p">where <math id="S3.SS2.SSS1.p2.7.m1.1" class="ltx_Math" alttext="\Pi" display="inline"><semantics id="S3.SS2.SSS1.p2.7.m1.1a"><mi mathvariant="normal" id="S3.SS2.SSS1.p2.7.m1.1.1" xref="S3.SS2.SSS1.p2.7.m1.1.1.cmml">Π</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.7.m1.1b"><ci id="S3.SS2.SSS1.p2.7.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.7.m1.1.1">Π</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.7.m1.1c">\Pi</annotation></semantics></math> represents the permutation matrix. In terms of this equation, we can transform Eq. <a href="#S3.E4" title="In 3.2.1. Weight-level aggregation ‣ 3.2. Aggregation optimization ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> to the following form</p>
<table id="S6.EGx6" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\displaystyle O_{j+1}=(l_{j+1}\Pi_{j+1}\Pi^{T}_{j+1})l_{j}I=(l_{j+1}\Pi_{j+1})(\Pi^{T}_{j+1}l_{j})I," display="inline"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.5" xref="S3.E6.m1.1.1.1.1.5.cmml"><mi id="S3.E6.m1.1.1.1.1.5.2" xref="S3.E6.m1.1.1.1.1.5.2.cmml">O</mi><mrow id="S3.E6.m1.1.1.1.1.5.3" xref="S3.E6.m1.1.1.1.1.5.3.cmml"><mi id="S3.E6.m1.1.1.1.1.5.3.2" xref="S3.E6.m1.1.1.1.1.5.3.2.cmml">j</mi><mo id="S3.E6.m1.1.1.1.1.5.3.1" xref="S3.E6.m1.1.1.1.1.5.3.1.cmml">+</mo><mn id="S3.E6.m1.1.1.1.1.5.3.3" xref="S3.E6.m1.1.1.1.1.5.3.3.cmml">1</mn></mrow></msub><mo id="S3.E6.m1.1.1.1.1.6" xref="S3.E6.m1.1.1.1.1.6.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.cmml">l</mi><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">j</mi><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">+</mo><mn id="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E6.m1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E6.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.cmml">Π</mi><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">j</mi><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">+</mo><mn id="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1a" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msubsup id="S3.E6.m1.1.1.1.1.1.1.1.1.4" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.cmml"><mi mathvariant="normal" id="S3.E6.m1.1.1.1.1.1.1.1.1.4.2.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.2.2.cmml">Π</mi><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.4.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.2.cmml">j</mi><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.1.cmml">+</mo><mn id="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.3.cmml">1</mn></mrow><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.4.2.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.2.3.cmml">T</mi></msubsup></mrow><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.2.cmml">​</mo><msub id="S3.E6.m1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.1.3.2.cmml">l</mi><mi id="S3.E6.m1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.3.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.2a" xref="S3.E6.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.1.4" xref="S3.E6.m1.1.1.1.1.1.4.cmml">I</mi></mrow><mo id="S3.E6.m1.1.1.1.1.7" xref="S3.E6.m1.1.1.1.1.7.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml"><mrow id="S3.E6.m1.1.1.1.1.2.1.1" xref="S3.E6.m1.1.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.1.1.1.1.2.1.1.2" xref="S3.E6.m1.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.2.1.1.1" xref="S3.E6.m1.1.1.1.1.2.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.2.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.cmml"><mi id="S3.E6.m1.1.1.1.1.2.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.2.cmml">l</mi><mrow id="S3.E6.m1.1.1.1.1.2.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.2" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.2.cmml">j</mi><mo id="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.1" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.1.cmml">+</mo><mn id="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.3" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.2.1.1.1.1" xref="S3.E6.m1.1.1.1.1.2.1.1.1.1.cmml">​</mo><msub id="S3.E6.m1.1.1.1.1.2.1.1.1.3" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E6.m1.1.1.1.1.2.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.2.cmml">Π</mi><mrow id="S3.E6.m1.1.1.1.1.2.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.2" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.2.cmml">j</mi><mo id="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.1" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.1.cmml">+</mo><mn id="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.3" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><mo stretchy="false" id="S3.E6.m1.1.1.1.1.2.1.1.3" xref="S3.E6.m1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.3.3.cmml">​</mo><mrow id="S3.E6.m1.1.1.1.1.3.2.1" xref="S3.E6.m1.1.1.1.1.3.2.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.1.1.1.1.3.2.1.2" xref="S3.E6.m1.1.1.1.1.3.2.1.1.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.3.2.1.1" xref="S3.E6.m1.1.1.1.1.3.2.1.1.cmml"><msubsup id="S3.E6.m1.1.1.1.1.3.2.1.1.2" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.cmml"><mi mathvariant="normal" id="S3.E6.m1.1.1.1.1.3.2.1.1.2.2.2" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.2.2.cmml">Π</mi><mrow id="S3.E6.m1.1.1.1.1.3.2.1.1.2.3" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.2" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.2.cmml">j</mi><mo id="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.1" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.1.cmml">+</mo><mn id="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.3" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E6.m1.1.1.1.1.3.2.1.1.2.2.3" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.2.1.1.1" xref="S3.E6.m1.1.1.1.1.3.2.1.1.1.cmml">​</mo><msub id="S3.E6.m1.1.1.1.1.3.2.1.1.3" xref="S3.E6.m1.1.1.1.1.3.2.1.1.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.2.1.1.3.2" xref="S3.E6.m1.1.1.1.1.3.2.1.1.3.2.cmml">l</mi><mi id="S3.E6.m1.1.1.1.1.3.2.1.1.3.3" xref="S3.E6.m1.1.1.1.1.3.2.1.1.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S3.E6.m1.1.1.1.1.3.2.1.3" xref="S3.E6.m1.1.1.1.1.3.2.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.3a" xref="S3.E6.m1.1.1.1.1.3.3.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.3.4" xref="S3.E6.m1.1.1.1.1.3.4.cmml">I</mi></mrow></mrow><mo id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><and id="S3.E6.m1.1.1.1.1a.cmml" xref="S3.E6.m1.1.1.1"></and><apply id="S3.E6.m1.1.1.1.1b.cmml" xref="S3.E6.m1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.6.cmml" xref="S3.E6.m1.1.1.1.1.6"></eq><apply id="S3.E6.m1.1.1.1.1.5.cmml" xref="S3.E6.m1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.5.1.cmml" xref="S3.E6.m1.1.1.1.1.5">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.5.2.cmml" xref="S3.E6.m1.1.1.1.1.5.2">𝑂</ci><apply id="S3.E6.m1.1.1.1.1.5.3.cmml" xref="S3.E6.m1.1.1.1.1.5.3"><plus id="S3.E6.m1.1.1.1.1.5.3.1.cmml" xref="S3.E6.m1.1.1.1.1.5.3.1"></plus><ci id="S3.E6.m1.1.1.1.1.5.3.2.cmml" xref="S3.E6.m1.1.1.1.1.5.3.2">𝑗</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.5.3.3.cmml" xref="S3.E6.m1.1.1.1.1.5.3.3">1</cn></apply></apply><apply id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"><times id="S3.E6.m1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.2"></times><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1"><times id="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.2">𝑙</ci><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.3"><plus id="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.1"></plus><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.2">𝑗</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.2">Π</ci><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.3"><plus id="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.1"></plus><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.2">𝑗</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4">subscript</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.4.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.2.2">Π</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.4.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.2.3">𝑇</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.3"><plus id="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.1"></plus><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.2">𝑗</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.4.3.3">1</cn></apply></apply></apply><apply id="S3.E6.m1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.3.2">𝑙</ci><ci id="S3.E6.m1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.3.3">𝑗</ci></apply><ci id="S3.E6.m1.1.1.1.1.1.4.cmml" xref="S3.E6.m1.1.1.1.1.1.4">𝐼</ci></apply></apply><apply id="S3.E6.m1.1.1.1.1c.cmml" xref="S3.E6.m1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.7.cmml" xref="S3.E6.m1.1.1.1.1.7"></eq><share href="#S3.E6.m1.1.1.1.1.1.cmml" id="S3.E6.m1.1.1.1.1d.cmml" xref="S3.E6.m1.1.1.1"></share><apply id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3"><times id="S3.E6.m1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3"></times><apply id="S3.E6.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1"><times id="S3.E6.m1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.1"></times><apply id="S3.E6.m1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.2.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.2">𝑙</ci><apply id="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.3"><plus id="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.1"></plus><ci id="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.2">𝑗</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.3.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E6.m1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.2.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.2">Π</ci><apply id="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.3"><plus id="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.1"></plus><ci id="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.2">𝑗</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.2.1.1.1.3.3.3">1</cn></apply></apply></apply><apply id="S3.E6.m1.1.1.1.1.3.2.1.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1"><times id="S3.E6.m1.1.1.1.1.3.2.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.1"></times><apply id="S3.E6.m1.1.1.1.1.3.2.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.2.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2">subscript</csymbol><apply id="S3.E6.m1.1.1.1.1.3.2.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.2.1.1.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.2.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.2.2">Π</ci><ci id="S3.E6.m1.1.1.1.1.3.2.1.1.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.2.3">𝑇</ci></apply><apply id="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.3"><plus id="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.1"></plus><ci id="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.2">𝑗</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E6.m1.1.1.1.1.3.2.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.2.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.2.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.3.2">𝑙</ci><ci id="S3.E6.m1.1.1.1.1.3.2.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.1.1.3.3">𝑗</ci></apply></apply><ci id="S3.E6.m1.1.1.1.1.3.4.cmml" xref="S3.E6.m1.1.1.1.1.3.4">𝐼</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\displaystyle O_{j+1}=(l_{j+1}\Pi_{j+1}\Pi^{T}_{j+1})l_{j}I=(l_{j+1}\Pi_{j+1})(\Pi^{T}_{j+1}l_{j})I,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p2.8" class="ltx_p">Based on Eq. <a href="#S3.E6" title="In 3.2.1. Weight-level aggregation ‣ 3.2. Aggregation optimization ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we can clearly see that the original layer weight can be losslessly transformed with a pair of well-designed permutation matrices, which we call it <span id="S3.SS2.SSS1.p2.8.1" class="ltx_text ltx_font_italic">weight permutation invariance</span>.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">In federated learning, traditional aggregation methods fuse local models according to their weight location, which may be sub-optimal since the <span id="S3.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_italic">weight permutation invariance</span> principle indicates that we can change the weight value in a specific location while ensuring the same performance. Thus, the location-based aggregation cannot achieve accurate knowledge fusion, leading to the weight mismatching problem.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2301.01299/assets/PFNM.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="398" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>The illustration of PFNM <cite class="ltx_cite ltx_citemacro_citep">(Yurochkin et al<span class="ltx_text">.</span>, <a href="#bib.bib163" title="" class="ltx_ref">2019b</a>)</cite>.</figcaption>
</figure>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p">To address this problem, a large number of federated optimation works attempt to achieve weight-level alignment. For example, Yurochkin <span id="S3.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Yurochkin et al<span class="ltx_text">.</span>, <a href="#bib.bib163" title="" class="ltx_ref">2019b</a>)</cite> developed Probabilistic Federated Neural Matching (PFNM). As shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2.1. Weight-level aggregation ‣ 3.2. Aggregation optimization ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the key idea is to identify subsets of
neurons in each local model that matches neurons in other local models and then combine the matched neurons to an improved global model by leveraging Bayesian nonparametric machinery. For single-layer neural matching, they presented a Beta Bernoulli Process <cite class="ltx_cite ltx_citemacro_citep">(Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib170" title="" class="ltx_ref">2013</a>)</cite> based model of MLP weight parameters, where the corresponding neurons in the
output layer are used to convert the neurons in each batch and form a cost matrix. Then the matched neurons can be
aggregated to generate the final global model. For multilayer neural matching, they extended the single strategy by defining a generative
model of deep neural network weights from outputs back to inputs. In this way, they could adopt a greedy inference procedure that first infers the matching of the top layer and then proceeds down the layers of the model.</p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para">
<p id="S3.SS2.SSS1.p5.6" class="ltx_p">Unfortunately, PFNM only performs well on simple architectures (e.g. fully connected feedforward networks). For more complex CNNs and LSTMs, it just receives minor improvements over location-based methods (e.g., FedAvg). To further achieve the weight alignment goal, Wang <span id="S3.SS2.SSS1.p5.6.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2020b</a>)</cite> proposed Federated Matched Averaging (FedMA) to effectively align advanced CNNs and LSTMs in a layer-wise manner. The key idea is to search for the best permutation matrices by addressing the following optimization problem</p>
<table id="S6.EGx7" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E7.m2.5" class="ltx_Math" alttext="\displaystyle\min_{\left\{\pi_{li}^{j}\right\}}\sum_{i=1}^{L}\sum_{j,l}\min_{\theta_{i}}\pi_{li}^{j}c\left(w_{jl},\theta_{i}\right)" display="inline"><semantics id="S3.E7.m2.5a"><mrow id="S3.E7.m2.5.5" xref="S3.E7.m2.5.5.cmml"><munder id="S3.E7.m2.5.5.4" xref="S3.E7.m2.5.5.4.cmml"><mi id="S3.E7.m2.5.5.4.2" xref="S3.E7.m2.5.5.4.2.cmml">min</mi><mrow id="S3.E7.m2.1.1.1.1" xref="S3.E7.m2.1.1.1.2.cmml"><mo id="S3.E7.m2.1.1.1.1.2" xref="S3.E7.m2.1.1.1.2.cmml">{</mo><msubsup id="S3.E7.m2.1.1.1.1.1" xref="S3.E7.m2.1.1.1.1.1.cmml"><mi id="S3.E7.m2.1.1.1.1.1.2.2" xref="S3.E7.m2.1.1.1.1.1.2.2.cmml">π</mi><mrow id="S3.E7.m2.1.1.1.1.1.2.3" xref="S3.E7.m2.1.1.1.1.1.2.3.cmml"><mi id="S3.E7.m2.1.1.1.1.1.2.3.2" xref="S3.E7.m2.1.1.1.1.1.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E7.m2.1.1.1.1.1.2.3.1" xref="S3.E7.m2.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E7.m2.1.1.1.1.1.2.3.3" xref="S3.E7.m2.1.1.1.1.1.2.3.3.cmml">i</mi></mrow><mi id="S3.E7.m2.1.1.1.1.1.3" xref="S3.E7.m2.1.1.1.1.1.3.cmml">j</mi></msubsup><mo id="S3.E7.m2.1.1.1.1.3" xref="S3.E7.m2.1.1.1.2.cmml">}</mo></mrow></munder><mo lspace="0.167em" rspace="0em" id="S3.E7.m2.5.5.3" xref="S3.E7.m2.5.5.3.cmml">​</mo><mrow id="S3.E7.m2.5.5.2" xref="S3.E7.m2.5.5.2.cmml"><mstyle displaystyle="true" id="S3.E7.m2.5.5.2.3" xref="S3.E7.m2.5.5.2.3.cmml"><munderover id="S3.E7.m2.5.5.2.3a" xref="S3.E7.m2.5.5.2.3.cmml"><mo movablelimits="false" id="S3.E7.m2.5.5.2.3.2.2" xref="S3.E7.m2.5.5.2.3.2.2.cmml">∑</mo><mrow id="S3.E7.m2.5.5.2.3.2.3" xref="S3.E7.m2.5.5.2.3.2.3.cmml"><mi id="S3.E7.m2.5.5.2.3.2.3.2" xref="S3.E7.m2.5.5.2.3.2.3.2.cmml">i</mi><mo id="S3.E7.m2.5.5.2.3.2.3.1" xref="S3.E7.m2.5.5.2.3.2.3.1.cmml">=</mo><mn id="S3.E7.m2.5.5.2.3.2.3.3" xref="S3.E7.m2.5.5.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E7.m2.5.5.2.3.3" xref="S3.E7.m2.5.5.2.3.3.cmml">L</mi></munderover></mstyle><mrow id="S3.E7.m2.5.5.2.2" xref="S3.E7.m2.5.5.2.2.cmml"><mstyle displaystyle="true" id="S3.E7.m2.5.5.2.2.3" xref="S3.E7.m2.5.5.2.2.3.cmml"><munder id="S3.E7.m2.5.5.2.2.3a" xref="S3.E7.m2.5.5.2.2.3.cmml"><mo movablelimits="false" id="S3.E7.m2.5.5.2.2.3.2" xref="S3.E7.m2.5.5.2.2.3.2.cmml">∑</mo><mrow id="S3.E7.m2.3.3.2.4" xref="S3.E7.m2.3.3.2.3.cmml"><mi id="S3.E7.m2.2.2.1.1" xref="S3.E7.m2.2.2.1.1.cmml">j</mi><mo id="S3.E7.m2.3.3.2.4.1" xref="S3.E7.m2.3.3.2.3.cmml">,</mo><mi id="S3.E7.m2.3.3.2.2" xref="S3.E7.m2.3.3.2.2.cmml">l</mi></mrow></munder></mstyle><mrow id="S3.E7.m2.5.5.2.2.2" xref="S3.E7.m2.5.5.2.2.2.cmml"><mrow id="S3.E7.m2.5.5.2.2.2.4" xref="S3.E7.m2.5.5.2.2.2.4.cmml"><munder id="S3.E7.m2.5.5.2.2.2.4.1" xref="S3.E7.m2.5.5.2.2.2.4.1.cmml"><mi id="S3.E7.m2.5.5.2.2.2.4.1.2" xref="S3.E7.m2.5.5.2.2.2.4.1.2.cmml">min</mi><msub id="S3.E7.m2.5.5.2.2.2.4.1.3" xref="S3.E7.m2.5.5.2.2.2.4.1.3.cmml"><mi id="S3.E7.m2.5.5.2.2.2.4.1.3.2" xref="S3.E7.m2.5.5.2.2.2.4.1.3.2.cmml">θ</mi><mi id="S3.E7.m2.5.5.2.2.2.4.1.3.3" xref="S3.E7.m2.5.5.2.2.2.4.1.3.3.cmml">i</mi></msub></munder><mo lspace="0.167em" id="S3.E7.m2.5.5.2.2.2.4a" xref="S3.E7.m2.5.5.2.2.2.4.cmml">⁡</mo><mrow id="S3.E7.m2.5.5.2.2.2.4.2" xref="S3.E7.m2.5.5.2.2.2.4.2.cmml"><msubsup id="S3.E7.m2.5.5.2.2.2.4.2.2" xref="S3.E7.m2.5.5.2.2.2.4.2.2.cmml"><mi id="S3.E7.m2.5.5.2.2.2.4.2.2.2.2" xref="S3.E7.m2.5.5.2.2.2.4.2.2.2.2.cmml">π</mi><mrow id="S3.E7.m2.5.5.2.2.2.4.2.2.2.3" xref="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.cmml"><mi id="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.2" xref="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.1" xref="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.1.cmml">​</mo><mi id="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.3" xref="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.3.cmml">i</mi></mrow><mi id="S3.E7.m2.5.5.2.2.2.4.2.2.3" xref="S3.E7.m2.5.5.2.2.2.4.2.2.3.cmml">j</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.E7.m2.5.5.2.2.2.4.2.1" xref="S3.E7.m2.5.5.2.2.2.4.2.1.cmml">​</mo><mi id="S3.E7.m2.5.5.2.2.2.4.2.3" xref="S3.E7.m2.5.5.2.2.2.4.2.3.cmml">c</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E7.m2.5.5.2.2.2.3" xref="S3.E7.m2.5.5.2.2.2.3.cmml">​</mo><mrow id="S3.E7.m2.5.5.2.2.2.2.2" xref="S3.E7.m2.5.5.2.2.2.2.3.cmml"><mo id="S3.E7.m2.5.5.2.2.2.2.2.3" xref="S3.E7.m2.5.5.2.2.2.2.3.cmml">(</mo><msub id="S3.E7.m2.4.4.1.1.1.1.1.1" xref="S3.E7.m2.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E7.m2.4.4.1.1.1.1.1.1.2" xref="S3.E7.m2.4.4.1.1.1.1.1.1.2.cmml">w</mi><mrow id="S3.E7.m2.4.4.1.1.1.1.1.1.3" xref="S3.E7.m2.4.4.1.1.1.1.1.1.3.cmml"><mi id="S3.E7.m2.4.4.1.1.1.1.1.1.3.2" xref="S3.E7.m2.4.4.1.1.1.1.1.1.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E7.m2.4.4.1.1.1.1.1.1.3.1" xref="S3.E7.m2.4.4.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E7.m2.4.4.1.1.1.1.1.1.3.3" xref="S3.E7.m2.4.4.1.1.1.1.1.1.3.3.cmml">l</mi></mrow></msub><mo id="S3.E7.m2.5.5.2.2.2.2.2.4" xref="S3.E7.m2.5.5.2.2.2.2.3.cmml">,</mo><msub id="S3.E7.m2.5.5.2.2.2.2.2.2" xref="S3.E7.m2.5.5.2.2.2.2.2.2.cmml"><mi id="S3.E7.m2.5.5.2.2.2.2.2.2.2" xref="S3.E7.m2.5.5.2.2.2.2.2.2.2.cmml">θ</mi><mi id="S3.E7.m2.5.5.2.2.2.2.2.2.3" xref="S3.E7.m2.5.5.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E7.m2.5.5.2.2.2.2.2.5" xref="S3.E7.m2.5.5.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m2.5b"><apply id="S3.E7.m2.5.5.cmml" xref="S3.E7.m2.5.5"><times id="S3.E7.m2.5.5.3.cmml" xref="S3.E7.m2.5.5.3"></times><apply id="S3.E7.m2.5.5.4.cmml" xref="S3.E7.m2.5.5.4"><csymbol cd="ambiguous" id="S3.E7.m2.5.5.4.1.cmml" xref="S3.E7.m2.5.5.4">subscript</csymbol><min id="S3.E7.m2.5.5.4.2.cmml" xref="S3.E7.m2.5.5.4.2"></min><set id="S3.E7.m2.1.1.1.2.cmml" xref="S3.E7.m2.1.1.1.1"><apply id="S3.E7.m2.1.1.1.1.1.cmml" xref="S3.E7.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m2.1.1.1.1.1.1.cmml" xref="S3.E7.m2.1.1.1.1.1">superscript</csymbol><apply id="S3.E7.m2.1.1.1.1.1.2.cmml" xref="S3.E7.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m2.1.1.1.1.1.2.1.cmml" xref="S3.E7.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.E7.m2.1.1.1.1.1.2.2.cmml" xref="S3.E7.m2.1.1.1.1.1.2.2">𝜋</ci><apply id="S3.E7.m2.1.1.1.1.1.2.3.cmml" xref="S3.E7.m2.1.1.1.1.1.2.3"><times id="S3.E7.m2.1.1.1.1.1.2.3.1.cmml" xref="S3.E7.m2.1.1.1.1.1.2.3.1"></times><ci id="S3.E7.m2.1.1.1.1.1.2.3.2.cmml" xref="S3.E7.m2.1.1.1.1.1.2.3.2">𝑙</ci><ci id="S3.E7.m2.1.1.1.1.1.2.3.3.cmml" xref="S3.E7.m2.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><ci id="S3.E7.m2.1.1.1.1.1.3.cmml" xref="S3.E7.m2.1.1.1.1.1.3">𝑗</ci></apply></set></apply><apply id="S3.E7.m2.5.5.2.cmml" xref="S3.E7.m2.5.5.2"><apply id="S3.E7.m2.5.5.2.3.cmml" xref="S3.E7.m2.5.5.2.3"><csymbol cd="ambiguous" id="S3.E7.m2.5.5.2.3.1.cmml" xref="S3.E7.m2.5.5.2.3">superscript</csymbol><apply id="S3.E7.m2.5.5.2.3.2.cmml" xref="S3.E7.m2.5.5.2.3"><csymbol cd="ambiguous" id="S3.E7.m2.5.5.2.3.2.1.cmml" xref="S3.E7.m2.5.5.2.3">subscript</csymbol><sum id="S3.E7.m2.5.5.2.3.2.2.cmml" xref="S3.E7.m2.5.5.2.3.2.2"></sum><apply id="S3.E7.m2.5.5.2.3.2.3.cmml" xref="S3.E7.m2.5.5.2.3.2.3"><eq id="S3.E7.m2.5.5.2.3.2.3.1.cmml" xref="S3.E7.m2.5.5.2.3.2.3.1"></eq><ci id="S3.E7.m2.5.5.2.3.2.3.2.cmml" xref="S3.E7.m2.5.5.2.3.2.3.2">𝑖</ci><cn type="integer" id="S3.E7.m2.5.5.2.3.2.3.3.cmml" xref="S3.E7.m2.5.5.2.3.2.3.3">1</cn></apply></apply><ci id="S3.E7.m2.5.5.2.3.3.cmml" xref="S3.E7.m2.5.5.2.3.3">𝐿</ci></apply><apply id="S3.E7.m2.5.5.2.2.cmml" xref="S3.E7.m2.5.5.2.2"><apply id="S3.E7.m2.5.5.2.2.3.cmml" xref="S3.E7.m2.5.5.2.2.3"><csymbol cd="ambiguous" id="S3.E7.m2.5.5.2.2.3.1.cmml" xref="S3.E7.m2.5.5.2.2.3">subscript</csymbol><sum id="S3.E7.m2.5.5.2.2.3.2.cmml" xref="S3.E7.m2.5.5.2.2.3.2"></sum><list id="S3.E7.m2.3.3.2.3.cmml" xref="S3.E7.m2.3.3.2.4"><ci id="S3.E7.m2.2.2.1.1.cmml" xref="S3.E7.m2.2.2.1.1">𝑗</ci><ci id="S3.E7.m2.3.3.2.2.cmml" xref="S3.E7.m2.3.3.2.2">𝑙</ci></list></apply><apply id="S3.E7.m2.5.5.2.2.2.cmml" xref="S3.E7.m2.5.5.2.2.2"><times id="S3.E7.m2.5.5.2.2.2.3.cmml" xref="S3.E7.m2.5.5.2.2.2.3"></times><apply id="S3.E7.m2.5.5.2.2.2.4.cmml" xref="S3.E7.m2.5.5.2.2.2.4"><apply id="S3.E7.m2.5.5.2.2.2.4.1.cmml" xref="S3.E7.m2.5.5.2.2.2.4.1"><csymbol cd="ambiguous" id="S3.E7.m2.5.5.2.2.2.4.1.1.cmml" xref="S3.E7.m2.5.5.2.2.2.4.1">subscript</csymbol><min id="S3.E7.m2.5.5.2.2.2.4.1.2.cmml" xref="S3.E7.m2.5.5.2.2.2.4.1.2"></min><apply id="S3.E7.m2.5.5.2.2.2.4.1.3.cmml" xref="S3.E7.m2.5.5.2.2.2.4.1.3"><csymbol cd="ambiguous" id="S3.E7.m2.5.5.2.2.2.4.1.3.1.cmml" xref="S3.E7.m2.5.5.2.2.2.4.1.3">subscript</csymbol><ci id="S3.E7.m2.5.5.2.2.2.4.1.3.2.cmml" xref="S3.E7.m2.5.5.2.2.2.4.1.3.2">𝜃</ci><ci id="S3.E7.m2.5.5.2.2.2.4.1.3.3.cmml" xref="S3.E7.m2.5.5.2.2.2.4.1.3.3">𝑖</ci></apply></apply><apply id="S3.E7.m2.5.5.2.2.2.4.2.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2"><times id="S3.E7.m2.5.5.2.2.2.4.2.1.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.1"></times><apply id="S3.E7.m2.5.5.2.2.2.4.2.2.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.2"><csymbol cd="ambiguous" id="S3.E7.m2.5.5.2.2.2.4.2.2.1.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.2">superscript</csymbol><apply id="S3.E7.m2.5.5.2.2.2.4.2.2.2.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.2"><csymbol cd="ambiguous" id="S3.E7.m2.5.5.2.2.2.4.2.2.2.1.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.2">subscript</csymbol><ci id="S3.E7.m2.5.5.2.2.2.4.2.2.2.2.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.2.2.2">𝜋</ci><apply id="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.2.2.3"><times id="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.1.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.1"></times><ci id="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.2.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.2">𝑙</ci><ci id="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.3.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.2.2.3.3">𝑖</ci></apply></apply><ci id="S3.E7.m2.5.5.2.2.2.4.2.2.3.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.2.3">𝑗</ci></apply><ci id="S3.E7.m2.5.5.2.2.2.4.2.3.cmml" xref="S3.E7.m2.5.5.2.2.2.4.2.3">𝑐</ci></apply></apply><interval closure="open" id="S3.E7.m2.5.5.2.2.2.2.3.cmml" xref="S3.E7.m2.5.5.2.2.2.2.2"><apply id="S3.E7.m2.4.4.1.1.1.1.1.1.cmml" xref="S3.E7.m2.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m2.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E7.m2.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E7.m2.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E7.m2.4.4.1.1.1.1.1.1.2">𝑤</ci><apply id="S3.E7.m2.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E7.m2.4.4.1.1.1.1.1.1.3"><times id="S3.E7.m2.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.E7.m2.4.4.1.1.1.1.1.1.3.1"></times><ci id="S3.E7.m2.4.4.1.1.1.1.1.1.3.2.cmml" xref="S3.E7.m2.4.4.1.1.1.1.1.1.3.2">𝑗</ci><ci id="S3.E7.m2.4.4.1.1.1.1.1.1.3.3.cmml" xref="S3.E7.m2.4.4.1.1.1.1.1.1.3.3">𝑙</ci></apply></apply><apply id="S3.E7.m2.5.5.2.2.2.2.2.2.cmml" xref="S3.E7.m2.5.5.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m2.5.5.2.2.2.2.2.2.1.cmml" xref="S3.E7.m2.5.5.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E7.m2.5.5.2.2.2.2.2.2.2.cmml" xref="S3.E7.m2.5.5.2.2.2.2.2.2.2">𝜃</ci><ci id="S3.E7.m2.5.5.2.2.2.2.2.2.3.cmml" xref="S3.E7.m2.5.5.2.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m2.5c">\displaystyle\min_{\left\{\pi_{li}^{j}\right\}}\sum_{i=1}^{L}\sum_{j,l}\min_{\theta_{i}}\pi_{li}^{j}c\left(w_{jl},\theta_{i}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(8)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E8.m1.3" class="ltx_Math" alttext="\displaystyle\text{ s.t. }\sum_{i}\pi_{li}^{j}=1\forall j,l;\sum_{l}\pi_{li}^{j}=1\forall i,j," display="inline"><semantics id="S3.E8.m1.3a"><mrow id="S3.E8.m1.3.3.1"><mrow id="S3.E8.m1.3.3.1.1.2" xref="S3.E8.m1.3.3.1.1.3.cmml"><mrow id="S3.E8.m1.3.3.1.1.1.1" xref="S3.E8.m1.3.3.1.1.1.1.cmml"><mrow id="S3.E8.m1.3.3.1.1.1.1.3" xref="S3.E8.m1.3.3.1.1.1.1.3.cmml"><mtext id="S3.E8.m1.3.3.1.1.1.1.3.2" xref="S3.E8.m1.3.3.1.1.1.1.3.2a.cmml">s.t. </mtext><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.3.1.1.1.1.3.1" xref="S3.E8.m1.3.3.1.1.1.1.3.1.cmml">​</mo><mrow id="S3.E8.m1.3.3.1.1.1.1.3.3" xref="S3.E8.m1.3.3.1.1.1.1.3.3.cmml"><mstyle displaystyle="true" id="S3.E8.m1.3.3.1.1.1.1.3.3.1" xref="S3.E8.m1.3.3.1.1.1.1.3.3.1.cmml"><munder id="S3.E8.m1.3.3.1.1.1.1.3.3.1a" xref="S3.E8.m1.3.3.1.1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S3.E8.m1.3.3.1.1.1.1.3.3.1.2" xref="S3.E8.m1.3.3.1.1.1.1.3.3.1.2.cmml">∑</mo><mi id="S3.E8.m1.3.3.1.1.1.1.3.3.1.3" xref="S3.E8.m1.3.3.1.1.1.1.3.3.1.3.cmml">i</mi></munder></mstyle><msubsup id="S3.E8.m1.3.3.1.1.1.1.3.3.2" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.cmml"><mi id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.2" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.2.cmml">π</mi><mrow id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.cmml"><mi id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.2" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.1" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.1.cmml">​</mo><mi id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.3" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.3.cmml">i</mi></mrow><mi id="S3.E8.m1.3.3.1.1.1.1.3.3.2.3" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.3.cmml">j</mi></msubsup></mrow></mrow><mo id="S3.E8.m1.3.3.1.1.1.1.2" xref="S3.E8.m1.3.3.1.1.1.1.2.cmml">=</mo><mrow id="S3.E8.m1.3.3.1.1.1.1.1.1" xref="S3.E8.m1.3.3.1.1.1.1.1.2.cmml"><mrow id="S3.E8.m1.3.3.1.1.1.1.1.1.1" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.cmml"><mn id="S3.E8.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.2.cmml">1</mn><mo lspace="0.167em" rspace="0em" id="S3.E8.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E8.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mo rspace="0.167em" id="S3.E8.m1.3.3.1.1.1.1.1.1.1.3.1" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.3.1.cmml">∀</mo><mi id="S3.E8.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">j</mi></mrow></mrow><mo id="S3.E8.m1.3.3.1.1.1.1.1.1.2" xref="S3.E8.m1.3.3.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E8.m1.2.2" xref="S3.E8.m1.2.2.cmml">l</mi></mrow></mrow><mo id="S3.E8.m1.3.3.1.1.2.3" xref="S3.E8.m1.3.3.1.1.3a.cmml">;</mo><mrow id="S3.E8.m1.3.3.1.1.2.2" xref="S3.E8.m1.3.3.1.1.2.2.cmml"><mrow id="S3.E8.m1.3.3.1.1.2.2.3" xref="S3.E8.m1.3.3.1.1.2.2.3.cmml"><mstyle displaystyle="true" id="S3.E8.m1.3.3.1.1.2.2.3.1" xref="S3.E8.m1.3.3.1.1.2.2.3.1.cmml"><munder id="S3.E8.m1.3.3.1.1.2.2.3.1a" xref="S3.E8.m1.3.3.1.1.2.2.3.1.cmml"><mo movablelimits="false" id="S3.E8.m1.3.3.1.1.2.2.3.1.2" xref="S3.E8.m1.3.3.1.1.2.2.3.1.2.cmml">∑</mo><mi id="S3.E8.m1.3.3.1.1.2.2.3.1.3" xref="S3.E8.m1.3.3.1.1.2.2.3.1.3.cmml">l</mi></munder></mstyle><msubsup id="S3.E8.m1.3.3.1.1.2.2.3.2" xref="S3.E8.m1.3.3.1.1.2.2.3.2.cmml"><mi id="S3.E8.m1.3.3.1.1.2.2.3.2.2.2" xref="S3.E8.m1.3.3.1.1.2.2.3.2.2.2.cmml">π</mi><mrow id="S3.E8.m1.3.3.1.1.2.2.3.2.2.3" xref="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.cmml"><mi id="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.2" xref="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.1" xref="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.1.cmml">​</mo><mi id="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.3" xref="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.3.cmml">i</mi></mrow><mi id="S3.E8.m1.3.3.1.1.2.2.3.2.3" xref="S3.E8.m1.3.3.1.1.2.2.3.2.3.cmml">j</mi></msubsup></mrow><mo id="S3.E8.m1.3.3.1.1.2.2.2" xref="S3.E8.m1.3.3.1.1.2.2.2.cmml">=</mo><mrow id="S3.E8.m1.3.3.1.1.2.2.1.1" xref="S3.E8.m1.3.3.1.1.2.2.1.2.cmml"><mrow id="S3.E8.m1.3.3.1.1.2.2.1.1.1" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.cmml"><mn id="S3.E8.m1.3.3.1.1.2.2.1.1.1.2" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.2.cmml">1</mn><mo lspace="0.167em" rspace="0em" id="S3.E8.m1.3.3.1.1.2.2.1.1.1.1" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.1.cmml">​</mo><mrow id="S3.E8.m1.3.3.1.1.2.2.1.1.1.3" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.3.cmml"><mo rspace="0.167em" id="S3.E8.m1.3.3.1.1.2.2.1.1.1.3.1" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.3.1.cmml">∀</mo><mi id="S3.E8.m1.3.3.1.1.2.2.1.1.1.3.2" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.3.2.cmml">i</mi></mrow></mrow><mo id="S3.E8.m1.3.3.1.1.2.2.1.1.2" xref="S3.E8.m1.3.3.1.1.2.2.1.2.cmml">,</mo><mi id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml">j</mi></mrow></mrow></mrow><mo id="S3.E8.m1.3.3.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.3b"><apply id="S3.E8.m1.3.3.1.1.3.cmml" xref="S3.E8.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.3a.cmml" xref="S3.E8.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E8.m1.3.3.1.1.1.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1"><eq id="S3.E8.m1.3.3.1.1.1.1.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.2"></eq><apply id="S3.E8.m1.3.3.1.1.1.1.3.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3"><times id="S3.E8.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.1"></times><ci id="S3.E8.m1.3.3.1.1.1.1.3.2a.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.2"><mtext id="S3.E8.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.2">s.t. </mtext></ci><apply id="S3.E8.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3"><apply id="S3.E8.m1.3.3.1.1.1.1.3.3.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.1.1.3.3.1.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.1">subscript</csymbol><sum id="S3.E8.m1.3.3.1.1.1.1.3.3.1.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.1.2"></sum><ci id="S3.E8.m1.3.3.1.1.1.1.3.3.1.3.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.1.3">𝑖</ci></apply><apply id="S3.E8.m1.3.3.1.1.1.1.3.3.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.1.1.3.3.2.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2">superscript</csymbol><apply id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.2">𝜋</ci><apply id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3"><times id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.1"></times><ci id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.2">𝑙</ci><ci id="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.3.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.2.3.3">𝑖</ci></apply></apply><ci id="S3.E8.m1.3.3.1.1.1.1.3.3.2.3.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.3.2.3">𝑗</ci></apply></apply></apply><list id="S3.E8.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1.1"><apply id="S3.E8.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1"><times id="S3.E8.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.1"></times><cn type="integer" id="S3.E8.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E8.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="latexml" id="S3.E8.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.3.1">for-all</csymbol><ci id="S3.E8.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1.1.1.3.2">𝑗</ci></apply></apply><ci id="S3.E8.m1.2.2.cmml" xref="S3.E8.m1.2.2">𝑙</ci></list></apply><apply id="S3.E8.m1.3.3.1.1.2.2.cmml" xref="S3.E8.m1.3.3.1.1.2.2"><eq id="S3.E8.m1.3.3.1.1.2.2.2.cmml" xref="S3.E8.m1.3.3.1.1.2.2.2"></eq><apply id="S3.E8.m1.3.3.1.1.2.2.3.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3"><apply id="S3.E8.m1.3.3.1.1.2.2.3.1.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.1"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.2.2.3.1.1.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.1">subscript</csymbol><sum id="S3.E8.m1.3.3.1.1.2.2.3.1.2.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.1.2"></sum><ci id="S3.E8.m1.3.3.1.1.2.2.3.1.3.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.1.3">𝑙</ci></apply><apply id="S3.E8.m1.3.3.1.1.2.2.3.2.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.2"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.2.2.3.2.1.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.2">superscript</csymbol><apply id="S3.E8.m1.3.3.1.1.2.2.3.2.2.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.2"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.2.2.3.2.2.1.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.2">subscript</csymbol><ci id="S3.E8.m1.3.3.1.1.2.2.3.2.2.2.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.2.2.2">𝜋</ci><apply id="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.2.2.3"><times id="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.1.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.1"></times><ci id="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.2.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.2">𝑙</ci><ci id="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.3.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.2.2.3.3">𝑖</ci></apply></apply><ci id="S3.E8.m1.3.3.1.1.2.2.3.2.3.cmml" xref="S3.E8.m1.3.3.1.1.2.2.3.2.3">𝑗</ci></apply></apply><list id="S3.E8.m1.3.3.1.1.2.2.1.2.cmml" xref="S3.E8.m1.3.3.1.1.2.2.1.1"><apply id="S3.E8.m1.3.3.1.1.2.2.1.1.1.cmml" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1"><times id="S3.E8.m1.3.3.1.1.2.2.1.1.1.1.cmml" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.1"></times><cn type="integer" id="S3.E8.m1.3.3.1.1.2.2.1.1.1.2.cmml" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.2">1</cn><apply id="S3.E8.m1.3.3.1.1.2.2.1.1.1.3.cmml" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.3"><csymbol cd="latexml" id="S3.E8.m1.3.3.1.1.2.2.1.1.1.3.1.cmml" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.3.1">for-all</csymbol><ci id="S3.E8.m1.3.3.1.1.2.2.1.1.1.3.2.cmml" xref="S3.E8.m1.3.3.1.1.2.2.1.1.1.3.2">𝑖</ci></apply></apply><ci id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1">𝑗</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.3c">\displaystyle\text{ s.t. }\sum_{i}\pi_{li}^{j}=1\forall j,l;\sum_{l}\pi_{li}^{j}=1\forall i,j,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p5.5" class="ltx_p">where <math id="S3.SS2.SSS1.p5.1.m1.1" class="ltx_Math" alttext="\theta_{i}" display="inline"><semantics id="S3.SS2.SSS1.p5.1.m1.1a"><msub id="S3.SS2.SSS1.p5.1.m1.1.1" xref="S3.SS2.SSS1.p5.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p5.1.m1.1.1.2" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.cmml">θ</mi><mi id="S3.SS2.SSS1.p5.1.m1.1.1.3" xref="S3.SS2.SSS1.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.1.m1.1b"><apply id="S3.SS2.SSS1.p5.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p5.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2">𝜃</ci><ci id="S3.SS2.SSS1.p5.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.1.m1.1c">\theta_{i}</annotation></semantics></math> is the <math id="S3.SS2.SSS1.p5.2.m2.1" class="ltx_Math" alttext="i_{th}" display="inline"><semantics id="S3.SS2.SSS1.p5.2.m2.1a"><msub id="S3.SS2.SSS1.p5.2.m2.1.1" xref="S3.SS2.SSS1.p5.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p5.2.m2.1.1.2" xref="S3.SS2.SSS1.p5.2.m2.1.1.2.cmml">i</mi><mrow id="S3.SS2.SSS1.p5.2.m2.1.1.3" xref="S3.SS2.SSS1.p5.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS1.p5.2.m2.1.1.3.2" xref="S3.SS2.SSS1.p5.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p5.2.m2.1.1.3.1" xref="S3.SS2.SSS1.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p5.2.m2.1.1.3.3" xref="S3.SS2.SSS1.p5.2.m2.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.2.m2.1b"><apply id="S3.SS2.SSS1.p5.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p5.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1.2">𝑖</ci><apply id="S3.SS2.SSS1.p5.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1.3"><times id="S3.SS2.SSS1.p5.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS1.p5.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1.3.2">𝑡</ci><ci id="S3.SS2.SSS1.p5.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS1.p5.2.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.2.m2.1c">i_{th}</annotation></semantics></math> neuron in the current global model, <math id="S3.SS2.SSS1.p5.3.m3.1" class="ltx_Math" alttext="w_{jl}" display="inline"><semantics id="S3.SS2.SSS1.p5.3.m3.1a"><msub id="S3.SS2.SSS1.p5.3.m3.1.1" xref="S3.SS2.SSS1.p5.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p5.3.m3.1.1.2" xref="S3.SS2.SSS1.p5.3.m3.1.1.2.cmml">w</mi><mrow id="S3.SS2.SSS1.p5.3.m3.1.1.3" xref="S3.SS2.SSS1.p5.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS1.p5.3.m3.1.1.3.2" xref="S3.SS2.SSS1.p5.3.m3.1.1.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p5.3.m3.1.1.3.1" xref="S3.SS2.SSS1.p5.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p5.3.m3.1.1.3.3" xref="S3.SS2.SSS1.p5.3.m3.1.1.3.3.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.3.m3.1b"><apply id="S3.SS2.SSS1.p5.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p5.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p5.3.m3.1.1.2">𝑤</ci><apply id="S3.SS2.SSS1.p5.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p5.3.m3.1.1.3"><times id="S3.SS2.SSS1.p5.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS1.p5.3.m3.1.1.3.1"></times><ci id="S3.SS2.SSS1.p5.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS1.p5.3.m3.1.1.3.2">𝑗</ci><ci id="S3.SS2.SSS1.p5.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS1.p5.3.m3.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.3.m3.1c">w_{jl}</annotation></semantics></math> is the output weights processed by permutation matrix <math id="S3.SS2.SSS1.p5.4.m4.1" class="ltx_Math" alttext="\pi_{li}^{j}" display="inline"><semantics id="S3.SS2.SSS1.p5.4.m4.1a"><msubsup id="S3.SS2.SSS1.p5.4.m4.1.1" xref="S3.SS2.SSS1.p5.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.p5.4.m4.1.1.2.2" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.2.cmml">π</mi><mrow id="S3.SS2.SSS1.p5.4.m4.1.1.2.3" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.3.cmml"><mi id="S3.SS2.SSS1.p5.4.m4.1.1.2.3.2" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p5.4.m4.1.1.2.3.1" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p5.4.m4.1.1.2.3.3" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.3.3.cmml">i</mi></mrow><mi id="S3.SS2.SSS1.p5.4.m4.1.1.3" xref="S3.SS2.SSS1.p5.4.m4.1.1.3.cmml">j</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.4.m4.1b"><apply id="S3.SS2.SSS1.p5.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.p5.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p5.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.2">𝜋</ci><apply id="S3.SS2.SSS1.p5.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.3"><times id="S3.SS2.SSS1.p5.4.m4.1.1.2.3.1.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.3.1"></times><ci id="S3.SS2.SSS1.p5.4.m4.1.1.2.3.2.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.3.2">𝑙</ci><ci id="S3.SS2.SSS1.p5.4.m4.1.1.2.3.3.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1.2.3.3">𝑖</ci></apply></apply><ci id="S3.SS2.SSS1.p5.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p5.4.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.4.m4.1c">\pi_{li}^{j}</annotation></semantics></math>. <math id="S3.SS2.SSS1.p5.5.m5.1" class="ltx_Math" alttext="c()" display="inline"><semantics id="S3.SS2.SSS1.p5.5.m5.1a"><mrow id="S3.SS2.SSS1.p5.5.m5.1.1" xref="S3.SS2.SSS1.p5.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.p5.5.m5.1.1.2" xref="S3.SS2.SSS1.p5.5.m5.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p5.5.m5.1.1.1" xref="S3.SS2.SSS1.p5.5.m5.1.1.1.cmml">​</mo><mrow id="S3.SS2.SSS1.p5.5.m5.1.1.3.2" xref="S3.SS2.SSS1.p5.5.m5.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p5.5.m5.1.1.3.2.1" xref="S3.SS2.SSS1.p5.5.m5.1.1.3.1.cmml">(</mo><mo stretchy="false" id="S3.SS2.SSS1.p5.5.m5.1.1.3.2.2" xref="S3.SS2.SSS1.p5.5.m5.1.1.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.5.m5.1b"><apply id="S3.SS2.SSS1.p5.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p5.5.m5.1.1"><times id="S3.SS2.SSS1.p5.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p5.5.m5.1.1.1"></times><ci id="S3.SS2.SSS1.p5.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p5.5.m5.1.1.2">𝑐</ci><list id="S3.SS2.SSS1.p5.5.m5.1.1.3.1.cmml" xref="S3.SS2.SSS1.p5.5.m5.1.1.3.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.5.m5.1c">c()</annotation></semantics></math> is the distance metric served as determining the similarity between neurons. To solve this optimization problem, unlike PFNM that used heuristic choices, FedMA addressed it by the Hungarian matching algorithm <cite class="ltx_cite ltx_citemacro_citep">(Kuhn, <a href="#bib.bib70" title="" class="ltx_ref">1955</a>)</cite>.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2301.01299/assets/fvg-fed2.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="298" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Comparison between FedAvg and <math id="S3.F5.2.m1.1" class="ltx_Math" alttext="Fed^{2}" display="inline"><semantics id="S3.F5.2.m1.1b"><mrow id="S3.F5.2.m1.1.1" xref="S3.F5.2.m1.1.1.cmml"><mi id="S3.F5.2.m1.1.1.2" xref="S3.F5.2.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.F5.2.m1.1.1.1" xref="S3.F5.2.m1.1.1.1.cmml">​</mo><mi id="S3.F5.2.m1.1.1.3" xref="S3.F5.2.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.F5.2.m1.1.1.1b" xref="S3.F5.2.m1.1.1.1.cmml">​</mo><msup id="S3.F5.2.m1.1.1.4" xref="S3.F5.2.m1.1.1.4.cmml"><mi id="S3.F5.2.m1.1.1.4.2" xref="S3.F5.2.m1.1.1.4.2.cmml">d</mi><mn id="S3.F5.2.m1.1.1.4.3" xref="S3.F5.2.m1.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.F5.2.m1.1c"><apply id="S3.F5.2.m1.1.1.cmml" xref="S3.F5.2.m1.1.1"><times id="S3.F5.2.m1.1.1.1.cmml" xref="S3.F5.2.m1.1.1.1"></times><ci id="S3.F5.2.m1.1.1.2.cmml" xref="S3.F5.2.m1.1.1.2">𝐹</ci><ci id="S3.F5.2.m1.1.1.3.cmml" xref="S3.F5.2.m1.1.1.3">𝑒</ci><apply id="S3.F5.2.m1.1.1.4.cmml" xref="S3.F5.2.m1.1.1.4"><csymbol cd="ambiguous" id="S3.F5.2.m1.1.1.4.1.cmml" xref="S3.F5.2.m1.1.1.4">superscript</csymbol><ci id="S3.F5.2.m1.1.1.4.2.cmml" xref="S3.F5.2.m1.1.1.4.2">𝑑</ci><cn type="integer" id="S3.F5.2.m1.1.1.4.3.cmml" xref="S3.F5.2.m1.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.2.m1.1d">Fed^{2}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib159" title="" class="ltx_ref">2021</a>)</cite>.</figcaption>
</figure>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2301.01299/assets/fed2.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1196" height="331" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>The illustration of <math id="S3.F6.2.m1.1" class="ltx_Math" alttext="Fed^{2}" display="inline"><semantics id="S3.F6.2.m1.1b"><mrow id="S3.F6.2.m1.1.1" xref="S3.F6.2.m1.1.1.cmml"><mi id="S3.F6.2.m1.1.1.2" xref="S3.F6.2.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.F6.2.m1.1.1.1" xref="S3.F6.2.m1.1.1.1.cmml">​</mo><mi id="S3.F6.2.m1.1.1.3" xref="S3.F6.2.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.F6.2.m1.1.1.1b" xref="S3.F6.2.m1.1.1.1.cmml">​</mo><msup id="S3.F6.2.m1.1.1.4" xref="S3.F6.2.m1.1.1.4.cmml"><mi id="S3.F6.2.m1.1.1.4.2" xref="S3.F6.2.m1.1.1.4.2.cmml">d</mi><mn id="S3.F6.2.m1.1.1.4.3" xref="S3.F6.2.m1.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.F6.2.m1.1c"><apply id="S3.F6.2.m1.1.1.cmml" xref="S3.F6.2.m1.1.1"><times id="S3.F6.2.m1.1.1.1.cmml" xref="S3.F6.2.m1.1.1.1"></times><ci id="S3.F6.2.m1.1.1.2.cmml" xref="S3.F6.2.m1.1.1.2">𝐹</ci><ci id="S3.F6.2.m1.1.1.3.cmml" xref="S3.F6.2.m1.1.1.3">𝑒</ci><apply id="S3.F6.2.m1.1.1.4.cmml" xref="S3.F6.2.m1.1.1.4"><csymbol cd="ambiguous" id="S3.F6.2.m1.1.1.4.1.cmml" xref="S3.F6.2.m1.1.1.4">superscript</csymbol><ci id="S3.F6.2.m1.1.1.4.2.cmml" xref="S3.F6.2.m1.1.1.4.2">𝑑</ci><cn type="integer" id="S3.F6.2.m1.1.1.4.3.cmml" xref="S3.F6.2.m1.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.2.m1.1d">Fed^{2}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib159" title="" class="ltx_ref">2021</a>)</cite>.</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Feature-level aggregation</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.2" class="ltx_p">Despite effectiveness, the performance of weight-level aggregation/alignment largely depends on the selection of distance metric, which may not fully reflect the inherent feature information embedded in the neurons. In addition, the computation cost of the matching process is significantly heavy. To address these limitations, Yu <span id="S3.SS2.SSS2.p1.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib159" title="" class="ltx_ref">2021</a>)</cite> designed a feature-level alignment method, named <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="Fed^{2}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mrow id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p1.1.m1.1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS2.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p1.1.m1.1.1.1a" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml">​</mo><msup id="S3.SS2.SSS2.p1.1.m1.1.1.4" xref="S3.SS2.SSS2.p1.1.m1.1.1.4.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.4.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.4.2.cmml">d</mi><mn id="S3.SS2.SSS2.p1.1.m1.1.1.4.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"><times id="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1"></times><ci id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2">𝐹</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3">𝑒</ci><apply id="S3.SS2.SSS2.p1.1.m1.1.1.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.4.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.4">superscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.1.1.4.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.4.2">𝑑</ci><cn type="integer" id="S3.SS2.SSS2.p1.1.m1.1.1.4.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">Fed^{2}</annotation></semantics></math> which is composed of a feature-oriented structure adaptation and a model fusion algorithm. As shown in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.2.1. Weight-level aggregation ‣ 3.2. Aggregation optimization ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, compared with traditional weight alignment, <math id="S3.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="Fed^{2}" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><mrow id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.1.1.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p1.2.m2.1.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS2.p1.2.m2.1.1.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p1.2.m2.1.1.1a" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.cmml">​</mo><msup id="S3.SS2.SSS2.p1.2.m2.1.1.4" xref="S3.SS2.SSS2.p1.2.m2.1.1.4.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.1.1.4.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.4.2.cmml">d</mi><mn id="S3.SS2.SSS2.p1.2.m2.1.1.4.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><apply id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1"><times id="S3.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1"></times><ci id="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.2">𝐹</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.3">𝑒</ci><apply id="S3.SS2.SSS2.p1.2.m2.1.1.4.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.1.1.4.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.4">superscript</csymbol><ci id="S3.SS2.SSS2.p1.2.m2.1.1.4.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.4.2">𝑑</ci><cn type="integer" id="S3.SS2.SSS2.p1.2.m2.1.1.4.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">Fed^{2}</annotation></semantics></math> paid more attention to the neuron features and then aggregated the corresponding neurons. As a result, similar knowledge can be fused to achieve better performance.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.3" class="ltx_p">Concretely, the authors developed two schemes to accomplish feature-based federated learning. Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.2.1. Weight-level aggregation ‣ 3.2. Aggregation optimization ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the pipeline of the proposed <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="Fed^{2}" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mrow id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.1.m1.1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS2.p2.1.m1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.1.m1.1.1.1a" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.cmml">​</mo><msup id="S3.SS2.SSS2.p2.1.m1.1.1.4" xref="S3.SS2.SSS2.p2.1.m1.1.1.4.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.4.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.4.2.cmml">d</mi><mn id="S3.SS2.SSS2.p2.1.m1.1.1.4.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><apply id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1"><times id="S3.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1"></times><ci id="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.2">𝐹</ci><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3">𝑒</ci><apply id="S3.SS2.SSS2.p2.1.m1.1.1.4.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.4.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.4">superscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.4.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.4.2">𝑑</ci><cn type="integer" id="S3.SS2.SSS2.p2.1.m1.1.1.4.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">Fed^{2}</annotation></semantics></math>. The first scheme is <span id="S3.SS2.SSS2.p2.3.1" class="ltx_text ltx_font_italic">model structure adaptation</span>, where <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="Fed^{2}" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><mrow id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p2.2.m2.1.1.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.2.m2.1.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS2.p2.2.m2.1.1.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.2.m2.1.1.1a" xref="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml">​</mo><msup id="S3.SS2.SSS2.p2.2.m2.1.1.4" xref="S3.SS2.SSS2.p2.2.m2.1.1.4.cmml"><mi id="S3.SS2.SSS2.p2.2.m2.1.1.4.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.4.2.cmml">d</mi><mn id="S3.SS2.SSS2.p2.2.m2.1.1.4.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><apply id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1"><times id="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.1"></times><ci id="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.2">𝐹</ci><ci id="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3">𝑒</ci><apply id="S3.SS2.SSS2.p2.2.m2.1.1.4.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.2.m2.1.1.4.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.4">superscript</csymbol><ci id="S3.SS2.SSS2.p2.2.m2.1.1.4.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.4.2">𝑑</ci><cn type="integer" id="S3.SS2.SSS2.p2.2.m2.1.1.4.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">Fed^{2}</annotation></semantics></math> takes advantage of the group-convolution technique to allocate and learn the distinctive neuron features. Next, a feature paired averaging policy is presented to aggregate different neurons according to the partitioned group features. In this way, <math id="S3.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="Fed^{2}" display="inline"><semantics id="S3.SS2.SSS2.p2.3.m3.1a"><mrow id="S3.SS2.SSS2.p2.3.m3.1.1" xref="S3.SS2.SSS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p2.3.m3.1.1.2" xref="S3.SS2.SSS2.p2.3.m3.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.3.m3.1.1.1" xref="S3.SS2.SSS2.p2.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS2.p2.3.m3.1.1.3" xref="S3.SS2.SSS2.p2.3.m3.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.3.m3.1.1.1a" xref="S3.SS2.SSS2.p2.3.m3.1.1.1.cmml">​</mo><msup id="S3.SS2.SSS2.p2.3.m3.1.1.4" xref="S3.SS2.SSS2.p2.3.m3.1.1.4.cmml"><mi id="S3.SS2.SSS2.p2.3.m3.1.1.4.2" xref="S3.SS2.SSS2.p2.3.m3.1.1.4.2.cmml">d</mi><mn id="S3.SS2.SSS2.p2.3.m3.1.1.4.3" xref="S3.SS2.SSS2.p2.3.m3.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.3.m3.1b"><apply id="S3.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1"><times id="S3.SS2.SSS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.1"></times><ci id="S3.SS2.SSS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.2">𝐹</ci><ci id="S3.SS2.SSS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.3">𝑒</ci><apply id="S3.SS2.SSS2.p2.3.m3.1.1.4.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.3.m3.1.1.4.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.4">superscript</csymbol><ci id="S3.SS2.SSS2.p2.3.m3.1.1.4.2.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.4.2">𝑑</ci><cn type="integer" id="S3.SS2.SSS2.p2.3.m3.1.1.4.3.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.3.m3.1c">Fed^{2}</annotation></semantics></math> enables more accurate feature alignment as well as avoiding the expensive distance-based optimization.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span>Other aggregation</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">The aforementioned works mainly focus on alignment, in fact, there are also many other literatures targeting federated aggregation. For example, Yin <span id="S3.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib156" title="" class="ltx_ref">2018</a>)</cite> proposed a robust aggregation method for distributed learning. In the beginning, this work mainly analyzed two robust distributed gradient descent (GD) algorithms, including the coordinate-wise median and the coordinate-wise trimmed mean. They proved statistical error rates for three kinds of population loss functions: strongly convex, non-strongly convex, and smooth non-convex. Furthermore, to reduce the communication cost, the authors designed a median-based distributed algorithm and demonstrate its effectiveness by extensive experiments. Chen <span id="S3.SS2.SSS3.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2020a</a>)</cite> further considered the federated learning scenario, and found that heterogeneous data in different nodes will harm the training convergence to some degree. Based on this observation, they developed a novel gradient correction mechanism that can perturb the local gradients with noise. The main advantage of the proposed scheme is that it offers a provable convergence guarantee even when data are non-iid.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">Besides, Yurochkin <span id="S3.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Yurochkin et al<span class="ltx_text">.</span>, <a href="#bib.bib161" title="" class="ltx_ref">2019a</a>)</cite> leveraged Bayesian nonparametrics to design a meta-model that can potentially capture the global structure through statistical parameter matching. The authors pointed out that their approach is model-independent and is applicable to a wide range of model types.
Chen <span id="S3.SS2.SSS3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen and Chao, <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite> proposed FEDBE, a novel method to apply bayesian model ensemble into conventional federated learning, aiming at making the aggregation more robust. Motivated by prior work <cite class="ltx_cite ltx_citemacro_citep">(Maddox et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2019</a>)</cite>, the authors utilized bayesian inference to construct an improved global model. In addition, stochastic weight average (SWA) <cite class="ltx_cite ltx_citemacro_citep">(Izmailov et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2018</a>)</cite> is also used to further boost the performance.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Heterogeneous federated learning</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Heterogeneous federated learning aims to effectively aggregate models generated from heterogeneous environments. Here the heterogeneous property could be reflected from data, models or device systems. We will dive into each aspect in the next parts.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Data heterogeneity</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Data heterogeneity indicates that collaborative clients might be in different situations, resulting in various data distributions. For example, the dog images collected from indoors and outdoors display highly heterogeneous data distribution. To address the issue, the research community borrows the idea from other AI techniques to alleviate the heterogeneity influence, which we list as follows.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.5" class="ltx_p"><span id="S3.SS3.SSS1.p2.5.1" class="ltx_text ltx_font_italic">Multi-task learning based methods.</span>
Multi-task learning enables learning models for multiple related tasks at the same time <cite class="ltx_cite ltx_citemacro_citep">(Rebuffi
et al<span class="ltx_text">.</span>, <a href="#bib.bib119" title="" class="ltx_ref">2017</a>; Bilen and Vedaldi, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>; Mallya and
Lazebnik, <a href="#bib.bib103" title="" class="ltx_ref">2018</a>)</cite>. The core design principle is to capture the relationship among tasks and leverage the relationship to facilitate the learning process. In federated learning, clients with different data distributions could also be considered as a type of multi-task learning, where each task has a distinctive statistical representation <cite class="ltx_cite ltx_citemacro_citep">(Vanhaesebrouck et al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2017</a>; Zantedeschi
et al<span class="ltx_text">.</span>, <a href="#bib.bib164" title="" class="ltx_ref">2020</a>; T Dinh
et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2020</a>; Hanzely et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2020</a>; Hanzely and
Richtárik, <a href="#bib.bib48" title="" class="ltx_ref">2020</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2021a</a>)</cite>.
For instance, Smith <span id="S3.SS3.SSS1.p2.5.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Smith
et al<span class="ltx_text">.</span>, <a href="#bib.bib128" title="" class="ltx_ref">2017</a>)</cite> first proposed to combine federated learning and multi-task learning. By a series of concept formulations and theoretical analyses, they suggested multi-task learning is a natural choice to handle the statistical problem in the federated setting. Based on the combination, they further developed a novel approach MOCHA, in order to accomplish their goal. Specifically, the authors formulated the problem as a dual optimization problem as follows</p>
<table id="S6.EGx8" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(9)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E9.m1.2" class="ltx_Math" alttext="\displaystyle\min_{\boldsymbol{\alpha}}\left\{\mathcal{D}(\boldsymbol{\alpha}):=\sum_{t=1}^{m}\sum_{i=1}^{n_{t}}\ell_{t}^{*}\left(-\boldsymbol{\alpha}_{t}^{i}\right)+\mathcal{R}^{*}(\mathbf{X}\boldsymbol{\alpha})\right\}," display="inline"><semantics id="S3.E9.m1.2a"><mrow id="S3.E9.m1.2.2.1"><mrow id="S3.E9.m1.2.2.1.1.2" xref="S3.E9.m1.2.2.1.1.3.cmml"><munder id="S3.E9.m1.2.2.1.1.1.1" xref="S3.E9.m1.2.2.1.1.1.1.cmml"><mi id="S3.E9.m1.2.2.1.1.1.1.2" xref="S3.E9.m1.2.2.1.1.1.1.2.cmml">min</mi><mi id="S3.E9.m1.2.2.1.1.1.1.3" xref="S3.E9.m1.2.2.1.1.1.1.3.cmml">𝜶</mi></munder><mo id="S3.E9.m1.2.2.1.1.2a" xref="S3.E9.m1.2.2.1.1.3.cmml">⁡</mo><mrow id="S3.E9.m1.2.2.1.1.2.2" xref="S3.E9.m1.2.2.1.1.3.cmml"><mo id="S3.E9.m1.2.2.1.1.2.2.2" xref="S3.E9.m1.2.2.1.1.3.cmml">{</mo><mrow id="S3.E9.m1.2.2.1.1.2.2.1" xref="S3.E9.m1.2.2.1.1.2.2.1.cmml"><mrow id="S3.E9.m1.2.2.1.1.2.2.1.4" xref="S3.E9.m1.2.2.1.1.2.2.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E9.m1.2.2.1.1.2.2.1.4.2" xref="S3.E9.m1.2.2.1.1.2.2.1.4.2.cmml">𝒟</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.2.1.1.2.2.1.4.1" xref="S3.E9.m1.2.2.1.1.2.2.1.4.1.cmml">​</mo><mrow id="S3.E9.m1.2.2.1.1.2.2.1.4.3.2" xref="S3.E9.m1.2.2.1.1.2.2.1.4.cmml"><mo stretchy="false" id="S3.E9.m1.2.2.1.1.2.2.1.4.3.2.1" xref="S3.E9.m1.2.2.1.1.2.2.1.4.cmml">(</mo><mi id="S3.E9.m1.1.1" xref="S3.E9.m1.1.1.cmml">𝜶</mi><mo rspace="0.278em" stretchy="false" id="S3.E9.m1.2.2.1.1.2.2.1.4.3.2.2" xref="S3.E9.m1.2.2.1.1.2.2.1.4.cmml">)</mo></mrow></mrow><mo rspace="0.278em" id="S3.E9.m1.2.2.1.1.2.2.1.3" xref="S3.E9.m1.2.2.1.1.2.2.1.3.cmml">:=</mo><mrow id="S3.E9.m1.2.2.1.1.2.2.1.2" xref="S3.E9.m1.2.2.1.1.2.2.1.2.cmml"><mrow id="S3.E9.m1.2.2.1.1.2.2.1.1.1" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.cmml"><munderover id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2a" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.cmml"><mi id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.2.cmml">t</mi><mo id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.1" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.3.cmml">m</mi></munderover></mstyle><mrow id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.cmml"><munderover id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2a" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.cmml"><mi id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.1" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><msub id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.cmml"><mi id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.2.cmml">n</mi><mi id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.3.cmml">t</mi></msub></munderover></mstyle><mrow id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.cmml"><msubsup id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.2.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.2.2.cmml">ℓ</mi><mi id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.2.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.2.3.cmml">t</mi><mo id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.3.cmml">∗</mo></msubsup><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1a" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml">𝜶</mi><mi id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.2.3.cmml">t</mi><mi id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msubsup></mrow><mo id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.3" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E9.m1.2.2.1.1.2.2.1.2.3" xref="S3.E9.m1.2.2.1.1.2.2.1.2.3.cmml">+</mo><mrow id="S3.E9.m1.2.2.1.1.2.2.1.2.2" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.cmml"><msup id="S3.E9.m1.2.2.1.1.2.2.1.2.2.3" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.2" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.2.cmml">ℛ</mi><mo id="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.3" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.3.cmml">∗</mo></msup><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.2.1.1.2.2.1.2.2.2" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.2.cmml">​</mo><mrow id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.2" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.cmml"><mi id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.2" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.1" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.1.cmml">​</mo><mi id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.3" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.3.cmml">𝜶</mi></mrow><mo stretchy="false" id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.3" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E9.m1.2.2.1.1.2.2.3" xref="S3.E9.m1.2.2.1.1.3.cmml">}</mo></mrow></mrow><mo id="S3.E9.m1.2.2.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.2b"><apply id="S3.E9.m1.2.2.1.1.3.cmml" xref="S3.E9.m1.2.2.1.1.2"><apply id="S3.E9.m1.2.2.1.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.1.1">subscript</csymbol><min id="S3.E9.m1.2.2.1.1.1.1.2.cmml" xref="S3.E9.m1.2.2.1.1.1.1.2"></min><ci id="S3.E9.m1.2.2.1.1.1.1.3.cmml" xref="S3.E9.m1.2.2.1.1.1.1.3">𝜶</ci></apply><apply id="S3.E9.m1.2.2.1.1.2.2.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1"><csymbol cd="latexml" id="S3.E9.m1.2.2.1.1.2.2.1.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.3">assign</csymbol><apply id="S3.E9.m1.2.2.1.1.2.2.1.4.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.4"><times id="S3.E9.m1.2.2.1.1.2.2.1.4.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.4.1"></times><ci id="S3.E9.m1.2.2.1.1.2.2.1.4.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.4.2">𝒟</ci><ci id="S3.E9.m1.1.1.cmml" xref="S3.E9.m1.1.1">𝜶</ci></apply><apply id="S3.E9.m1.2.2.1.1.2.2.1.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2"><plus id="S3.E9.m1.2.2.1.1.2.2.1.2.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.3"></plus><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1"><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2">superscript</csymbol><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2">subscript</csymbol><sum id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.2"></sum><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3"><eq id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.1"></eq><ci id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.2">𝑡</ci><cn type="integer" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.2.3">𝑚</ci></apply><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1"><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2">superscript</csymbol><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2">subscript</csymbol><sum id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.2"></sum><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3"><eq id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.1"></eq><ci id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.2">𝑛</ci><ci id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1"><times id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.2"></times><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.2.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.2.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.2.2">ℓ</ci><ci id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.2.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.2.3">𝑡</ci></apply><times id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.3.3"></times></apply><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1"><minus id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1"></minus><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.2.2">𝜶</ci><ci id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.2.3">𝑡</ci></apply><ci id="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply></apply></apply></apply></apply><apply id="S3.E9.m1.2.2.1.1.2.2.1.2.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2"><times id="S3.E9.m1.2.2.1.1.2.2.1.2.2.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.2"></times><apply id="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.3"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.3">superscript</csymbol><ci id="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.2">ℛ</ci><times id="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.3.3"></times></apply><apply id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1"><times id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.1"></times><ci id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.2.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.2">𝐗</ci><ci id="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.3.cmml" xref="S3.E9.m1.2.2.1.1.2.2.1.2.2.1.1.1.3">𝜶</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.2c">\displaystyle\min_{\boldsymbol{\alpha}}\left\{\mathcal{D}(\boldsymbol{\alpha}):=\sum_{t=1}^{m}\sum_{i=1}^{n_{t}}\ell_{t}^{*}\left(-\boldsymbol{\alpha}_{t}^{i}\right)+\mathcal{R}^{*}(\mathbf{X}\boldsymbol{\alpha})\right\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS3.SSS1.p2.4" class="ltx_p">where <math id="S3.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="l_{t}^{*}" display="inline"><semantics id="S3.SS3.SSS1.p2.1.m1.1a"><msubsup id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.1.m1.1.1.2.2" xref="S3.SS3.SSS1.p2.1.m1.1.1.2.2.cmml">l</mi><mi id="S3.SS3.SSS1.p2.1.m1.1.1.2.3" xref="S3.SS3.SSS1.p2.1.m1.1.1.2.3.cmml">t</mi><mo id="S3.SS3.SSS1.p2.1.m1.1.1.3" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.1b"><apply id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1">superscript</csymbol><apply id="S3.SS3.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.1.m1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.1.m1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.2.2">𝑙</ci><ci id="S3.SS3.SSS1.p2.1.m1.1.1.2.3.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.2.3">𝑡</ci></apply><times id="S3.SS3.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.1c">l_{t}^{*}</annotation></semantics></math> and <math id="S3.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{R}^{*}" display="inline"><semantics id="S3.SS3.SSS1.p2.2.m2.1a"><msup id="S3.SS3.SSS1.p2.2.m2.1.1" xref="S3.SS3.SSS1.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.2.m2.1.1.2" xref="S3.SS3.SSS1.p2.2.m2.1.1.2.cmml">ℛ</mi><mo id="S3.SS3.SSS1.p2.2.m2.1.1.3" xref="S3.SS3.SSS1.p2.2.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.2.m2.1b"><apply id="S3.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.SSS1.p2.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1.2">ℛ</ci><times id="S3.SS3.SSS1.p2.2.m2.1.1.3.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.2.m2.1c">\mathcal{R}^{*}</annotation></semantics></math> are the conjugate dual functions of <math id="S3.SS3.SSS1.p2.3.m3.1" class="ltx_Math" alttext="l_{t}" display="inline"><semantics id="S3.SS3.SSS1.p2.3.m3.1a"><msub id="S3.SS3.SSS1.p2.3.m3.1.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.cmml"><mi id="S3.SS3.SSS1.p2.3.m3.1.1.2" xref="S3.SS3.SSS1.p2.3.m3.1.1.2.cmml">l</mi><mi id="S3.SS3.SSS1.p2.3.m3.1.1.3" xref="S3.SS3.SSS1.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.3.m3.1b"><apply id="S3.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.3.m3.1.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.3.m3.1.1.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1.2">𝑙</ci><ci id="S3.SS3.SSS1.p2.3.m3.1.1.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.3.m3.1c">l_{t}</annotation></semantics></math> and <math id="S3.SS3.SSS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics id="S3.SS3.SSS1.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.4.m4.1.1" xref="S3.SS3.SSS1.p2.4.m4.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.4.m4.1b"><ci id="S3.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.4.m4.1c">\mathcal{R}</annotation></semantics></math>, respectively. To solve <a href="#S3.E9" title="In 3.3.1. Data heterogeneity ‣ 3.3. Heterogeneous federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, they carefully designed the quadratic approximation of the dual problem to separate computation across the nodes.</p>
</div>
<div id="S3.SS3.SSS1.p3" class="ltx_para">
<p id="S3.SS3.SSS1.p3.1" class="ltx_p">Despite federated multi-task learning being demonstrated effective, it has been applied only on convex models. To address the limitation, Corinzia <span id="S3.SS3.SSS1.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Corinzia
et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> proposed a more general approach, named VIRTUAL, to achieve federation on non-convex models.
The key idea is to construct a hierarchical Bayesian network in terms of the central server and the clients, such that the inference could be performed with variational methods. In this way, each client can obtain a task-specific model that benefits from the server model in a transfer learning manner.</p>
</div>
<div id="S3.SS3.SSS1.p4" class="ltx_para">
<p id="S3.SS3.SSS1.p4.1" class="ltx_p">Marfoq <span id="S3.SS3.SSS1.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Marfoq et al<span class="ltx_text">.</span>, <a href="#bib.bib104" title="" class="ltx_ref">2021</a>)</cite> further proposed to study federated multi-task learning under the flexible assumption that each local data distribution is a mixture of unknown underlying distributions, which is a more challenging and practical scenario. In the
beginning, the authors showed the fact that t federated learning is impossible without
assumptions on local data distributions. Then they made the flexible assumption and developed Federated Expectation-Maximization to accomplish their objective. Besides, the proposed approach is proven generalizable to unseen clients.</p>
</div>
<div id="S3.SS3.SSS1.p5" class="ltx_para">
<p id="S3.SS3.SSS1.p5.1" class="ltx_p"><span id="S3.SS3.SSS1.p5.1.1" class="ltx_text ltx_font_italic">Meta-learning based methods.</span>
Meta-learning is commonly considered as learning to learn <cite class="ltx_cite ltx_citemacro_citep">(Thrun and Pratt, <a href="#bib.bib136" title="" class="ltx_ref">2012</a>)</cite>. Compared with conventional deep learning algorithms that learn specific feature knowledge, meta-learning focus more on learning the learning ability. In the field of federated learning, meta-learning techniques can also be applied to generate a more personalized federation model. Jiang <span id="S3.SS3.SSS1.p5.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2019</a>)</cite> first proposed to combine them, where they believed meta-learning had a number of similarities with the objective of addressing the statistical challenge in FL. Concretely, they developed a novel algorithm to further combine FedAvg <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2017</a>)</cite> and Reptile <cite class="ltx_cite ltx_citemacro_citep">(Nichol and
Schulman, <a href="#bib.bib111" title="" class="ltx_ref">2018</a>)</cite>, with two modifications: the first one is to decrease the local learning rate to make training more stable; another is to design a fine-tuning stage based on Reptile with smaller K and Adam as the server optimizer, which could improve the initial model as well as preserving and stabilizing the personalized model.</p>
</div>
<div id="S3.SS3.SSS1.p6" class="ltx_para">
<p id="S3.SS3.SSS1.p6.1" class="ltx_p">Khodak <span id="S3.SS3.SSS1.p6.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Khodak
et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2019</a>)</cite> built a theoretical framework to further characterize meta-learning methods and apply them into federated learning. They introduced Average Regret-Upper-Bound Analysis (ARUBA), which enables meta-learning to leverage more sophisticated structures. With ARUBA, researchers could improve the results of many ML
tasks, including adapting to the task-similarity, adapting to dynamic environments, adapting to the inter-task geometry and statistical learning-to-Learn. Towards FL, they improved meta-test-time performance on few-shot learning
and effectively added user-personalization to FedAvg.</p>
</div>
<div id="S3.SS3.SSS1.p7" class="ltx_para">
<p id="S3.SS3.SSS1.p7.3" class="ltx_p">Fallah <span id="S3.SS3.SSS1.p7.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Fallah
et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite> aims to find an initial shared model that can be easily fitted to their local data with one or a few steps
of gradient descent. They achieved their objective by incorporating Model-Agnostic Meta-Learning (MAML) <cite class="ltx_cite ltx_citemacro_citep">(Finn
et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2017</a>, <a href="#bib.bib38" title="" class="ltx_ref">2018</a>)</cite> into current FL pipelines. Specifically, the authors proposed a personalized variant of the FedAvg algorithm, named Per-FedAvg, which can be formulated as optimizing the following equation</p>
<table id="S6.EGx9" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(10)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E10.m1.3" class="ltx_Math" alttext="\displaystyle\min_{w\in\mathbb{R}^{d}}F(w):=\frac{1}{n}\sum_{i=1}^{n}f_{i}\left(w-\alpha\nabla f_{i}(w)\right)," display="inline"><semantics id="S3.E10.m1.3a"><mrow id="S3.E10.m1.3.3.1" xref="S3.E10.m1.3.3.1.1.cmml"><mrow id="S3.E10.m1.3.3.1.1" xref="S3.E10.m1.3.3.1.1.cmml"><mrow id="S3.E10.m1.3.3.1.1.3" xref="S3.E10.m1.3.3.1.1.3.cmml"><mrow id="S3.E10.m1.3.3.1.1.3.2" xref="S3.E10.m1.3.3.1.1.3.2.cmml"><munder id="S3.E10.m1.3.3.1.1.3.2.1" xref="S3.E10.m1.3.3.1.1.3.2.1.cmml"><mi id="S3.E10.m1.3.3.1.1.3.2.1.2" xref="S3.E10.m1.3.3.1.1.3.2.1.2.cmml">min</mi><mrow id="S3.E10.m1.3.3.1.1.3.2.1.3" xref="S3.E10.m1.3.3.1.1.3.2.1.3.cmml"><mi id="S3.E10.m1.3.3.1.1.3.2.1.3.2" xref="S3.E10.m1.3.3.1.1.3.2.1.3.2.cmml">w</mi><mo id="S3.E10.m1.3.3.1.1.3.2.1.3.1" xref="S3.E10.m1.3.3.1.1.3.2.1.3.1.cmml">∈</mo><msup id="S3.E10.m1.3.3.1.1.3.2.1.3.3" xref="S3.E10.m1.3.3.1.1.3.2.1.3.3.cmml"><mi id="S3.E10.m1.3.3.1.1.3.2.1.3.3.2" xref="S3.E10.m1.3.3.1.1.3.2.1.3.3.2.cmml">ℝ</mi><mi id="S3.E10.m1.3.3.1.1.3.2.1.3.3.3" xref="S3.E10.m1.3.3.1.1.3.2.1.3.3.3.cmml">d</mi></msup></mrow></munder><mo lspace="0.167em" id="S3.E10.m1.3.3.1.1.3.2a" xref="S3.E10.m1.3.3.1.1.3.2.cmml">⁡</mo><mi id="S3.E10.m1.3.3.1.1.3.2.2" xref="S3.E10.m1.3.3.1.1.3.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E10.m1.3.3.1.1.3.1" xref="S3.E10.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S3.E10.m1.3.3.1.1.3.3.2" xref="S3.E10.m1.3.3.1.1.3.cmml"><mo stretchy="false" id="S3.E10.m1.3.3.1.1.3.3.2.1" xref="S3.E10.m1.3.3.1.1.3.cmml">(</mo><mi id="S3.E10.m1.1.1" xref="S3.E10.m1.1.1.cmml">w</mi><mo rspace="0.278em" stretchy="false" id="S3.E10.m1.3.3.1.1.3.3.2.2" xref="S3.E10.m1.3.3.1.1.3.cmml">)</mo></mrow></mrow><mo rspace="0.278em" id="S3.E10.m1.3.3.1.1.2" xref="S3.E10.m1.3.3.1.1.2.cmml">:=</mo><mrow id="S3.E10.m1.3.3.1.1.1" xref="S3.E10.m1.3.3.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E10.m1.3.3.1.1.1.3" xref="S3.E10.m1.3.3.1.1.1.3.cmml"><mfrac id="S3.E10.m1.3.3.1.1.1.3a" xref="S3.E10.m1.3.3.1.1.1.3.cmml"><mn id="S3.E10.m1.3.3.1.1.1.3.2" xref="S3.E10.m1.3.3.1.1.1.3.2.cmml">1</mn><mi id="S3.E10.m1.3.3.1.1.1.3.3" xref="S3.E10.m1.3.3.1.1.1.3.3.cmml">n</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E10.m1.3.3.1.1.1.2" xref="S3.E10.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E10.m1.3.3.1.1.1.1" xref="S3.E10.m1.3.3.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E10.m1.3.3.1.1.1.1.2" xref="S3.E10.m1.3.3.1.1.1.1.2.cmml"><munderover id="S3.E10.m1.3.3.1.1.1.1.2a" xref="S3.E10.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E10.m1.3.3.1.1.1.1.2.2.2" xref="S3.E10.m1.3.3.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E10.m1.3.3.1.1.1.1.2.2.3" xref="S3.E10.m1.3.3.1.1.1.1.2.2.3.cmml"><mi id="S3.E10.m1.3.3.1.1.1.1.2.2.3.2" xref="S3.E10.m1.3.3.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E10.m1.3.3.1.1.1.1.2.2.3.1" xref="S3.E10.m1.3.3.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E10.m1.3.3.1.1.1.1.2.2.3.3" xref="S3.E10.m1.3.3.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E10.m1.3.3.1.1.1.1.2.3" xref="S3.E10.m1.3.3.1.1.1.1.2.3.cmml">n</mi></munderover></mstyle><mrow id="S3.E10.m1.3.3.1.1.1.1.1" xref="S3.E10.m1.3.3.1.1.1.1.1.cmml"><msub id="S3.E10.m1.3.3.1.1.1.1.1.3" xref="S3.E10.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.E10.m1.3.3.1.1.1.1.1.3.2" xref="S3.E10.m1.3.3.1.1.1.1.1.3.2.cmml">f</mi><mi id="S3.E10.m1.3.3.1.1.1.1.1.3.3" xref="S3.E10.m1.3.3.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E10.m1.3.3.1.1.1.1.1.2" xref="S3.E10.m1.3.3.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E10.m1.3.3.1.1.1.1.1.1.1" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E10.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">w</mi><mo id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">α</mi><mo lspace="0.167em" rspace="0em" id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.1" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml"><mo rspace="0.167em" id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.1.cmml">∇</mo><msub id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.cmml"><mi id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.2" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.2.cmml">f</mi><mi id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.3" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.1a" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.4.2" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.4.2.1" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E10.m1.2.2" xref="S3.E10.m1.2.2.cmml">w</mi><mo stretchy="false" id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.4.2.2" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E10.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E10.m1.3.3.1.2" xref="S3.E10.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m1.3b"><apply id="S3.E10.m1.3.3.1.1.cmml" xref="S3.E10.m1.3.3.1"><csymbol cd="latexml" id="S3.E10.m1.3.3.1.1.2.cmml" xref="S3.E10.m1.3.3.1.1.2">assign</csymbol><apply id="S3.E10.m1.3.3.1.1.3.cmml" xref="S3.E10.m1.3.3.1.1.3"><times id="S3.E10.m1.3.3.1.1.3.1.cmml" xref="S3.E10.m1.3.3.1.1.3.1"></times><apply id="S3.E10.m1.3.3.1.1.3.2.cmml" xref="S3.E10.m1.3.3.1.1.3.2"><apply id="S3.E10.m1.3.3.1.1.3.2.1.cmml" xref="S3.E10.m1.3.3.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E10.m1.3.3.1.1.3.2.1.1.cmml" xref="S3.E10.m1.3.3.1.1.3.2.1">subscript</csymbol><min id="S3.E10.m1.3.3.1.1.3.2.1.2.cmml" xref="S3.E10.m1.3.3.1.1.3.2.1.2"></min><apply id="S3.E10.m1.3.3.1.1.3.2.1.3.cmml" xref="S3.E10.m1.3.3.1.1.3.2.1.3"><in id="S3.E10.m1.3.3.1.1.3.2.1.3.1.cmml" xref="S3.E10.m1.3.3.1.1.3.2.1.3.1"></in><ci id="S3.E10.m1.3.3.1.1.3.2.1.3.2.cmml" xref="S3.E10.m1.3.3.1.1.3.2.1.3.2">𝑤</ci><apply id="S3.E10.m1.3.3.1.1.3.2.1.3.3.cmml" xref="S3.E10.m1.3.3.1.1.3.2.1.3.3"><csymbol cd="ambiguous" id="S3.E10.m1.3.3.1.1.3.2.1.3.3.1.cmml" xref="S3.E10.m1.3.3.1.1.3.2.1.3.3">superscript</csymbol><ci id="S3.E10.m1.3.3.1.1.3.2.1.3.3.2.cmml" xref="S3.E10.m1.3.3.1.1.3.2.1.3.3.2">ℝ</ci><ci id="S3.E10.m1.3.3.1.1.3.2.1.3.3.3.cmml" xref="S3.E10.m1.3.3.1.1.3.2.1.3.3.3">𝑑</ci></apply></apply></apply><ci id="S3.E10.m1.3.3.1.1.3.2.2.cmml" xref="S3.E10.m1.3.3.1.1.3.2.2">𝐹</ci></apply><ci id="S3.E10.m1.1.1.cmml" xref="S3.E10.m1.1.1">𝑤</ci></apply><apply id="S3.E10.m1.3.3.1.1.1.cmml" xref="S3.E10.m1.3.3.1.1.1"><times id="S3.E10.m1.3.3.1.1.1.2.cmml" xref="S3.E10.m1.3.3.1.1.1.2"></times><apply id="S3.E10.m1.3.3.1.1.1.3.cmml" xref="S3.E10.m1.3.3.1.1.1.3"><divide id="S3.E10.m1.3.3.1.1.1.3.1.cmml" xref="S3.E10.m1.3.3.1.1.1.3"></divide><cn type="integer" id="S3.E10.m1.3.3.1.1.1.3.2.cmml" xref="S3.E10.m1.3.3.1.1.1.3.2">1</cn><ci id="S3.E10.m1.3.3.1.1.1.3.3.cmml" xref="S3.E10.m1.3.3.1.1.1.3.3">𝑛</ci></apply><apply id="S3.E10.m1.3.3.1.1.1.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1"><apply id="S3.E10.m1.3.3.1.1.1.1.2.cmml" xref="S3.E10.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1.2">superscript</csymbol><apply id="S3.E10.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E10.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="S3.E10.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S3.E10.m1.3.3.1.1.1.1.2.2.2"></sum><apply id="S3.E10.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S3.E10.m1.3.3.1.1.1.1.2.2.3"><eq id="S3.E10.m1.3.3.1.1.1.1.2.2.3.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1.2.2.3.1"></eq><ci id="S3.E10.m1.3.3.1.1.1.1.2.2.3.2.cmml" xref="S3.E10.m1.3.3.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E10.m1.3.3.1.1.1.1.2.2.3.3.cmml" xref="S3.E10.m1.3.3.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E10.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E10.m1.3.3.1.1.1.1.2.3">𝑛</ci></apply><apply id="S3.E10.m1.3.3.1.1.1.1.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1"><times id="S3.E10.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.2"></times><apply id="S3.E10.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E10.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.E10.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1"><minus id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.2">𝑤</ci><apply id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3"><times id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.2">𝛼</ci><apply id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3"><ci id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.1">∇</ci><apply id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.1.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.2.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.2">𝑓</ci><ci id="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.3.cmml" xref="S3.E10.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.3">𝑖</ci></apply></apply><ci id="S3.E10.m1.2.2.cmml" xref="S3.E10.m1.2.2">𝑤</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.3c">\displaystyle\min_{w\in\mathbb{R}^{d}}F(w):=\frac{1}{n}\sum_{i=1}^{n}f_{i}\left(w-\alpha\nabla f_{i}(w)\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS3.SSS1.p7.2" class="ltx_p">where <math id="S3.SS3.SSS1.p7.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.SSS1.p7.1.m1.1a"><mi id="S3.SS3.SSS1.p7.1.m1.1.1" xref="S3.SS3.SSS1.p7.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p7.1.m1.1b"><ci id="S3.SS3.SSS1.p7.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p7.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p7.1.m1.1c">n</annotation></semantics></math> is the number of clients and <math id="S3.SS3.SSS1.p7.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS3.SSS1.p7.2.m2.1a"><mi id="S3.SS3.SSS1.p7.2.m2.1.1" xref="S3.SS3.SSS1.p7.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p7.2.m2.1b"><ci id="S3.SS3.SSS1.p7.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p7.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p7.2.m2.1c">\alpha</annotation></semantics></math> is the learning rate. The detailed solution for the optimization problem can be seen in the paper if readers have an interest.</p>
</div>
<div id="S3.SS3.SSS1.p8" class="ltx_para">
<p id="S3.SS3.SSS1.p8.1" class="ltx_p">Acar <span id="S3.SS3.SSS1.p8.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Acar et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite> further modified meta-learning to benefit federated learning. As shown in Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.3.1. Data heterogeneity ‣ 3.3. Heterogeneous federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, they proposed PFL, a gradient correction method based on prior works, which explicitly de-biased the
meta-model in the distributed heterogeneous data setting to learn a more personalized device model. During the process, convergence guarantees of PFL for strongly convex, convex and nonconvex meta objectives are provided.</p>
</div>
<div id="S3.SS3.SSS1.p9" class="ltx_para">
<p id="S3.SS3.SSS1.p9.1" class="ltx_p"><span id="S3.SS3.SSS1.p9.1.1" class="ltx_text ltx_font_italic">Transfer learning based methods.</span>
Transfer learning aims to transfer the information learned from a source task to a target task <cite class="ltx_cite ltx_citemacro_citep">(Pan and Yang, <a href="#bib.bib114" title="" class="ltx_ref">2009</a>)</cite>. A large number of research works have been proposed to advance this promising field <cite class="ltx_cite ltx_citemacro_citep">(Yosinski
et al<span class="ltx_text">.</span>, <a href="#bib.bib158" title="" class="ltx_ref">2014</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2018</a>, <a href="#bib.bib85" title="" class="ltx_ref">2019b</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib171" title="" class="ltx_ref">2022b</a>)</cite>. In federated learning, transferring the knowledge of the federated model to each client model will significantly facilitate the personalization performance under the data heterogeneity environment. Wang <span id="S3.SS3.SSS1.p9.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2019b</a>)</cite> proposed to use fine-tuning, a typical transfer learning algorithm to achieve personalization. They first conducted traditional FL to obtain a global model. Then the federated model is regarded as the source model and further retrained using individual client’s training cache data. In this way, each client model can acquire and benefit the transferred knowledge, outputting an improved customized model.</p>
</div>
<div id="S3.SS3.SSS1.p10" class="ltx_para">
<p id="S3.SS3.SSS1.p10.1" class="ltx_p">Based on the aforementioned work, Yu <span id="S3.SS3.SSS1.p10.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Yu
et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2020</a>)</cite> extended the simple fine-tuning strategy. They investigated how three adaptation mechanisms: fine-tuning,
multi-task learning, and knowledge distillation affect the personalization performance. The authors characterized these mechanisms as <span id="S3.SS3.SSS1.p10.1.2" class="ltx_text ltx_font_italic">local adaptation</span>. In addition, different model protection techniques such as differential privacy and robust aggregation were applied to further validate the effectiveness of local adaptation. Finally, they used both CV and NLP datasets to demonstrate the superiority and necessity to conduct local adaptation.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2301.01299/assets/PFL.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="574" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>The illustration of PFL <cite class="ltx_cite ltx_citemacro_citep">(Acar et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>.</figcaption>
</figure>
<div id="S3.SS3.SSS1.p11" class="ltx_para">
<p id="S3.SS3.SSS1.p11.1" class="ltx_p">Peng <span id="S3.SS3.SSS1.p11.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Peng
et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2020</a>)</cite> considered a new FL+TL scenario beyond fine-tuning. Instead, they paid more attention to domain shift, which means that the labeled data collected by source nodes statistically differ from the target node’s unlabeled data. Based on this setting, they proposed the problem of federated domain adaptation and address it by Federated Adversarial Domain Adaptation (FADA). The key idea is to apply adversarial adaptation and representation disentanglement to FL settings.</p>
</div>
<div id="S3.SS3.SSS1.p12" class="ltx_para">
<p id="S3.SS3.SSS1.p12.1" class="ltx_p">Ozkara <span id="S3.SS3.SSS1.p12.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Ozkara
et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2021</a>)</cite> introduced a quantized and personalized FL algorithm to deal with the data issue. The quantized training process is conducted via knowledge distillation (KD) among clients who have access to heterogeneous data and resources. Besides, they developed an alternating proximal gradient update to address this compressed personalization challenge and analyzed its convergence properties.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2301.01299/assets/IFCA.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="274" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>The illustration of IFCA <cite class="ltx_cite ltx_citemacro_citep">(Ghosh
et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite>.</figcaption>
</figure>
<div id="S3.SS3.SSS1.p13" class="ltx_para">
<p id="S3.SS3.SSS1.p13.1" class="ltx_p"><span id="S3.SS3.SSS1.p13.1.1" class="ltx_text ltx_font_italic">Clustering-based methods.</span>
Clustering-based FL attempts to tackle the data heterogeneity issue via partitioning clients into different clusters, each of which conforms to a similar distribution. In terms of this key idea, much research effort is made to explore cluster-based FL. Sattler <span id="S3.SS3.SSS1.p13.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Sattler
et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2020</a>)</cite> proposed Clustered Federated Learning (CFL), to utilize geometric properties of the FL loss surface, in order to group
the client population into clusters with jointly trainable data distributions. It is worth noting that CFL is orthogonal to the current FL communication protocol and can be applied to
general non-convex objectives beyond DNNs.</p>
</div>
<div id="S3.SS3.SSS1.p14" class="ltx_para">
<p id="S3.SS3.SSS1.p14.1" class="ltx_p">Ghosh <span id="S3.SS3.SSS1.p14.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Ghosh
et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite> proposed the Iterative Federated Clustering Algorithm
(IFCA), which alternately estimated the cluster identities of the users and optimized model parameters for the user clusters via gradient descent. As shown in Fig. <a href="#S3.F8" title="Figure 8 ‣ 3.3.1. Data heterogeneity ‣ 3.3. Heterogeneous federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, the server broadcasted models and the workers dynamically identified their cluster memberships and run local updates. This process will continue to operate until the clusters become stable.</p>
</div>
<div id="S3.SS3.SSS1.p15" class="ltx_para">
<p id="S3.SS3.SSS1.p15.1" class="ltx_p">To train high-quality cluster models, Ruan <span id="S3.SS3.SSS1.p15.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Ruan and Joe-Wong, <a href="#bib.bib120" title="" class="ltx_ref">2022</a>)</cite> suggested FedSoft, which uses proximal updates to restrict client burden by asking a subset of clients to complete just one optimization task per communication round.</p>
</div>
<div id="S3.SS3.SSS1.p16" class="ltx_para">
<p id="S3.SS3.SSS1.p16.1" class="ltx_p">Liu <span id="S3.SS3.SSS1.p16.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2021d</a>)</cite> proposed a framework to accomplish privacy-preserving federated adaptation. The key idea is to group the clients with similar distribution to collaboratively adapt the federated model, rather than just adapting it with the data in a single device. PFA leveraged the sparsity property of neural networks to generate privacy-preserving representations and used them to efficiently identify clients with similar data distributions. In this way, PFA can conduct an FL process in a group-wise way on the federated model to achieve adaptation.</p>
</div>
<div id="S3.SS3.SSS1.p17" class="ltx_para">
<p id="S3.SS3.SSS1.p17.1" class="ltx_p">Besides, in order to achieve clustering without uploading any extra information, Liu <span id="S3.SS3.SSS1.p17.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2021b</a>)</cite> further proposed DistFL, targeting at finishing accurate, automated and efficient cluster-based FL in terms of distribution feature. Specifically, they extracted the distribution knowledge from the uploaded model via existing synthesis techniques <cite class="ltx_cite ltx_citemacro_citep">(Mahendran and
Vedaldi, <a href="#bib.bib102" title="" class="ltx_ref">2015</a>)</cite> and then compared them to obtain the clustering results. Finally, they aggregated models in each cluster, getting rid of the influence of heterogeneous data.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Model heterogeneity</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Model heterogeneity means that the federated model might not be identical due to the different hardware and data distributions of clients. For example, in order to fit various
computation capabilities of clients, we require deploying different model architectures to match each client. On the other hand, NAS techniques <cite class="ltx_cite ltx_citemacro_citep">(Zoph and Le, <a href="#bib.bib182" title="" class="ltx_ref">2016</a>)</cite> have been widely used to search a crafted architecture based on the data in each device, thus leading to the model heterogeneity situation.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2301.01299/assets/heteroFL.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="386" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>The illustration of HeteroFL <cite class="ltx_cite ltx_citemacro_citep">(Diao
et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>.</figcaption>
</figure>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">To tackle the problem, Li <span id="S3.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib76" title="" class="ltx_ref">2019</a>)</cite> used transfer learning
and knowledge distillation to develop a universal framework, which enabled federated
learning with uniquely designed models. Lin <span id="S3.SS3.SSS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Lin
et al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2020</a>)</cite> further proposed a distillation framework for robust federated model fusion and leveraged entropy-reduction to accelerate convergence. Diao <span id="S3.SS3.SSS2.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Diao
et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite> designed HeteroFL to address heterogeneous clients equipped with highly
different computation and communication capabilities. As shown in Fig. <a href="#S3.F9" title="Figure 9 ‣ 3.3.2. Model heterogeneity ‣ 3.3. Heterogeneous federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, the federation is achieved by aggregating parameters on the same location while unlearning the other non-overlapping area.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3. </span>System heterogeneity</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">System heterogeneity is a practical property in FL scenarios because different clients/parties naturally own heterogeneous hardware and memory limitation. Therefore, how to accomplish FL under the condition of system heterogeneity is worth exploring.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2301.01299/assets/oort.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>The illustration of Oort <cite class="ltx_cite ltx_citemacro_citep">(Lai
et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2021b</a>)</cite>.</figcaption>
</figure>
<div id="S3.SS3.SSS3.p2" class="ltx_para">
<p id="S3.SS3.SSS3.p2.1" class="ltx_p">A key design for system acceleration is to develop different client selection strategies for avoiding the influence of latency stragglers. Here stragglers refer to the clients with weak computing power and thus could slow down the overall FL process. Lai <span id="S3.SS3.SSS3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Lai
et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2021b</a>)</cite> proposed Oort, a system to improve the performance
of federated training and testing with guided participant selection. As shown in Fig. <a href="#S3.F10" title="Figure 10 ‣ 3.3.3. System heterogeneity ‣ 3.3. Heterogeneous federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, Oort cherry-picked participants according to the tradeoff between statistical and system efficiency. Specifically, they defined ”Client Statistical Utility” to measure the importance of each client. Shin <span id="S3.SS3.SSS3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Shin
et al<span class="ltx_text">.</span>, <a href="#bib.bib125" title="" class="ltx_ref">2022</a>)</cite> developed FedBalancer, a framework to actively select clients’ training samples in terms of the more “informative” data. Besides, they introduced an adaptive deadline control scheme
to predict the optimal deadline for each round, in order to further speed up global training. Li <span id="S3.SS3.SSS3.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2022a</a>)</cite> observed that current client selection was coarse-grained due to their under-exploitation on the clients’ data
and system heterogeneity. Based on this finding, they proposed PyramidFL, a fine-grained client selection framework to speed up the FL training. The key idea is to not only focus on the divergence of those selected participants but also fully exploited the data and system heterogeneity within selected clients to profile their utility
more efficiently. As a result, PyramidFL is able to achieve better performance compared to other baselines.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Secure federated learning</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The original design of federated learning considers the security problem via exchanging parameters while keeping raw data in their own devices. However, recent studies have proved that attackers might steal the privacy information from the uploaded models. Therefore, more rigorous secure FL should be investigated. In the following parts, we will introduce the attack methods and defense methods in FL scenarios.</p>
</div>
<figure id="S3.F11" class="ltx_figure"><img src="/html/2301.01299/assets/model_replace.png" id="S3.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="345" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>The illustration of model replacement <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>.</figcaption>
</figure>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span>Attack methods</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p"><span id="S3.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_italic">Backdoor attack.</span>
The goal of backdoor attacks is to manipulate a subset of training data by injecting adversarial triggers such that DNN models will
output incorrect prediction on the test set when the same trigger occurs. In federated learning, directly applying current backdoor attacks is unsuitable since the aggregation process might destroy the triggers. Bagdasaryan <span id="S3.SS4.SSS1.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> is the first to backdoor federated learning. They achieved their objective by proposing model replacement, which means the backdoor is injected to the joint model rather than raw data. As shown in Fig. <a href="#S3.F11" title="Figure 11 ‣ 3.4. Secure federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, the attacker trained a model on the backdoor data using the constrain-and-scale technique. In this way, the averaging function is largely affected by this attack model. Wang <span id="S3.SS4.SSS1.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib142" title="" class="ltx_ref">2020a</a>)</cite> proposed edge-case backdoors, which forced a model
to misclassify on seemingly easy inputs that are unlikely to be part of the
training or testing data. For example, they may exist on the tail of the input distribution. As a result, it is extremely hard to detect them. Xie <span id="S3.SS4.SSS1.p1.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib151" title="" class="ltx_ref">2020</a>)</cite> further developed distributed backdoor attack (DBA) to compromise FL. They mainly took advantage of the distributed nature of FL, decomposing a
global trigger pattern into separate local patterns and introducing them into the training set of different adversarial parties respectively. Therefore, DBA is more persistent and stealthy compared to centralized ones.
In FL models, backdoors can be inserted, but these backdoors are often not durable, i.e., they do not remain in the model after poisoned updates stop being uploaded. Since training occurs gradually in FL systems, an inserted backdoor may not survive until deployment. Zhang <span id="S3.SS4.SSS1.p1.1.5" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_citep"><span id="S3.SS4.SSS1.p1.1.5.1.1" class="ltx_text ltx_font_upright">(</span>Zhang et al<span class="ltx_text">.</span><span id="S3.SS4.SSS1.p1.1.5.2.2.1.1" class="ltx_text ltx_font_upright">, </span><a href="#bib.bib173" title="" class="ltx_ref">2022d</a><span id="S3.SS4.SSS1.p1.1.5.3.3" class="ltx_text ltx_font_upright">)</span></cite></span> proposed Neurotoxin, which is a simple modification to existing backdoor attacks that target parameters that are not changed in magnitude as much during training.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p"><span id="S3.SS4.SSS1.p2.1.1" class="ltx_text ltx_font_italic">Gradients attack.</span>
Gradients attack targets at reverse some privacy information from gradients. In federated learning, exchanging gradients is a typical step for knowledge update and aggregation. Therefore, gradient attack poses a high risk to the federal participants. Zhu <span id="S3.SS4.SSS1.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib181" title="" class="ltx_ref">2019</a>)</cite> found since training occurs gradually in FL systems, an inserted backdoor may not survive until deployment. that it is possible to obtain the
private training data from the publicly shared gradients. They first randomly generated a pair of “dummy” inputs and labels and used them to compute corresponding gradients. Then the gradients were compared to the shared ones and continually optimize the dummy inputs and labels to minimize the distance between them. As a result, the dummy data are close to the original ones and can peek into user privacy.
Lam <span id="S3.SS4.SSS1.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Lam
et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2021</a>)</cite> further realized gradients attack from the aggregated model updates/gradients. The authors leveraged the summary information from device analytics and reconstructed the user participant matrix, which invalided the current secure aggregation protocols <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>.
Zhu <span id="S3.SS4.SSS1.p2.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhu and Blaschko, <a href="#bib.bib180" title="" class="ltx_ref">2021</a>)</cite> proposed Recursive Gradient Attack on Privacy (R-GAP), an approach to analyze how and when the target gradients can lead to the unique recovery of original data. Concretely, the authors designed a recursive, depth-wise algorithm for recovering training data from the gradient
information, which is the first closed-form algorithm that works on both CNN layers and FC layers.
Li <span id="S3.SS4.SSS1.p2.1.5" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2022b</a>)</cite> found that under certain defense settings, generative gradient leakage can still leak private training information.</p>
</div>
<div id="S3.SS4.SSS1.p3" class="ltx_para">
<p id="S3.SS4.SSS1.p3.1" class="ltx_p"><span id="S3.SS4.SSS1.p3.1.1" class="ltx_text ltx_font_italic">Model poison attack.</span>
The goal of poison attacks is to induce the FL model to output the target label specified by the adversary. For example, Tolpegin <span id="S3.SS4.SSS1.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Tolpegin
et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2020</a>)</cite> implemented data poison attack by flipping the labels of training data from one class to another class in the local training epoch to mislead the global model output. Although the aggregation process in FL can mitigate the attack to some extent, when the number of malicious clients becomes large, FL is inevitably poisoned.
Fang <span id="S3.SS4.SSS1.p3.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Fang
et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite> conducted the first systematic study
on local model poisoning attacks to federated learning. Based on this study, they proposed local model poisoning attacks to Byzantine robust federated learning via manipulating the
local model parameters on compromised worker devices during the learning process. Besides, the authors further stated two defense strategies and test their performance on the proposed attack.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span>Defense methods</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p"><span id="S3.SS4.SSS2.p1.1.1" class="ltx_text ltx_font_italic">DP-based defense.</span>
Differential privacy (DP) <cite class="ltx_cite ltx_citemacro_citep">(Dwork, <a href="#bib.bib31" title="" class="ltx_ref">2008</a>)</cite> has been widely used to prevent information leakage. The key idea is to add some noises to obfuscate the original information. As a result, attackers are hard to infer the privacy properties. Federated learning also requires this type of protection since the uploaded model parameters can be easily exploited to extract sensitive information.
Wei <span id="S3.SS4.SSS2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a href="#bib.bib147" title="" class="ltx_ref">2020</a>)</cite> proposed NbAFL, a framework that applied DP into FL. Specifically, they added noises to parameters of the local model at the client side before aggregation. Besides, the authors theoretically analyzed the convergence property of differentially private FL algorithms and proved the effectiveness of the proposed framework.</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<p id="S3.SS4.SSS2.p2.1" class="ltx_p">Kairouz <span id="S3.SS4.SSS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2021a</a>)</cite>
presented a comprehensive end-to-end
system, where they discretized the data and added discrete Gaussian noise before conducting secure aggregation. In addition, the authors provided a novel privacy analysis for sums of discrete Gaussians and carefully analyzed the effects of data quantization and modular summation arithmetic. Experiments demonstrated that their method can achieve comparable performance with 16 bits of precision per value.
Agarwal <span id="S3.SS4.SSS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Agarwal
et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite> proposed a multi-dimensional Skellam mechanism, where two independent poisson random
variables are used to measure the difference. The authors applied their mechanism to FL and provided a novel algorithm that
appropriately discretized the data and used the Skellam mechanism along with modular arithmetic to bound the range of the data and communication costs before secure aggregation. As a result, they could achieve better privacy-accuracy trade-offs in a more efficient manner.</p>
</div>
<figure id="S3.F12" class="ltx_figure"><img src="/html/2301.01299/assets/EHE.png" id="S3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="306" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>The illustration of BatchCrypt <cite class="ltx_cite ltx_citemacro_citep">(Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib166" title="" class="ltx_ref">2020b</a>)</cite>.</figcaption>
</figure>
<figure id="S3.F13" class="ltx_figure"><img src="/html/2301.01299/assets/PPFL.png" id="S3.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1196" height="304" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>The illustration of PPFL <cite class="ltx_cite ltx_citemacro_citep">(Mo et al<span class="ltx_text">.</span>, <a href="#bib.bib107" title="" class="ltx_ref">2021</a>)</cite>.</figcaption>
</figure>
<div id="S3.SS4.SSS2.p3" class="ltx_para">
<p id="S3.SS4.SSS2.p3.1" class="ltx_p"><span id="S3.SS4.SSS2.p3.1.1" class="ltx_text ltx_font_italic">HE-based defense.</span>
HE-based FL aims to combine traditional Homomorphic Encryption (HE) and FL in a more suitable way. By applying HE, FL is able to aggregate client models without revealing the information of the concrete model parameters. Therefore, it is impossible to infer user privacy from the model.
Hardy <span id="S3.SS4.SSS2.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Hardy et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2017</a>)</cite> proposed to encrypt FL with the homomorphic scheme in the field of privacy-preserving entity resolution and federated logistic regression. They bounded the difference between the empirical loss of their classifier on the true data and showed an improved convergence speed. Besides, their experiments found that even rates for generalization cannot be significantly affected by entity resolution.
Liu <span id="S3.SS4.SSS2.p3.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib95" title="" class="ltx_ref">2019a</a>)</cite> designed a secure FL framework through leveraging the additive property of partial homomorphic encryption, which effectively avoids the exposure of client models at the server side. Besides, the authors introduced two optimization mechanisms to further enhance efficiency.
Zhang <span id="S3.SS4.SSS2.p3.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib166" title="" class="ltx_ref">2020b</a>)</cite> proposed BatchCrypt, an efficient homomorphic encryption system for cross-Silo federated learning. As shown in Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.4.2. Defense methods ‣ 3.4. Secure federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, there exist five typical steps to achieve a cross-silo FL system. In the beginning, the aggregator needed to select a client to generate an HE key-pair and distribute it to others. Then for each iteration, clients conducted local gradient updates and further encrypted them by the public key. These encrypted parameters were uploaded to the server where aggregation happened and the aggregated model is transferred to each client. Finally, the client side decrypted the received information and implemented the local training as the next round. BatchCrypt proposed two novel schemes to further improve efficiency. First, a feasible batch encryption scheme was presented to directly sum up the ciphertexts of two batches. Second, an efficient analytical model dACIQ was presented to choose optimal clipping thresholds with
the minimum cumulative error. As a result, BatchCrypt achieved 23×-93× training speedup while reducing the communication overhead by 66×-101×.</p>
</div>
<div id="S3.SS4.SSS2.p4" class="ltx_para">
<p id="S3.SS4.SSS2.p4.1" class="ltx_p"><span id="S3.SS4.SSS2.p4.1.1" class="ltx_text ltx_font_italic">TEE-based defense.</span>
The aforementioned secure FL approaches provide security guarantee mainly from the perspective of software. In real-world scenarios, hardware protection is also widely applied by designing crafted architecture. Trusted Execution Environment (TEE) is
a trusted component that establishes an isolated region on the main processor to ensure the confidentiality and integrity of data and programs <cite class="ltx_cite ltx_citemacro_citep">(ARM, <a href="#bib.bib6" title="" class="ltx_ref">2009</a>; Costan and
Devadas, <a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite>. Compared to traditional encryption schemes such as homomorphic encryption, TEE is more efficient with respect to the computation cost since it only requires some simple operations to connect the trusted and untrusted part in OS. Recently there have been a large number of works targeting at applying TEE to deep/federated learning, in order to achieve protection from hardware level. For example, Mo <span id="S3.SS4.SSS2.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Mo et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2020</a>)</cite> proposed DarkneTZ that enabled executing DNNs more secure with TEE in an edge device. They partitioned DNNs into a set of non-sensitive layers and sensitive layers, which are respectively processed by TEE or normal OS. Here the partition choice is based on the underlying system’s CPU execution time, memory usage, and accurate power consumption of different DNN layers. Besides, the authors developed a threat model to validate DarkneTZ’s robustness under the membership inference attack and the results showed that DarkneTZ could defend against this type of attack with negligible performance overhead.</p>
</div>
<div id="S3.SS4.SSS2.p5" class="ltx_para">
<p id="S3.SS4.SSS2.p5.1" class="ltx_p">Based on the combination of DNNs and TEE, Mo <span id="S3.SS4.SSS2.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Mo et al<span class="ltx_text">.</span>, <a href="#bib.bib107" title="" class="ltx_ref">2021</a>)</cite> further attempted to apply TEEs to federated learning. Specifically, they proposed PPFL, a framework that limited privacy leakages in federated learning via implementing local training in TEEs. As shown in Fig. <a href="#S3.F13" title="Figure 13 ‣ 3.4.2. Defense methods ‣ 3.4. Secure federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, to address the challenge of limited memory size of TEEs, the authors designed a greedy layer-wise training to conduct local updates until convergence. In this way, this approach could support sophisticated settings such as training one or more layers (block) each time, which potentially speed up the training process.
Zhang <span id="S3.SS4.SSS2.p5.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2022c</a>)</cite> proposed TEESlice, a system to provide a strong security guarantee while maintaining low inference latency with the help of TEEs. Concretely, TEESlice executed the more private model slices on TEEs and others on normal AI accelerators. As a result, TEESlice can achieve more than 10× throughput promotion with the same level of strong security guarantee.</p>
</div>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Fair federated learning</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Existing works of federated learning pay more attention to improving learning performance based on the accuracy of the model and the time of learning task completion. However, the interests of the FL clients are often ignored and this may lead to unfairness. The problem of fairness can occur in the whole FL training process, including client selection, model optimization, incentive distribution, and contribution evaluation. The unfairness can have a negative impact on both the FL clients and the FL server, as clients are discouraged to join FL training, and servers are less likely to attract potentially high-quality clients. Recently, to achieve fairness from different angles, various Fairness-Aware Federated Learning (FAFL) approaches have been proposed. In this section, we will discuss recent FAFL methods in detail.</p>
</div>
<section id="S3.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1. </span>Fair client selection</h4>

<div id="S3.SS5.SSS1.p1" class="ltx_para">
<p id="S3.SS5.SSS1.p1.1" class="ltx_p">Unfairness in FL Client Selection mainly consists of three types, over-representation, under-representation, and never-representation. Suppose an FL system prefers to select clients with high performance (such as a faster GPU), and clients with the highest performance may be selected much more than any other clients (i.e., over-representation), while clients with poor performance may be selected just a few times (i.e., under-representation). At the same time, the client with the lowest performance may never be selected (i.e., never-representation). Additionally, due to the heterogeneity among clients, fairness does not indicate giving everyone the same possibility to be selected. It is important to balance the interests of the server and the interests of the clients. If clients from specific groups are oversampled, the global FL model will be partial to their data, so the model’s performance will deteriorate <cite class="ltx_cite ltx_citemacro_citep">(Cho
et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>. Existing FAFL client selection methods can be partitioned into two categories, considering fairness factors and customization for each client.</p>
</div>
<div id="S3.SS5.SSS1.p2" class="ltx_para">
<p id="S3.SS5.SSS1.p2.1" class="ltx_p"><span id="S3.SS5.SSS1.p2.1.1" class="ltx_text ltx_font_italic">1) Fairness factors.</span>
Fairness factors are designed to allow rarely selected clients, such as clients with lower computational abilities or smaller datasets, to join the FL training more frequently. Yang <span id="S3.SS5.SSS1.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib153" title="" class="ltx_ref">2021</a>)</cite> proposed a client selection algorithm based on the Combinatorial Multi-Armed Bandit (CMAB) framework to reduce the class imbalance effect. Inspired by <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib77" title="" class="ltx_ref">2019a</a>)</cite>, Huang <span id="S3.SS5.SSS1.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Huang
et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite> converts the original offline problem to an online Lyapunov optimization problem and uses dynamic queues to quantify the long-term guarantee of the client participation rate. Moreover, Huang introduces a long-term fairness constraint to make sure the average client’s long-term chosen rate is above a constant. After <cite class="ltx_cite ltx_citemacro_citep">(Huang
et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite>, Huang <span id="S3.SS5.SSS1.p2.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Huang
et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite> improves the performance by replacing dynamic queues to the Exp3 algorithms <cite class="ltx_cite ltx_citemacro_citep">(Auer
et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2002</a>)</cite>, and the fairness parameter determining the selection possibility in each round can be different. However, these works all design the fairness factor without considering the real-time contribution of individual clients. Song <span id="S3.SS5.SSS1.p2.1.5" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Song
et al<span class="ltx_text">.</span>, <a href="#bib.bib129" title="" class="ltx_ref">2021</a>)</cite> addresses this problem and proposed a client selection policy with fairness constraints based on reputation, using a fairness parameter to balance reputation and the number of successful transmissions.</p>
</div>
<div id="S3.SS5.SSS1.p3" class="ltx_para">
<p id="S3.SS5.SSS1.p3.1" class="ltx_p"><span id="S3.SS5.SSS1.p3.1.1" class="ltx_text ltx_font_italic">2) Client customization.</span>
This approach pays attention to customized model settings or customized model procedures. Clients often receive the same initial models at the first training round in most current FL paradigms. Therefore, clients with lower capabilities, such as bad network connections, require more time to complete each training round and are likely to be kept out of subsequent rounds, leading to under-presented and never-presented problems. To alleviate this problem, dynamically adapting the FL model framework or the training procedure based on client capabilities is often used.</p>
</div>
<div id="S3.SS5.SSS1.p4" class="ltx_para">
<p id="S3.SS5.SSS1.p4.1" class="ltx_p">Caldas <span id="S3.SS5.SSS1.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite> proposed Federated Dropout (FD), which distributes sub-models with sizes suitable for each client based on their computational resources. The process of FD is shown in Fig <a href="#S3.F14" title="Figure 14 ‣ 3.5.1. Fair client selection ‣ 3.5. Fair federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>. Although FD diminishes communication and local computation costs largely, it uses dropout operations and treats the neural networks as black-box functions. Bouaciada <span id="S3.SS5.SSS1.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> noticed this problem and proposed Adaptive Federated Dropout (AFD) <cite class="ltx_cite ltx_citemacro_citep">(Bouacida
et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>. AFD keeps an activation score map to generate the best-fit sub-model for each client. FD and AFD both make sure clients with low capabilities could participate in FL training, but they do not provide custom pruned submodels to different clients. To address this limitation, Horvath <span id="S3.SS5.SSS1.p4.1.3" class="ltx_text ltx_font_italic">et al. </span> <cite class="ltx_cite ltx_citemacro_citep">(Horvath et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2021</a>)</cite> augmented FD to Ordered Dropout (OD). Different from FD, OD drops neighboring components of the model despite random neurons. OD divides clients with comparable computational capabilities into clusters, and clients in the same cluster apply the same dropout rate. Moreover, OD applies the knowledge distillation method <cite class="ltx_cite ltx_citemacro_citep">(Hinton
et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2015</a>)</cite> to enhance feature extraction for smaller submodels.</p>
</div>
<figure id="S3.F14" class="ltx_figure"><img src="/html/2301.01299/assets/FDmodel.png" id="S3.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14. </span>The summary of the Federated Dropout (FD) training procedure <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>. </figcaption>
</figure>
<figure id="S3.F15" class="ltx_figure"><img src="/html/2301.01299/assets/TRAmethod.png" id="S3.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15. </span>The illustration of ThrowRightAway (TRA) scheme <cite class="ltx_cite ltx_citemacro_citep">(Zhou
et al<span class="ltx_text">.</span>, <a href="#bib.bib179" title="" class="ltx_ref">2021</a>)</cite>.</figcaption>
</figure>
<div id="S3.SS5.SSS1.p5" class="ltx_para">
<p id="S3.SS5.SSS1.p5.1" class="ltx_p">Clients’ communication capabilities can also affect client selection. A poor network may cause too much retransmission and lead to extra delays in FL model training, which makes clients with a poorer network less likely to aggregate their model updates into the final model and leads to model bias. To deal with this issue, Zhou <span id="S3.SS5.SSS1.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhou
et al<span class="ltx_text">.</span>, <a href="#bib.bib179" title="" class="ltx_ref">2021</a>)</cite> proposed ThrowRightAway (TRA), a loss-tolerant FL framework that makes the FL training faster by ignoring few lost packets. As is shown in Fig <a href="#S3.F15" title="Figure 15 ‣ 3.5.1. Fair client selection ‣ 3.5. Fair federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, at first every participating FL client reports their network conditions to the FL server, and the server divides the clients into two categories: sufficient type and insufficient type. Only the clients in the sufficient type can get a re-transmission request and then re-transmit their loss packets. Apparently, the method can only be effective when the category is accurate.</p>
</div>
<div id="S3.SS5.SSS1.p6" class="ltx_para">
<p id="S3.SS5.SSS1.p6.1" class="ltx_p">This method means assigning less work to clients with lower capabilities to make them available to pass threshold-based FL client selection. Li <span id="S3.SS5.SSS1.p6.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020b</a>)</cite> proposed FedProx which allowed each client performed partial training based on its accessible resources. FedProx allows various local epochs, and thus more clients are encouraged to join the training process.</p>
</div>
</section>
<section id="S3.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2. </span>Fair model optimization</h4>

<div id="S3.SS5.SSS2.p1" class="ltx_para">
<p id="S3.SS5.SSS2.p1.1" class="ltx_p">In the optimization during FL model training, the model may discriminate against definite preserved groups, or overfit some clients at the expense of others. Recent works dealing with this issue can be approximately divided into two types: 1) objective function-based and 2) gradient-based.</p>
</div>
<div id="S3.SS5.SSS2.p2" class="ltx_para">
<p id="S3.SS5.SSS2.p2.1" class="ltx_p"><span id="S3.SS5.SSS2.p2.1.1" class="ltx_text ltx_font_italic">1) Objective function-based methods: </span>
Objective function-based methods focus on the global/local objectives of the FL model, such as minimizing the loss function. Mohri <span id="S3.SS5.SSS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Mohri
et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2019</a>)</cite> proposed AFL, which aims to prevent the model overfitting any specific client at the expense of others. AFL just optimizes the global model for the target distribution made up of a mixture of clients. However, this method only works for a small number of clients. Zhou <span id="S3.SS5.SSS2.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhou
et al<span class="ltx_text">.</span>, <a href="#bib.bib179" title="" class="ltx_ref">2021</a>)</cite> proposed q-FFL to diminish the scalability limitation of AFL. q-FFL adds parameter q to reweigh the aggregate loss. To improve the model robustness and maintain good-intent fairness at the same time, Hu <span id="S3.SS5.SSS2.p2.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Hu
et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2022</a>)</cite> proposed fedMGDA+ which optimizes each FL client’s loss function respectively and simultaneously. Addressing the same issue, Li <span id="S3.SS5.SSS2.p2.1.5" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2021a</a>)</cite> proposed Ditto, which improves fairness and robustness at the same time.</p>
</div>
<div id="S3.SS5.SSS2.p3" class="ltx_para">
<p id="S3.SS5.SSS2.p3.1" class="ltx_p">While the methods mentioned all pay attention to the accuracy parity notion of fairness, there are also many kinds of research focusing on group fairness. Du <span id="S3.SS5.SSS2.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Du
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite> proposed AgnosticFair, which incorporates an agnostic fairness constraint. Although it has good accuracy and fairness on unknown testing data distribution, it needs prior knowledge to design the re-weighting function, which limits its application in dynamic systems. Cui <span id="S3.SS5.SSS2.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Cui
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite> proposed FCFL, a multi-objective optimization framework that achieves good-intent fairness and group fairness at the same time. Different from AFL, it minimizes the loss of the client with the worst performance and uses a smooth surrogate maximum function considering all clients. A fairness constraint is also added to calculate the disparities among all clients.</p>
</div>
<div id="S3.SS5.SSS2.p4" class="ltx_para">
<p id="S3.SS5.SSS2.p4.1" class="ltx_p"><span id="S3.SS5.SSS2.p4.1.1" class="ltx_text ltx_font_italic">2) Gradient-based approaches: </span>
Here, gradient means the local updated gradient of each client in every local iteration. Wang <span id="S3.SS5.SSS2.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang
et al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2021</a>)</cite> proposed the federated fair averaging (FedFV) algorithm, which aims to average clients’ gradients after mitigating potential conflicts among clients. FedFV detects gradient conflicts through the cosine similarity and modifies both the direction and magnitude of the gradients by iteratively eliminating such conflicts. However, the estimated gradients may be incompatible with the latest updates.</p>
</div>
</section>
<section id="S3.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.3. </span>Fair contribution evaluation</h4>

<div id="S3.SS5.SSS3.p1" class="ltx_para">
<p id="S3.SS5.SSS3.p1.1" class="ltx_p">Contribution evaluation in FL learning indicates that an FL system can evaluate the contribution of different clients without accessing data from the clients. Many methods designed for non-privacy machine learning environments cannot be applied to FL scenarios directly. A general method is to evaluate each client’s model contribution to the aggregated FL model, and a fair evaluation is critical. Unfairness in contribution evaluation may lead to the free-rider issue <cite class="ltx_cite ltx_citemacro_citep">(Hardin and
Cullity, <a href="#bib.bib50" title="" class="ltx_ref">2003</a>)</cite>, which implies that clients contribute little but can get similar benefits as the clients who contribute more. In this part we will introduce five types of existing FL contribution evaluation methods with their typical works.</p>
</div>
<div id="S3.SS5.SSS3.p2" class="ltx_para">
<p id="S3.SS5.SSS3.p2.1" class="ltx_p"><span id="S3.SS5.SSS3.p2.1.1" class="ltx_text ltx_font_italic">1) Self-reported information: </span>
This method of evaluation contribution is based on clients reporting their information actively. Most works based on this method believe their clients are reliable, which is not always correct in practice. Proposed by Zhang <span id="S3.SS5.SSS3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib167" title="" class="ltx_ref">2020a</a>)</cite>, Hierarchically fair federated learning (HFFL) follows the idea of ’contribute more, get more reward’, which is proved effective in social psychology <cite class="ltx_cite ltx_citemacro_citep">(Tornblom and
Jonsson, <a href="#bib.bib138" title="" class="ltx_ref">1985</a>)</cite>, game theory <cite class="ltx_cite ltx_citemacro_citep">(Rabin, <a href="#bib.bib118" title="" class="ltx_ref">1993</a>)</cite> and bandwidth allocation <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2008</a>)</cite>. Hence, it’s critical to figure out how to evaluate a client’s contribution and how much proportion of reward a client should get to ensure fairness. Data Shapley can be used to evaluate contribution in machine learning, but Shapley value is model-dependent <cite class="ltx_cite ltx_citemacro_citep">(Ghorbani and Zou, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite> and incompatible with FL tasks. As a result, Zhang proposes evaluating contributions based on publicly verifiable factors of clients, such as cost of data collection, data volume, and data quality, to avoid the inconsistency of model-dependent methods. To distribute proportional rewards to clients, Zhang introduces hierarchically fair federated learning (HFFL), as is shown in Figure <a href="#S3.F16" title="Figure 16 ‣ 3.5.3. Fair contribution evaluation ‣ 3.5. Fair federated learning ‣ 3. Approaches of federated learning ‣ Recent Advances on Federated Learning: A Systematic Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. The publicly verifiable factors determined by the clients’ consensus about each client are reported to the FL server, and the FL server then uses the information to rate each client, which at the same level are supposed to contribute to the model equally and will get the equal reward.</p>
</div>
<figure id="S3.F16" class="ltx_figure"><img src="/html/2301.01299/assets/HFFL.png" id="S3.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16. </span>The illustration of hierarchically fair federated learning (HFFL) <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib167" title="" class="ltx_ref">2020a</a>)</cite>. </figcaption>
</figure>
<div id="S3.SS5.SSS3.p3" class="ltx_para">
<p id="S3.SS5.SSS3.p3.1" class="ltx_p"><span id="S3.SS5.SSS3.p3.1.1" class="ltx_text ltx_font_italic">2) Individual evaluation: </span>
Individual evaluation implies evaluating contribution through performance on specific tasks and pays more attention to individual performance instead of global performance. The method often adopts two assumptions that both the server and the client are reliable and clients with a similar model to others are regarded to supply more contribution, which is not always feasible. To achieve fairness without sacrificing the model performance, Lyu <span id="S3.SS5.SSS3.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Lyu
et al<span class="ltx_text">.</span>, <a href="#bib.bib99" title="" class="ltx_ref">2020a</a>)</cite> proposed a Collaborative Fair Federated Learning (CFFL) framework based on reputation, which uses a reputation mechanism to achieve collaborative fairness. Lyu definites collaborative fairness as the reward is proportional to the client’s contribution. Different standard FL process, CFFL allows clients to receive only the allocated aggregated updates according to their reputations, and the server is in charge of a reputation list which is updated in each communication round relying on the quality of the uploaded gradients of each participant.</p>
</div>
<div id="S3.SS5.SSS3.p4" class="ltx_para">
<p id="S3.SS5.SSS3.p4.1" class="ltx_p"><span id="S3.SS5.SSS3.p4.1.1" class="ltx_text ltx_font_italic">3) Utility game: </span>
The utility game <cite class="ltx_cite ltx_citemacro_citep">(Gollapudi et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>)</cite> refers to a game where each player chooses an available team to maximize their payoffs, while the universal social welfare is the total utility produced by all the teams cumulatively. FL contribution evaluation methods based on utility games have a deep connection with profit-sharing schemes, and there are three diffusely used profit-sharing schemes: (1) Egalitarian: any part of the utility produced by a team is separated equally between the members.
(2) Marginal gain: the payoff of a player in a team is equal to the team gained when the player joined.
(3) Marginal loss: the payoff of a player in a team is equal to the team will lose if the player leaves.</p>
</div>
<div id="S3.SS5.SSS3.p5" class="ltx_para">
<p id="S3.SS5.SSS3.p5.3" class="ltx_p">Among the three types above, the marginal loss scheme is the most commonly adopted. Wang <span id="S3.SS5.SSS3.p5.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang
et al<span class="ltx_text">.</span>, <a href="#bib.bib141" title="" class="ltx_ref">2019a</a>)</cite> proposed a deletion method to evaluate contributions in horizontal federated learning. This evaluation method consists of removing the instances supplied from one definite party, retraining the model, calculating the difference between the original model and the new model, and using this difference to define the contribution of this party. Wang formulates the influence measure as follows,</p>
<table id="S3.E11" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(11)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E11.m1.1" class="ltx_Math" alttext="Influence^{-i}=\frac{1}{n}\sum_{j=1}^{n}\left|\hat{y}_{j}-\hat{y}_{j}^{-i}\right|," display="block"><semantics id="S3.E11.m1.1a"><mrow id="S3.E11.m1.1.1.1" xref="S3.E11.m1.1.1.1.1.cmml"><mrow id="S3.E11.m1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.cmml"><mrow id="S3.E11.m1.1.1.1.1.3" xref="S3.E11.m1.1.1.1.1.3.cmml"><mi id="S3.E11.m1.1.1.1.1.3.2" xref="S3.E11.m1.1.1.1.1.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.1.1.1.1.3.1" xref="S3.E11.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E11.m1.1.1.1.1.3.3" xref="S3.E11.m1.1.1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.1.1.1.1.3.1a" xref="S3.E11.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E11.m1.1.1.1.1.3.4" xref="S3.E11.m1.1.1.1.1.3.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.1.1.1.1.3.1b" xref="S3.E11.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E11.m1.1.1.1.1.3.5" xref="S3.E11.m1.1.1.1.1.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.1.1.1.1.3.1c" xref="S3.E11.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E11.m1.1.1.1.1.3.6" xref="S3.E11.m1.1.1.1.1.3.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.1.1.1.1.3.1d" xref="S3.E11.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E11.m1.1.1.1.1.3.7" xref="S3.E11.m1.1.1.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.1.1.1.1.3.1e" xref="S3.E11.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E11.m1.1.1.1.1.3.8" xref="S3.E11.m1.1.1.1.1.3.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.1.1.1.1.3.1f" xref="S3.E11.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E11.m1.1.1.1.1.3.9" xref="S3.E11.m1.1.1.1.1.3.9.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.1.1.1.1.3.1g" xref="S3.E11.m1.1.1.1.1.3.1.cmml">​</mo><msup id="S3.E11.m1.1.1.1.1.3.10" xref="S3.E11.m1.1.1.1.1.3.10.cmml"><mi id="S3.E11.m1.1.1.1.1.3.10.2" xref="S3.E11.m1.1.1.1.1.3.10.2.cmml">e</mi><mrow id="S3.E11.m1.1.1.1.1.3.10.3" xref="S3.E11.m1.1.1.1.1.3.10.3.cmml"><mo id="S3.E11.m1.1.1.1.1.3.10.3a" xref="S3.E11.m1.1.1.1.1.3.10.3.cmml">−</mo><mi id="S3.E11.m1.1.1.1.1.3.10.3.2" xref="S3.E11.m1.1.1.1.1.3.10.3.2.cmml">i</mi></mrow></msup></mrow><mo id="S3.E11.m1.1.1.1.1.2" xref="S3.E11.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E11.m1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.cmml"><mfrac id="S3.E11.m1.1.1.1.1.1.3" xref="S3.E11.m1.1.1.1.1.1.3.cmml"><mn id="S3.E11.m1.1.1.1.1.1.3.2" xref="S3.E11.m1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S3.E11.m1.1.1.1.1.1.3.3" xref="S3.E11.m1.1.1.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E11.m1.1.1.1.1.1.2" xref="S3.E11.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E11.m1.1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.1.cmml"><munderover id="S3.E11.m1.1.1.1.1.1.1.2" xref="S3.E11.m1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E11.m1.1.1.1.1.1.1.2.2.2" xref="S3.E11.m1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E11.m1.1.1.1.1.1.1.2.2.3" xref="S3.E11.m1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E11.m1.1.1.1.1.1.1.2.2.3.2" xref="S3.E11.m1.1.1.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S3.E11.m1.1.1.1.1.1.1.2.2.3.1" xref="S3.E11.m1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E11.m1.1.1.1.1.1.1.2.2.3.3" xref="S3.E11.m1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E11.m1.1.1.1.1.1.1.2.3" xref="S3.E11.m1.1.1.1.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S3.E11.m1.1.1.1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E11.m1.1.1.1.1.1.1.1.1.2" xref="S3.E11.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E11.m1.1.1.1.1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">y</mi><mo id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2.1" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S3.E11.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml"><mi id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2.2" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml">y</mi><mo id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2.1" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2.1.cmml">^</mo></mover><mi id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">j</mi><mrow id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mo id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3a" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">−</mo><mi id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">i</mi></mrow></msubsup></mrow><mo id="S3.E11.m1.1.1.1.1.1.1.1.1.3" xref="S3.E11.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><mo id="S3.E11.m1.1.1.1.2" xref="S3.E11.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E11.m1.1b"><apply id="S3.E11.m1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1"><eq id="S3.E11.m1.1.1.1.1.2.cmml" xref="S3.E11.m1.1.1.1.1.2"></eq><apply id="S3.E11.m1.1.1.1.1.3.cmml" xref="S3.E11.m1.1.1.1.1.3"><times id="S3.E11.m1.1.1.1.1.3.1.cmml" xref="S3.E11.m1.1.1.1.1.3.1"></times><ci id="S3.E11.m1.1.1.1.1.3.2.cmml" xref="S3.E11.m1.1.1.1.1.3.2">𝐼</ci><ci id="S3.E11.m1.1.1.1.1.3.3.cmml" xref="S3.E11.m1.1.1.1.1.3.3">𝑛</ci><ci id="S3.E11.m1.1.1.1.1.3.4.cmml" xref="S3.E11.m1.1.1.1.1.3.4">𝑓</ci><ci id="S3.E11.m1.1.1.1.1.3.5.cmml" xref="S3.E11.m1.1.1.1.1.3.5">𝑙</ci><ci id="S3.E11.m1.1.1.1.1.3.6.cmml" xref="S3.E11.m1.1.1.1.1.3.6">𝑢</ci><ci id="S3.E11.m1.1.1.1.1.3.7.cmml" xref="S3.E11.m1.1.1.1.1.3.7">𝑒</ci><ci id="S3.E11.m1.1.1.1.1.3.8.cmml" xref="S3.E11.m1.1.1.1.1.3.8">𝑛</ci><ci id="S3.E11.m1.1.1.1.1.3.9.cmml" xref="S3.E11.m1.1.1.1.1.3.9">𝑐</ci><apply id="S3.E11.m1.1.1.1.1.3.10.cmml" xref="S3.E11.m1.1.1.1.1.3.10"><csymbol cd="ambiguous" id="S3.E11.m1.1.1.1.1.3.10.1.cmml" xref="S3.E11.m1.1.1.1.1.3.10">superscript</csymbol><ci id="S3.E11.m1.1.1.1.1.3.10.2.cmml" xref="S3.E11.m1.1.1.1.1.3.10.2">𝑒</ci><apply id="S3.E11.m1.1.1.1.1.3.10.3.cmml" xref="S3.E11.m1.1.1.1.1.3.10.3"><minus id="S3.E11.m1.1.1.1.1.3.10.3.1.cmml" xref="S3.E11.m1.1.1.1.1.3.10.3"></minus><ci id="S3.E11.m1.1.1.1.1.3.10.3.2.cmml" xref="S3.E11.m1.1.1.1.1.3.10.3.2">𝑖</ci></apply></apply></apply><apply id="S3.E11.m1.1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1.1.1"><times id="S3.E11.m1.1.1.1.1.1.2.cmml" xref="S3.E11.m1.1.1.1.1.1.2"></times><apply id="S3.E11.m1.1.1.1.1.1.3.cmml" xref="S3.E11.m1.1.1.1.1.1.3"><divide id="S3.E11.m1.1.1.1.1.1.3.1.cmml" xref="S3.E11.m1.1.1.1.1.1.3"></divide><cn type="integer" id="S3.E11.m1.1.1.1.1.1.3.2.cmml" xref="S3.E11.m1.1.1.1.1.1.3.2">1</cn><ci id="S3.E11.m1.1.1.1.1.1.3.3.cmml" xref="S3.E11.m1.1.1.1.1.1.3.3">𝑛</ci></apply><apply id="S3.E11.m1.1.1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1"><apply id="S3.E11.m1.1.1.1.1.1.1.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E11.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E11.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E11.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E11.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E11.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2.2.3"><eq id="S3.E11.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E11.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2.2.3.2">𝑗</ci><cn type="integer" id="S3.E11.m1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E11.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2.3">𝑛</ci></apply><apply id="S3.E11.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1"><abs id="S3.E11.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.2"></abs><apply id="S3.E11.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1"><minus id="S3.E11.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2"><ci id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.2.2">𝑦</ci></apply><ci id="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2"><ci id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2.1">^</ci><ci id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.2.2">𝑦</ci></apply><ci id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.2.3">𝑗</ci></apply><apply id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3"><minus id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3"></minus><ci id="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1.1.1.3.3.2">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E11.m1.1c">Influence^{-i}=\frac{1}{n}\sum_{j=1}^{n}\left|\hat{y}_{j}-\hat{y}_{j}^{-i}\right|,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS5.SSS3.p5.2" class="ltx_p">where n is the size of the dataset, <math id="S3.SS5.SSS3.p5.1.m1.1" class="ltx_Math" alttext="\hat{y}_{j}" display="inline"><semantics id="S3.SS5.SSS3.p5.1.m1.1a"><msub id="S3.SS5.SSS3.p5.1.m1.1.1" xref="S3.SS5.SSS3.p5.1.m1.1.1.cmml"><mover accent="true" id="S3.SS5.SSS3.p5.1.m1.1.1.2" xref="S3.SS5.SSS3.p5.1.m1.1.1.2.cmml"><mi id="S3.SS5.SSS3.p5.1.m1.1.1.2.2" xref="S3.SS5.SSS3.p5.1.m1.1.1.2.2.cmml">y</mi><mo id="S3.SS5.SSS3.p5.1.m1.1.1.2.1" xref="S3.SS5.SSS3.p5.1.m1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS5.SSS3.p5.1.m1.1.1.3" xref="S3.SS5.SSS3.p5.1.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p5.1.m1.1b"><apply id="S3.SS5.SSS3.p5.1.m1.1.1.cmml" xref="S3.SS5.SSS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS3.p5.1.m1.1.1.1.cmml" xref="S3.SS5.SSS3.p5.1.m1.1.1">subscript</csymbol><apply id="S3.SS5.SSS3.p5.1.m1.1.1.2.cmml" xref="S3.SS5.SSS3.p5.1.m1.1.1.2"><ci id="S3.SS5.SSS3.p5.1.m1.1.1.2.1.cmml" xref="S3.SS5.SSS3.p5.1.m1.1.1.2.1">^</ci><ci id="S3.SS5.SSS3.p5.1.m1.1.1.2.2.cmml" xref="S3.SS5.SSS3.p5.1.m1.1.1.2.2">𝑦</ci></apply><ci id="S3.SS5.SSS3.p5.1.m1.1.1.3.cmml" xref="S3.SS5.SSS3.p5.1.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p5.1.m1.1c">\hat{y}_{j}</annotation></semantics></math> is the model trained on all data prediction on <span id="S3.SS5.SSS3.p5.2.1" class="ltx_text ltx_font_italic">j</span>th instance, and <math id="S3.SS5.SSS3.p5.2.m2.1" class="ltx_Math" alttext="\hat{y}_{j}^{-i}" display="inline"><semantics id="S3.SS5.SSS3.p5.2.m2.1a"><msubsup id="S3.SS5.SSS3.p5.2.m2.1.1" xref="S3.SS5.SSS3.p5.2.m2.1.1.cmml"><mover accent="true" id="S3.SS5.SSS3.p5.2.m2.1.1.2.2" xref="S3.SS5.SSS3.p5.2.m2.1.1.2.2.cmml"><mi id="S3.SS5.SSS3.p5.2.m2.1.1.2.2.2" xref="S3.SS5.SSS3.p5.2.m2.1.1.2.2.2.cmml">y</mi><mo id="S3.SS5.SSS3.p5.2.m2.1.1.2.2.1" xref="S3.SS5.SSS3.p5.2.m2.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.SS5.SSS3.p5.2.m2.1.1.2.3" xref="S3.SS5.SSS3.p5.2.m2.1.1.2.3.cmml">j</mi><mrow id="S3.SS5.SSS3.p5.2.m2.1.1.3" xref="S3.SS5.SSS3.p5.2.m2.1.1.3.cmml"><mo id="S3.SS5.SSS3.p5.2.m2.1.1.3a" xref="S3.SS5.SSS3.p5.2.m2.1.1.3.cmml">−</mo><mi id="S3.SS5.SSS3.p5.2.m2.1.1.3.2" xref="S3.SS5.SSS3.p5.2.m2.1.1.3.2.cmml">i</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p5.2.m2.1b"><apply id="S3.SS5.SSS3.p5.2.m2.1.1.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS3.p5.2.m2.1.1.1.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1">superscript</csymbol><apply id="S3.SS5.SSS3.p5.2.m2.1.1.2.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS3.p5.2.m2.1.1.2.1.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1">subscript</csymbol><apply id="S3.SS5.SSS3.p5.2.m2.1.1.2.2.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1.2.2"><ci id="S3.SS5.SSS3.p5.2.m2.1.1.2.2.1.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1.2.2.1">^</ci><ci id="S3.SS5.SSS3.p5.2.m2.1.1.2.2.2.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1.2.2.2">𝑦</ci></apply><ci id="S3.SS5.SSS3.p5.2.m2.1.1.2.3.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1.2.3">𝑗</ci></apply><apply id="S3.SS5.SSS3.p5.2.m2.1.1.3.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1.3"><minus id="S3.SS5.SSS3.p5.2.m2.1.1.3.1.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1.3"></minus><ci id="S3.SS5.SSS3.p5.2.m2.1.1.3.2.cmml" xref="S3.SS5.SSS3.p5.2.m2.1.1.3.2">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p5.2.m2.1c">\hat{y}_{j}^{-i}</annotation></semantics></math> s the model trained without the <span id="S3.SS5.SSS3.p5.2.2" class="ltx_text ltx_font_italic">i</span>th instance prediction on <span id="S3.SS5.SSS3.p5.2.3" class="ltx_text ltx_font_italic">j</span>th instance.</p>
</div>
<div id="S3.SS5.SSS3.p6" class="ltx_para">
<p id="S3.SS5.SSS3.p6.1" class="ltx_p">Then Huang defines a party’s contribution as the total influence of all instances it possesses.</p>
<table id="S3.E12" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(12)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E12.m1.1" class="ltx_Math" alttext="Influence^{-D}=\sum_{i\in D}Influence^{-i}," display="block"><semantics id="S3.E12.m1.1a"><mrow id="S3.E12.m1.1.1.1" xref="S3.E12.m1.1.1.1.1.cmml"><mrow id="S3.E12.m1.1.1.1.1" xref="S3.E12.m1.1.1.1.1.cmml"><mrow id="S3.E12.m1.1.1.1.1.2" xref="S3.E12.m1.1.1.1.1.2.cmml"><mi id="S3.E12.m1.1.1.1.1.2.2" xref="S3.E12.m1.1.1.1.1.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.2.1" xref="S3.E12.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.2.3" xref="S3.E12.m1.1.1.1.1.2.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.2.1a" xref="S3.E12.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.2.4" xref="S3.E12.m1.1.1.1.1.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.2.1b" xref="S3.E12.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.2.5" xref="S3.E12.m1.1.1.1.1.2.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.2.1c" xref="S3.E12.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.2.6" xref="S3.E12.m1.1.1.1.1.2.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.2.1d" xref="S3.E12.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.2.7" xref="S3.E12.m1.1.1.1.1.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.2.1e" xref="S3.E12.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.2.8" xref="S3.E12.m1.1.1.1.1.2.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.2.1f" xref="S3.E12.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.2.9" xref="S3.E12.m1.1.1.1.1.2.9.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.2.1g" xref="S3.E12.m1.1.1.1.1.2.1.cmml">​</mo><msup id="S3.E12.m1.1.1.1.1.2.10" xref="S3.E12.m1.1.1.1.1.2.10.cmml"><mi id="S3.E12.m1.1.1.1.1.2.10.2" xref="S3.E12.m1.1.1.1.1.2.10.2.cmml">e</mi><mrow id="S3.E12.m1.1.1.1.1.2.10.3" xref="S3.E12.m1.1.1.1.1.2.10.3.cmml"><mo id="S3.E12.m1.1.1.1.1.2.10.3a" xref="S3.E12.m1.1.1.1.1.2.10.3.cmml">−</mo><mi id="S3.E12.m1.1.1.1.1.2.10.3.2" xref="S3.E12.m1.1.1.1.1.2.10.3.2.cmml">D</mi></mrow></msup></mrow><mo rspace="0.111em" id="S3.E12.m1.1.1.1.1.1" xref="S3.E12.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E12.m1.1.1.1.1.3" xref="S3.E12.m1.1.1.1.1.3.cmml"><munder id="S3.E12.m1.1.1.1.1.3.1" xref="S3.E12.m1.1.1.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E12.m1.1.1.1.1.3.1.2" xref="S3.E12.m1.1.1.1.1.3.1.2.cmml">∑</mo><mrow id="S3.E12.m1.1.1.1.1.3.1.3" xref="S3.E12.m1.1.1.1.1.3.1.3.cmml"><mi id="S3.E12.m1.1.1.1.1.3.1.3.2" xref="S3.E12.m1.1.1.1.1.3.1.3.2.cmml">i</mi><mo id="S3.E12.m1.1.1.1.1.3.1.3.1" xref="S3.E12.m1.1.1.1.1.3.1.3.1.cmml">∈</mo><mi id="S3.E12.m1.1.1.1.1.3.1.3.3" xref="S3.E12.m1.1.1.1.1.3.1.3.3.cmml">D</mi></mrow></munder><mrow id="S3.E12.m1.1.1.1.1.3.2" xref="S3.E12.m1.1.1.1.1.3.2.cmml"><mi id="S3.E12.m1.1.1.1.1.3.2.2" xref="S3.E12.m1.1.1.1.1.3.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.3.2.1" xref="S3.E12.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.3.2.3" xref="S3.E12.m1.1.1.1.1.3.2.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.3.2.1a" xref="S3.E12.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.3.2.4" xref="S3.E12.m1.1.1.1.1.3.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.3.2.1b" xref="S3.E12.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.3.2.5" xref="S3.E12.m1.1.1.1.1.3.2.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.3.2.1c" xref="S3.E12.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.3.2.6" xref="S3.E12.m1.1.1.1.1.3.2.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.3.2.1d" xref="S3.E12.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.3.2.7" xref="S3.E12.m1.1.1.1.1.3.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.3.2.1e" xref="S3.E12.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.3.2.8" xref="S3.E12.m1.1.1.1.1.3.2.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.3.2.1f" xref="S3.E12.m1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.3.2.9" xref="S3.E12.m1.1.1.1.1.3.2.9.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.3.2.1g" xref="S3.E12.m1.1.1.1.1.3.2.1.cmml">​</mo><msup id="S3.E12.m1.1.1.1.1.3.2.10" xref="S3.E12.m1.1.1.1.1.3.2.10.cmml"><mi id="S3.E12.m1.1.1.1.1.3.2.10.2" xref="S3.E12.m1.1.1.1.1.3.2.10.2.cmml">e</mi><mrow id="S3.E12.m1.1.1.1.1.3.2.10.3" xref="S3.E12.m1.1.1.1.1.3.2.10.3.cmml"><mo id="S3.E12.m1.1.1.1.1.3.2.10.3a" xref="S3.E12.m1.1.1.1.1.3.2.10.3.cmml">−</mo><mi id="S3.E12.m1.1.1.1.1.3.2.10.3.2" xref="S3.E12.m1.1.1.1.1.3.2.10.3.2.cmml">i</mi></mrow></msup></mrow></mrow></mrow><mo id="S3.E12.m1.1.1.1.2" xref="S3.E12.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E12.m1.1b"><apply id="S3.E12.m1.1.1.1.1.cmml" xref="S3.E12.m1.1.1.1"><eq id="S3.E12.m1.1.1.1.1.1.cmml" xref="S3.E12.m1.1.1.1.1.1"></eq><apply id="S3.E12.m1.1.1.1.1.2.cmml" xref="S3.E12.m1.1.1.1.1.2"><times id="S3.E12.m1.1.1.1.1.2.1.cmml" xref="S3.E12.m1.1.1.1.1.2.1"></times><ci id="S3.E12.m1.1.1.1.1.2.2.cmml" xref="S3.E12.m1.1.1.1.1.2.2">𝐼</ci><ci id="S3.E12.m1.1.1.1.1.2.3.cmml" xref="S3.E12.m1.1.1.1.1.2.3">𝑛</ci><ci id="S3.E12.m1.1.1.1.1.2.4.cmml" xref="S3.E12.m1.1.1.1.1.2.4">𝑓</ci><ci id="S3.E12.m1.1.1.1.1.2.5.cmml" xref="S3.E12.m1.1.1.1.1.2.5">𝑙</ci><ci id="S3.E12.m1.1.1.1.1.2.6.cmml" xref="S3.E12.m1.1.1.1.1.2.6">𝑢</ci><ci id="S3.E12.m1.1.1.1.1.2.7.cmml" xref="S3.E12.m1.1.1.1.1.2.7">𝑒</ci><ci id="S3.E12.m1.1.1.1.1.2.8.cmml" xref="S3.E12.m1.1.1.1.1.2.8">𝑛</ci><ci id="S3.E12.m1.1.1.1.1.2.9.cmml" xref="S3.E12.m1.1.1.1.1.2.9">𝑐</ci><apply id="S3.E12.m1.1.1.1.1.2.10.cmml" xref="S3.E12.m1.1.1.1.1.2.10"><csymbol cd="ambiguous" id="S3.E12.m1.1.1.1.1.2.10.1.cmml" xref="S3.E12.m1.1.1.1.1.2.10">superscript</csymbol><ci id="S3.E12.m1.1.1.1.1.2.10.2.cmml" xref="S3.E12.m1.1.1.1.1.2.10.2">𝑒</ci><apply id="S3.E12.m1.1.1.1.1.2.10.3.cmml" xref="S3.E12.m1.1.1.1.1.2.10.3"><minus id="S3.E12.m1.1.1.1.1.2.10.3.1.cmml" xref="S3.E12.m1.1.1.1.1.2.10.3"></minus><ci id="S3.E12.m1.1.1.1.1.2.10.3.2.cmml" xref="S3.E12.m1.1.1.1.1.2.10.3.2">𝐷</ci></apply></apply></apply><apply id="S3.E12.m1.1.1.1.1.3.cmml" xref="S3.E12.m1.1.1.1.1.3"><apply id="S3.E12.m1.1.1.1.1.3.1.cmml" xref="S3.E12.m1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E12.m1.1.1.1.1.3.1.1.cmml" xref="S3.E12.m1.1.1.1.1.3.1">subscript</csymbol><sum id="S3.E12.m1.1.1.1.1.3.1.2.cmml" xref="S3.E12.m1.1.1.1.1.3.1.2"></sum><apply id="S3.E12.m1.1.1.1.1.3.1.3.cmml" xref="S3.E12.m1.1.1.1.1.3.1.3"><in id="S3.E12.m1.1.1.1.1.3.1.3.1.cmml" xref="S3.E12.m1.1.1.1.1.3.1.3.1"></in><ci id="S3.E12.m1.1.1.1.1.3.1.3.2.cmml" xref="S3.E12.m1.1.1.1.1.3.1.3.2">𝑖</ci><ci id="S3.E12.m1.1.1.1.1.3.1.3.3.cmml" xref="S3.E12.m1.1.1.1.1.3.1.3.3">𝐷</ci></apply></apply><apply id="S3.E12.m1.1.1.1.1.3.2.cmml" xref="S3.E12.m1.1.1.1.1.3.2"><times id="S3.E12.m1.1.1.1.1.3.2.1.cmml" xref="S3.E12.m1.1.1.1.1.3.2.1"></times><ci id="S3.E12.m1.1.1.1.1.3.2.2.cmml" xref="S3.E12.m1.1.1.1.1.3.2.2">𝐼</ci><ci id="S3.E12.m1.1.1.1.1.3.2.3.cmml" xref="S3.E12.m1.1.1.1.1.3.2.3">𝑛</ci><ci id="S3.E12.m1.1.1.1.1.3.2.4.cmml" xref="S3.E12.m1.1.1.1.1.3.2.4">𝑓</ci><ci id="S3.E12.m1.1.1.1.1.3.2.5.cmml" xref="S3.E12.m1.1.1.1.1.3.2.5">𝑙</ci><ci id="S3.E12.m1.1.1.1.1.3.2.6.cmml" xref="S3.E12.m1.1.1.1.1.3.2.6">𝑢</ci><ci id="S3.E12.m1.1.1.1.1.3.2.7.cmml" xref="S3.E12.m1.1.1.1.1.3.2.7">𝑒</ci><ci id="S3.E12.m1.1.1.1.1.3.2.8.cmml" xref="S3.E12.m1.1.1.1.1.3.2.8">𝑛</ci><ci id="S3.E12.m1.1.1.1.1.3.2.9.cmml" xref="S3.E12.m1.1.1.1.1.3.2.9">𝑐</ci><apply id="S3.E12.m1.1.1.1.1.3.2.10.cmml" xref="S3.E12.m1.1.1.1.1.3.2.10"><csymbol cd="ambiguous" id="S3.E12.m1.1.1.1.1.3.2.10.1.cmml" xref="S3.E12.m1.1.1.1.1.3.2.10">superscript</csymbol><ci id="S3.E12.m1.1.1.1.1.3.2.10.2.cmml" xref="S3.E12.m1.1.1.1.1.3.2.10.2">𝑒</ci><apply id="S3.E12.m1.1.1.1.1.3.2.10.3.cmml" xref="S3.E12.m1.1.1.1.1.3.2.10.3"><minus id="S3.E12.m1.1.1.1.1.3.2.10.3.1.cmml" xref="S3.E12.m1.1.1.1.1.3.2.10.3"></minus><ci id="S3.E12.m1.1.1.1.1.3.2.10.3.2.cmml" xref="S3.E12.m1.1.1.1.1.3.2.10.3.2">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E12.m1.1c">Influence^{-D}=\sum_{i\in D}Influence^{-i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.SSS3.p7" class="ltx_para">
<p id="S3.SS5.SSS3.p7.1" class="ltx_p">For vertical horizontal learning, Huang uses shapley value which will be introduced in the next part.</p>
</div>
<div id="S3.SS5.SSS3.p8" class="ltx_para">
<p id="S3.SS5.SSS3.p8.1" class="ltx_p"><span id="S3.SS5.SSS3.p8.1.1" class="ltx_text ltx_font_italic">4) Shapley value: </span>
Shapley value (SV) was first introduced in cooperative game theory <cite class="ltx_cite ltx_citemacro_citep">(Shapley, <a href="#bib.bib123" title="" class="ltx_ref">1997</a>)</cite>. Different from marginal loss, SV-based FL contribution evaluation approaches can reflect the contribution of a client’s own data, in spite of its joining order, and can produce a fairer evaluation. However, SV’s computational complexity is <math id="S3.SS5.SSS3.p8.1.m1.1" class="ltx_Math" alttext="O(2^{n})" display="inline"><semantics id="S3.SS5.SSS3.p8.1.m1.1a"><mrow id="S3.SS5.SSS3.p8.1.m1.1.1" xref="S3.SS5.SSS3.p8.1.m1.1.1.cmml"><mi id="S3.SS5.SSS3.p8.1.m1.1.1.3" xref="S3.SS5.SSS3.p8.1.m1.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS5.SSS3.p8.1.m1.1.1.2" xref="S3.SS5.SSS3.p8.1.m1.1.1.2.cmml">​</mo><mrow id="S3.SS5.SSS3.p8.1.m1.1.1.1.1" xref="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS5.SSS3.p8.1.m1.1.1.1.1.2" xref="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1" xref="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.cmml"><mn id="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.2" xref="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.2.cmml">2</mn><mi id="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.3" xref="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.3.cmml">n</mi></msup><mo stretchy="false" id="S3.SS5.SSS3.p8.1.m1.1.1.1.1.3" xref="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p8.1.m1.1b"><apply id="S3.SS5.SSS3.p8.1.m1.1.1.cmml" xref="S3.SS5.SSS3.p8.1.m1.1.1"><times id="S3.SS5.SSS3.p8.1.m1.1.1.2.cmml" xref="S3.SS5.SSS3.p8.1.m1.1.1.2"></times><ci id="S3.SS5.SSS3.p8.1.m1.1.1.3.cmml" xref="S3.SS5.SSS3.p8.1.m1.1.1.3">𝑂</ci><apply id="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.cmml" xref="S3.SS5.SSS3.p8.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS5.SSS3.p8.1.m1.1.1.1.1">superscript</csymbol><cn type="integer" id="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.2">2</cn><ci id="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS5.SSS3.p8.1.m1.1.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p8.1.m1.1c">O(2^{n})</annotation></semantics></math>, so many approaches have been proposed to improve efficiency.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Prevalent frameworks of federated learning</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we will introduce several prevalent frameworks of federated learning, including FedLab, Flower, FedML, FATE, and FedScale.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">FedLab.</span>
Since most FL schemes follow the same basic steps and just a few changes in some steps are needed in different scenarios, Zeng <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Zeng
et al<span class="ltx_text">.</span>, <a href="#bib.bib165" title="" class="ltx_ref">2021</a>)</cite> proposed FedLab <cite class="ltx_cite ltx_citemacro_citep">(SMILELab-FL, <a href="#bib.bib127" title="" class="ltx_ref">2021</a>)</cite>, which is designed flexible and customizable, offers essential functional modules, and has highly customizable interfaces. Two main roles in FL settings are provided: Server and Client, and both of them are made up of two components, NetworkManager and ParameterServerHandler/Trainer. The design focuses more on communication efficiency and FL algorithm effectiveness. To support methods improving Communication Efficiency, FedLab uses tensor-based communication, supports customizable communication agreement, and implements both Synchronous and Asynchronous communication patterns according to Federated Optimization algorithms. For Optimization Effectiveness, FedLab applies a ”high-cohesion and low-coupling” optimization module which provides aggregation and data partition methods. Additionally, FedLab can be used in various scenarios, such as Standalone, Cross-process and Hierarchical FL simulation.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Flower.</span>
Due to the lack of frameworks that are able to support scalably executing FL methods on mobile and edge devices, Beutel <span id="S4.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Beutel et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> proposed Flower <cite class="ltx_cite ltx_citemacro_citep">(adap, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, which can run large-scale FL experiments on different FL device scenarios. Flower makes it possible to smoothly transition from experimental research to system research on a large group of real edge devices. Designed to be scalable, client-agnostic, communication-agnostic, privacy-agnostic, and flexible, Flower has extensive implementations, such as communication stack, serialization, ClientProxy, and Virtual Client Engine(VCE).</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">FedML.</span>
Proposed by He <span id="S4.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite>, FedML <cite class="ltx_cite ltx_citemacro_citep">(FedML-AI, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> aims to solve the lack of support for diverse FL computing paradigms, support of diverse FL configurations, and standardized FL algorithm implementations and benchmarks. FedML library is mainly made up of high-level API FedML-API and low-level API FedML-core. To support FL on real-world hardware platforms, FedML offers on-device FL testbeds called FedML-Mobile and FedML-IoT which are built upon real-world hardware platforms. FedML programming interface allows worker/client-oriented programming, message definition beyond gradient and model, topology management, trainer and coordinator, privacy, security, and robustness, so users can just pay attention to algorithms implementations and ignore the backend details.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">FATE.</span>
Since most open-sourced frameworks are research-oriented and lack the implementation on industry, Liu <span id="S4.p5.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021c</a>)</cite> proposed FATE(Federated AI Technology Enabler) <cite class="ltx_cite ltx_citemacro_citep">(WeBank, <a href="#bib.bib146" title="" class="ltx_ref">2019</a>)</cite>, which is the first production-oriented platform. Built on FederatedML, FATE provides Private Set Intersection(PSI), and uses distributed computation framework Eggroll to improve computation efficiency. FATE provides three main components, scheduling system FATE-Flow, visualization tool FATE-Board, and high-performance inference platform FATE-Serving. In addition, kinds of deployments are supported, including building FATE on top of Kubernetes in data centers through KubeFATE, manual or docker deployments on Mac and Linux, and cross-cloud deployment and management through FATE-cloud.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">FedScale.</span>
Lai <span id="S4.p6.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Lai
et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2021a</a>)</cite> proposed FedScale <cite class="ltx_cite ltx_citemacro_citep">(SymbioticLab, <a href="#bib.bib132" title="" class="ltx_ref">2021</a>)</cite>, which contains many realistic FL datasets for different tasks, and FedScale Runtime which is an automated evaluation platform aiming to simplify and standardize FL evaluation in more realistic environments. The raw data of FedScale datasets are collected from various sources, processed into consistent formats, sorted into different FL use cases and packed into standardized APIs for users to easily use in other frameworks. The evaluation platform, FedScale Runtime, is equipped with both mobile and cluster backends to enable both on-device FL evaluation on smartphones, and FL evaluations in real deployments and in-cluster simulations.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section summarizes some limitations of current FL approaches and discusses possible future directions.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Dynamic federated learning.</span>
Current federated learning approaches assume that data in each client are stable and unchanged. However, in real-world scenarios, clients may be in an ever-changing environment, where the local data are continuously observed and processed by sensors. Under this condition, directly conducting conventional training and aggregation will suffer from the catastrophic forgetting problem, which indicates that the prior knowledge learned by the model might be forgotten as new data arrive. Incremental learning <cite class="ltx_cite ltx_citemacro_citep">(Castro et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2018</a>; Wu
et al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2019</a>; Lomonaco
et al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">2021</a>)</cite> is a hot research topic to address the issue, targeting at learning new knowledge
while maintaining the ability to recognize previous ones. In the future, how to effectively combine federated learning and incremental learning is worth exploring.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Decentralized federated learning.</span>
A central server is of vital importance to traditional federated learning since aggregation needs to be conducted in this side. Considering that the third-party server may not be honest, uploading parameters or gradients to it potentially exists security risks. Therefore, it is necessary to achieve federated learning without a server involved.
Although He <span id="S5.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(He
et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite> has made a preliminary attempt to decentralized FL, they only target logistic regression and the experiments are insufficient.
How to accomplish general decentralized FL still remains an open problem.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Scalability of federated learning.</span>
Recent FL papers paid more attention to designing new algorithms to improve FL performance under different conditions. However, they ignore the scalability property, which determines whether we could operate large-scale FL. In many cooperation scenarios, there might be a huge number of parties and we should provide guidance to the cooperation improvement as the number of participants increases. In a word, FL scalability deserves future investigation.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Unified benchmark.</span>
Although a large number of datasets have been used for evaluating the performance of FL, there is still a lack of a unified benchmark to align the results for a fair comparison. On one hand, in order to achieve different federated goals (e.g., personalization, robustness), researchers use different datasets to test the performance. On the other hand, two typical types of FL, horizontal FL and vertical FL, also apply distinctive datasets to demonstrate the performance of different FL types. Thus a unified benchmark will definitely benefit the FL community.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Federated learning has gained more and more attention due to its ability of collaboratively generating a global model without leaking sensitive information. Recent surveys have summarized many related works devoted to offering a comprehensive understanding to developers and readers in this community. However, most of them focus on a specific aspect of FL or fail to catch the latest progress of this hot research topic. This paper provides a systematic survey, which investigates recent development on federated learning. By analyzing the pipeline and challenges of FL, we propose a taxonomy with different FL aspects involved. In addition, we also explore some practical FL frameworks and characterize their features. Finally, some limitations and future direction are concluded in order to promote the evolution of the FL community.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acar et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Durmus Alp Emre Acar, Yue
Zhao, Ruizhao Zhu, Ramon Matas,
Matthew Mattina, Paul Whatmough, and
Venkatesh Saligrama. 2021.

</span>
<span class="ltx_bibblock">Debiasing model updates for improving personalized
federated training. In <em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">International Conference on
Machine Learning</em>. PMLR, 21–31.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">adap (2020)</span>
<span class="ltx_bibblock">
adap. 2020.

</span>
<span class="ltx_bibblock">Flower - A Friendly Federated Learning Framework.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/adap/flower" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/adap/flower</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal
et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Naman Agarwal, Peter
Kairouz, and Ziyu Liu. 2021.

</span>
<span class="ltx_bibblock">The skellam mechanism for differentially private
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 34 (2021),
5052–5064.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Albrecht (2016)</span>
<span class="ltx_bibblock">
Jan Philipp Albrecht.
2016.

</span>
<span class="ltx_bibblock">How the GDPR will change the world.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Eur. Data Prot. L. Rev.</em>
2 (2016), 287.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ARM (2009)</span>
<span class="ltx_bibblock">
Architecure ARM.
2009.

</span>
<span class="ltx_bibblock">Security technology building a secure system using
trustzone technology (white paper).

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ARM Limited</em> (2009).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Auer
et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2002)</span>
<span class="ltx_bibblock">
Peter Auer, Nicolo
Cesa-Bianchi, Yoav Freund, and Robert E
Schapire. 2002.

</span>
<span class="ltx_bibblock">The nonstochastic multiarmed bandit problem.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">SIAM journal on computing</em>
32, 1 (2002),
48–77.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan,
Andreas Veit, Yiqing Hua,
Deborah Estrin, and Vitaly Shmatikov.
2020.

</span>
<span class="ltx_bibblock">How to backdoor federated learning. In
<em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence
and Statistics</em>. PMLR, 2938–2948.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baruch
et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Gilad Baruch, Moran
Baruch, and Yoav Goldberg.
2019.

</span>
<span class="ltx_bibblock">A little is enough: Circumventing defenses for
distributed learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Daniel J Beutel, Taner
Topal, Akhil Mathur, Xinchi Qiu,
Titouan Parcollet, Pedro PB de
Gusmão, and Nicholas D Lane.
2020.

</span>
<span class="ltx_bibblock">Flower: A friendly federated learning research
framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.14390</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhagoji et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Arjun Nitin Bhagoji,
Supriyo Chakraborty, Prateek Mittal,
and Seraphin Calo. 2019.

</span>
<span class="ltx_bibblock">Analyzing federated learning through an adversarial
lens. In <em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>. PMLR, 634–643.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bilen and Vedaldi (2017)</span>
<span class="ltx_bibblock">
Hakan Bilen and Andrea
Vedaldi. 2017.

</span>
<span class="ltx_bibblock">Universal representations: The missing link between
faces, text, planktons, and cat breeds.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1701.07275</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir
Ivanov, Ben Kreuter, Antonio Marcedone,
H Brendan McMahan, Sarvar Patel,
Daniel Ramage, Aaron Segal, and
Karn Seth. 2017.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving
machine learning. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security</em>.
1175–1191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bouacida
et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Nader Bouacida, Jiahui
Hou, Hui Zang, and Xin Liu.
2021.

</span>
<span class="ltx_bibblock">Adaptive Federated Dropout: Improving Communication
Efficiency and Generalization for Federated Learning. In
<em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM 2021 - IEEE Conference on Computer
Communications Workshops (INFOCOM WKSHPS)</em>. 1–6.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/INFOCOMWKSHPS51825.2021.9484526" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/INFOCOMWKSHPS51825.2021.9484526</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Sebastian Caldas, Jakub
Konečny, H Brendan McMahan, and
Ameet Talwalkar. 2018.

</span>
<span class="ltx_bibblock">Expanding the reach of federated learning by
reducing client resource requirements.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.07210</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castro et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Francisco M Castro,
Manuel J Marín-Jiménez,
Nicolás Guil, Cordelia Schmid, and
Karteek Alahari. 2018.

</span>
<span class="ltx_bibblock">End-to-end incremental learning. In
<em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer
vision (ECCV)</em>. 233–248.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Chong Chen, Fei Sun,
Min Zhang, and Bolin Ding.
2022.

</span>
<span class="ltx_bibblock">Recommendation Unlearning. In
<em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM Web Conference 2022</em>
(Virtual Event, Lyon, France) <em id="bib.bib17.4.2" class="ltx_emph ltx_font_italic">(WWW ’22)</em>.
Association for Computing Machinery,
New York, NY, USA, 2768–2777.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3485447.3511997" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3485447.3511997</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Chong Chen, Min Zhang,
Weizhi Ma, Yiqun Liu, and
Shaoping Ma. 2020b.

</span>
<span class="ltx_bibblock">Efficient non-sampling factorization machines for
optimal context-aware recommendation. In
<em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of The Web Conference 2020</em>.
2400–2410.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Chong Chen, Min Zhang,
Weizhi Ma, Yiqun Liu, and
Shaoping Ma. 2020c.

</span>
<span class="ltx_bibblock">Jointly non-sampling learning for knowledge graph
enhanced recommendation. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval</em>. 189–198.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Fei Chen, Mi Luo,
Zhenhua Dong, Zhenguo Li, and
Xiuqiang He. 2018.

</span>
<span class="ltx_bibblock">Federated meta-learning with fast convergence and
efficient communication.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.07876</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Chao (2020)</span>
<span class="ltx_bibblock">
Hong-You Chen and
Wei-Lun Chao. 2020.

</span>
<span class="ltx_bibblock">Fedbe: Making bayesian model ensemble applicable to
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ICLR</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Zhang (2022)</span>
<span class="ltx_bibblock">
Jiayi Chen and Aidong
Zhang. 2022.

</span>
<span class="ltx_bibblock">FedMSplit: Correlation-Adaptive Federated
Multi-Task Learning across Multimodal Split Networks. In
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining</em>. 87–96.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Xiangyi Chen, Tiancong
Chen, Haoran Sun, Steven Z Wu, and
Mingyi Hong. 2020a.

</span>
<span class="ltx_bibblock">Distributed training with heterogeneous data:
Bridging median-and mean-based algorithms.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
21616–21626.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho
et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yae Jee Cho, Jianyu Wang,
and Gauri Joshi. 2020.

</span>
<span class="ltx_bibblock">Client selection in federated learning: Convergence
analysis and power-of-choice selection strategies.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.01243</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corinzia
et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Luca Corinzia, Ami
Beuret, and Joachim M Buhmann.
2019.

</span>
<span class="ltx_bibblock">Variational federated multi-task learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.06268</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costan and
Devadas (2016)</span>
<span class="ltx_bibblock">
Victor Costan and
Srinivas Devadas. 2016.

</span>
<span class="ltx_bibblock">Intel SGX explained.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Cryptology ePrint Archive</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui
et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Sen Cui, Weishen Pan,
Jian Liang, Changshui Zhang, and
Fei Wang. 2021.

</span>
<span class="ltx_bibblock">Addressing algorithmic disparity and performance
inconsistency in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 34 (2021),
26091–26102.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin
et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei
Chang, Kenton Lee, and Kristina
Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional
transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diao
et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Enmao Diao, Jie Ding,
and Vahid Tarokh. 2021.

</span>
<span class="ltx_bibblock">HeteroFL: Computation and communication efficient
federated learning for heterogeneous clients.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">ICLR</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du
et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Wei Du, Depeng Xu,
Xintao Wu, and Hanghang Tong.
2021.

</span>
<span class="ltx_bibblock">Fairness-aware agnostic federated learning. In
<em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 SIAM International
Conference on Data Mining (SDM)</em>. SIAM, 181–189.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2008)</span>
<span class="ltx_bibblock">
Cynthia Dwork.
2008.

</span>
<span class="ltx_bibblock">Differential privacy: A survey of results. In
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">International conference on theory and applications
of models of computation</em>. Springer, 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Cynthia Dwork, Moritz
Hardt, Toniann Pitassi, Omer Reingold,
and Richard Zemel. 2012.

</span>
<span class="ltx_bibblock">Fairness through awareness. In
<em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd innovations in theoretical
computer science conference</em>. 214–226.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Enthoven and
Al-Ars (2021)</span>
<span class="ltx_bibblock">
David Enthoven and Zaid
Al-Ars. 2021.

</span>
<span class="ltx_bibblock">Fidel: Reconstructing private training samples from
weight updates in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00159</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fallah
et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Alireza Fallah, Aryan
Mokhtari, and Asuman Ozdaglar.
2020.

</span>
<span class="ltx_bibblock">Personalized federated learning: A meta-learning
approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.07948</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang
et al<span id="bib.bib35.4.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Minghong Fang, Xiaoyu
Cao, Jinyuan Jia, and Neil Gong.
2020.

</span>
<span class="ltx_bibblock">Local model poisoning attacks to
<math id="bib.bib35.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib35.1.m1.1a"><mo stretchy="false" id="bib.bib35.1.m1.1.1" xref="bib.bib35.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib35.1.m1.1b"><ci id="bib.bib35.1.m1.1.1.cmml" xref="bib.bib35.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib35.1.m1.1c">\{</annotation></semantics></math>Byzantine-Robust<math id="bib.bib35.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib35.2.m2.1a"><mo stretchy="false" id="bib.bib35.2.m2.1.1" xref="bib.bib35.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib35.2.m2.1b"><ci id="bib.bib35.2.m2.1.1.cmml" xref="bib.bib35.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib35.2.m2.1c">\}</annotation></semantics></math> federated learning. In
<em id="bib.bib35.5.1" class="ltx_emph ltx_font_italic">29th USENIX Security Symposium (USENIX Security
20)</em>. 1605–1622.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FedML-AI (2020)</span>
<span class="ltx_bibblock">
FedML-AI. 2020.

</span>
<span class="ltx_bibblock">FedML: The Community Building Open and Collaborative
AI Anywhere at Any Scale.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/FedML-AI/FedML" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/FedML-AI/FedML</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finn
et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Chelsea Finn, Pieter
Abbeel, and Sergey Levine.
2017.

</span>
<span class="ltx_bibblock">Model-agnostic meta-learning for fast adaptation of
deep networks. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">International conference on
machine learning</em>. PMLR, 1126–1135.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finn
et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Chelsea Finn, Kelvin Xu,
and Sergey Levine. 2018.

</span>
<span class="ltx_bibblock">Probabilistic model-agnostic meta-learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 31 (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut
Bauermeister, Hannah Dröge, and
Michael Moeller. 2020.

</span>
<span class="ltx_bibblock">Inverting gradients-how easy is it to break privacy
in federated learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
16937–16947.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gentry (2009)</span>
<span class="ltx_bibblock">
Craig Gentry.
2009.

</span>
<span class="ltx_bibblock">Fully homomorphic encryption using ideal lattices.
In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the forty-first annual ACM
symposium on Theory of computing</em>. 169–178.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geyer
et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Robin C Geyer, Tassilo
Klein, and Moin Nabi. 2017.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client
level perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.07557</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghorbani and Zou (2019)</span>
<span class="ltx_bibblock">
Amirata Ghorbani and
James Zou. 2019.

</span>
<span class="ltx_bibblock">Data shapley: Equitable valuation of data for
machine learning. In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">International Conference on
Machine Learning</em>. PMLR, 2242–2251.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh
et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Avishek Ghosh, Jichan
Chung, Dong Yin, and Kannan
Ramchandran. 2020.

</span>
<span class="ltx_bibblock">An efficient framework for clustered federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
19586–19597.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh
et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Avishek Ghosh, Justin
Hong, Dong Yin, and Kannan
Ramchandran. 2019.

</span>
<span class="ltx_bibblock">Robust federated learning in a heterogeneous
environment.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.06629</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girgis et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Antonious Girgis, Deepesh
Data, Suhas Diggavi, Peter Kairouz,
and Ananda Theertha Suresh.
2021.

</span>
<span class="ltx_bibblock">Shuffled model of differential privacy in federated
learning. In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">International Conference on
Artificial Intelligence and Statistics</em>. PMLR, 2521–2529.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gollapudi et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Sreenivas Gollapudi,
Kostas Kollias, Debmalya Panigrahi, and
Venetia Pliatsika. 2017.

</span>
<span class="ltx_bibblock">Profit sharing and efficiency in utility games. In
<em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">25th Annual European Symposium on Algorithms (ESA
2017)</em>. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanzely et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Filip Hanzely,
Slavomír Hanzely, Samuel
Horváth, and Peter Richtárik.
2020.

</span>
<span class="ltx_bibblock">Lower bounds and optimal algorithms for
personalized federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
2304–2315.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanzely and
Richtárik (2020)</span>
<span class="ltx_bibblock">
Filip Hanzely and Peter
Richtárik. 2020.

</span>
<span class="ltx_bibblock">Federated learning of a mixture of global and local
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.05516</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka
Rao, Rajiv Mathews, Swaroop Ramaswamy,
Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé
Kiddon, and Daniel Ramage.
2018.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardin and
Cullity (2003)</span>
<span class="ltx_bibblock">
Russell Hardin and
Garrett Cullity. 2003.

</span>
<span class="ltx_bibblock">The free rider problem.

</span>
<span class="ltx_bibblock">(2003).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardy et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Stephen Hardy, Wilko
Henecka, Hamish Ivey-Law, Richard Nock,
Giorgio Patrini, Guillaume Smith, and
Brian Thorne. 2017.

</span>
<span class="ltx_bibblock">Private federated learning on vertically
partitioned data via entity resolution and additively homomorphic
encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.10677</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Chaoyang He, Emir Ceyani,
Keshav Balasubramanian, Murali Annavaram,
and Salman Avestimehr. 2021.

</span>
<span class="ltx_bibblock">Spreadgnn: Serverless multi-task federated learning
for graph neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.02743</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib53.3.3.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li,
Jinhyun So, Xiao Zeng,
Mi Zhang, Hongyi Wang,
Xiaoyang Wang, Praneeth Vepakomma,
Abhishek Singh, Hang Qiu,
et al<span id="bib.bib53.4.1" class="ltx_text">.</span> 2020.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for
federated machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He
et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Chaoyang He, Conghui Tan,
Hanlin Tang, Shuang Qiu, and
Ji Liu. 2019.

</span>
<span class="ltx_bibblock">Central server free federated learning over
single-sided trust social networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.04956</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton
et al<span id="bib.bib55.3.3.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol
Vinyals, Jeff Dean, et al<span id="bib.bib55.4.1" class="ltx_text">.</span>
2015.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1503.02531</em>
2, 7 (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hitaj
et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Briland Hitaj, Giuseppe
Ateniese, and Fernando Perez-Cruz.
2017.

</span>
<span class="ltx_bibblock">Deep models under the GAN: information leakage from
collaborative deep learning. In <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
2017 ACM SIGSAC conference on computer and communications security</em>.
603–618.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horvath et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Samuel Horvath, Stefanos
Laskaridis, Mario Almeida, Ilias
Leontiadis, Stylianos Venieris, and
Nicholas Lane. 2021.

</span>
<span class="ltx_bibblock">Fjord: Fair and accurate federated learning under
heterogeneous targets with ordered dropout.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 34 (2021),
12876–12889.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu
et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zeou Hu, Kiarash
Shaloudegi, Guojun Zhang, and Yaoliang
Yu. 2022.

</span>
<span class="ltx_bibblock">Federated learning meets multi-objective
optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network Science and
Engineering</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang
et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Tiansheng Huang, Weiwei
Lin, Li Shen, Keqin Li, and
Albert Y Zomaya. 2022.

</span>
<span class="ltx_bibblock">Stochastic client selection for federated learning
with volatile clients.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang
et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tiansheng Huang, Weiwei
Lin, Wentai Wu, Ligang He,
Keqin Li, and Albert Y Zomaya.
2020.

</span>
<span class="ltx_bibblock">An efficiency-boosting client selection scheme for
federated learning with fairness guarantee.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed
Systems</em> 32, 7 (2020),
1552–1564.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Yutao Huang, Lingyang
Chu, Zirui Zhou, Lanjun Wang,
Jiangchuan Liu, Jian Pei, and
Yong Zhang. 2021a.

</span>
<span class="ltx_bibblock">Personalized Cross-Silo Federated Learning on
Non-IID Data.. In <em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">AAAI</em>.
7865–7873.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang
et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Yangsibo Huang, Samyak
Gupta, Zhao Song, Kai Li, and
Sanjeev Arora. 2021b.

</span>
<span class="ltx_bibblock">Evaluating gradient inversion attacks and defenses
in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 34 (2021),
7232–7241.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izmailov et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Pavel Izmailov, Dmitrii
Podoprikhin, Timur Garipov, Dmitry
Vetrov, and Andrew Gordon Wilson.
2018.

</span>
<span class="ltx_bibblock">Averaging weights leads to wider optima and better
generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05407</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yihan Jiang, Jakub
Konečnỳ, Keith Rush, and
Sreeram Kannan. 2019.

</span>
<span class="ltx_bibblock">Improving federated learning personalization via
model agnostic meta learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.12488</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz
et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Peter Kairouz, Ziyu Liu,
and Thomas Steinke. 2021a.

</span>
<span class="ltx_bibblock">The distributed discrete gaussian mechanism for
federated learning with secure aggregation. In
<em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.
PMLR, 5201–5212.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz
et al<span id="bib.bib66.3.3.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan
McMahan, Brendan Avent, Aurélien
Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary
Charles, Graham Cormode, Rachel
Cummings, et al<span id="bib.bib66.4.1" class="ltx_text">.</span> 2021b.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.5.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in
Machine Learning</em> 14, 1–2
(2021), 1–210.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khodak
et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Mikhail Khodak,
Maria-Florina F Balcan, and Ameet S
Talwalkar. 2019.

</span>
<span class="ltx_bibblock">Adaptive gradient-based meta-learning methods.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky
et al<span id="bib.bib68.3.3.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Geoffrey
Hinton, et al<span id="bib.bib68.4.1" class="ltx_text">.</span> 2009.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny
images.

</span>
<span class="ltx_bibblock">(2009).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky
et al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya
Sutskever, and Geoffrey E Hinton.
2012.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional
neural networks. In <em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>. 1097–1105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuhn (1955)</span>
<span class="ltx_bibblock">
Harold W Kuhn.
1955.

</span>
<span class="ltx_bibblock">The Hungarian method for the assignment problem.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Naval research logistics quarterly</em>
2, 1-2 (1955),
83–97.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai
et al<span id="bib.bib71.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Fan Lai, Yinwei Dai,
Xiangfeng Zhu, Harsha V Madhyastha, and
Mosharaf Chowdhury. 2021a.

</span>
<span class="ltx_bibblock">FedScale: Benchmarking model and system performance
of federated learning. In <em id="bib.bib71.3.1" class="ltx_emph ltx_font_italic">Proceedings of the First
Workshop on Systems Challenges in Reliable and Secure Federated Learning</em>.
1–3.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai
et al<span id="bib.bib72.6.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Fan Lai, Xiangfeng Zhu,
Harsha V Madhyastha, and Mosharaf
Chowdhury. 2021b.

</span>
<span class="ltx_bibblock">Oort: Efficient federated learning via guided
participant selection. In <em id="bib.bib72.4.4" class="ltx_emph ltx_font_italic">15th <math id="bib.bib72.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib72.1.1.m1.1a"><mo stretchy="false" id="bib.bib72.1.1.m1.1.1" xref="bib.bib72.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib72.1.1.m1.1b"><ci id="bib.bib72.1.1.m1.1.1.cmml" xref="bib.bib72.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib72.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib72.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib72.2.2.m2.1a"><mo stretchy="false" id="bib.bib72.2.2.m2.1.1" xref="bib.bib72.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib72.2.2.m2.1b"><ci id="bib.bib72.2.2.m2.1.1.cmml" xref="bib.bib72.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib72.2.2.m2.1c">\}</annotation></semantics></math>
Symposium on Operating Systems Design and Implementation (<math id="bib.bib72.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib72.3.3.m3.1a"><mo stretchy="false" id="bib.bib72.3.3.m3.1.1" xref="bib.bib72.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib72.3.3.m3.1b"><ci id="bib.bib72.3.3.m3.1.1.cmml" xref="bib.bib72.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib72.3.3.m3.1c">\{</annotation></semantics></math>OSDI<math id="bib.bib72.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib72.4.4.m4.1a"><mo stretchy="false" id="bib.bib72.4.4.m4.1.1" xref="bib.bib72.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib72.4.4.m4.1b"><ci id="bib.bib72.4.4.m4.1.1.cmml" xref="bib.bib72.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib72.4.4.m4.1c">\}</annotation></semantics></math> 21)</em>.
19–35.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam
et al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Maximilian Lam, Gu-Yeon
Wei, David Brooks, Vijay Janapa Reddi,
and Michael Mitzenmacher.
2021.

</span>
<span class="ltx_bibblock">Gradient disaggregation: Breaking privacy in
federated learning by reconstructing the user participant matrix. In
<em id="bib.bib73.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.
PMLR, 5959–5968.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun
et al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (1998)</span>
<span class="ltx_bibblock">
Yann LeCun, Léon
Bottou, Yoshua Bengio, and Patrick
Haffner. 1998.

</span>
<span class="ltx_bibblock">Gradient-based learning applied to document
recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.3.1" class="ltx_emph ltx_font_italic">Proc. IEEE</em> 86,
11 (1998), 2278–2324.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Chenning Li, Xiao Zeng,
Mi Zhang, and Zhichao Cao.
2022a.

</span>
<span class="ltx_bibblock">PyramidFL: A Fine-grained Client Selection
Framework for Efficient Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">Mobicom</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Wang (2019)</span>
<span class="ltx_bibblock">
Daliang Li and Junpu
Wang. 2019.

</span>
<span class="ltx_bibblock">Fedmd: Heterogenous federated learning via model
distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03581</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib77.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Fengjiao Li, Jia Liu,
and Bo Ji. 2019a.

</span>
<span class="ltx_bibblock">Combinatorial sleeping bandits with fairness
constraints.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network Science and
Engineering</em> 7, 3
(2019), 1799–1813.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2008)</span>
<span class="ltx_bibblock">
Li Li, Martin Pal, and
Yang Richard Yang. 2008.

</span>
<span class="ltx_bibblock">Proportional fairness in multi-rate wireless LANs.
In <em id="bib.bib78.3.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM 2008-The 27th Conference on
Computer Communications</em>. IEEE, 1004–1012.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib79.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen,
Zhaomin Wu, Sixu Hu,
Naibo Wang, Yuan Li, Xu
Liu, and Bingsheng He.
2021b.

</span>
<span class="ltx_bibblock">A survey on federated learning systems: vision,
hype and reality for data privacy and protection.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data
Engineering</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib80.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Tian Li, Shengyuan Hu,
Ahmad Beirami, and Virginia Smith.
2021a.

</span>
<span class="ltx_bibblock">Ditto: Fair and robust federated learning through
personalization. In <em id="bib.bib80.3.1" class="ltx_emph ltx_font_italic">International Conference on
Machine Learning</em>. PMLR, 6357–6368.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib81.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu,
Ameet Talwalkar, and Virginia Smith.
2020b.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future
directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.3.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>
37, 3 (2020),
50–60.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu,
Manzil Zaheer, Maziar Sanjabi,
Ameet Talwalkar, and Virginia Smith.
2020c.

</span>
<span class="ltx_bibblock">Federated Optimization in Heterogeneous Networks.
In <em id="bib.bib82.3.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>,
I. Dhillon,
D. Papailiopoulos, and V. Sze (Eds.),
Vol. 2. 429–450.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib83.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Xuhong Li, Yves
Grandvalet, and Franck Davoine.
2018.

</span>
<span class="ltx_bibblock">Explicit inductive bias for transfer learning with
convolutional networks. In <em id="bib.bib83.3.1" class="ltx_emph ltx_font_italic">International
Conference on Machine Learning</em>. 2825–2834.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib84.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Xiang Li, Kaixuan Huang,
Wenhao Yang, Shusen Wang, and
Zhihua Zhang. 2020a.

</span>
<span class="ltx_bibblock">On the convergence of fedavg on non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations (ICLR)</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib85.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Xingjian Li, Haoyi Xiong,
Hanchao Wang, Yuxuan Rao,
Liping Liu, and Jun Huan.
2019b.

</span>
<span class="ltx_bibblock">Delta: Deep learning transfer using feature map
with attention for convolutional networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations (ICLR)</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib86.2.2.1" class="ltx_text">.</span> (2021c)</span>
<span class="ltx_bibblock">
Yuanchun Li, Ziqi Zhang,
Bingyan Liu, Ziyue Yang, and
Yunxin Liu. 2021c.

</span>
<span class="ltx_bibblock">ModelDiff: testing-based DNN similarity comparison
for model reuse detection. In <em id="bib.bib86.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
30th ACM SIGSOFT International Symposium on Software Testing and Analysis</em>.
139–151.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib87.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Zhuohang Li, Jiaxin
Zhang, Luyang Liu, and Jian Liu.
2022b.

</span>
<span class="ltx_bibblock">Auditing Privacy Defenses in Federated Learning via
Generative Gradient Leakage. In <em id="bib.bib87.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.
10132–10142.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al<span id="bib.bib88.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Wei Yang Bryan Lim,
Nguyen Cong Luong, Dinh Thai Hoang,
Yutao Jiao, Ying-Chang Liang,
Qiang Yang, Dusit Niyato, and
Chunyan Miao. 2020.

</span>
<span class="ltx_bibblock">Federated learning in mobile edge networks: A
comprehensive survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.3.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>
22, 3 (2020),
2031–2063.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin
et al<span id="bib.bib89.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong,
Sebastian U Stich, and Martin Jaggi.
2020.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
2351–2363.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
et al<span id="bib.bib90.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Bingyan Liu, Yifeng Cai,
Yao Guo, and Xiangqun Chen.
2021a.

</span>
<span class="ltx_bibblock">TransTailor: Pruning the pre-trained model for
improved transfer learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.3.1" class="ltx_emph ltx_font_italic">AAAI</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib91.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Bingyan Liu, Yifeng Cai,
Ziqi Zhang, Yuanchun Li,
Leye Wang, Ding Li, Yao
Guo, and Xiangqun Chen.
2021b.

</span>
<span class="ltx_bibblock">DistFL: Distribution-aware Federated Learning for
Mobile Scenarios.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies</em> 5,
4 (2021), 1–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
et al<span id="bib.bib92.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Bingyan Liu, Yao Guo,
and Xiangqun Chen. 2019b.

</span>
<span class="ltx_bibblock">WealthAdapt: A general network adaptation framework
for small data tasks. In <em id="bib.bib92.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th
ACM International Conference on Multimedia</em>. 2179–2187.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
et al<span id="bib.bib93.2.2.1" class="ltx_text">.</span> (2021d)</span>
<span class="ltx_bibblock">
Bingyan Liu, Yao Guo,
and Xiangqun Chen. 2021d.

</span>
<span class="ltx_bibblock">PFA: Privacy-preserving Federated Adaptation for
Effective Model Personalization. In <em id="bib.bib93.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the Web Conference 2021</em>. 923–934.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
et al<span id="bib.bib94.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Bingyan Liu, Yuanchun Li,
Yunxin Liu, Yao Guo, and
Xiangqun Chen. 2020.

</span>
<span class="ltx_bibblock">Pmc: A privacy-preserving deep learning model
customization framework for edge computing.

</span>
<span class="ltx_bibblock"><em id="bib.bib94.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies</em> 4,
4 (2020), 1–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
et al<span id="bib.bib95.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Changchang Liu, Supriyo
Chakraborty, and Dinesh Verma.
2019a.

</span>
<span class="ltx_bibblock">Secure model fusion for distributed learning using
partial homomorphic encryption.

</span>
<span class="ltx_bibblock">In <em id="bib.bib95.3.1" class="ltx_emph ltx_font_italic">Policy-Based Autonomic Data
Governance</em>. Springer, 154–179.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
et al<span id="bib.bib96.2.2.1" class="ltx_text">.</span> (2021c)</span>
<span class="ltx_bibblock">
Yang Liu, Tao Fan,
Tianjian Chen, Qian Xu, and
Qiang Yang. 2021c.

</span>
<span class="ltx_bibblock">FATE: An Industrial Grade Platform for
Collaborative Learning With Data Protection.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.3.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em> 22,
226 (2021), 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lomonaco
et al<span id="bib.bib97.3.3.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Vincenzo Lomonaco, Lorenzo
Pellegrini, Andrea Cossu, Antonio Carta,
Gabriele Graffieti, Tyler L Hayes,
Matthias De Lange, Marc Masana,
Jary Pomponi, Gido M Van de Ven,
et al<span id="bib.bib97.4.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Avalanche: an end-to-end library for continual
learning. In <em id="bib.bib97.5.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
3600–3610.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lubana
et al<span id="bib.bib98.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Ekdeep Singh Lubana,
Chi Ian Tang, Fahim Kawsar,
Robert P Dick, and Akhil Mathur.
2022.

</span>
<span class="ltx_bibblock">Orchestra: Unsupervised Federated Learning via
Globally Consistent Clustering.

</span>
<span class="ltx_bibblock"><em id="bib.bib98.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.11506</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu
et al<span id="bib.bib99.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Xinyi Xu,
Qian Wang, and Han Yu.
2020a.

</span>
<span class="ltx_bibblock">Collaborative fairness in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib99.3.1" class="ltx_emph ltx_font_italic">Federated Learning</em>.
Springer, 189–204.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al<span id="bib.bib100.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Han Yu,
and Qiang Yang. 2020b.

</span>
<span class="ltx_bibblock">Threats to federated learning: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.02133</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maddox et al<span id="bib.bib101.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Wesley J Maddox, Pavel
Izmailov, Timur Garipov, Dmitry P
Vetrov, and Andrew Gordon Wilson.
2019.

</span>
<span class="ltx_bibblock">A simple baseline for bayesian uncertainty in deep
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib101.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahendran and
Vedaldi (2015)</span>
<span class="ltx_bibblock">
Aravindh Mahendran and
Andrea Vedaldi. 2015.

</span>
<span class="ltx_bibblock">Understanding deep image representations by
inverting them. In <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>.
5188–5196.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallya and
Lazebnik (2018)</span>
<span class="ltx_bibblock">
Arun Mallya and Svetlana
Lazebnik. 2018.

</span>
<span class="ltx_bibblock">Packnet: Adding multiple tasks to a single network
by iterative pruning. In <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</em>.
7765–7773.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marfoq et al<span id="bib.bib104.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Othmane Marfoq, Giovanni
Neglia, Aurélien Bellet, Laetitia
Kameni, and Richard Vidal.
2021.

</span>
<span class="ltx_bibblock">Federated multi-task learning under a mixture of
distributions.

</span>
<span class="ltx_bibblock"><em id="bib.bib104.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 34 (2021),
15434–15447.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib105.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and Blaise Aguera y Arcas.
2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data. In <em id="bib.bib105.3.1" class="ltx_emph ltx_font_italic">Artificial
Intelligence and Statistics</em>. PMLR, 1273–1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan
et al<span id="bib.bib106.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
H Brendan McMahan, Daniel
Ramage, Kunal Talwar, and Li Zhang.
2018.

</span>
<span class="ltx_bibblock">Learning Differentially Private Recurrent Language
Models. In <em id="bib.bib106.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et al<span id="bib.bib107.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Fan Mo, Hamed Haddadi,
Kleomenis Katevas, Eduard Marin,
Diego Perino, and Nicolas Kourtellis.
2021.

</span>
<span class="ltx_bibblock">PPFL: privacy-preserving federated learning with
trusted execution environments. In <em id="bib.bib107.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the 19th Annual International Conference on Mobile Systems, Applications, and
Services</em>. 94–108.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et al<span id="bib.bib108.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Fan Mo, Ali Shahin
Shamsabadi, Kleomenis Katevas, Soteris
Demetriou, Ilias Leontiadis, Andrea
Cavallaro, and Hamed Haddadi.
2020.

</span>
<span class="ltx_bibblock">DarkneTZ: towards model privacy at the edge using
trusted execution environments. In <em id="bib.bib108.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the 18th International Conference on Mobile Systems, Applications, and
Services</em>. 161–174.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohri
et al<span id="bib.bib109.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary
Sivek, and Ananda Theertha Suresh.
2019.

</span>
<span class="ltx_bibblock">Agnostic federated learning. In
<em id="bib.bib109.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.
PMLR, 4615–4625.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagalapatti and
Narayanam (2021)</span>
<span class="ltx_bibblock">
Lokesh Nagalapatti and
Ramasuri Narayanam. 2021.

</span>
<span class="ltx_bibblock">Game of gradients: Mitigating irrelevant clients in
federated learning. In <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, Vol. 35.
9046–9054.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nichol and
Schulman (2018)</span>
<span class="ltx_bibblock">
Alex Nichol and John
Schulman. 2018.

</span>
<span class="ltx_bibblock">Reptile: a scalable metalearning algorithm.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.02999</em>
2, 3 (2018),
4.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozdayi
et al<span id="bib.bib112.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Mustafa Safa Ozdayi, Murat
Kantarcioglu, and Yulia R Gel.
2021.

</span>
<span class="ltx_bibblock">Defending against backdoors in federated learning
with robust learning rate. In <em id="bib.bib112.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
AAAI Conference on Artificial Intelligence</em>, Vol. 35.
9268–9276.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozkara
et al<span id="bib.bib113.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Kaan Ozkara, Navjot
Singh, Deepesh Data, and Suhas
Diggavi. 2021.

</span>
<span class="ltx_bibblock">QuPeD: Quantized Personalization via Distillation
with Applications to Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib113.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 34 (2021),
3622–3634.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan and Yang (2009)</span>
<span class="ltx_bibblock">
Sinno Jialin Pan and
Qiang Yang. 2009.

</span>
<span class="ltx_bibblock">A survey on transfer learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on knowledge and data
engineering</em> 22, 10
(2009), 1345–1359.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panda et al<span id="bib.bib115.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Ashwinee Panda, Saeed
Mahloujifar, Arjun Nitin Bhagoji, Supriyo
Chakraborty, and Prateek Mittal.
2022.

</span>
<span class="ltx_bibblock">SparseFed: Mitigating Model Poisoning Attacks in
Federated Learning with Sparsification. In
<em id="bib.bib115.3.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence
and Statistics</em>. PMLR, 7587–7624.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng
et al<span id="bib.bib116.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xingchao Peng, Zijun
Huang, Yizhe Zhu, and Kate Saenko.
2020.

</span>
<span class="ltx_bibblock">Federated adversarial domain adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bib116.3.1" class="ltx_emph ltx_font_italic">ICLR</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peterson
et al<span id="bib.bib117.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Daniel Peterson, Pallika
Kanani, and Virendra J Marathe.
2019.

</span>
<span class="ltx_bibblock">Private federated learning with domain adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bib117.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.06733</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rabin (1993)</span>
<span class="ltx_bibblock">
Matthew Rabin.
1993.

</span>
<span class="ltx_bibblock">Incorporating fairness into game theory and
economics.

</span>
<span class="ltx_bibblock"><em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">The American economic review</em>
(1993), 1281–1302.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rebuffi
et al<span id="bib.bib119.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Sylvestre-Alvise Rebuffi,
Hakan Bilen, and Andrea Vedaldi.
2017.

</span>
<span class="ltx_bibblock">Learning multiple visual domains with residual
adapters. In <em id="bib.bib119.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>. 506–516.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan and Joe-Wong (2022)</span>
<span class="ltx_bibblock">
Yichen Ruan and Carlee
Joe-Wong. 2022.

</span>
<span class="ltx_bibblock">Fedsoft: Soft clustered federated learning with
proximal local updating. In <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
AAAI Conference on Artificial Intelligence</em>, Vol. 36.
8124–8131.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler
et al<span id="bib.bib121.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Felix Sattler,
Klaus-Robert Müller, and Wojciech
Samek. 2020.

</span>
<span class="ltx_bibblock">Clustered federated learning: Model-agnostic
distributed multitask optimization under privacy constraints.

</span>
<span class="ltx_bibblock"><em id="bib.bib121.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and
learning systems</em> 32, 8
(2020), 3710–3722.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao
et al<span id="bib.bib122.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Yingxia Shao, Bin Cui,
Lei Chen, Lin Ma, Junjie
Yao, and Ning Xu. 2014.

</span>
<span class="ltx_bibblock">Parallel subgraph listing in a large-scale graph.
In <em id="bib.bib122.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 ACM SIGMOD International
Conference on Management of Data</em>. 625–636.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shapley (1997)</span>
<span class="ltx_bibblock">
Lloyd S Shapley.
1997.

</span>
<span class="ltx_bibblock">A value for n-person games.

</span>
<span class="ltx_bibblock"><em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">Classics in game theory</em>
69 (1997).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma
et al<span id="bib.bib124.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Pranay Sharma, Rohan
Panda, Gauri Joshi, and Pramod
Varshney. 2022.

</span>
<span class="ltx_bibblock">Federated minimax optimization: Improved
convergence analyses and algorithms. In
<em id="bib.bib124.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.
PMLR, 19683–19730.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin
et al<span id="bib.bib125.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jaemin Shin, Yuanchun Li,
Yunxin Liu, and Sung-Ju Lee.
2022.

</span>
<span class="ltx_bibblock">FedBalancer: Data and Pace Control for Efficient
Federated Learning on Heterogeneous Clients.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.3.1" class="ltx_emph ltx_font_italic">MobiSys</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and
Zisserman (2014)</span>
<span class="ltx_bibblock">
Karen Simonyan and
Andrew Zisserman. 2014.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale
image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.1556</em>
(2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SMILELab-FL (2021)</span>
<span class="ltx_bibblock">
SMILELab-FL.
2021.

</span>
<span class="ltx_bibblock">FedLab: A Flexible Federated Learning Framework.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/SMILELab-FL/FedLab" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/SMILELab-FL/FedLab</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith
et al<span id="bib.bib128.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Virginia Smith, Chao-Kai
Chiang, Maziar Sanjabi, and Ameet S
Talwalkar. 2017.

</span>
<span class="ltx_bibblock">Federated multi-task learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib128.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song
et al<span id="bib.bib129.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zhendong Song, Hongguang
Sun, Howard H Yang, Xijun Wang,
Yan Zhang, and Tony QS Quek.
2021.

</span>
<span class="ltx_bibblock">Reputation-based Federated Learning for Secure
Wireless Networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
9, 2 (2021),
1212–1226.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span id="bib.bib130.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jingwei Sun, Ang Li,
Louis DiValentin, Amin Hassanzadeh,
Yiran Chen, and Hai Li.
2021.

</span>
<span class="ltx_bibblock">Fl-wbc: Enhancing robustness against model
poisoning attacks in federated learning from a client perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib130.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 34 (2021),
12613–12624.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun
et al<span id="bib.bib131.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ziteng Sun, Peter
Kairouz, Ananda Theertha Suresh, and
H Brendan McMahan. 2019.

</span>
<span class="ltx_bibblock">Can you really backdoor federated learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib131.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.07963</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SymbioticLab (2021)</span>
<span class="ltx_bibblock">
SymbioticLab.
2021.

</span>
<span class="ltx_bibblock">FedScale: Benchmarking Model and System Performance
of Federated Learning at Scale.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/symbioticlab/fedscale" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/symbioticlab/fedscale</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">T Dinh
et al<span id="bib.bib133.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Canh T Dinh, Nguyen Tran,
and Josh Nguyen. 2020.

</span>
<span class="ltx_bibblock">Personalized federated learning with moreau
envelopes.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
21394–21405.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang
et al<span id="bib.bib134.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhenheng Tang, Yonggang
Zhang, Shaohuai Shi, Xin He,
Bo Han, and Xiaowen Chu.
2022.

</span>
<span class="ltx_bibblock">Virtual Homogeneity Learning: Defending against
Data Heterogeneity in Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib134.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.02465</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tarzanagh et al<span id="bib.bib135.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Davoud Ataee Tarzanagh,
Mingchen Li, Christos Thrampoulidis,
and Samet Oymak. 2022.

</span>
<span class="ltx_bibblock">FEDNEST: Federated Bilevel, Minimax, and
Compositional Optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.02215</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thrun and Pratt (2012)</span>
<span class="ltx_bibblock">
Sebastian Thrun and
Lorien Pratt. 2012.

</span>
<span class="ltx_bibblock"><em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">Learning to learn</em>.

</span>
<span class="ltx_bibblock">Springer Science &amp; Business Media.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tolpegin
et al<span id="bib.bib137.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Vale Tolpegin, Stacey
Truex, Mehmet Emre Gursoy, and Ling
Liu. 2020.

</span>
<span class="ltx_bibblock">Data poisoning attacks against federated learning
systems. In <em id="bib.bib137.3.1" class="ltx_emph ltx_font_italic">European Symposium on Research in
Computer Security</em>. Springer, 480–501.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tornblom and
Jonsson (1985)</span>
<span class="ltx_bibblock">
Kjell Y Tornblom and
Dan R Jonsson. 1985.

</span>
<span class="ltx_bibblock">Subrules of the equality and contribution
principles: Their perceived fairness in distribution and retribution.

</span>
<span class="ltx_bibblock"><em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">Social Psychology Quarterly</em>
(1985), 249–261.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vanhaesebrouck et al<span id="bib.bib139.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Paul Vanhaesebrouck,
Aurélien Bellet, and Marc Tommasi.
2017.

</span>
<span class="ltx_bibblock">Decentralized collaborative learning of
personalized models over networks. In <em id="bib.bib139.3.1" class="ltx_emph ltx_font_italic">Artificial
Intelligence and Statistics</em>. PMLR, 509–517.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span id="bib.bib140.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam
Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia
Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
et al<span id="bib.bib141.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Guan Wang,
Charlie Xiaoqian Dang, and Ziye Zhou.
2019a.

</span>
<span class="ltx_bibblock">Measure contribution of participants in federated
learning. In <em id="bib.bib141.3.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on
Big Data (Big Data)</em>. IEEE, 2597–2604.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib142.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Hongyi Wang, Kartik
Sreenivasan, Shashank Rajput, Harit
Vishwakarma, Saurabh Agarwal, Jy-yong
Sohn, Kangwook Lee, and Dimitris
Papailiopoulos. 2020a.

</span>
<span class="ltx_bibblock">Attack of the tails: Yes, you really can backdoor
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
16070–16084.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib143.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Hongyi Wang, Mikhail
Yurochkin, Yuekai Sun, Dimitris
Papailiopoulos, and Yasaman Khazaeni.
2020b.

</span>
<span class="ltx_bibblock">Federated learning with matched averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib143.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations (ICLR)</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib144.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Kangkang Wang, Rajiv
Mathews, Chloé Kiddon, Hubert
Eichner, Françoise Beaufays, and
Daniel Ramage. 2019b.

</span>
<span class="ltx_bibblock">Federated evaluation of on-device personalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.10252</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
et al<span id="bib.bib145.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zheng Wang, Xiaoliang
Fan, Jianzhong Qi, Chenglu Wen,
Cheng Wang, and Rongshan Yu.
2021.

</span>
<span class="ltx_bibblock">Federated learning with fair averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib145.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.14937</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WeBank (2019)</span>
<span class="ltx_bibblock">
WeBank. 2019.

</span>
<span class="ltx_bibblock">An Industrial Level Federated Learning Framework.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/FederatedAI/FATE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/FederatedAI/FATE</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span id="bib.bib147.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li,
Ming Ding, Chuan Ma,
Howard H Yang, Farhad Farokhi,
Shi Jin, Tony QS Quek, and
H Vincent Poor. 2020.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy:
Algorithms and performance analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib147.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics
and Security</em> 15 (2020),
3454–3469.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu
et al<span id="bib.bib148.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu,
Tao Qi, Yongfeng Huang, and
Xing Xie. 2022.

</span>
<span class="ltx_bibblock">FedAttack: Effective and Covert Poisoning Attack on
Federated Recommendation via Hard Sampling.

</span>
<span class="ltx_bibblock"><em id="bib.bib148.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.04975</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu
et al<span id="bib.bib149.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yue Wu, Yinpeng Chen,
Lijuan Wang, Yuancheng Ye,
Zicheng Liu, Yandong Guo, and
Yun Fu. 2019.

</span>
<span class="ltx_bibblock">Large scale incremental learning. In
<em id="bib.bib149.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</em>. 374–382.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie
et al<span id="bib.bib150.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Chulin Xie, Minghao Chen,
Pin-Yu Chen, and Bo Li.
2021.

</span>
<span class="ltx_bibblock">Crfl: Certifiably robust federated learning against
backdoor attacks. In <em id="bib.bib150.3.1" class="ltx_emph ltx_font_italic">International Conference on
Machine Learning</em>. PMLR, 11372–11382.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie
et al<span id="bib.bib151.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Chulin Xie, Keli Huang,
Pin-Yu Chen, and Bo Li.
2020.

</span>
<span class="ltx_bibblock">Dba: Distributed backdoor attacks against federated
learning. In <em id="bib.bib151.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu
et al<span id="bib.bib152.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jingjing Xu, Hao Zhou,
Chun Gan, Zaixiang Zheng, and
Lei Li. 2021.

</span>
<span class="ltx_bibblock">Vocabulary Learning via Optimal Transport for
Neural Machine Translation. In <em id="bib.bib152.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing,
ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6,
2021</em>, Chengqing Zong,
Fei Xia, Wenjie Li, and
Roberto Navigli (Eds.). Association
for Computational Linguistics, 7361–7373.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.571" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/2021.acl-long.571</a>

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
et al<span id="bib.bib153.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Miao Yang, Ximin Wang,
Hongbin Zhu, Haifeng Wang, and
Hua Qian. 2021.

</span>
<span class="ltx_bibblock">Federated learning with class imbalance reduction.
In <em id="bib.bib153.3.1" class="ltx_emph ltx_font_italic">2021 29th European Signal Processing Conference
(EUSIPCO)</em>. IEEE, 2174–2178.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
et al<span id="bib.bib154.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu,
Tianjian Chen, and Yongxin Tong.
2019.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and
applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib154.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em> 10, 2
(2019), 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib155.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Timothy Yang, Galen
Andrew, Hubert Eichner, Haicheng Sun,
Wei Li, Nicholas Kong,
Daniel Ramage, and Françoise
Beaufays. 2018.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google
keyboard query suggestions.

</span>
<span class="ltx_bibblock"><em id="bib.bib155.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.02903</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin
et al<span id="bib.bib156.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Dong Yin, Yudong Chen,
Ramchandran Kannan, and Peter
Bartlett. 2018.

</span>
<span class="ltx_bibblock">Byzantine-robust distributed learning: Towards
optimal statistical rates. In <em id="bib.bib156.3.1" class="ltx_emph ltx_font_italic">International
Conference on Machine Learning</em>. PMLR, 5650–5659.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span id="bib.bib157.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Hongxu Yin, Arun Mallya,
Arash Vahdat, Jose M Alvarez,
Jan Kautz, and Pavlo Molchanov.
2021.

</span>
<span class="ltx_bibblock">See through gradients: Image batch recovery via
gradinversion. In <em id="bib.bib157.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
16337–16346.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yosinski
et al<span id="bib.bib158.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Jason Yosinski, Jeff
Clune, Yoshua Bengio, and Hod Lipson.
2014.

</span>
<span class="ltx_bibblock">How transferable are features in deep neural
networks?. In <em id="bib.bib158.3.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>. 3320–3328.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span id="bib.bib159.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Fuxun Yu, Weishan Zhang,
Zhuwei Qin, Zirui Xu, Di
Wang, Chenchen Liu, Zhi Tian, and
Xiang Chen. 2021.

</span>
<span class="ltx_bibblock">Fed2: Feature-aligned federated learning. In
<em id="bib.bib159.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery &amp; Data Mining</em>. 2066–2074.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu
et al<span id="bib.bib160.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tao Yu, Eugene
Bagdasaryan, and Vitaly Shmatikov.
2020.

</span>
<span class="ltx_bibblock">Salvaging federated learning by local adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bib160.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.04758</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yurochkin et al<span id="bib.bib161.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Mikhail Yurochkin, Mayank
Agarwal, Soumya Ghosh, Kristjan
Greenewald, and Nghia Hoang.
2019a.

</span>
<span class="ltx_bibblock">Statistical model aggregation via parameter
matching.

</span>
<span class="ltx_bibblock"><em id="bib.bib161.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yurochkin et al<span id="bib.bib162.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Mikhail Yurochkin, Mayank
Agarwal, Soumya Ghosh, Kristjan
Greenewald, Nghia Hoang, and Yasaman
Khazaeni. 2018.

</span>
<span class="ltx_bibblock">Probabilistic Federated Neural Matching.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yurochkin et al<span id="bib.bib163.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Mikhail Yurochkin, Mayank
Agarwal, Soumya Ghosh, Kristjan
Greenewald, Nghia Hoang, and Yasaman
Khazaeni. 2019b.

</span>
<span class="ltx_bibblock">Bayesian nonparametric federated learning of neural
networks. In <em id="bib.bib163.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>. PMLR, 7252–7261.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zantedeschi
et al<span id="bib.bib164.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Valentina Zantedeschi,
Aurélien Bellet, and Marc Tommasi.
2020.

</span>
<span class="ltx_bibblock">Fully decentralized joint learning of personalized
models and collaboration graphs. In <em id="bib.bib164.3.1" class="ltx_emph ltx_font_italic">International
Conference on Artificial Intelligence and Statistics</em>. PMLR,
864–874.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng
et al<span id="bib.bib165.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Dun Zeng, Siqi Liang,
Xiangjing Hu, and Zenglin Xu.
2021.

</span>
<span class="ltx_bibblock">FedLab: A Flexible Federated Learning Framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib165.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.11621</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
et al<span id="bib.bib166.6.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Chengliang Zhang, Suyi
Li, Junzhe Xia, Wei Wang,
Feng Yan, and Yang Liu.
2020b.

</span>
<span class="ltx_bibblock"><math id="bib.bib166.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib166.1.m1.1a"><mo stretchy="false" id="bib.bib166.1.m1.1.1" xref="bib.bib166.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib166.1.m1.1b"><ci id="bib.bib166.1.m1.1.1.cmml" xref="bib.bib166.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib166.1.m1.1c">\{</annotation></semantics></math>BatchCrypt<math id="bib.bib166.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib166.2.m2.1a"><mo stretchy="false" id="bib.bib166.2.m2.1.1" xref="bib.bib166.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib166.2.m2.1b"><ci id="bib.bib166.2.m2.1.1.cmml" xref="bib.bib166.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib166.2.m2.1c">\}</annotation></semantics></math>: Efficient homomorphic
encryption for <math id="bib.bib166.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib166.3.m3.1a"><mo stretchy="false" id="bib.bib166.3.m3.1.1" xref="bib.bib166.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib166.3.m3.1b"><ci id="bib.bib166.3.m3.1.1.cmml" xref="bib.bib166.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib166.3.m3.1c">\{</annotation></semantics></math>Cross-Silo<math id="bib.bib166.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib166.4.m4.1a"><mo stretchy="false" id="bib.bib166.4.m4.1.1" xref="bib.bib166.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib166.4.m4.1b"><ci id="bib.bib166.4.m4.1.1.cmml" xref="bib.bib166.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib166.4.m4.1c">\}</annotation></semantics></math> federated learning. In
<em id="bib.bib166.7.1" class="ltx_emph ltx_font_italic">2020 USENIX annual technical conference (USENIX ATC
20)</em>. 493–506.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib167.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Jingfeng Zhang, Cheng Li,
Antonio Robles-Kelly, and Mohan
Kankanhalli. 2020a.

</span>
<span class="ltx_bibblock">Hierarchically fair federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib167.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.10386</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib168.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Michael Zhang, Karan
Sapra, Sanja Fidler, Serena Yeung, and
Jose M Alvarez. 2020c.

</span>
<span class="ltx_bibblock">Personalized federated learning with first order
model optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib168.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.08565</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
et al<span id="bib.bib169.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Xinwei Zhang, Xiangyi
Chen, Mingyi Hong, Steven Wu, and
Jinfeng Yi. 2022a.

</span>
<span class="ltx_bibblock">Understanding clipping for federated learning:
Convergence and client-level differential privacy. In
<em id="bib.bib169.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.
PMLR, 26048–26067.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
et al<span id="bib.bib170.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Yuchen Zhang, John Duchi,
Michael I Jordan, and Martin J
Wainwright. 2013.

</span>
<span class="ltx_bibblock">Information-theoretic lower bounds for distributed
statistical estimation with communication constraints.

</span>
<span class="ltx_bibblock"><em id="bib.bib170.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 26 (2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib171.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Ziqi Zhang, Yuanchun Li,
Jindong Wang, Bingyan Liu,
Ding Li, Yao Guo,
Xiangqun Chen, and Yunxin Liu.
2022b.

</span>
<span class="ltx_bibblock">ReMoS: Reducing Defect Inheritance in Transfer
Learning via Relevant Model Slicing. In <em id="bib.bib171.3.1" class="ltx_emph ltx_font_italic">2022
IEEE/ACM 44th International Conference on Software Engineering (ICSE)</em>.
IEEE, 1856–1868.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
et al<span id="bib.bib172.2.2.1" class="ltx_text">.</span> (2022c)</span>
<span class="ltx_bibblock">
Ziqi Zhang, Lucien KL Ng,
Bingyan Liu, Yifeng Cai,
Ding Li, Yao Guo, and
Xiangqun Chen. 2022c.

</span>
<span class="ltx_bibblock">TEESlice: slicing DNN models for secure and
efficient deployment. In <em id="bib.bib172.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd
ACM International Workshop on AI and Software Testing/Analysis</em>.
1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib173.2.2.1" class="ltx_text">.</span> (2022d)</span>
<span class="ltx_bibblock">
Zhengming Zhang, Ashwinee
Panda, Linyue Song, Yaoqing Yang,
Michael Mahoney, Prateek Mittal,
Ramchandran Kannan, and Joseph
Gonzalez. 2022d.

</span>
<span class="ltx_bibblock">Neurotoxin: Durable backdoors in federated
learning. In <em id="bib.bib173.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>. PMLR, 26429–26446.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
et al<span id="bib.bib174.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Bo Zhao, Konda Reddy
Mopuri, and Hakan Bilen.
2020.

</span>
<span class="ltx_bibblock">idlg: Improved deep leakage from gradients.

</span>
<span class="ltx_bibblock"><em id="bib.bib174.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.02610</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
et al<span id="bib.bib175.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li,
Liangzhen Lai, Naveen Suda,
Damon Civin, and Vikas Chandra.
2018.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib175.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao and Joshi (2022)</span>
<span class="ltx_bibblock">
Zhiyuan Zhao and Gauri
Joshi. 2022.

</span>
<span class="ltx_bibblock">A Dynamic Reweighting Strategy For Fair Federated
Learning. In <em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022 - 2022 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.
8772–8776.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICASSP43922.2022.9746300" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICASSP43922.2022.9746300</a>

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng
et al<span id="bib.bib177.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Wenbo Zheng, Lan Yan,
Chao Gou, and Fei-Yue Wang.
2021.

</span>
<span class="ltx_bibblock">Federated meta-learning for fraudulent credit card
detection. In <em id="bib.bib177.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Ninth
International Conference on International Joint Conferences on Artificial
Intelligence</em>. 4654–4660.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
et al<span id="bib.bib178.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Chendi Zhou, Ji Liu,
Juncheng Jia, Jingbo Zhou,
Yang Zhou, Huaiyu Dai, and
Dejing Dou. 2022.

</span>
<span class="ltx_bibblock">Efficient device scheduling with multi-job
federated learning. In <em id="bib.bib178.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, Vol. 36.
9971–9979.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
et al<span id="bib.bib179.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Pengyuan Zhou, Pei Fang,
and Pan Hui. 2021.

</span>
<span class="ltx_bibblock">Loss Tolerant Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib179.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.03591</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu and Blaschko (2021)</span>
<span class="ltx_bibblock">
Junyi Zhu and Matthew
Blaschko. 2021.

</span>
<span class="ltx_bibblock">R-gap: Recursive gradient attack on privacy.

</span>
<span class="ltx_bibblock"><em id="bib.bib180.1.1" class="ltx_emph ltx_font_italic">ICLR</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib181.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu,
and Song Han. 2019.

</span>
<span class="ltx_bibblock">Deep leakage from gradients.

</span>
<span class="ltx_bibblock"><em id="bib.bib181.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zoph and Le (2016)</span>
<span class="ltx_bibblock">
Barret Zoph and Quoc V
Le. 2016.

</span>
<span class="ltx_bibblock">Neural architecture search with reinforcement
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.01578</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2301.01298" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2301.01299" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2301.01299">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2301.01299" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2301.01300" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 08:07:02 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
