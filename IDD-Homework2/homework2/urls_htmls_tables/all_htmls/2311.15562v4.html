<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.15562] Fully Authentic Visual Question Answering Dataset from Online Communities</title><meta property="og:description" content="Visual Question Answering (VQA) entails answering questions about images. We introduce the first VQA dataset in which all contents originate from an authentic use case. Sourced from online question answering community …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fully Authentic Visual Question Answering Dataset from Online Communities">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fully Authentic Visual Question Answering Dataset from Online Communities">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.15562">

<!--Generated on Tue Feb 27 17:11:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fully Authentic Visual Question Answering Dataset from Online Communities</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Chongyan Chen<sup id="id12.12.id1" class="ltx_sup">1</sup>, Mengchen Liu<sup id="id13.13.id2" class="ltx_sup">2</sup>, Noel Codella <sup id="id14.14.id3" class="ltx_sup">2</sup>, Yunsheng Li<sup id="id15.15.id4" class="ltx_sup">2</sup> , Lu Yuan <sup id="id16.16.id5" class="ltx_sup">2</sup> , Danna Gurari <sup id="id17.17.id6" class="ltx_sup">1</sup><sup id="id18.18.id7" class="ltx_sup">,</sup><sup id="id19.19.id8" class="ltx_sup">3</sup>
<br class="ltx_break"> <math id="id9.9.m9.1" class="ltx_Math" alttext="~{}^{1}" display="inline"><semantics id="id9.9.m9.1a"><msup id="id9.9.m9.1.1" xref="id9.9.m9.1.1.cmml"><mi id="id9.9.m9.1.1a" xref="id9.9.m9.1.1.cmml"></mi><mn mathsize="90%" id="id9.9.m9.1.1.1" xref="id9.9.m9.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id9.9.m9.1b"><apply id="id9.9.m9.1.1.cmml" xref="id9.9.m9.1.1"><cn type="integer" id="id9.9.m9.1.1.1.cmml" xref="id9.9.m9.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.m9.1c">~{}^{1}</annotation></semantics></math><span id="id20.20.id9" class="ltx_text" style="font-size:90%;"> University of Texas at Austin</span>
<math id="id10.10.m10.1" class="ltx_Math" alttext="~{}^{2}" display="inline"><semantics id="id10.10.m10.1a"><msup id="id10.10.m10.1.1" xref="id10.10.m10.1.1.cmml"><mi id="id10.10.m10.1.1a" xref="id10.10.m10.1.1.cmml"></mi><mn mathsize="90%" id="id10.10.m10.1.1.1" xref="id10.10.m10.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id10.10.m10.1b"><apply id="id10.10.m10.1.1.cmml" xref="id10.10.m10.1.1"><cn type="integer" id="id10.10.m10.1.1.1.cmml" xref="id10.10.m10.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.10.m10.1c">~{}^{2}</annotation></semantics></math><span id="id11.11.1" class="ltx_text" style="font-size:90%;"> Microsoft<math id="id11.11.1.m1.1" class="ltx_Math" alttext="~{}^{3}" display="inline"><semantics id="id11.11.1.m1.1a"><msup id="id11.11.1.m1.1.1" xref="id11.11.1.m1.1.1.cmml"><mi id="id11.11.1.m1.1.1a" xref="id11.11.1.m1.1.1.cmml"></mi><mn id="id11.11.1.m1.1.1.1" xref="id11.11.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="id11.11.1.m1.1b"><apply id="id11.11.1.m1.1.1.cmml" xref="id11.11.1.m1.1.1"><cn type="integer" id="id11.11.1.m1.1.1.1.cmml" xref="id11.11.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.11.1.m1.1c">~{}^{3}</annotation></semantics></math> University of Colorado Boulder</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id21.id1" class="ltx_p">Visual Question Answering (VQA) entails answering questions about images. We introduce the first VQA dataset in which all contents originate from an authentic use case. Sourced from online question answering community forums, we call it VQAonline. We then characterize our dataset and how it relates to eight other VQA datasets. Observing that answers in our dataset tend to be much longer (e.g., with a mean of 173 words) and thus incompatible with standard VQA evaluation metrics, we next analyze which of the six popular metrics for longer text evaluation align best with human judgments. We then use the best-suited metrics to evaluate six state-of-the-art vision and language foundation models on VQAonline and reveal where they struggle most. To facilitate future extensions, we publicly share the dataset at: https://vqaonline.github.io/.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">We are witnessing a renewed excitement in visual question answering (VQA), the task of answering questions about images, with recent successes of large foundation models in <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">zero-shot settings</em>. Moreover, many papers are reporting that such models can achieve or even exceed human performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While modern VQA datasets enabled this exciting progress in VQA model development, they have two key limitations. First, most publicly-available VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> are artificially-constructed to probe model performance rather stemming from authentic use cases. Yet, repeatedly, prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> has revealed that models developed for artificially-constructed settings generalize poorly to authentic use cases due to significant domain shifts. The second key limitation of existing datasets lies in a limited diversity of content representing authentic use cases. The only VQA dataset reflecting an authentic use case is VizWiz-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>. However, VizWiz-VQA is constrained to one population, specifically blind individuals seeking assistance in learning about their visual surroundings. A further limitation of the dataset is that the answers were collected years later to support model benchmarking, rather than having been validated as acceptable by those who asked the questions.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2311.15562/assets/Images/main_figure3.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="350" height="338" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text" style="font-size:90%;"> Shown is an example from our VQAonline dataset alongside an example from each of three related VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> to illustrate key distinctions. VQAonline is the first to originate from an <em id="S1.F1.4.2.1" class="ltx_emph ltx_font_italic">authentic</em> VQA use case end to end, including with authentic context and answers, as well as the first VQA dataset sourced from an online question answering community. These distinctions result in many unique properties, including lengthy answers. </span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We address these limitations by introducing a new dataset called <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">VQAonline.</em> Sourced from online question answering communities, it is the the first VQA dataset with all contents originating from an authentic use case. We conduct analysis to highlight how this dataset differs from prior datasets; examples include that it contains: (1) authentic context that clarifies the question, (2) an answer the individual asking the question validated as acceptable from all community-provided answers, (3) answers that are considerably longer (e.g., a mean of 173 words versus typically 11 words or fewer in prior work), and (4) user-chosen topics for each visual question from 105 diverse topics revealing the dataset’s inherent diversity. Such differences are exemplified in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Next, observing that previous VQA evaluation metrics are ill-suited for lengthy textual answers, we assess which related evaluation metrics for long-form text best align with human judgement for our dataset. Using these metrics, we then benchmark the performance of six modern foundation models in the zero-shot setting. Our findings reveal that modern models perform poorly overall and our fine-grained analysis reveals opportunities for how to improve such models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We expect improved models designed for VQAonline can also benefit many other real-world applications including other community question answering platforms (e.g., Yahoo! Answers, Wiki Answers, Quora), learning assistance VQA tools for children <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, virtual assistants, visual dialog and query, as well as visual search and browsing. To encourage community-wide progress, we publicly-share the dataset at: https://vqaonline.github.io/.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Question Answering Datasets. </h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">A commonality of all VQA datasets (e.g.,  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>) is the inclusion of artificially-generated content. The closest dataset to reflect an authentic use case is VizWiz-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, as the visual questions were all submitted by individuals seeking assistance to learn about their visual surroundings. However, the answers were crowdsourced and never shared with or validated by those asking the questions. To our knowledge, our work is the first to provide an authentic dataset end-to-end, with each example including a natural language question, image, context, and answer validated by the individual asking the question. We conduct extensive analysis to reveal unique aspects of our dataset (Section <a href="#S3.SS2" title="3.2 Dataset Analysis ‣ 3 VQAonline Dataset ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">Our work also aligns with VQA datasets providing context that supplements each visual question: Context-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> and ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>. However, <em id="S2.SS0.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">context</em> in Context-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> is defined as the kind of website the image is sourced from rather than our definition of supplementary data that supports answering the visual question. More similar to our work is ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>, however the context is both artificially-generated and has distinct compositions and content, as discussed in Section <a href="#S3.SS2" title="3.2 Dataset Analysis ‣ 3 VQAonline Dataset ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Community Question Answering (CQA) Datasets.</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Numerous datasets have been developed around CQA. However, all focus on text-based question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> or natural language questions with multimedia answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>. To our knowledge, our work is the first to introduce a community multimodal question answering dataset specifically for visual question answering.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">VQA Models.</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Traditional VQA models generate short answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> and do not consider extra context to help answer the questions. Yet, our dataset features answers that are lengthy, typically containing multiple sentences, and provides additional context that can help answer visual questions. Accordingly, we benchmark large Vision and Language Models (VLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> which, by their generative design, are better-suited to generate long-form answers using their strong natural language understanding capabilities. Overall, we find these models perform poorly and provide fine-grained analysis to reveal opportunities for improving such models.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>VQAonline Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We now introduce our VQAonline dataset originating from an authentic use case where people used a web-based platform to post and answer visual questions.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset Creation</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Source.</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">We created our dataset by scraping data from the online question-answering platform Stack Exchange. We used the publicly-available data that spanned from August 2010 to November 2022, and adhered to all data license restrictions (cc-by-sa 4.0) set forth by Stack Exchange.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2311.15562/assets/Images/vqs_per_category.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="713" height="268" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Number of visual questions per topic (in log scale). The colors, as defined in the legend, indicate the super-category for each topic.</span></figcaption>
</figure>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p2.1" class="ltx_p">From the Stack Exchange archive, we collected all examples that contained each the following four components: a natural language question, a natural language context, an image, and at least one answer. Originally, this information was collected from Stack Exchange users via four user entry interactions. The natural language question was collected as the post "title", using a text entry field paired with instructions "be specific and imagine you’re asking a question to another person" when writing the question. The image was collected as a URL link. The natural language context was provided via a "body" text entry field that supported inline images (shared via a URL link) with users instructed to supply "all the information someone would need to answer your question". These three pieces of information were then shared publicly, after which users on the site could publicly provide answers.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Stack Exchange users could either propose edits to existing answers, which had to then be approved by high-reputation users, or could propose new answers. Further community deliberation to arrive at high-quality answers could occur when users posted comments under the question or answers. The mean number of answers per visual question is 1.45. </span></span></span> Finally, the individuals who posted the visual questions could mark which answer was the “accepted answer"; we only included examples for which the individuals did this.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p3.1" class="ltx_p">Of note, the Stack Exchange platform has two key mechanisms that contribute to high quality VQAs. First, the platform enforces a stringent reputation system with moderation policies, as documented at <span id="S3.SS1.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_typewriter">https://meta.stackexchange.com/help</span>. Second, it features topic-specific sites for users to engage with a relevant community; e.g., music.stackexchange.com and gardening.stackexchange.com. We used the data from 105 topical sites that belong to five super-categories, with 46 about culture &amp; recreation (e.g., travel, sports, Christianity, French language), 25 about life &amp; arts (e.g., physical fitness, parenting, pets), 24 about science (e.g., physics, chemistry, biology), 7 about being professional, and 3 about business.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We excluded the 75 technology topics (e.g., Stack Overflow, LaTeX), because they typically are language problems disguised as VQA problems. In particular, the questions typically ask about code shown in screenshots.</span></span></span> All topics are shown in Figure <a href="#S3.F2" title="Figure 2 ‣ Source. ‣ 3.1 Dataset Creation ‣ 3 VQAonline Dataset ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Filtering.</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">We started with 331,000 candidate VQAs from the source and removed visual questions without accepted answers. Then, we conducted two filtering steps to obtain our final dataset with <span id="S3.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_bold">64,696 visual questions</span>. First, we removed low-quality VQAs based on Stack Exchange’s feature that enables users to up-vote or down-vote each post (either question or answer). Each up-vote added 1 to the score and each down-vote subtracted 1 from the score. We removed visual questions with scores 0 or less for either the question or answer. Second, we excluded visual questions with multiple images (e.g., GIF) or with visual answers.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Dataset Splits.</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">While our work is inspired by the zero-shot setting, we designed our dataset to support few-shot settings given the recent exciting developments around in-context few-shot learning with foundation models. To do so, we randomly selected from each of the 105 topics covered in our dataset 7 examples for training and 3 examples for validation for all topics with at least 20 examples. The result is 665 examples in the training set, 285 in the validation set, and 63,746 examples in the test set.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dataset Analysis</h3>

<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:660.6pt;height:180pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S3.T1.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.3.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t">VQA Dataset</td>
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\overline{Q}" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><mover accent="true" id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.1.m1.1.1.2.cmml">Q</mi><mo id="S3.T1.1.1.1.1.m1.1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1"><ci id="S3.T1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1.1">¯</ci><ci id="S3.T1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.1.m1.1.1.2">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\overline{Q}</annotation></semantics></math></td>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t"><math id="S3.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\overline{A}" display="inline"><semantics id="S3.T1.2.2.2.2.m1.1a"><mover accent="true" id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml"><mi id="S3.T1.2.2.2.2.m1.1.1.2" xref="S3.T1.2.2.2.2.m1.1.1.2.cmml">A</mi><mo id="S3.T1.2.2.2.2.m1.1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><apply id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1"><ci id="S3.T1.2.2.2.2.m1.1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1.1">¯</ci><ci id="S3.T1.2.2.2.2.m1.1.1.2.cmml" xref="S3.T1.2.2.2.2.m1.1.1.2">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">\overline{A}</annotation></semantics></math></td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_align_left ltx_border_t"><math id="S3.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="N_{\text{img}}" display="inline"><semantics id="S3.T1.3.3.3.3.m1.1a"><msub id="S3.T1.3.3.3.3.m1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.cmml"><mi id="S3.T1.3.3.3.3.m1.1.1.2" xref="S3.T1.3.3.3.3.m1.1.1.2.cmml">N</mi><mtext id="S3.T1.3.3.3.3.m1.1.1.3" xref="S3.T1.3.3.3.3.m1.1.1.3a.cmml">img</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><apply id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.3.3.3.3.m1.1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S3.T1.3.3.3.3.m1.1.1.2.cmml" xref="S3.T1.3.3.3.3.m1.1.1.2">𝑁</ci><ci id="S3.T1.3.3.3.3.m1.1.1.3a.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3"><mtext mathsize="70%" id="S3.T1.3.3.3.3.m1.1.1.3.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">N_{\text{img}}</annotation></semantics></math></td>
<td id="S3.T1.3.3.3.5" class="ltx_td ltx_align_left ltx_border_t"># Auth. Topics</td>
<td id="S3.T1.3.3.3.6" class="ltx_td ltx_align_left ltx_border_t">Context</td>
<td id="S3.T1.3.3.3.7" class="ltx_td ltx_align_left ltx_border_t">Auth. VQ</td>
<td id="S3.T1.3.3.3.8" class="ltx_td ltx_align_left ltx_border_t">Auth. Context</td>
<td id="S3.T1.3.3.3.9" class="ltx_td ltx_align_left ltx_border_t">Vld. A</td>
</tr>
<tr id="S3.T1.3.3.4" class="ltx_tr">
<td id="S3.T1.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t">VQAonline (Ours)</td>
<td id="S3.T1.3.3.4.2" class="ltx_td ltx_align_left ltx_border_t">9.3</td>
<td id="S3.T1.3.3.4.3" class="ltx_td ltx_align_left ltx_border_t">173.2</td>
<td id="S3.T1.3.3.4.4" class="ltx_td ltx_align_left ltx_border_t">64.7K</td>
<td id="S3.T1.3.3.4.5" class="ltx_td ltx_align_left ltx_border_t">105</td>
<td id="S3.T1.3.3.4.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.3.3.4.6.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S3.T1.3.3.4.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.3.3.4.7.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S3.T1.3.3.4.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.3.3.4.8.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S3.T1.3.3.4.9" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.3.3.4.9.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
</tr>
<tr id="S3.T1.3.3.5" class="ltx_tr">
<td id="S3.T1.3.3.5.1" class="ltx_td ltx_align_left">Context-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>
</td>
<td id="S3.T1.3.3.5.2" class="ltx_td ltx_align_left">6.5</td>
<td id="S3.T1.3.3.5.3" class="ltx_td ltx_align_left">11.0</td>
<td id="S3.T1.3.3.5.4" class="ltx_td ltx_align_left">42</td>
<td id="S3.T1.3.3.5.5" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.5.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.5.6" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.5.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.5.7" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.5.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.5.8" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.5.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.5.9" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.5.9.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S3.T1.3.3.6" class="ltx_tr">
<td id="S3.T1.3.3.6.1" class="ltx_td ltx_align_left">VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>
</td>
<td id="S3.T1.3.3.6.2" class="ltx_td ltx_align_left">6.1</td>
<td id="S3.T1.3.3.6.3" class="ltx_td ltx_align_left">1.2</td>
<td id="S3.T1.3.3.6.4" class="ltx_td ltx_align_left">204.7K</td>
<td id="S3.T1.3.3.6.5" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.6.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.6.6" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.6.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.6.7" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.6.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.6.8" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.6.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.6.9" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.6.9.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S3.T1.3.3.7" class="ltx_tr">
<td id="S3.T1.3.3.7.1" class="ltx_td ltx_align_left">VizWiz-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>
</td>
<td id="S3.T1.3.3.7.2" class="ltx_td ltx_align_left">6.7</td>
<td id="S3.T1.3.3.7.3" class="ltx_td ltx_align_left">1.7</td>
<td id="S3.T1.3.3.7.4" class="ltx_td ltx_align_left">20.5K</td>
<td id="S3.T1.3.3.7.5" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.7.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.7.6" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.7.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.7.7" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.7.7.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S3.T1.3.3.7.8" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.7.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.7.9" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.7.9.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S3.T1.3.3.8" class="ltx_tr">
<td id="S3.T1.3.3.8.1" class="ltx_td ltx_align_left">OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>
</td>
<td id="S3.T1.3.3.8.2" class="ltx_td ltx_align_left">8.1</td>
<td id="S3.T1.3.3.8.3" class="ltx_td ltx_align_left">1.3</td>
<td id="S3.T1.3.3.8.4" class="ltx_td ltx_align_left">14K</td>
<td id="S3.T1.3.3.8.5" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.8.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.8.6" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.8.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.8.7" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.8.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.8.8" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.8.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.8.9" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.8.9.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S3.T1.3.3.9" class="ltx_tr">
<td id="S3.T1.3.3.9.1" class="ltx_td ltx_align_left">DocVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>
</td>
<td id="S3.T1.3.3.9.2" class="ltx_td ltx_align_left">9.5</td>
<td id="S3.T1.3.3.9.3" class="ltx_td ltx_align_left">2.4</td>
<td id="S3.T1.3.3.9.4" class="ltx_td ltx_align_left">12.8K</td>
<td id="S3.T1.3.3.9.5" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.9.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.9.6" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.9.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.9.7" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.9.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.9.8" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.9.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.9.9" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.9.9.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S3.T1.3.3.10" class="ltx_tr">
<td id="S3.T1.3.3.10.1" class="ltx_td ltx_align_left">ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>
</td>
<td id="S3.T1.3.3.10.2" class="ltx_td ltx_align_left">12.11</td>
<td id="S3.T1.3.3.10.3" class="ltx_td ltx_align_left">4.4</td>
<td id="S3.T1.3.3.10.4" class="ltx_td ltx_align_left">6.5K</td>
<td id="S3.T1.3.3.10.5" class="ltx_td ltx_align_left">26</td>
<td id="S3.T1.3.3.10.6" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.10.6.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S3.T1.3.3.10.7" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.10.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.10.8" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.10.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.10.9" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.10.9.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S3.T1.3.3.11" class="ltx_tr">
<td id="S3.T1.3.3.11.1" class="ltx_td ltx_align_left">InfographicVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>
</td>
<td id="S3.T1.3.3.11.2" class="ltx_td ltx_align_left">11.5</td>
<td id="S3.T1.3.3.11.3" class="ltx_td ltx_align_left">1.6</td>
<td id="S3.T1.3.3.11.4" class="ltx_td ltx_align_left">5.5K</td>
<td id="S3.T1.3.3.11.5" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.11.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.11.6" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.11.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.11.7" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.11.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.11.8" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.11.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.11.9" class="ltx_td ltx_align_left"><span id="S3.T1.3.3.11.9.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S3.T1.3.3.12" class="ltx_tr">
<td id="S3.T1.3.3.12.1" class="ltx_td ltx_align_left ltx_border_b">INFOSEEK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>
</td>
<td id="S3.T1.3.3.12.2" class="ltx_td ltx_align_left ltx_border_b">8.9</td>
<td id="S3.T1.3.3.12.3" class="ltx_td ltx_align_left ltx_border_b">1.5</td>
<td id="S3.T1.3.3.12.4" class="ltx_td ltx_align_left ltx_border_b">1.36M</td>
<td id="S3.T1.3.3.12.5" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T1.3.3.12.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.12.6" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T1.3.3.12.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.12.7" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T1.3.3.12.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.12.8" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T1.3.3.12.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.3.3.12.9" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T1.3.3.12.9.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.11.4.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.9.3" class="ltx_text" style="font-size:90%;">Characterization of existing VQA datasets and our new VQAonline in terms of mean question length (i.e., <math id="S3.T1.7.1.m1.1" class="ltx_Math" alttext="\overline{Q}" display="inline"><semantics id="S3.T1.7.1.m1.1b"><mover accent="true" id="S3.T1.7.1.m1.1.1" xref="S3.T1.7.1.m1.1.1.cmml"><mi id="S3.T1.7.1.m1.1.1.2" xref="S3.T1.7.1.m1.1.1.2.cmml">Q</mi><mo id="S3.T1.7.1.m1.1.1.1" xref="S3.T1.7.1.m1.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.T1.7.1.m1.1c"><apply id="S3.T1.7.1.m1.1.1.cmml" xref="S3.T1.7.1.m1.1.1"><ci id="S3.T1.7.1.m1.1.1.1.cmml" xref="S3.T1.7.1.m1.1.1.1">¯</ci><ci id="S3.T1.7.1.m1.1.1.2.cmml" xref="S3.T1.7.1.m1.1.1.2">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.1.m1.1d">\overline{Q}</annotation></semantics></math>), mean answer length (i.e., <math id="S3.T1.8.2.m2.1" class="ltx_Math" alttext="\overline{A}" display="inline"><semantics id="S3.T1.8.2.m2.1b"><mover accent="true" id="S3.T1.8.2.m2.1.1" xref="S3.T1.8.2.m2.1.1.cmml"><mi id="S3.T1.8.2.m2.1.1.2" xref="S3.T1.8.2.m2.1.1.2.cmml">A</mi><mo id="S3.T1.8.2.m2.1.1.1" xref="S3.T1.8.2.m2.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.T1.8.2.m2.1c"><apply id="S3.T1.8.2.m2.1.1.cmml" xref="S3.T1.8.2.m2.1.1"><ci id="S3.T1.8.2.m2.1.1.1.cmml" xref="S3.T1.8.2.m2.1.1.1">¯</ci><ci id="S3.T1.8.2.m2.1.1.2.cmml" xref="S3.T1.8.2.m2.1.1.2">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.2.m2.1d">\overline{A}</annotation></semantics></math>), number of images (i.e., <math id="S3.T1.9.3.m3.1" class="ltx_Math" alttext="N_{\text{img}}" display="inline"><semantics id="S3.T1.9.3.m3.1b"><msub id="S3.T1.9.3.m3.1.1" xref="S3.T1.9.3.m3.1.1.cmml"><mi id="S3.T1.9.3.m3.1.1.2" xref="S3.T1.9.3.m3.1.1.2.cmml">N</mi><mtext id="S3.T1.9.3.m3.1.1.3" xref="S3.T1.9.3.m3.1.1.3a.cmml">img</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T1.9.3.m3.1c"><apply id="S3.T1.9.3.m3.1.1.cmml" xref="S3.T1.9.3.m3.1.1"><csymbol cd="ambiguous" id="S3.T1.9.3.m3.1.1.1.cmml" xref="S3.T1.9.3.m3.1.1">subscript</csymbol><ci id="S3.T1.9.3.m3.1.1.2.cmml" xref="S3.T1.9.3.m3.1.1.2">𝑁</ci><ci id="S3.T1.9.3.m3.1.1.3a.cmml" xref="S3.T1.9.3.m3.1.1.3"><mtext mathsize="70%" id="S3.T1.9.3.m3.1.1.3.cmml" xref="S3.T1.9.3.m3.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.3.m3.1d">N_{\text{img}}</annotation></semantics></math>), number of authentic topics (i.e., # Auth. Topics), inclusion of context, inclusion of authentic visual questions (i.e., Auth. VQ), inclusion of authentic context, and inclusion of answers validated by those asking the questions (i.e., Vld. A)).</span></figcaption>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Comparison to Existing Datasets</h4>

<section id="S3.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Setup.</h5>

<div id="S3.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p1.1" class="ltx_p">We start by contextualizing how our dataset compares to the current focus of the research community. To do so, we compare VQAs in our dataset to those in eight existing popular VQA datasets. We selected VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> because it is the most popular (e.g., highly cited) VQA dataset, Context-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> and ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We focus on the subset of ScienceQA with both image and context.</span></span></span> because they similarly claim to provide context, VizWiz-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> because it similarly originates from an authentic use case, INFOSEEK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> because it similarly centers on information seeking rather than probing an AI model, OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> because it similarly requires domain-specific knowledge, as well as DocVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> and Infographic VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> because they similarly contain VQAs with documents and infographics.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p2.1" class="ltx_p">For each dataset, we report the mean number of words in each question, mean number of words in each answer, number of images, number of topics which were authentically used to label clusters of VQAs by theme, whether context is included, whether the visual questions are authentic, whether the context is authentic, and whether the answers are validated by those asking the questions. Results are reported in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Dataset Analysis ‣ 3 VQAonline Dataset ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p3.1" class="ltx_p">Our findings reveal commonalities of our VQAonline dataset with existing datasets. For example, we observe that the visual questions themselves are similar in terms of the typical question length and number of images. Specifically, questions have a mean of 9 words in our dataset compared to 6 to 11 words in other datasets and the nearly 65,000 images in our dataset are comparable to existing datasets that typically contain tens of thousands of examples.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p4.1" class="ltx_p">Our analysis also demonstrates significant differences from existing datasets across two key dimensions. The first dimension is the <em id="S3.SS2.SSS1.Px1.p4.1.1" class="ltx_emph ltx_font_italic">dataset authenticity</em>. Our VQAonline dataset is the only fully authentic dataset for the entire VQA pipeline, meaning all contents originate from an authentic use case. The second key difference of our dataset is the typical <em id="S3.SS2.SSS1.Px1.p4.1.2" class="ltx_emph ltx_font_italic">answer length</em>. Answers in our VQAonline dataset are orders of magnitude longer than in existing datasets, with a mean length of 173 words in VQAonline versus 11 or fewer words in existing datasets. This larger length is not anomalous, as the median answer length in VQAonline is 120 words. This longer length in part reflects responses are provided in paragraph form, with most commonly 3 sentences per answer. This distinction renders existing evaluation metrics for VQA models largely ineffective, as existing metrics assume brief responses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. This issue will be addressed in Section <a href="#S4.SS1" title="4.1 Evaluation Methodology ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
</section>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Fine-Grained Analysis</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">We next provide a richer analysis of the questions, images, context, and answers in our VQAonline dataset.</p>
</div>
<section id="S3.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Topic.</h5>

<div id="S3.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px1.p1.1" class="ltx_p">The authentic user-based posting of each VQA within a topic community reveals the diversity of content in our dataset. We report the number of VQAs in each of the 105 topics in Figure <a href="#S3.F2" title="Figure 2 ‣ Source. ‣ 3.1 Dataset Creation ‣ 3 VQAonline Dataset ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. As shown, there can be as few as 10 examples in a topic to over 20,000.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Questions.</h5>

<div id="S3.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px2.p1.1" class="ltx_p">Our VQAonline dataset is the first with authentic visual questions asked by users of an online question-answering platform. This results in a domain shift from the only other VQA dataset with authentically asked visual questions, VizWiz-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, which were asked by blind users of a mobile phone application. For instance, blind people typically asked about their physical surroundings (e.g., "What is this?", "What does this say?") while Stack Exchange users often asked for more fine-grained or specialized information about the images (e.g., "What’s a better term to describe these ‘stripes’ in blouse (Screen shot attached)", "What is the third character on this sign seen in Taipei?"). Another distinction stems from Stack Exchange’s feature for avoiding redundant questions by letting users remove duplicates of other already submitted questions. The consequence is that only 0.2% of natural language questions in our VQAonline dataset are duplicated compared to 37.9% of questions in VizWiz-VQA.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Context.</h5>

<div id="S3.SS2.SSS2.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px3.p1.1" class="ltx_p">Our VQAonline dataset is the first to include authentic context. The only other VQA dataset containing context, as defined by supplementary data that supports answering the visual question, is ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>. However, the context was contrived with heuristic rules for extracting data from elementary and high school science curricula. Consequently, context in ScienceQA is typically narrated in third-person, while context in our VQAonline often are narrated in first-person. Another difference is ScienceQA has a shorter average length of the context compared to that in our dataset; i.e., a mean of 41 words, median of 28 words, and most commonly 3 sentences versus a mean of 127 words, median of 94 words, and most commonly 4 sentences. A further difference is that the context in ScienceQA only relates to 26 science topics while the context in our dataset spans 105 topics that extend beyond science.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Image Contents.</h5>

<div id="S3.SS2.SSS2.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px4.p1.1" class="ltx_p">We next quantify the tendency for images to include screenshots and infographics to reflect the tendency for visual questions to be related to the digital realm and data analysis. We identified an image as a "screenshot" when contents were copied digitally from a computing device and an "infographic" when the image shows a chart, table, graph, attention map, map, or diagram. We manually labeled 105 images sampled <em id="S3.SS2.SSS2.Px4.p1.1.1" class="ltx_emph ltx_font_italic">from each of the 105 topics</em> as well as an independent random selection of 100 images from the <em id="S3.SS2.SSS2.Px4.p1.1.2" class="ltx_emph ltx_font_italic">entire dataset</em>. We found for the topic-sampled set that 79% are screenshots and 17% show infographics and for the samples from the entire dataset that 84% are screenshots and 33% show infographics. The similarly high prevalence of screenshots across both samples underscores Stack Exchange users’ tendency to ask questions about the digital realm. We hypothesized the different prevalence of infographics across both samples was due to topical differences, as 52% of all VQAs (i.e., 33,384 out of 64,696) belong to only 23% of the topics (i.e., 24 science topics). Supporting this hypothesis, we found from inspecting 100 random VQAs from each subset of the science topics and non-science topics that infographics were in 58% and 6% of images respectively.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Importance of Images.</h5>

<div id="S3.SS2.SSS2.Px5.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px5.p1.1" class="ltx_p">We next examine the value of images for answering the questions. To do so, we flagged each image as <em id="S3.SS2.SSS2.Px5.p1.1.1" class="ltx_emph ltx_font_italic">question-aligned</em> if elements described in the question are found in the image and <em id="S3.SS2.SSS2.Px5.p1.1.2" class="ltx_emph ltx_font_italic">context-aligned</em> if elements described in the context are found in the image. We additionally flagged whether an image is <em id="S3.SS2.SSS2.Px5.p1.1.3" class="ltx_emph ltx_font_italic">necessary</em>, based on whether the answer can be obtained without the image. We manually labeled 105 examples from each of the 105 topics. We found that 91.4% of the images are <em id="S3.SS2.SSS2.Px5.p1.1.4" class="ltx_emph ltx_font_italic">question-aligned</em> and 99% of the images are <em id="S3.SS2.SSS2.Px5.p1.1.5" class="ltx_emph ltx_font_italic">context-aligned</em>. This finding contrasts that of the only other dataset with authentic visual questions, VizWiz-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, where only 71% are deemed answerable because blind photographers struggled to capture contents of interest in their images. We also found that 65.7% images are <em id="S3.SS2.SSS2.Px5.p1.1.6" class="ltx_emph ltx_font_italic">necessary</em> for answering the question in our dataset. Often, the purpose of unnecessary images was to motivate or show scenarios in which the asker encounters the question. Altogether, these findings underscore the value of VQA models analyzing the images to arrive at the answers.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2311.15562/assets/Images/user_intention_examples.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Examples of three common user intentions for visual questions in our dataset: instruction, evidence, and verification. </span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS2.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">User Intention.</h5>

<div id="S3.SS2.SSS2.Px6.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px6.p1.1" class="ltx_p">We finally characterized intents behind the visual questions to inform how AI models can better meet users’ information wants.
Our work complements two prior works that identified user intentions for three community question answering platforms (Answerbag, Metafilter, and Yahoo! Answer) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> and the music StackExchange community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. Our work is the first to investigate user intentions for <em id="S3.SS2.SSS2.Px6.p1.1.1" class="ltx_emph ltx_font_italic">VQAs</em>. Through a multi-round analysis of the data, described in the Supplementary Materials, we defined eight potential intents: <em id="S3.SS2.SSS2.Px6.p1.1.2" class="ltx_emph ltx_font_italic">instruction</em>, <em id="S3.SS2.SSS2.Px6.p1.1.3" class="ltx_emph ltx_font_italic">evidence</em>, <em id="S3.SS2.SSS2.Px6.p1.1.4" class="ltx_emph ltx_font_italic">verification</em>, <em id="S3.SS2.SSS2.Px6.p1.1.5" class="ltx_emph ltx_font_italic">advice</em>, <em id="S3.SS2.SSS2.Px6.p1.1.6" class="ltx_emph ltx_font_italic">identification</em>, <em id="S3.SS2.SSS2.Px6.p1.1.7" class="ltx_emph ltx_font_italic">opinion</em>, <em id="S3.SS2.SSS2.Px6.p1.1.8" class="ltx_emph ltx_font_italic">reason</em>, and <em id="S3.SS2.SSS2.Px6.p1.1.9" class="ltx_emph ltx_font_italic">other</em>. Examples for three of these categories are shown in Figure <a href="#S3.F3" title="Figure 3 ‣ Importance of Images. ‣ 3.2.2 Fine-Grained Analysis ‣ 3.2 Dataset Analysis ‣ 3 VQAonline Dataset ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We then recruited vetted crowdworkers from Amazon Mechanical Turk to label one primary intent per visual question when shown the question, image, context, and answer. They labeled 105 examples, with one randomly selected from each of the 105 topics. We used the majority vote label per VQA. The resulting distribution of assigned categories is 19% are instruction, 18% evidence, 17% verification, 16% advice, 10% identification, 10% opinion, 8% reason, and 1% other. Examples from the three most common categories are shown in Figure <a href="#S3.F3" title="Figure 3 ‣ Importance of Images. ‣ 3.2.2 Fine-Grained Analysis ‣ 3.2 Dataset Analysis ‣ 3 VQAonline Dataset ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Methodology</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As noted in Section <a href="#S3.SS2" title="3.2 Dataset Analysis ‣ 3 VQAonline Dataset ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, the uniquely lengthy answers in our VQAonline dataset render the mainstream VQA evaluation metric unsuitable since it assumes short answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>. Consequently, we first establish an appropriate evaluation methodology. To do so, we assess which widely-used metrics for longer text evaluation align best with human judgments of the quality of model-generated results.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Candidate Evaluation Metrics.</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We identified six popular evaluation metrics for longer text from related fields, including long-form question answering and image captioning. We primarily focused on reference-based metrics, which assess the similarity between model-generated answers and human-generated answers, because of their widespread use. Specifically, we chose ROUGE-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>, RefCLIP-S  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, BERTscore <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>, and the GPT-4 metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>For GPT-4 metric, we prompted GPT-4 to ”compare the ground truth and prediction from AI models and give a correctness score for the prediction. The correctness score is 0 (totally wrong), 1 (partially correct), or 2 (totally correct).” We then rescale GPT-4 metric to range from 0 to 1. </span></span></span>. For completeness, we also include the reference-free metric CLIP-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, which assesses compatibility between the image and model-generated answer. All metrics are set to output values in the range of 0 to 1.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Models.</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">We chose to evaluate the following six modern zero-shot Vision and Language Models (VLMs) that achieved strong performance in early 2023 for various VQA datasets and subtasks.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">BLIP-2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>: state-of-the-art (SOTA) for zero-shot setting on four VQA datasets, including ScienceQA and VizWiz-VQA according to LVLM-eHub, a comprehensive evaluation benchmark for VLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">MiniGPT-4</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>: ranked first overall in open-world VQA scenario according to LLVM-eHub <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">LLaVA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>: top-two performance in the zero-shot setting for three VQA datasets, including for DocVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> according to LLVM-eHub <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">mPLUG-Owl</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>: SOTA for VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> in early 2023 and ranked second overall in open-world VQA scenario according to LLVM-eHub <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">InstructBLIP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>: SOTA for zero-shot setting on four VQA datasets, including OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> according to LLVM-eHub  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p"><span id="S4.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">GPT-4V</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>: SOTA for numerous VQA datasets, such as Infographic VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> and TVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Human Judgement Collection.</h5>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">We hired ten human annotators for an IRB-approved human evaluation study. Each individual was a domain expert in one of ten topics covered in our dataset (i.e., Physical Fitness, Botany, Music, Mechanics, Politics, Law, Chinese Language, Economics, Statistics, and Artificial Intelligence), with at least a Master’s degree in a field related to their assigned VQA topic. Each domain expert was assigned 20 VQAs on their topic, and was required to rate the performance of six model-generated answers per VQA. As a result, we collected for 200 VQAs a total of 1200 human judgments (i.e., <math id="S4.SS1.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="10*20*6" display="inline"><semantics id="S4.SS1.SSS0.Px3.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml">10</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml">∗</mo><mn id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml">20</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1a" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml">∗</mo><mn id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.4" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.4.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1"><times id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2">10</cn><cn type="integer" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3">20</cn><cn type="integer" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.4.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.4">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.1.m1.1c">10*20*6</annotation></semantics></math>).</p>
</div>
<div id="S4.SS1.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p2.1" class="ltx_p">For the annotation task, each domain expert was shown for each VQA the original visual question, context, and answer alongside the six model-generated answers in a home-grown web-based annotation tool. The task was to rate each model-generated answer as either incorrect (i.e., score of 0), partially correct (i.e, score of 0.5), or correct (i.e., score of 1). The definitions for these ratings are: <span id="S4.SS1.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_bold">Correct</span>: At least one of these two items needs to be true for the answer to be correct: (1) Aligns with all the key points in the reference answer. (2) The answer also makes sense based on your best judgment, even if not included in the reference answer. <span id="S4.SS1.SSS0.Px3.p2.1.2" class="ltx_text ltx_font_bold">Partially Correct:</span> The answer matches part of the key points in the reference answer that answers the question but has errors in the content. <span id="S4.SS1.SSS0.Px3.p2.1.3" class="ltx_text ltx_font_bold">Incorrect:</span> The response doesn’t include any key points from the reference answer and doesn’t make sense.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.2.1" class="ltx_tr">
<td id="S4.T2.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S4.T2.2.1.1.1" class="ltx_text" style="font-size:90%;">Metric</span></td>
<td id="S4.T2.2.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T2.2.1.2.1" class="ltx_text" style="font-size:90%;">Correlation</span></td>
<td id="S4.T2.2.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T2.2.1.3.1" class="ltx_text" style="font-size:90%;">p-value</span></td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T2.2.2.1.1" class="ltx_text" style="font-size:90%;">METEOR</span></td>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.2.2.2.1" class="ltx_text" style="font-size:90%;">0.971</span></td>
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.2.2.3.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
</tr>
<tr id="S4.T2.2.3" class="ltx_tr">
<td id="S4.T2.2.3.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T2.2.3.1.1" class="ltx_text" style="font-size:90%;">GPT-4 metric</span></td>
<td id="S4.T2.2.3.2" class="ltx_td ltx_align_left"><span id="S4.T2.2.3.2.1" class="ltx_text" style="font-size:90%;">0.944</span></td>
<td id="S4.T2.2.3.3" class="ltx_td ltx_align_left"><span id="S4.T2.2.3.3.1" class="ltx_text" style="font-size:90%;">0.005</span></td>
</tr>
<tr id="S4.T2.2.4" class="ltx_tr">
<td id="S4.T2.2.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T2.2.4.1.1" class="ltx_text" style="font-size:90%;">BERTscore</span></td>
<td id="S4.T2.2.4.2" class="ltx_td ltx_align_left"><span id="S4.T2.2.4.2.1" class="ltx_text" style="font-size:90%;">0.900</span></td>
<td id="S4.T2.2.4.3" class="ltx_td ltx_align_left"><span id="S4.T2.2.4.3.1" class="ltx_text" style="font-size:90%;">0.015</span></td>
</tr>
<tr id="S4.T2.2.5" class="ltx_tr">
<td id="S4.T2.2.5.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T2.2.5.1.1" class="ltx_text" style="font-size:90%;">RefCLIP-S</span></td>
<td id="S4.T2.2.5.2" class="ltx_td ltx_align_left"><span id="S4.T2.2.5.2.1" class="ltx_text" style="font-size:90%;">0.892</span></td>
<td id="S4.T2.2.5.3" class="ltx_td ltx_align_left"><span id="S4.T2.2.5.3.1" class="ltx_text" style="font-size:90%;">0.017</span></td>
</tr>
<tr id="S4.T2.2.6" class="ltx_tr">
<td id="S4.T2.2.6.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T2.2.6.1.1" class="ltx_text" style="font-size:90%;">ROUGEL</span></td>
<td id="S4.T2.2.6.2" class="ltx_td ltx_align_left"><span id="S4.T2.2.6.2.1" class="ltx_text" style="font-size:90%;">0.874</span></td>
<td id="S4.T2.2.6.3" class="ltx_td ltx_align_left"><span id="S4.T2.2.6.3.1" class="ltx_text" style="font-size:90%;">0.023</span></td>
</tr>
<tr id="S4.T2.2.7" class="ltx_tr">
<td id="S4.T2.2.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T2.2.7.1.1" class="ltx_text" style="font-size:90%;">CLIP-S</span></td>
<td id="S4.T2.2.7.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.2.7.2.1" class="ltx_text" style="font-size:90%;">0.662</span></td>
<td id="S4.T2.2.7.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.2.7.3.1" class="ltx_text" style="font-size:90%;">0.152</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Correlation between human judgment and popular automated evaluation metric scores for 220 VQAs. </figcaption>
</figure>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Analysis Design and Results.</h5>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">We calculated the mean score for each model across the 200 VQAs for humans<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>The resulting scores are GPT-4V: 0.76, mPLUG-Owl: 0.57, LLaVA:0.5, MiniGPT-4: 0.37, InstructBLIP:0.12, and BLIP2: 0.1.</span></span></span> as well as using each evaluation metric. Then we calculated the Pearson correlation between mean scores between humans and each evaluation metric. Results are shown in Table <a href="#S4.T2" title="Table 2 ‣ Human Judgement Collection. ‣ 4.1 Evaluation Methodology ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS1.SSS0.Px4.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p2.1" class="ltx_p">Overall, we observe a high positive correlation between all the reference-based metrics and human judgments with statistical significance (i.e., p-value less than 0.05). Metrics with the strongest correlation to human judgments are METEOR metric followed by GPT-4 metric and BERTscore. Consequently, we will use these three top-performing metrics for our algorithm benchmarking.</p>
</div>
<div id="S4.SS1.SSS0.Px4.p3" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p3.1" class="ltx_p">For the reference-free metric CLIP-S, although the results are not statistically significant, we found it performs the worst of all metrics. This contrasts the finding from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> that CLIP-S can outperform other reference-based metrics for image captioning.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model Benchmarking</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We next analyze the performance of modern models on our VQAonline dataset to reveal what makes it challenging for the research community.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Models and Evaluation Metrics.</h5>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We evaluate the same six models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite> analyzed in Section <a href="#S4.SS1" title="4.1 Evaluation Methodology ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, with the three most human-aligned evaluation metrics <span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We provided the overall results for the six evaluation metrics in supplementary materials.</span></span></span>. However, due to GPT-based models requiring a fee for their usage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, we only evaluate GPT-4V on a subset of data and only use the GPT-4 metric for that subset of data. We created this subset by randomly selecting 20 VQAs from each of the 105 topics, except for the 17 topics containing less than 20 examples where we used all available VQAs. The resulting subset contains 1903 VQAs. For a fair comparison, we also report the performance of all other models on this subset.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Overall Results.</h5>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">Results are shown in Table <a href="#S3.T9" title="Table 9 ‣ III.3 Automated Model Benchmarking ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
We observe that all models perform poorly, especially when considering the results from the METEOR and GPT-4 metrics. In other words, considerable progress still remains to achieve the status of some modern models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> which are purported to match or even surpass human performance on some VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p2.1" class="ltx_p">When comparing across models on the whole test set, we found that mPLUG-Owl is the best VLM model, and if on the subset, GPT-4V is the top-performing model, followed closely by mPLUG-Owl.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.2.1" class="ltx_tr">
<td id="S4.T3.2.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T3.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.2.1.2.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S4.T3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T3.2.1.3.1" class="ltx_text ltx_font_bold">Subset</span></td>
</tr>
<tr id="S4.T3.2.2" class="ltx_tr">
<td id="S4.T3.2.2.1" class="ltx_td"></td>
<td id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">M.</span></td>
<td id="S4.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">B.</span></td>
<td id="S4.T3.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.4.1" class="ltx_text ltx_font_italic" style="font-size:90%;">M.</span></td>
<td id="S4.T3.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">B.</span></td>
<td id="S4.T3.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">G.</span></td>
</tr>
<tr id="S4.T3.2.3" class="ltx_tr">
<td id="S4.T3.2.3.1" class="ltx_td ltx_align_left ltx_border_t">GPT-4V  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>
</td>
<td id="S4.T3.2.3.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.2.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T3.2.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.3.4.1" class="ltx_text ltx_font_bold">0.12</span></td>
<td id="S4.T3.2.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.3.5.1" class="ltx_text ltx_font_bold">0.76</span></td>
<td id="S4.T3.2.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.3.6.1" class="ltx_text ltx_font_bold">0.55</span></td>
</tr>
<tr id="S4.T3.2.4" class="ltx_tr">
<td id="S4.T3.2.4.1" class="ltx_td ltx_align_left">mPLUG-Owl <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>
</td>
<td id="S4.T3.2.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.2.4.2.1" class="ltx_text ltx_font_bold">0.12</span></td>
<td id="S4.T3.2.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.4.3.1" class="ltx_text ltx_font_bold">0.76</span></td>
<td id="S4.T3.2.4.4" class="ltx_td ltx_align_center">0.10</td>
<td id="S4.T3.2.4.5" class="ltx_td ltx_align_center">0.75</td>
<td id="S4.T3.2.4.6" class="ltx_td ltx_align_center">0.40</td>
</tr>
<tr id="S4.T3.2.5" class="ltx_tr">
<td id="S4.T3.2.5.1" class="ltx_td ltx_align_left">LLaVA  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>
</td>
<td id="S4.T3.2.5.2" class="ltx_td ltx_align_center">0.08</td>
<td id="S4.T3.2.5.3" class="ltx_td ltx_align_center ltx_border_r">0.75</td>
<td id="S4.T3.2.5.4" class="ltx_td ltx_align_center">0.08</td>
<td id="S4.T3.2.5.5" class="ltx_td ltx_align_center">0.75</td>
<td id="S4.T3.2.5.6" class="ltx_td ltx_align_center">0.38</td>
</tr>
<tr id="S4.T3.2.6" class="ltx_tr">
<td id="S4.T3.2.6.1" class="ltx_td ltx_align_left">MiniGPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>
</td>
<td id="S4.T3.2.6.2" class="ltx_td ltx_align_center">0.09</td>
<td id="S4.T3.2.6.3" class="ltx_td ltx_align_center ltx_border_r">0.75</td>
<td id="S4.T3.2.6.4" class="ltx_td ltx_align_center">0.08</td>
<td id="S4.T3.2.6.5" class="ltx_td ltx_align_center">0.74</td>
<td id="S4.T3.2.6.6" class="ltx_td ltx_align_center">0.32</td>
</tr>
<tr id="S4.T3.2.7" class="ltx_tr">
<td id="S4.T3.2.7.1" class="ltx_td ltx_align_left">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>
</td>
<td id="S4.T3.2.7.2" class="ltx_td ltx_align_center">0.05</td>
<td id="S4.T3.2.7.3" class="ltx_td ltx_align_center ltx_border_r">0.67</td>
<td id="S4.T3.2.7.4" class="ltx_td ltx_align_center">0.06</td>
<td id="S4.T3.2.7.5" class="ltx_td ltx_align_center">0.69</td>
<td id="S4.T3.2.7.6" class="ltx_td ltx_align_center">0.30</td>
</tr>
<tr id="S4.T3.2.8" class="ltx_tr">
<td id="S4.T3.2.8.1" class="ltx_td ltx_align_left ltx_border_bb">BLIP2  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</td>
<td id="S4.T3.2.8.2" class="ltx_td ltx_align_center ltx_border_bb">0.04</td>
<td id="S4.T3.2.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.69</td>
<td id="S4.T3.2.8.4" class="ltx_td ltx_align_center ltx_border_bb">0.04</td>
<td id="S4.T3.2.8.5" class="ltx_td ltx_align_center ltx_border_bb">0.69</td>
<td id="S4.T3.2.8.6" class="ltx_td ltx_align_center ltx_border_bb">0.27</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">Performance of six modern VQA models on our dataset with respect to three evaluation metrics. (M. = METEOR, B. = BERTscore, G.= GPT-4 Metric)</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Analysis With Respect to Input Types.</h5>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">We next analyze the predictive power of each input by analyzing the performance of ablated model variants where one input type is removed from each: i.e., Q+C, Q+I, and C+I. Results for the two top-performing models are shown in Table <a href="#S4.T4" title="Table 4 ‣ Analysis With Respect to Input Types. ‣ 4.2 Model Benchmarking ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Unsurprisingly, we find that the model receiving the maximum amount of information (i.e., Q+C+I) performs the best.</p>
</div>
<div id="S4.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p2.1" class="ltx_p">We observe that the worst-performing variant across both models and all evaluation metrics are the models lacking <em id="S4.SS2.SSS0.Px3.p2.1.1" class="ltx_emph ltx_font_italic">context</em> as input. This highlights that current models find the context (which is typically language-based) to be the most valuable source for arriving at an answer.</p>
</div>
<div id="S4.SS2.SSS0.Px3.p3" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p3.1" class="ltx_p">When comparing the models that lack questions as input (i.e., Q+C) with models receiving all inputs (Q+C+I), the questions’ predictive power appears to be nearly negligible. We suspect this is because often the information provided by the question is included in the context. For example, a question asking "What test am I taking, anyways?" while the part of the context includes "can you tell me what field of math the test was on?", which overlaps with the question.</p>
</div>
<div id="S4.SS2.SSS0.Px3.p4" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p4.1" class="ltx_p">When comparing the models lacking images as input (i.e., Q+C) with models receiving all inputs (Q+C+I), we similarly observe that the predictive power of the image seems very small. We suspect this result does not imply the irrelevance of images for the task, as further validated through a small-scale study in Section <a href="#S3.SS2" title="3.2 Dataset Analysis ‣ 3 VQAonline Dataset ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. Instead, we believe this argues for improving models so they can better utilize images when predicting answers.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T4.2.1" class="ltx_tr">
<td id="S4.T4.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S4.T4.2.1.1.1" class="ltx_text" style="font-size:90%;">Models</span></td>
<td id="S4.T4.2.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T4.2.1.2.1" class="ltx_text" style="font-size:90%;">METEOR</span></td>
<td id="S4.T4.2.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T4.2.1.3.1" class="ltx_text" style="font-size:90%;">BERT.</span></td>
<td id="S4.T4.2.1.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T4.2.1.4.1" class="ltx_text" style="font-size:90%;">~GPT-4 Metric</span></td>
</tr>
<tr id="S4.T4.2.2" class="ltx_tr">
<td id="S4.T4.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T4.2.2.1.1" class="ltx_text" style="font-size:90%;">mPLUG(Q+C+I)</span></td>
<td id="S4.T4.2.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.12</span></td>
<td id="S4.T4.2.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.76</span></td>
<td id="S4.T4.2.2.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.40</span></td>
</tr>
<tr id="S4.T4.2.3" class="ltx_tr">
<td id="S4.T4.2.3.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.2.3.1.1" class="ltx_text" style="font-size:90%;">mPLUG(C+I)</span></td>
<td id="S4.T4.2.3.2" class="ltx_td ltx_align_left"><span id="S4.T4.2.3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.12</span></td>
<td id="S4.T4.2.3.3" class="ltx_td ltx_align_left"><span id="S4.T4.2.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.76</span></td>
<td id="S4.T4.2.3.4" class="ltx_td ltx_align_left"><span id="S4.T4.2.3.4.1" class="ltx_text" style="font-size:90%;">0.37</span></td>
</tr>
<tr id="S4.T4.2.4" class="ltx_tr">
<td id="S4.T4.2.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.2.4.1.1" class="ltx_text" style="font-size:90%;">mPLUG(Q+C)</span></td>
<td id="S4.T4.2.4.2" class="ltx_td ltx_align_left"><span id="S4.T4.2.4.2.1" class="ltx_text" style="font-size:90%;">0.11</span></td>
<td id="S4.T4.2.4.3" class="ltx_td ltx_align_left"><span id="S4.T4.2.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.76</span></td>
<td id="S4.T4.2.4.4" class="ltx_td ltx_align_left"><span id="S4.T4.2.4.4.1" class="ltx_text" style="font-size:90%;">0.36</span></td>
</tr>
<tr id="S4.T4.2.5" class="ltx_tr">
<td id="S4.T4.2.5.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.2.5.1.1" class="ltx_text" style="font-size:90%;">mPLUG(Q+I)</span></td>
<td id="S4.T4.2.5.2" class="ltx_td ltx_align_left"><span id="S4.T4.2.5.2.1" class="ltx_text" style="font-size:90%;">0.07</span></td>
<td id="S4.T4.2.5.3" class="ltx_td ltx_align_left"><span id="S4.T4.2.5.3.1" class="ltx_text" style="font-size:90%;">0.74</span></td>
<td id="S4.T4.2.5.4" class="ltx_td ltx_align_left"><span id="S4.T4.2.5.4.1" class="ltx_text" style="font-size:90%;">0.25</span></td>
</tr>
<tr id="S4.T4.2.6" class="ltx_tr">
<td id="S4.T4.2.6.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.2.6.1.1" class="ltx_text" style="font-size:90%;">~GPT-4V (Q+C+I)</span></td>
<td id="S4.T4.2.6.2" class="ltx_td ltx_align_left"><span id="S4.T4.2.6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.12</span></td>
<td id="S4.T4.2.6.3" class="ltx_td ltx_align_left"><span id="S4.T4.2.6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.76</span></td>
<td id="S4.T4.2.6.4" class="ltx_td ltx_align_left"><span id="S4.T4.2.6.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.55</span></td>
</tr>
<tr id="S4.T4.2.7" class="ltx_tr">
<td id="S4.T4.2.7.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.2.7.1.1" class="ltx_text" style="font-size:90%;">~GPT-4V (C+I)</span></td>
<td id="S4.T4.2.7.2" class="ltx_td ltx_align_left"><span id="S4.T4.2.7.2.1" class="ltx_text" style="font-size:90%;">0.11</span></td>
<td id="S4.T4.2.7.3" class="ltx_td ltx_align_left"><span id="S4.T4.2.7.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.76</span></td>
<td id="S4.T4.2.7.4" class="ltx_td ltx_align_left"><span id="S4.T4.2.7.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.55</span></td>
</tr>
<tr id="S4.T4.2.8" class="ltx_tr">
<td id="S4.T4.2.8.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.2.8.1.1" class="ltx_text" style="font-size:90%;">~GPT-4V (Q+C)</span></td>
<td id="S4.T4.2.8.2" class="ltx_td ltx_align_left"><span id="S4.T4.2.8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.12</span></td>
<td id="S4.T4.2.8.3" class="ltx_td ltx_align_left"><span id="S4.T4.2.8.3.1" class="ltx_text" style="font-size:90%;">0.75</span></td>
<td id="S4.T4.2.8.4" class="ltx_td ltx_align_left"><span id="S4.T4.2.8.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.55</span></td>
</tr>
<tr id="S4.T4.2.9" class="ltx_tr">
<td id="S4.T4.2.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T4.2.9.1.1" class="ltx_text" style="font-size:90%;">~GPT-4V (Q+I)</span></td>
<td id="S4.T4.2.9.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T4.2.9.2.1" class="ltx_text" style="font-size:90%;">0.07</span></td>
<td id="S4.T4.2.9.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T4.2.9.3.1" class="ltx_text" style="font-size:90%;">0.74</span></td>
<td id="S4.T4.2.9.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T4.2.9.4.1" class="ltx_text" style="font-size:90%;">0.40</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance of the top-two VQA models with different input types (Q+C+I, C+I, Q+C, Q+I). Models or scores labeled with "~" are evaluated on the dataset subset. </figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.2.1" class="ltx_tr">
<td id="S4.T5.2.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T5.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T5.2.1.2.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S4.T5.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="6"><span id="S4.T5.2.1.3.1" class="ltx_text ltx_font_bold">Subset</span></td>
</tr>
<tr id="S4.T5.2.2" class="ltx_tr">
<td id="S4.T5.2.2.1" class="ltx_td ltx_align_left"><span id="S4.T5.2.2.1.1" class="ltx_text ltx_font_bold">Metric</span></td>
<td id="S4.T5.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">METEOR</span></td>
<td id="S4.T5.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">BERTscore</span></td>
<td id="S4.T5.2.2.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T5.2.2.4.1" class="ltx_text ltx_font_italic" style="font-size:90%;">METEOR</span></td>
<td id="S4.T5.2.2.5" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T5.2.2.5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">BERTscore</span></td>
<td id="S4.T5.2.2.6" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T5.2.2.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GPT-4 Metric</span></td>
</tr>
<tr id="S4.T5.2.3" class="ltx_tr">
<td id="S4.T5.2.3.1" class="ltx_td ltx_align_left"><span id="S4.T5.2.3.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T5.2.3.2" class="ltx_td ltx_align_center"><span id="S4.T5.2.3.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">mPLUG-Owl</span></td>
<td id="S4.T5.2.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.2.3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">mPLUG-Owl</span></td>
<td id="S4.T5.2.3.4" class="ltx_td ltx_align_center"><span id="S4.T5.2.3.4.1" class="ltx_text ltx_font_italic" style="font-size:90%;">mPLUG-Owl</span></td>
<td id="S4.T5.2.3.5" class="ltx_td ltx_align_center"><span id="S4.T5.2.3.5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GPT-4V</span></td>
<td id="S4.T5.2.3.6" class="ltx_td ltx_align_center"><span id="S4.T5.2.3.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">mPLUG-Owl</span></td>
<td id="S4.T5.2.3.7" class="ltx_td ltx_align_center"><span id="S4.T5.2.3.7.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GPT-4V</span></td>
<td id="S4.T5.2.3.8" class="ltx_td ltx_align_center"><span id="S4.T5.2.3.8.1" class="ltx_text ltx_font_italic" style="font-size:90%;">mPLUG-Owl</span></td>
<td id="S4.T5.2.3.9" class="ltx_td ltx_align_center"><span id="S4.T5.2.3.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;"> GPT-4V</span></td>
</tr>
<tr id="S4.T5.2.4" class="ltx_tr">
<td id="S4.T5.2.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.2.4.1.1" class="ltx_text ltx_font_bold">Science</span></td>
<td id="S4.T5.2.4.2" class="ltx_td ltx_align_center ltx_border_t">0.14</td>
<td id="S4.T5.2.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.77</td>
<td id="S4.T5.2.4.4" class="ltx_td ltx_align_center ltx_border_t">0.13</td>
<td id="S4.T5.2.4.5" class="ltx_td ltx_align_center ltx_border_t">0.13</td>
<td id="S4.T5.2.4.6" class="ltx_td ltx_align_center ltx_border_t">0.75</td>
<td id="S4.T5.2.4.7" class="ltx_td ltx_align_center ltx_border_t">0.77</td>
<td id="S4.T5.2.4.8" class="ltx_td ltx_align_center ltx_border_t">0.38</td>
<td id="S4.T5.2.4.9" class="ltx_td ltx_align_center ltx_border_t">0.56</td>
</tr>
<tr id="S4.T5.2.5" class="ltx_tr">
<td id="S4.T5.2.5.1" class="ltx_td ltx_align_left"><span id="S4.T5.2.5.1.1" class="ltx_text ltx_font_bold">Life&amp;Arts</span></td>
<td id="S4.T5.2.5.2" class="ltx_td ltx_align_center">0.13</td>
<td id="S4.T5.2.5.3" class="ltx_td ltx_align_center ltx_border_r">0.76</td>
<td id="S4.T5.2.5.4" class="ltx_td ltx_align_center">0.12</td>
<td id="S4.T5.2.5.5" class="ltx_td ltx_align_center">0.12</td>
<td id="S4.T5.2.5.6" class="ltx_td ltx_align_center">0.75</td>
<td id="S4.T5.2.5.7" class="ltx_td ltx_align_center">0.77</td>
<td id="S4.T5.2.5.8" class="ltx_td ltx_align_center">0.42</td>
<td id="S4.T5.2.5.9" class="ltx_td ltx_align_center">0.54</td>
</tr>
<tr id="S4.T5.2.6" class="ltx_tr">
<td id="S4.T5.2.6.1" class="ltx_td ltx_align_left"><span id="S4.T5.2.6.1.1" class="ltx_text ltx_font_bold">Culture&amp;Rec.</span></td>
<td id="S4.T5.2.6.2" class="ltx_td ltx_align_center">0.11</td>
<td id="S4.T5.2.6.3" class="ltx_td ltx_align_center ltx_border_r">0.75</td>
<td id="S4.T5.2.6.4" class="ltx_td ltx_align_center">0.11</td>
<td id="S4.T5.2.6.5" class="ltx_td ltx_align_center">0.11</td>
<td id="S4.T5.2.6.6" class="ltx_td ltx_align_center">0.75</td>
<td id="S4.T5.2.6.7" class="ltx_td ltx_align_center">0.77</td>
<td id="S4.T5.2.6.8" class="ltx_td ltx_align_center">0.39</td>
<td id="S4.T5.2.6.9" class="ltx_td ltx_align_center">0.56</td>
</tr>
<tr id="S4.T5.2.7" class="ltx_tr">
<td id="S4.T5.2.7.1" class="ltx_td ltx_align_left"><span id="S4.T5.2.7.1.1" class="ltx_text ltx_font_bold">Business</span></td>
<td id="S4.T5.2.7.2" class="ltx_td ltx_align_center">0.13</td>
<td id="S4.T5.2.7.3" class="ltx_td ltx_align_center ltx_border_r">0.76</td>
<td id="S4.T5.2.7.4" class="ltx_td ltx_align_center">0.14</td>
<td id="S4.T5.2.7.5" class="ltx_td ltx_align_center">0.14</td>
<td id="S4.T5.2.7.6" class="ltx_td ltx_align_center">0.77</td>
<td id="S4.T5.2.7.7" class="ltx_td ltx_align_center">0.77</td>
<td id="S4.T5.2.7.8" class="ltx_td ltx_align_center">0.42</td>
<td id="S4.T5.2.7.9" class="ltx_td ltx_align_center">0.58</td>
</tr>
<tr id="S4.T5.2.8" class="ltx_tr">
<td id="S4.T5.2.8.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.2.8.1.1" class="ltx_text ltx_font_bold">Professional</span></td>
<td id="S4.T5.2.8.2" class="ltx_td ltx_align_center ltx_border_bb">0.13</td>
<td id="S4.T5.2.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.76</td>
<td id="S4.T5.2.8.4" class="ltx_td ltx_align_center ltx_border_bb">0.13</td>
<td id="S4.T5.2.8.5" class="ltx_td ltx_align_center ltx_border_bb">0.13</td>
<td id="S4.T5.2.8.6" class="ltx_td ltx_align_center ltx_border_bb">0.76</td>
<td id="S4.T5.2.8.7" class="ltx_td ltx_align_center ltx_border_bb">0.77</td>
<td id="S4.T5.2.8.8" class="ltx_td ltx_align_center ltx_border_bb">0.48</td>
<td id="S4.T5.2.8.9" class="ltx_td ltx_align_center ltx_border_bb">0.57</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.4.2" class="ltx_text" style="font-size:90%;">Results for mPLUG-Owl and GPT-4V on five super-categories. (Culture&amp; Rec. = Culture&amp;Recreation.)</span></figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2311.15562/assets/Images/GPV_language_topics_two_metrics.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="343" height="402" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Performance of the two top-performing VQA models, mPLUG-Owl and GPT-4V, for each of 105 topics with their five super-categories represented in 5 different colors. Results are shown with respect to the top two human-aligned evaluation metrics: (a) METEOR and (b) GPT-4 Metric. For visualization simplicity, we show text labels only for topics with interesting identified trends (We omitted "language" for each language topic; e.g., "Esperanto" instead of "Esperanto Language"). </span></figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2311.15562/assets/Images/qualitative_example.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="343" height="594" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Results from the GPT-4V model on three examples in our dataset from three topics, puzzling and two language topics.</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Analysis With Respect to VQA Topics.</h5>

<div id="S4.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p1.1" class="ltx_p">We next analyzed each model’s performance per topic. First, we visualize performance for the two top-performing models with respect to the top two human-aligned evaluation metrics, METEOR and GPT-4 metric, in Figure <a href="#S4.F4" title="Figure 4 ‣ Analysis With Respect to Input Types. ‣ 4.2 Model Benchmarking ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Scores associated with GPT-4V model or GPT-4 metric are reported on the subset and otherwise are on the entire test set. We also report performance with respect to the five super-categories in Table <a href="#S4.T5" title="Table 5 ‣ Analysis With Respect to Input Types. ‣ 4.2 Model Benchmarking ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S4.SS2.SSS0.Px4.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p2.1" class="ltx_p">One common outlier for the per-topic visualization (i.e., Figure <a href="#S4.F4" title="Figure 4 ‣ Analysis With Respect to Input Types. ‣ 4.2 Model Benchmarking ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) is consistently very low performance for the <em id="S4.SS2.SSS0.Px4.p2.1.1" class="ltx_emph ltx_font_italic">puzzling</em> topic across both evaluation metrics. We attribute this to such topics requiring jointly strong vision understanding and reasoning skills. An example of the <em id="S4.SS2.SSS0.Px4.p2.1.2" class="ltx_emph ltx_font_italic">puzzling</em> topic with GPT-4V is shown in Figure <a href="#S4.F5" title="Figure 5 ‣ Analysis With Respect to Input Types. ‣ 4.2 Model Benchmarking ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, Example 1.</p>
</div>
<div id="S4.SS2.SSS0.Px4.p3" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p3.1" class="ltx_p">Another trend we observe for the per-topic visualization (i.e., Figure <a href="#S4.F4" title="Figure 4 ‣ Analysis With Respect to Input Types. ‣ 4.2 Model Benchmarking ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) is different performance for topics related to different languages. The models work best on visual questions related to Germanic Languages; e.g., English as exemplified in Figure <a href="#S4.F5" title="Figure 5 ‣ Analysis With Respect to Input Types. ‣ 4.2 Model Benchmarking ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, Example 2. The next best languages are Romance Languages (e.g., Spanish, French, Italian, Portuguese), then East Asian Languages (e.g., Chinese, Korean), and finally Russian. We suspect causes of such consistent performance discrepancies across languages are: (1) Data availability: languages which have fewer datasets and less diversity (e.g., one exemplified in Figure <a href="#S4.F5" title="Figure 5 ‣ Analysis With Respect to Input Types. ‣ 4.2 Model Benchmarking ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, Example 3) perform worse than those with greater diversity and abundance of datasets (e.g., English), (2) Language Complexity: Languages with simpler grammar (e.g., Esperanto) or familiar grammatical structures (e.g., Romance languages for an English-trained model) are easier for models to learn, and (3) Text recognition can be more challenging for languages with unique scripts, such as Chinese or Korean while these language-related VQAs often pertain to text recognition. We believe these findings shed light on future research directions for multilingual VQA and text VQA research.</p>
</div>
<div id="S4.SS2.SSS0.Px4.p4" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p4.1" class="ltx_p">With respect to the five super-categories (i.e., Table <a href="#S4.T5" title="Table 5 ‣ Analysis With Respect to Input Types. ‣ 4.2 Model Benchmarking ‣ 4 Experiments ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), overall across all evaluation metrics, the models perform worst on Culture &amp; Recreation. We suspect this is because such visual questions require a higher reliance on vision understanding, an area where prevailing VLMs fail.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We introduce the first fully authentic end-to-end VQA dataset. Our analysis reveals numerous distinct features of our new dataset, most importantly atypically long answers. As a result, we conducted human-based studies to establish a new VQA evaluation methodology for our dataset and found that well-suited metrics are GPT-4 metric, METEOR, and BERTscore. Subsequent benchmarking of six modern VQA models in zero-shot settings reveals they all perform poorly. Our fine-grained analysis across the 105 topics covered in the dataset reveals some topics are easier (e.g., English-based and Computer Science-based topics) while other topics are very challenging (e.g., puzzling). To facilitate extensions of this work, we publicly share our dataset at https://vqaonline.github.io/.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Antol et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib1.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span id="bib.bib1.11.3" class="ltx_text" style="font-size:90%;">, pages 2425–2433, 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Biten et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, and R Manmatha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Latr: Layout-aware transformer for scene-text vqa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, pages 16548–16558, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Bitton-Guetta et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, pages 2616–2627, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Bolotova et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Valeriia Bolotova, Vladislav Blinov, Falk Scholer, W Bruce Croft, and Mark Sanderson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, pages 1196–1207, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Brady et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Erin Brady, Meredith Ringel Morris, Yu Zhong, Samuel White, and Jeffrey P Bigham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Visual challenges in the everyday lives of blind people.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the SIGCHI conference on human factors in computing systems</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, pages 2117–2126, 2013.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Byeon et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Coyo-700m: Image-text pair dataset, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Cambazoglu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
B Barla Cambazoglu, Leila Tavakoli, Falk Scholer, Mark Sanderson, and Bruce Croft.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">An intent taxonomy for questions asked in web search.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2021 Conference on Human Information Interaction and Retrieval</em><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, pages 85–94, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Changpinyo et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, pages 3558–3568, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Long Chen, Dell Zhang, and Levene Mark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Understanding user intent in community question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 21st international conference on world wide web</em><span id="bib.bib9.11.3" class="ltx_text" style="font-size:90%;">, pages 823–828, 2012.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Microsoft coco captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1504.00325</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Can pre-trained vision and language models answer visual information-seeking questions?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.11713</em><span id="bib.bib11.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Dai et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.4.4.1" class="ltx_text" style="font-size:90%;">Elliott and Keller [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text" style="font-size:90%;">
Desmond Elliott and Frank Keller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">Image description using visual dependency representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</em><span id="bib.bib13.10.3" class="ltx_text" style="font-size:90%;">, pages 1292–1302, 2013.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.4.4.1" class="ltx_text" style="font-size:90%;">Exchange [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">
Stack Exchange.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">Gpt-4v pricing, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Accessed: 2023-11-16.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.4.4.1" class="ltx_text" style="font-size:90%;">Fu and Fan [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">
Hengyi Fu and Yun Fan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">Music information seeking via social q&amp;a: An analysis of questions in music stackexchange community.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries</em><span id="bib.bib15.10.3" class="ltx_text" style="font-size:90%;">, pages 139–142, 2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Gilardi et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Chatgpt outperforms crowd-workers for text-annotation tasks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.15056</em><span id="bib.bib16.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Goyal et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.4.4.1" class="ltx_text" style="font-size:90%;">Gurari and Grauman [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">
Danna Gurari and Kristen Grauman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">Crowdverge: Predicting if people will agree on the answer to a visual question.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</em><span id="bib.bib18.10.3" class="ltx_text" style="font-size:90%;">, pages 3511–3522, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Gurari et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Vizwiz grand challenge: Answering visual questions from blind people.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, pages 3608–3617, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Gurari et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Danna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhattacharya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Captioning images taken by people who are blind.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2002.08565</em><span id="bib.bib20.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Harper et al. [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
F Maxwell Harper, Joseph Weinberg, John Logie, and Joseph A Konstan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Question types in social q&amp;a sites.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib21.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">First Monday</em><span id="bib.bib21.10.2" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Bin He, Meng Xia, Xinguo Yu, Pengpeng Jian, Hao Meng, and Zhanwen Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">An educational robot system of visual question answering for preschoolers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib22.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 2nd International Conference on Robotics and Automation Engineering (ICRAE)</em><span id="bib.bib22.11.3" class="ltx_text" style="font-size:90%;">, pages 441–445. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Hessel et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">CLIPScore: a reference-free evaluation metric for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">EMNLP</em><span id="bib.bib23.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Hu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.11154</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.4.4.1" class="ltx_text" style="font-size:90%;">Hudson and Manning [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">
Drew A Hudson and Christopher D Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">Gqa: A new dataset for real-world visual reasoning and compositional question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib25.10.3" class="ltx_text" style="font-size:90%;">, pages 6700–6709, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Ignatova et al. [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Kateryna Ignatova, Cigdem Toprak, Delphine Bernhard, and Iryna Gurevych.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Annotating question types in social q&amp;a sites.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Tagungsband des GSCL Symposiums ‘Sprachtechnologie und eHumanities</em><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, pages 44–49, 2009.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Johnson et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, pages 2901–2910, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Kofler et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Christoph Kofler, Martha Larson, and Alan Hanjalic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">User intent in multimedia search: a survey of the state of the art and future challenges.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Computing Surveys (CSUR)</em><span id="bib.bib28.10.2" class="ltx_text" style="font-size:90%;">, 49(2):1–37, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Krishna et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib29.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International journal of computer vision</em><span id="bib.bib29.10.2" class="ltx_text" style="font-size:90%;">, 123(1):32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Lei et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Tvqa: Localized, compositional video question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, pages 1369–1379, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Otter: A multi-modal model with in-context instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.03726</em><span id="bib.bib31.10.2" class="ltx_text" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.12597</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.4.4.1" class="ltx_text" style="font-size:90%;">Lin [2004]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">
Chin-Yew Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">Rouge: A package for automatic evaluation of summaries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Text summarization branches out</em><span id="bib.bib33.10.3" class="ltx_text" style="font-size:90%;">, pages 74–81, 2004.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Mitigating hallucination in large multi-modal models via robust instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.14565</em><span id="bib.bib34.10.2" class="ltx_text" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Visual instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib35.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.08485</em><span id="bib.bib35.10.2" class="ltx_text" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.4.4.1" class="ltx_text" style="font-size:90%;">Liu [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text" style="font-size:90%;">
Zhe Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">Understanding and modeling user behavior in social question and answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">2015.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Lu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib37.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.02255</em><span id="bib.bib37.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Marino et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Ok-vqa: A visual question answering benchmark requiring external knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Mathew et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Docvqa: A dataset for vqa on document images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em><span id="bib.bib39.11.3" class="ltx_text" style="font-size:90%;">, pages 2200–2209, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Mathew et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and C.V. Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Infographicvqa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em><span id="bib.bib40.11.3" class="ltx_text" style="font-size:90%;">, pages 1697–1706, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Mishra et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Ocr-vqa: Visual question answering by reading text in images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 international conference on document analysis and recognition (ICDAR)</em><span id="bib.bib41.11.3" class="ltx_text" style="font-size:90%;">, pages 947–952. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Naik et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Nandita Naik, Christopher Potts, and Elisa Kreiss.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Context-vqa: Towards context-aware and purposeful visual question answering.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib42.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2307.15745</em><span id="bib.bib42.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Nie et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Liqiang Nie, Meng Wang, Zhengjun Zha, Guangda Li, and Tat-Seng Chua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Multimedia answering: enriching text qa with media information.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, pages 695–704, 2011.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.4.4.1" class="ltx_text" style="font-size:90%;">OpenAI [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.6.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">Gpt-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib44.9.2" class="ltx_text" style="font-size:90%;">, abs/2303.08774, 2023.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Ordonez et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Im2text: Describing images using 1 million captioned photographs.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib45.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib45.10.2" class="ltx_text" style="font-size:90%;">, 24, 2011.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Saikh et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Scienceqa: a novel resource for question answering on scholarly articles.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib46.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal on Digital Libraries</em><span id="bib.bib46.10.2" class="ltx_text" style="font-size:90%;">, 23(3):289–301, 2022.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Schuhmann et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.02114</em><span id="bib.bib47.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Sharma et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib48.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em><span id="bib.bib48.11.3" class="ltx_text" style="font-size:90%;">, pages 2556–2565, 2018.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Singh et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Towards vqa models that can read.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib49.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib49.11.3" class="ltx_text" style="font-size:90%;">, pages 8317–8326, 2019.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.4.4.1" class="ltx_text" style="font-size:90%;">Srba and Bielikova [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.6.1" class="ltx_text" style="font-size:90%;">
Ivan Srba and Maria Bielikova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">A comprehensive survey and classification of approaches for community question answering.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib50.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Transactions on the Web (TWEB)</em><span id="bib.bib50.9.2" class="ltx_text" style="font-size:90%;">, 10(3):1–63, 2016.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Toba et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Hapnes Toba, Zhao-Yan Ming, Mirna Adriani, and Tat-Seng Chua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">Discovering high quality answers in community question answering archives using a hierarchy of classifiers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Information Sciences</em><span id="bib.bib51.10.2" class="ltx_text" style="font-size:90%;">, 261:101–115, 2014.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Visual question answering: A survey of methods and datasets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib52.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</em><span id="bib.bib52.10.2" class="ltx_text" style="font-size:90%;">, 163:21–40, 2017.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Xu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib53.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.09265</em><span id="bib.bib53.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">The dawn of lmms: Preliminary explorations with gpt-4v (ision).
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib54.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.17421</em><span id="bib.bib54.10.2" class="ltx_text" style="font-size:90%;">, 9, 2023.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Ye et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">mplug-owl: Modularization empowers large language models with multimodality, 2023.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">Mm-vet: Evaluating large multimodal models for integrated capabilities.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib56.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.02490</em><span id="bib.bib56.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Zeng et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Xiaoyu Zeng, Yanan Wang, Tai-Yin Chiu, Nilavra Bhattacharya, and Danna Gurari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">Vision skills needed to answer visual questions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib57.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the ACM on Human-Computer Interaction</em><span id="bib.bib57.10.2" class="ltx_text" style="font-size:90%;">, 4(CSCW2):1–31, 2020.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Bertscore: Evaluating text generation with bert.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib58.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.09675</em><span id="bib.bib58.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">Minigpt-4: Enhancing vision-language understanding with advanced large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib59.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.10592</em><span id="bib.bib59.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Supplementary Material</h2>

<figure id="S6.T6" class="ltx_table">
<div id="S6.T6.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:631.2pt;height:198pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S6.T6.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T6.2.1.1" class="ltx_tr">
<td id="S6.T6.2.1.1.1" class="ltx_td ltx_align_left ltx_border_t">VQA Dataset</td>
<td id="S6.T6.2.1.1.2" class="ltx_td ltx_align_left ltx_border_t">Which Images?</td>
<td id="S6.T6.2.1.1.3" class="ltx_td ltx_align_left ltx_border_t">Who Asked?</td>
<td id="S6.T6.2.1.1.4" class="ltx_td ltx_align_left ltx_border_t">From User?</td>
</tr>
<tr id="S6.T6.2.1.2" class="ltx_tr">
<td id="S6.T6.2.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Our dataset</td>
<td id="S6.T6.2.1.2.2" class="ltx_td ltx_align_left ltx_border_t">StackExchange Users</td>
<td id="S6.T6.2.1.2.3" class="ltx_td ltx_align_left ltx_border_t">StackExchange Users</td>
<td id="S6.T6.2.1.2.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S6.T6.2.1.2.4.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
</tr>
<tr id="S6.T6.2.1.3" class="ltx_tr">
<td id="S6.T6.2.1.3.1" class="ltx_td ltx_align_left">Context-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>
</td>
<td id="S6.T6.2.1.3.2" class="ltx_td ltx_align_left">Six types of websites</td>
<td id="S6.T6.2.1.3.3" class="ltx_td ltx_align_left">Annotators</td>
<td id="S6.T6.2.1.3.4" class="ltx_td ltx_align_left"><span id="S6.T6.2.1.3.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S6.T6.2.1.4" class="ltx_tr">
<td id="S6.T6.2.1.4.1" class="ltx_td ltx_align_left">VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>
</td>
<td id="S6.T6.2.1.4.2" class="ltx_td ltx_align_left">MSCOCO</td>
<td id="S6.T6.2.1.4.3" class="ltx_td ltx_align_left">Crowd workers (AMT)</td>
<td id="S6.T6.2.1.4.4" class="ltx_td ltx_align_left"><span id="S6.T6.2.1.4.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S6.T6.2.1.5" class="ltx_tr">
<td id="S6.T6.2.1.5.1" class="ltx_td ltx_align_left">VizWiz-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>
</td>
<td id="S6.T6.2.1.5.2" class="ltx_td ltx_align_left">Captured by Blind people</td>
<td id="S6.T6.2.1.5.3" class="ltx_td ltx_align_left">Blind people</td>
<td id="S6.T6.2.1.5.4" class="ltx_td ltx_align_left"><span id="S6.T6.2.1.5.4.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
</tr>
<tr id="S6.T6.2.1.6" class="ltx_tr">
<td id="S6.T6.2.1.6.1" class="ltx_td ltx_align_left">OKVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>
</td>
<td id="S6.T6.2.1.6.2" class="ltx_td ltx_align_left">MSCOCO</td>
<td id="S6.T6.2.1.6.3" class="ltx_td ltx_align_left">Crowd workers (AMT)</td>
<td id="S6.T6.2.1.6.4" class="ltx_td ltx_align_left"><span id="S6.T6.2.1.6.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S6.T6.2.1.7" class="ltx_tr">
<td id="S6.T6.2.1.7.1" class="ltx_td ltx_align_left">DocVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>
</td>
<td id="S6.T6.2.1.7.2" class="ltx_td ltx_align_left">UCSF Industry Documents Library</td>
<td id="S6.T6.2.1.7.3" class="ltx_td ltx_align_left">Remote workers</td>
<td id="S6.T6.2.1.7.4" class="ltx_td ltx_align_left"><span id="S6.T6.2.1.7.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S6.T6.2.1.8" class="ltx_tr">
<td id="S6.T6.2.1.8.1" class="ltx_td ltx_align_left">ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>
</td>
<td id="S6.T6.2.1.8.2" class="ltx_td ltx_align_left">Online K-12 learning platform</td>
<td id="S6.T6.2.1.8.3" class="ltx_td ltx_align_left">Online K-12 learning platform</td>
<td id="S6.T6.2.1.8.4" class="ltx_td ltx_align_left"><span id="S6.T6.2.1.8.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S6.T6.2.1.9" class="ltx_tr">
<td id="S6.T6.2.1.9.1" class="ltx_td ltx_align_left">InfographicVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>
</td>
<td id="S6.T6.2.1.9.2" class="ltx_td ltx_align_left">Bing and Google Image Search</td>
<td id="S6.T6.2.1.9.3" class="ltx_td ltx_align_left">Annotators</td>
<td id="S6.T6.2.1.9.4" class="ltx_td ltx_align_left"><span id="S6.T6.2.1.9.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S6.T6.2.1.10" class="ltx_tr">
<td id="S6.T6.2.1.10.1" class="ltx_td ltx_align_left">INFOSEEK-Wikidata <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>
</td>
<td id="S6.T6.2.1.10.2" class="ltx_td ltx_align_left">9 datasets following OVEN</td>
<td id="S6.T6.2.1.10.3" class="ltx_td ltx_align_left">Template-based auto-generation</td>
<td id="S6.T6.2.1.10.4" class="ltx_td ltx_align_left"><span id="S6.T6.2.1.10.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S6.T6.2.1.11" class="ltx_tr">
<td id="S6.T6.2.1.11.1" class="ltx_td ltx_align_left ltx_border_b">INFOSEEK-Human <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>
</td>
<td id="S6.T6.2.1.11.2" class="ltx_td ltx_align_left ltx_border_b">9 datasets following OVEN</td>
<td id="S6.T6.2.1.11.3" class="ltx_td ltx_align_left ltx_border_b">Annotators</td>
<td id="S6.T6.2.1.11.4" class="ltx_td ltx_align_left ltx_border_b"><span id="S6.T6.2.1.11.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S6.T6.4.2" class="ltx_text" style="font-size:90%;">Comparison of visual questions from eight existing VQA datasets and our new VQAonline dataset regarding image and question sources. Like OVEN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, INFOSEEK sources images from nine image classification and retrieval datasets. </span></figcaption>
</figure>
<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This document supplements the main paper with more information about:</p>
<ol id="S6.I1" class="ltx_enumerate">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">Dataset Collection (Supplements Section 3.1)</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">Dataset Analysis (Supplements Section 3.2)</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">Algorithm Benchmarking (Supplements Section 4.2)</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S1a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span>Dataset Collection</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">I.1 </span>Dataset Source and Filtration</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">The Stack Exchange data is hosted at <span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_italic">https://archive.org/details/stackexchange</span> in XML format.
We started with 331,000 candidate visual questions. After removing visual questions without an accepted answer, it resulted in 165,766 visual questions. As mentioned in the main paper, we then conducted two filtering steps. After removing visual questions with scores of 0 or less for either the question or answer, we had 119,177 visual questions. Next, after removing visual questions with multiple images, we had 85,573 visual questions. Subsequently, removing visual questions with visual answers, left 65,849 visual questions. Examples of filtered visual questions with visual answers and multiple images are shown in Figures <a href="#S1.F6" title="Figure 6 ‣ I.1 Dataset Source and Filtration ‣ I Dataset Collection ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S1.F7" title="Figure 7 ‣ I.1 Dataset Source and Filtration ‣ I Dataset Collection ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> respectively. Finally, after removing examples for which the image could not be downloaded from the provided link, we got to our final dataset of 64,696 visual questions. For data format consistency, we converted all images to png format.</p>
</div>
<figure id="S1.F6" class="ltx_figure"><img src="/html/2311.15562/assets/Images/visualanswer.png" id="S1.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="329" height="382" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S1.F6.3.2" class="ltx_text" style="font-size:90%;">An example of a visual question with a "visual answer".</span></figcaption>
</figure>
<figure id="S1.F7" class="ltx_figure"><img src="/html/2311.15562/assets/Images/multiple_images.png" id="S1.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="329" height="365" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S1.F7.3.2" class="ltx_text" style="font-size:90%;">An example of a visual question with multiple images. The answer is omitted to save space. </span></figcaption>
</figure>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">I.2 </span>User Intention Taxonomy and Definitions. </h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p"><span id="S1.SS2.p1.1.1" class="ltx_text ltx_font_bold">User Intention Taxonomy.</span> We first brainstormed 11 free-form user intention categories. To do so, we solicited help from GPT-3.5 by having it indicate via a zero-shot prompt the user intentions when given a question and context. We identified an initial 11 free-form user intention categories, which are shown in the second column of Table <a href="#S1.T7" title="Table 7 ‣ Hiring Crowdworkers. ‣ I.3 User Intention Annotations. ‣ I Dataset Collection ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We then refined these categories based on the frequency of their occurrence in the literature on user intention (shown in the third to the last column in Table <a href="#S1.T7" title="Table 7 ‣ Hiring Crowdworkers. ‣ I.3 User Intention Annotations. ‣ I Dataset Collection ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) for related fields such as question answering and search queries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> to the following 7 categories: advice, evidence, identification, instruction, opinion, reason, and verification. We then drafted the definitions for each taxonomy, mainly by adapting definitions from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">We then finalized the taxonomy with definitions via four rounds of annotator analysis on a total of 105 visual questions. Specifically, two annotators (authors) tagged the category for each visual question. Any disagreements that arose were discussed and resolved, and the definitions associated with each taxonomy category were adjusted accordingly. We observed slight agreement accuracy <span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>The agreement accuracy is calculated by dividing the number of agreements between two annotators by the total number of examples annotated.</span></span></span> improvement and Cohen’s kappa agreement improvement after each round. Specifically, from round 1 to round 4, agreement accuracy improved from 53.3% to 87.5% and Cohen’s kappa agreement improved from 45.24% to 84.69%. This process culminated in our final taxonomy and their definition, which includes the following eight categories mentioned in the main paper: <em id="S1.SS2.p2.1.1" class="ltx_emph ltx_font_italic">advice, evidence, identification, instruction, opinion, reason, verification</em>, and <em id="S1.SS2.p2.1.2" class="ltx_emph ltx_font_italic">other</em>.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">As shown in Table <a href="#S1.T7" title="Table 7 ‣ Hiring Crowdworkers. ‣ I.3 User Intention Annotations. ‣ I Dataset Collection ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, our final taxonomy is different from  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> as they focus solely on text-based question or search queries whereas our work centers on visual questions. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> also explores user intentions for authentic visual questions for blind individuals.
However, blind people’s user intents of visual questions significantly differ from that of online users’ visual questions. In fact, only one of the user intents identified for blind individuals (i.e., identification) overlaps with the user intents we have identified for our dataset.</p>
</div>
<section id="S1.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Final user intention taxonomy and definitions. </h5>

<div id="S1.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS2.SSS0.Px1.p1.1" class="ltx_p">We show the final user intention taxonomy and definitions as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Verification</span>: These fact-checking questions aim to confirm or refute a hypothesis through affirmative answers (yes/no or multiple choice options). Since this question is based on fact, it cannot include subjective questions which should be classified as Opinion.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Identification</span>: The expected answer for these questions is a named entity or object identification. The answer is objective.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Reason</span>: These questions require answers that explain causes underlying a particular action or event. Most "why" questions fall into this category.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Evidence-based</span>: This category primarily includes questions that ask for the definition, features, description, or process of a concept, idea, object, event, or one of its attributes. It also covers translation questions of seeking the meaning of a sentence or word. If a question can be classified as Verification, it should not be classified as Evidence-based.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p"><span id="S1.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Instruction</span>: These questions typically involve "how to do" inquiries, seeking instructions, guidelines, or procedures for a specific action in real life.</p>
</div>
</li>
<li id="S1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i6.p1" class="ltx_para">
<p id="S1.I1.i6.p1.1" class="ltx_p"><span id="S1.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Advice</span>: This category includes questions where the user seeks personalized guidance on a specific topic. Advice questions differ from Instruction questions in that they expect subjective recommendations, while Instruction questions seek objective, step-by-step processes for performing a specific action. Advice questions may also involve finding resources or seeking better translations of sentences/words. Additionally, this category can include questions where users are looking for ideas or comments on how to improve an existing solution. If a question can be classified as Instruction, it should not be categorized as Advice.</p>
</div>
</li>
<li id="S1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i7.p1" class="ltx_para">
<p id="S1.I1.i7.p1.1" class="ltx_p"><span id="S1.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Opinion</span>: These questions aim to elicit subjective opinions on a topic of interest (e.g., "what do you think about" or "is X good/bad"). It might include religious questions. This category excludes Advice questions, where the focus is on the user asking the question.</p>
</div>
</li>
<li id="S1.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i8.p1" class="ltx_para">
<p id="S1.I1.i8.p1.1" class="ltx_p"><span id="S1.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">Other</span>: Other.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">I.3 </span>User Intention Annotations. </h3>

<section id="S1.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Hiring Crowdworkers. </h5>

<div id="S1.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS3.SSS0.Px1.p1.1" class="ltx_p">We hired three crowdworkers from Amazon Mechanical Turk to perform our task. For quality control, we only accepted workers located in the United States who had completed more than 500 Human Intelligence Tasks (HITs) with over a 95% approval rating.</p>
</div>
<figure id="S1.T7" class="ltx_table">
<div id="S1.T7.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:1026.8pt;height:277.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-220.0pt,59.4pt) scale(0.7,0.7) ;">
<table id="S1.T7.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T7.2.1.1" class="ltx_tr">
<td id="S1.T7.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Ours</td>
<td id="S1.T7.2.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">free-form intent</td>
<td id="S1.T7.2.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">Ignatova (2009) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>
</td>
<td id="S1.T7.2.1.1.4" class="ltx_td ltx_align_left ltx_border_tt">Harper (2010)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>
</td>
<td id="S1.T7.2.1.1.5" class="ltx_td ltx_align_left ltx_border_tt">Toba (2014) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</td>
<td id="S1.T7.2.1.1.6" class="ltx_td ltx_align_left ltx_border_tt">Cambazoglu(2021) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>
</td>
<td id="S1.T7.2.1.1.7" class="ltx_td ltx_align_left ltx_border_tt">Bolotova (2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>
</td>
<td id="S1.T7.2.1.1.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt">Fu (2016) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>
</td>
</tr>
<tr id="S1.T7.2.1.2" class="ltx_tr">
<td id="S1.T7.2.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Instruction</td>
<td id="S1.T7.2.1.2.2" class="ltx_td ltx_align_left ltx_border_t">How</td>
<td id="S1.T7.2.1.2.3" class="ltx_td ltx_align_left ltx_border_t">Procedural</td>
<td id="S1.T7.2.1.2.4" class="ltx_td ltx_align_left ltx_border_t">Prescriptive</td>
<td id="S1.T7.2.1.2.5" class="ltx_td ltx_align_left ltx_border_t">Proceure</td>
<td id="S1.T7.2.1.2.6" class="ltx_td ltx_align_left ltx_border_t">Process</td>
<td id="S1.T7.2.1.2.7" class="ltx_td ltx_align_left ltx_border_t">Instruction</td>
<td id="S1.T7.2.1.2.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">Instruction</td>
</tr>
<tr id="S1.T7.2.1.3" class="ltx_tr">
<td id="S1.T7.2.1.3.1" class="ltx_td ltx_align_left">Evidence-based</td>
<td id="S1.T7.2.1.3.2" class="ltx_td ltx_align_left">Comprehend fact</td>
<td id="S1.T7.2.1.3.3" class="ltx_td ltx_align_left">General Info need</td>
<td id="S1.T7.2.1.3.4" class="ltx_td ltx_align_left">Factual</td>
<td id="S1.T7.2.1.3.5" class="ltx_td ltx_align_left">Factoid</td>
<td id="S1.T7.2.1.3.6" class="ltx_td ltx_align_left">Description</td>
<td id="S1.T7.2.1.3.7" class="ltx_td ltx_align_left">Evidence-based</td>
<td id="S1.T7.2.1.3.8" class="ltx_td ltx_nopad_r ltx_align_left">Factual</td>
</tr>
<tr id="S1.T7.2.1.4" class="ltx_tr">
<td id="S1.T7.2.1.4.1" class="ltx_td ltx_align_left">Reason</td>
<td id="S1.T7.2.1.4.2" class="ltx_td ltx_align_left">Why</td>
<td id="S1.T7.2.1.4.3" class="ltx_td ltx_align_left">Causal</td>
<td id="S1.T7.2.1.4.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.4.5" class="ltx_td ltx_align_left">Reason</td>
<td id="S1.T7.2.1.4.6" class="ltx_td ltx_align_left">Reason</td>
<td id="S1.T7.2.1.4.7" class="ltx_td ltx_align_left">Reason</td>
<td id="S1.T7.2.1.4.8" class="ltx_td ltx_nopad_r ltx_align_left">-</td>
</tr>
<tr id="S1.T7.2.1.5" class="ltx_tr">
<td id="S1.T7.2.1.5.1" class="ltx_td ltx_align_left">Verification</td>
<td id="S1.T7.2.1.5.2" class="ltx_td ltx_align_left">Validate</td>
<td id="S1.T7.2.1.5.3" class="ltx_td ltx_align_left">Verification</td>
<td id="S1.T7.2.1.5.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.5.5" class="ltx_td ltx_align_left">Yes/No</td>
<td id="S1.T7.2.1.5.6" class="ltx_td ltx_align_left">Verification</td>
<td id="S1.T7.2.1.5.7" class="ltx_td ltx_align_left">Debate</td>
<td id="S1.T7.2.1.5.8" class="ltx_td ltx_nopad_r ltx_align_left">-</td>
</tr>
<tr id="S1.T7.2.1.6" class="ltx_tr">
<td id="S1.T7.2.1.6.1" class="ltx_td ltx_align_left">Opinion</td>
<td id="S1.T7.2.1.6.2" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.6.3" class="ltx_td ltx_align_left">Disjunctive</td>
<td id="S1.T7.2.1.6.4" class="ltx_td ltx_align_left">Disapproval/Quality</td>
<td id="S1.T7.2.1.6.5" class="ltx_td ltx_align_left">Opinion</td>
<td id="S1.T7.2.1.6.6" class="ltx_td ltx_align_left">Opinion</td>
<td id="S1.T7.2.1.6.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.6.8" class="ltx_td ltx_nopad_r ltx_align_left">Opinion</td>
</tr>
<tr id="S1.T7.2.1.7" class="ltx_tr">
<td id="S1.T7.2.1.7.1" class="ltx_td ltx_align_left">Identification</td>
<td id="S1.T7.2.1.7.2" class="ltx_td ltx_align_left">Recognize</td>
<td id="S1.T7.2.1.7.3" class="ltx_td ltx_align_left">Concept completion</td>
<td id="S1.T7.2.1.7.4" class="ltx_td ltx_align_left">Identification</td>
<td id="S1.T7.2.1.7.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.7.6" class="ltx_td ltx_align_left">Entity</td>
<td id="S1.T7.2.1.7.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.7.8" class="ltx_td ltx_nopad_r ltx_align_left">Identifying resources</td>
</tr>
<tr id="S1.T7.2.1.8" class="ltx_tr">
<td id="S1.T7.2.1.8.1" class="ltx_td ltx_align_left">Advice</td>
<td id="S1.T7.2.1.8.2" class="ltx_td ltx_align_left">Advise</td>
<td id="S1.T7.2.1.8.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.8.4" class="ltx_td ltx_align_left">Advice</td>
<td id="S1.T7.2.1.8.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.8.6" class="ltx_td ltx_align_left">Advice</td>
<td id="S1.T7.2.1.8.7" class="ltx_td ltx_align_left">Experience</td>
<td id="S1.T7.2.1.8.8" class="ltx_td ltx_nopad_r ltx_align_left">Recommend/Solution</td>
</tr>
<tr id="S1.T7.2.1.9" class="ltx_tr">
<td id="S1.T7.2.1.9.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.9.2" class="ltx_td ltx_align_left">Prove</td>
<td id="S1.T7.2.1.9.3" class="ltx_td ltx_align_left">Quantification</td>
<td id="S1.T7.2.1.9.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.9.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.9.6" class="ltx_td ltx_align_left">Quantity</td>
<td id="S1.T7.2.1.9.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.9.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.10" class="ltx_tr">
<td id="S1.T7.2.1.10.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.10.2" class="ltx_td ltx_align_left">Compare</td>
<td id="S1.T7.2.1.10.3" class="ltx_td ltx_align_left">Comparison</td>
<td id="S1.T7.2.1.10.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.10.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.10.6" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.10.7" class="ltx_td ltx_align_left">Comparison</td>
<td id="S1.T7.2.1.10.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.11" class="ltx_tr">
<td id="S1.T7.2.1.11.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.11.2" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.11.3" class="ltx_td ltx_align_left">Definition</td>
<td id="S1.T7.2.1.11.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.11.5" class="ltx_td ltx_align_left">Definition</td>
<td id="S1.T7.2.1.11.6" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.11.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.11.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.12" class="ltx_tr">
<td id="S1.T7.2.1.12.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.12.2" class="ltx_td ltx_align_left">Find</td>
<td id="S1.T7.2.1.12.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.12.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.12.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.12.6" class="ltx_td ltx_align_left">Resource</td>
<td id="S1.T7.2.1.12.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.12.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.13" class="ltx_tr">
<td id="S1.T7.2.1.13.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.13.2" class="ltx_td ltx_align_left">Language</td>
<td id="S1.T7.2.1.13.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.13.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.13.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.13.6" class="ltx_td ltx_align_left">Language</td>
<td id="S1.T7.2.1.13.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.13.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.14" class="ltx_tr">
<td id="S1.T7.2.1.14.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.14.2" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.14.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.14.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.14.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.14.6" class="ltx_td ltx_align_left">Temporal</td>
<td id="S1.T7.2.1.14.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.14.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.15" class="ltx_tr">
<td id="S1.T7.2.1.15.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.15.2" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.15.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.15.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.15.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.15.6" class="ltx_td ltx_align_left">Calculation</td>
<td id="S1.T7.2.1.15.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.15.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.16" class="ltx_tr">
<td id="S1.T7.2.1.16.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.16.2" class="ltx_td"></td>
<td id="S1.T7.2.1.16.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.16.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.16.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.16.6" class="ltx_td ltx_align_left">Attribute</td>
<td id="S1.T7.2.1.16.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.16.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.17" class="ltx_tr">
<td id="S1.T7.2.1.17.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.17.2" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.17.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.17.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.17.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.17.6" class="ltx_td ltx_align_left">List</td>
<td id="S1.T7.2.1.17.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.17.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.18" class="ltx_tr">
<td id="S1.T7.2.1.18.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.18.2" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.18.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.18.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.18.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.18.6" class="ltx_td ltx_align_left">Weather</td>
<td id="S1.T7.2.1.18.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.18.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.19" class="ltx_tr">
<td id="S1.T7.2.1.19.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.19.2" class="ltx_td"></td>
<td id="S1.T7.2.1.19.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.19.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.19.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.19.6" class="ltx_td ltx_align_left">Location</td>
<td id="S1.T7.2.1.19.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.19.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.20" class="ltx_tr">
<td id="S1.T7.2.1.20.1" class="ltx_td"></td>
<td id="S1.T7.2.1.20.2" class="ltx_td ltx_align_left">Explain</td>
<td id="S1.T7.2.1.20.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.20.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.20.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.20.6" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.20.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.20.8" class="ltx_td"></td>
</tr>
<tr id="S1.T7.2.1.21" class="ltx_tr">
<td id="S1.T7.2.1.21.1" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.21.2" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.21.3" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.21.4" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.21.5" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.21.6" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.21.7" class="ltx_td ltx_align_left">-</td>
<td id="S1.T7.2.1.21.8" class="ltx_td ltx_nopad_r ltx_align_left">Request research</td>
</tr>
<tr id="S1.T7.2.1.22" class="ltx_tr">
<td id="S1.T7.2.1.22.1" class="ltx_td ltx_align_left ltx_border_bb">Other</td>
<td id="S1.T7.2.1.22.2" class="ltx_td ltx_align_left ltx_border_bb">-</td>
<td id="S1.T7.2.1.22.3" class="ltx_td ltx_align_left ltx_border_bb">-</td>
<td id="S1.T7.2.1.22.4" class="ltx_td ltx_align_left ltx_border_bb">-</td>
<td id="S1.T7.2.1.22.5" class="ltx_td ltx_align_left ltx_border_bb">-</td>
<td id="S1.T7.2.1.22.6" class="ltx_td ltx_align_left ltx_border_bb">-</td>
<td id="S1.T7.2.1.22.7" class="ltx_td ltx_align_left ltx_border_bb">-</td>
<td id="S1.T7.2.1.22.8" class="ltx_td ltx_border_bb"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T7.3.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S1.T7.4.2" class="ltx_text" style="font-size:90%;">User intent taxonomy for our dataset, first round free-form intent (Bootstrapping from GPT-3.5 and Manual Labeling) and existing research. Except that ours is user intention taxonomy for visual question answering and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> for Bing query search, others  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> are for community question answering (text-only). </span></figcaption>
</figure>
</section>
<section id="S1.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Annotation Task Design and Collection. </h5>

<div id="S1.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S1.SS3.SSS0.Px2.p1.1" class="ltx_p">We provided instructions, taxonomy definitions, and two examples per user intention category. The annotation task interface showed for each visual question the question, context, and answer.</p>
</div>
<div id="S1.SS3.SSS0.Px2.p2" class="ltx_para">
<p id="S1.SS3.SSS0.Px2.p2.1" class="ltx_p">To facilitate collecting high-quality results, we provided each crowdworker with one-on-one Zoom training. We also provided a qualifying annotation test that all workers passed to verify they understood the instructions.</p>
</div>
<div id="S1.SS3.SSS0.Px2.p3" class="ltx_para">
<p id="S1.SS3.SSS0.Px2.p3.1" class="ltx_p">To enable the assessment of high-quality results, we collected two user intention annotations from two independent workers (worker A and B) per VQA instance (i.e. image-question-context-answer) for 105 randomly selected VQAs, one from each of 105 topics. We then detected if their annotations matched. We found that 25 out of 105 didn’t match. Thus, we hired a third worker (worker C) to break the tie by instructing that individual to select one of the two provided user intentions. We paid $40 Amazon Gift Card to workers A and B and $5 Amazon Gift Card to worker C.</p>
</div>
</section>
<section id="S1.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Instructions for User Intention Annotation.</h5>

<div id="S1.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S1.SS3.SSS0.Px3.p1.1" class="ltx_p">The following are the instructions provided for workers A and B:</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">Read definitions and two examples for each category.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">We will present to you 105 questions. Each question contains a question, context, and a reference answer. Please read them and categorize the primary user intention according to step 1.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">If there are any non-English questions, or if the questions contain many proper nouns that are difficult to interpret, you can use a translation tool to translate them into your native language.</p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i4.p1" class="ltx_para">
<p id="S1.I2.i4.p1.1" class="ltx_p">Fill in the user intent category for each question in the provided spreadsheet.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section id="S2a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span>Dataset Analysis</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II.1 </span>Comparison of VQAonline and Other Datasets</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We supplement the main paper by comparing the sources of the visual questions for eight datasets in Table <a href="#S6.T6" title="Table 6 ‣ 6 Supplementary Material ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Only our dataset and VizWiz-VQA dataset are sourced from authentic use cases with both the images and visual questions coming from real users.</p>
</div>
<figure id="S2.T8" class="ltx_table">
<table id="S2.T8.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T8.3.4" class="ltx_tr">
<td id="S2.T8.3.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S2.T8.3.4.1.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="S2.T8.3.4.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S2.T8.3.4.2.1" class="ltx_text" style="font-size:90%;">Model Configuration</span></td>
<td id="S2.T8.3.4.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_tt" colspan="2"><span id="S2.T8.3.4.3.1" class="ltx_text" style="font-size:90%;">Img-text Data</span></td>
<td id="S2.T8.3.4.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_tt" colspan="2"><span id="S2.T8.3.4.4.1" class="ltx_text" style="font-size:90%;">Visual Instruction Data</span></td>
</tr>
<tr id="S2.T8.3.5" class="ltx_tr">
<td id="S2.T8.3.5.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T8.3.5.1.1" class="ltx_text" style="font-size:90%;">Models</span></td>
<td id="S2.T8.3.5.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T8.3.5.2.1" class="ltx_text" style="font-size:90%;">Img Encoder</span></td>
<td id="S2.T8.3.5.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T8.3.5.3.1" class="ltx_text" style="font-size:90%;">Lang Encoder</span></td>
<td id="S2.T8.3.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T8.3.5.4.1" class="ltx_text" style="font-size:90%;">Adapter</span></td>
<td id="S2.T8.3.5.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T8.3.5.5.1" class="ltx_text" style="font-size:90%;">Source</span></td>
<td id="S2.T8.3.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T8.3.5.6.1" class="ltx_text" style="font-size:90%;">Size</span></td>
<td id="S2.T8.3.5.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T8.3.5.7.1" class="ltx_text" style="font-size:90%;">Source</span></td>
<td id="S2.T8.3.5.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T8.3.5.8.1" class="ltx_text" style="font-size:90%;">Size</span></td>
</tr>
<tr id="S2.T8.1.1" class="ltx_tr">
<td id="S2.T8.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S2.T8.1.1.2.1" class="ltx_text" style="font-size:90%;">BLIP2  </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T8.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a><span id="S2.T8.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T8.1.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T8.1.1.3.1" class="ltx_text" style="font-size:90%;">ViT-g /14</span></td>
<td id="S2.T8.1.1.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T8.1.1.4.1" class="ltx_text" style="font-size:90%;">FlanT5XL</span></td>
<td id="S2.T8.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T8.1.1.5.1" class="ltx_text" style="font-size:90%;">Q-Former</span></td>
<td id="S2.T8.1.1.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S2.T8.1.1.1.1" class="ltx_text" style="font-size:90%;">CC</span><sup id="S2.T8.1.1.1.2" class="ltx_sup"><span id="S2.T8.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup><span id="S2.T8.1.1.1.3" class="ltx_text" style="font-size:90%;">, VG, SBU, L400</span>
</td>
<td id="S2.T8.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T8.1.1.6.1" class="ltx_text" style="font-size:90%;">129M</span></td>
<td id="S2.T8.1.1.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T8.1.1.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T8.1.1.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T8.1.1.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S2.T8.3.6" class="ltx_tr">
<td id="S2.T8.3.6.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S2.T8.3.6.1.1" class="ltx_text" style="font-size:90%;">MiniGPT-4 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T8.3.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a><span id="S2.T8.3.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T8.3.6.2" class="ltx_td ltx_align_left"><span id="S2.T8.3.6.2.1" class="ltx_text" style="font-size:90%;">BLIP2-VE</span></td>
<td id="S2.T8.3.6.3" class="ltx_td ltx_align_left"><span id="S2.T8.3.6.3.1" class="ltx_text" style="font-size:90%;">Vicuna 13B</span></td>
<td id="S2.T8.3.6.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T8.3.6.4.1" class="ltx_text" style="font-size:90%;">FC layer</span></td>
<td id="S2.T8.3.6.5" class="ltx_td ltx_align_left"><span id="S2.T8.3.6.5.1" class="ltx_text" style="font-size:90%;">CC, SBU, L400</span></td>
<td id="S2.T8.3.6.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T8.3.6.6.1" class="ltx_text" style="font-size:90%;">5M</span></td>
<td id="S2.T8.3.6.7" class="ltx_td ltx_align_left"><span id="S2.T8.3.6.7.1" class="ltx_text" style="font-size:90%;">CC+ChatGPT</span></td>
<td id="S2.T8.3.6.8" class="ltx_td ltx_align_left"><span id="S2.T8.3.6.8.1" class="ltx_text" style="font-size:90%;">3.5K</span></td>
</tr>
<tr id="S2.T8.3.7" class="ltx_tr">
<td id="S2.T8.3.7.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S2.T8.3.7.1.1" class="ltx_text" style="font-size:90%;">LLaVA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T8.3.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a><span id="S2.T8.3.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T8.3.7.2" class="ltx_td ltx_align_left"><span id="S2.T8.3.7.2.1" class="ltx_text" style="font-size:90%;">ViT-L/14</span></td>
<td id="S2.T8.3.7.3" class="ltx_td ltx_align_left"><span id="S2.T8.3.7.3.1" class="ltx_text" style="font-size:90%;">LLaMA 13B</span></td>
<td id="S2.T8.3.7.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T8.3.7.4.1" class="ltx_text" style="font-size:90%;">FC layer</span></td>
<td id="S2.T8.3.7.5" class="ltx_td ltx_align_left"><span id="S2.T8.3.7.5.1" class="ltx_text" style="font-size:90%;">CC3M</span></td>
<td id="S2.T8.3.7.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T8.3.7.6.1" class="ltx_text" style="font-size:90%;">595K</span></td>
<td id="S2.T8.3.7.7" class="ltx_td ltx_align_left"><span id="S2.T8.3.7.7.1" class="ltx_text" style="font-size:90%;">LLaVA-I</span></td>
<td id="S2.T8.3.7.8" class="ltx_td ltx_align_left"><span id="S2.T8.3.7.8.1" class="ltx_text" style="font-size:90%;">158K</span></td>
</tr>
<tr id="S2.T8.2.2" class="ltx_tr">
<td id="S2.T8.2.2.2" class="ltx_td ltx_align_left ltx_border_r">
<span id="S2.T8.2.2.2.1" class="ltx_text" style="font-size:90%;">mPLUG-Owl </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T8.2.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a><span id="S2.T8.2.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T8.2.2.3" class="ltx_td ltx_align_left"><span id="S2.T8.2.2.3.1" class="ltx_text" style="font-size:90%;">ViT-L/14</span></td>
<td id="S2.T8.2.2.4" class="ltx_td ltx_align_left"><span id="S2.T8.2.2.4.1" class="ltx_text" style="font-size:90%;">LLaMA 7B</span></td>
<td id="S2.T8.2.2.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T8.2.2.5.1" class="ltx_text" style="font-size:90%;">LoRA</span></td>
<td id="S2.T8.2.2.1" class="ltx_td ltx_align_left">
<span id="S2.T8.2.2.1.1" class="ltx_text" style="font-size:90%;">CC</span><sup id="S2.T8.2.2.1.2" class="ltx_sup"><span id="S2.T8.2.2.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup><span id="S2.T8.2.2.1.3" class="ltx_text" style="font-size:90%;">, CY, L400</span>
</td>
<td id="S2.T8.2.2.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T8.2.2.6.1" class="ltx_text" style="font-size:90%;">204M</span></td>
<td id="S2.T8.2.2.7" class="ltx_td ltx_align_left"><span id="S2.T8.2.2.7.1" class="ltx_text" style="font-size:90%;">LLaVA-I</span></td>
<td id="S2.T8.2.2.8" class="ltx_td ltx_align_left"><span id="S2.T8.2.2.8.1" class="ltx_text" style="font-size:90%;">158K</span></td>
</tr>
<tr id="S2.T8.3.3" class="ltx_tr">
<td id="S2.T8.3.3.2" class="ltx_td ltx_align_left ltx_border_r">
<span id="S2.T8.3.3.2.1" class="ltx_text" style="font-size:90%;">InstructBLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T8.3.3.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a><span id="S2.T8.3.3.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T8.3.3.3" class="ltx_td ltx_align_left"><span id="S2.T8.3.3.3.1" class="ltx_text" style="font-size:90%;">ViT-g/14</span></td>
<td id="S2.T8.3.3.4" class="ltx_td ltx_align_left"><span id="S2.T8.3.3.4.1" class="ltx_text" style="font-size:90%;">FlanT5XL</span></td>
<td id="S2.T8.3.3.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T8.3.3.5.1" class="ltx_text" style="font-size:90%;">Q-Former</span></td>
<td id="S2.T8.3.3.6" class="ltx_td ltx_align_left"><span id="S2.T8.3.3.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T8.3.3.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T8.3.3.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T8.3.3.1" class="ltx_td ltx_align_left">
<span id="S2.T8.3.3.1.1" class="ltx_text" style="font-size:90%;">QA</span><sup id="S2.T8.3.3.1.2" class="ltx_sup"><span id="S2.T8.3.3.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup>
</td>
<td id="S2.T8.3.3.8" class="ltx_td ltx_align_left"><span id="S2.T8.3.3.8.1" class="ltx_text" style="font-size:90%;">16M</span></td>
</tr>
<tr id="S2.T8.3.8" class="ltx_tr">
<td id="S2.T8.3.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="S2.T8.3.8.1.1" class="ltx_text" style="font-size:90%;">GPT-4V  </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T8.3.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a><span id="S2.T8.3.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S2.T8.3.8.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S2.T8.3.8.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T8.3.8.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S2.T8.3.8.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T8.3.8.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S2.T8.3.8.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T8.3.8.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S2.T8.3.8.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T8.3.8.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S2.T8.3.8.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T8.3.8.7" class="ltx_td ltx_align_left ltx_border_bb"><span id="S2.T8.3.8.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T8.3.8.8" class="ltx_td ltx_align_left ltx_border_bb"><span id="S2.T8.3.8.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Details about the six benchmarked models’ model configuration and training data. CC<sup id="S2.T8.45.1" class="ltx_sup"><span id="S2.T8.45.1.1" class="ltx_text ltx_font_italic">∗</span></sup> comprises datasets from COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, CC3M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, and CC12M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>. CC stands for Conceptual Caption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>; VG stands for Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>; CY stands for COYO-700M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>; L400 stands for LAION 400M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>; SBU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> contains 1 million images with captions. LLaVA-I stands for 158K multimodal instruction-following data in LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>. QA<sup id="S2.T8.46.2" class="ltx_sup"><span id="S2.T8.46.2.1" class="ltx_text ltx_font_italic">∗</span></sup> stands for 13 question-answering datasets in InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. </figcaption>
</figure>
</section>
</section>
<section id="S3a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span>Algorithm Benchmarking</h2>

<section id="S3.SS1a" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III.1 </span>Algorithm Architecture</h3>

<div id="S3.SS1a.p1" class="ltx_para">
<p id="S3.SS1a.p1.1" class="ltx_p">Details about each of the six benchmarked models are provided in Table <a href="#S2.T8" title="Table 8 ‣ II.1 Comparison of VQAonline and Other Datasets ‣ II Dataset Analysis ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Specifically, we report each model’s image encoder, language encoder, adapter, and their training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>. Due to computational constraints, as we only have V100 GPU, we only evaluated FlanT5XL as the language encoder for BLIP2 and InstructBLIP, though other larger language encoders are available (i.e., FlanT5XXL for BLIP2 and Vicuna-13B-1.1 for InstructBlip).</p>
</div>
</section>
<section id="S3.SS2a" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III.2 </span>Experiments with Domain Experts</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Domain Expert Hiring.</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">We first manually selected 20 candidate topics for human evaluation. Then based on expert availability, we hired ten domain experts to represent each of ten fields (topics). To guarantee English proficiency, as most of our visual questions are written in English, we only accepted experts located at the United States.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Annotation Task Interface.</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">We show a screenshot of the annotation instructions in Figure <a href="#S3.F8" title="Figure 8 ‣ III.3 Automated Model Benchmarking ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and task interface in Figure <a href="#S3.F9" title="Figure 9 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The link to the code for our web-based annotation tool is available at https://github.com/VQAonline/VQAonlineVisualization.git.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data Collection Quality Control Mechanism.</h5>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">We provided a one-on-one training session with each domain expert to introduce the task and instructions. Afterwards, we gave each expert our contact information so that they could contact us with any questions about their tasks and receive feedback quickly. We also inspected the time each expert spent on each visual question as a proxy for assessing whether the hired domain experts were finishing the task by quickly selecting random options. The median and mean times are 1.4 minutes and 1.29 minutes respectively for assessing each model answer per visual question. We compensated each domain expert with a $75 Amazon Gift Card, resulting in an average hourly wage of $26/hour.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Correct, Partially Correct, and Incorrect Examples From Expert Evaluation. </h5>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p1.1" class="ltx_p">We show examples of expert annotations for the six benchmarked models in Tables <a href="#S3.T10" title="Table 10 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, <a href="#S3.T11" title="Table 11 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, <a href="#S3.T12" title="Table 12 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, <a href="#S3.T13" title="Table 13 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, <a href="#S3.T14" title="Table 14 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, <a href="#S3.T15" title="Table 15 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, spanning those that are correct (<a href="#S3.T10" title="Table 10 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>), partially correct (Table <a href="#S3.T11" title="Table 11 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> and Table <a href="#S3.T12" title="Table 12 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>), and incorrect (<a href="#S3.T13" title="Table 13 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, <a href="#S3.T14" title="Table 14 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, <a href="#S3.T15" title="Table 15 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>).</p>
</div>
<div id="S3.SS2.SSS0.Px4.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p2.1" class="ltx_p">One mechanics’s example shows where most of the models can answer correctly for a closed-ended visual question related to recognition (Table <a href="#S3.T10" title="Table 10 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>).</p>
</div>
<div id="S3.SS2.SSS0.Px4.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p3.1" class="ltx_p">Two economics examples show where the models partially fail for analyzing an <em id="S3.SS2.SSS0.Px4.p3.1.1" class="ltx_emph ltx_font_italic">infographic</em> and requiring <em id="S3.SS2.SSS0.Px4.p3.1.2" class="ltx_emph ltx_font_italic">specific domain knowledge</em> (Table <a href="#S3.T11" title="Table 11 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> and Table <a href="#S3.T12" title="Table 12 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>). These highlight that models labeled "partially correct" can occur because of correctly answered closed-ended questions with incorrect explanations or insufficient explanations matching the reference answer. For example, GPT-4V, mPLUG-OWl, MiniGPT-4, and LLaVA answer "yes" but don’t match any key points of the reason in the reference answer and have factual errors in their given reasons <span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Of note, the errors in all models’ answers to the economic VQ examples are highlighted in red by an economic Ph.D. student. </span></span></span>. InstructBLIP answered "yes" without providing any reason, but simply rephrased the context. For each example, key points in the reference answer were either unsatisfactorily conveyed in the model’s answer or were altogether absent.</p>
</div>
<div id="S3.SS2.SSS0.Px4.p4" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p4.1" class="ltx_p">We also show examples where models gave incorrect answers from a failure to handle the conflict in the question, context, and image. These are shown in Tables <a href="#S3.T13" title="Table 13 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, <a href="#S3.T14" title="Table 14 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, and <a href="#S3.T15" title="Table 15 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>. In Table <a href="#S3.T13" title="Table 13 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> and <a href="#S3.T14" title="Table 14 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, while the context and question provided incorrect information by indicating the plant has mold and asking how to remove the mold, the ground-truth answer corrects the questioner saying it’s not mold but moss/algae so there is no need to remove it. All the models trusted the natural language context and question more than the image and then thus were misled to answer the question (partially) incorrectly<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Among them, GPT-4V and LLaVA are partially correct as they each matched one key point with the reference answers: The method 2 mentioned by GPT-4V can also remove moss/algae; LLaVA mentioned the moss (though LLaVA identified as mold) might not be harmful. </span></span></span> by offering methods to remove mold<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Of note, models’ answers to the Gardening &amp; Landscaping VQ examples are verified with a Botany Ph.D. student.</span></span></span>.
The failure to handle conflict in question, context, and image is also common in other topics (e.g., Chinese Language), as shown in Table <a href="#S3.T15" title="Table 15 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>. While the context provided incorrect information by indicating the characters are 心兴, the ground-truth answer corrects the questioner saying it’s indeed 心头. All the models trusted the context more than the image and then thus were misled to answer the question incorrectly. This type of failure, also known as the hallucination issue, is also discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, where VLMs may generate descriptions or answers conflicting with the given image to align with the natural language instructions or questions, influenced by the strong language priors of large language models. We will release our dataset to facilitate further exploration of the hallucination issue in the authentic use cases.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III.3 </span>Automated Model Benchmarking</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Supplementing the main paper, where we reported the performance of the six models with respect to the three most human-aligned metrics (i.e., METEOR, GPT-4 metric, and BERTscore), we report here performance with respect to all six metrics. Overall results are shown in Table <a href="#S3.T9" title="Table 9 ‣ III.3 Automated Model Benchmarking ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. We observed that all models perform poorly on our dataset, especially when considering ROUGE-L, METEOR, and GPT-4 metrics.
When comparing across metrics, we find that all metrics except CLIP-S indicate GPT-4 and mPLUG-Owl are the best two models.</p>
</div>
<figure id="S3.T9" class="ltx_table">
<table id="S3.T9.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T9.2.1" class="ltx_tr">
<td id="S3.T9.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Models</td>
<td id="S3.T9.2.1.2" class="ltx_td ltx_align_left ltx_border_tt">ROUGE-L</td>
<td id="S3.T9.2.1.3" class="ltx_td ltx_align_left ltx_border_tt">METEOR</td>
<td id="S3.T9.2.1.4" class="ltx_td ltx_align_left ltx_border_tt">BERTscore</td>
<td id="S3.T9.2.1.5" class="ltx_td ltx_align_left ltx_border_tt">CLIP-S</td>
<td id="S3.T9.2.1.6" class="ltx_td ltx_align_left ltx_border_tt">RefCLIP-S</td>
<td id="S3.T9.2.1.7" class="ltx_td ltx_align_left ltx_border_tt">~GPT-4 Metric</td>
</tr>
<tr id="S3.T9.2.2" class="ltx_tr">
<td id="S3.T9.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">~GPT-4V<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>
</td>
<td id="S3.T9.2.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T9.2.2.2.1" class="ltx_text ltx_font_bold">0.16</span></td>
<td id="S3.T9.2.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T9.2.2.3.1" class="ltx_text ltx_font_bold">0.12</span></td>
<td id="S3.T9.2.2.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T9.2.2.4.1" class="ltx_text ltx_font_bold">0.76</span></td>
<td id="S3.T9.2.2.5" class="ltx_td ltx_align_left ltx_border_t">0.70</td>
<td id="S3.T9.2.2.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T9.2.2.6.1" class="ltx_text ltx_font_bold">0.75</span></td>
<td id="S3.T9.2.2.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T9.2.2.7.1" class="ltx_text ltx_font_bold">0.55</span></td>
</tr>
<tr id="S3.T9.2.3" class="ltx_tr">
<td id="S3.T9.2.3.1" class="ltx_td ltx_align_left ltx_border_r">mPLUG-Owl <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>
</td>
<td id="S3.T9.2.3.2" class="ltx_td ltx_align_left"><span id="S3.T9.2.3.2.1" class="ltx_text ltx_font_bold">0.16</span></td>
<td id="S3.T9.2.3.3" class="ltx_td ltx_align_left"><span id="S3.T9.2.3.3.1" class="ltx_text ltx_font_bold">0.12</span></td>
<td id="S3.T9.2.3.4" class="ltx_td ltx_align_left"><span id="S3.T9.2.3.4.1" class="ltx_text ltx_font_bold">0.76</span></td>
<td id="S3.T9.2.3.5" class="ltx_td ltx_align_left">0.72</td>
<td id="S3.T9.2.3.6" class="ltx_td ltx_align_left"><span id="S3.T9.2.3.6.1" class="ltx_text ltx_font_bold">0.75</span></td>
<td id="S3.T9.2.3.7" class="ltx_td ltx_align_left">0.4</td>
</tr>
<tr id="S3.T9.2.4" class="ltx_tr">
<td id="S3.T9.2.4.1" class="ltx_td ltx_align_left ltx_border_r">LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>
</td>
<td id="S3.T9.2.4.2" class="ltx_td ltx_align_left">0.15</td>
<td id="S3.T9.2.4.3" class="ltx_td ltx_align_left">0.08</td>
<td id="S3.T9.2.4.4" class="ltx_td ltx_align_left">0.75</td>
<td id="S3.T9.2.4.5" class="ltx_td ltx_align_left"><span id="S3.T9.2.4.5.1" class="ltx_text ltx_font_bold">0.73</span></td>
<td id="S3.T9.2.4.6" class="ltx_td ltx_align_left">0.75</td>
<td id="S3.T9.2.4.7" class="ltx_td ltx_align_left">0.38</td>
</tr>
<tr id="S3.T9.2.5" class="ltx_tr">
<td id="S3.T9.2.5.1" class="ltx_td ltx_align_left ltx_border_r">MiniGPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>
</td>
<td id="S3.T9.2.5.2" class="ltx_td ltx_align_left">0.15</td>
<td id="S3.T9.2.5.3" class="ltx_td ltx_align_left">0.09</td>
<td id="S3.T9.2.5.4" class="ltx_td ltx_align_left">0.75</td>
<td id="S3.T9.2.5.5" class="ltx_td ltx_align_left">0.70</td>
<td id="S3.T9.2.5.6" class="ltx_td ltx_align_left">0.74</td>
<td id="S3.T9.2.5.7" class="ltx_td ltx_align_left">0.34</td>
</tr>
<tr id="S3.T9.2.6" class="ltx_tr">
<td id="S3.T9.2.6.1" class="ltx_td ltx_align_left ltx_border_r">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>
</td>
<td id="S3.T9.2.6.2" class="ltx_td ltx_align_left">0.09</td>
<td id="S3.T9.2.6.3" class="ltx_td ltx_align_left">0.05</td>
<td id="S3.T9.2.6.4" class="ltx_td ltx_align_left">0.67</td>
<td id="S3.T9.2.6.5" class="ltx_td ltx_align_left">0.68</td>
<td id="S3.T9.2.6.6" class="ltx_td ltx_align_left">0.71</td>
<td id="S3.T9.2.6.7" class="ltx_td ltx_align_left">0.29</td>
</tr>
<tr id="S3.T9.2.7" class="ltx_tr">
<td id="S3.T9.2.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">BLIP2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</td>
<td id="S3.T9.2.7.2" class="ltx_td ltx_align_left ltx_border_bb">0.09</td>
<td id="S3.T9.2.7.3" class="ltx_td ltx_align_left ltx_border_bb">0.04</td>
<td id="S3.T9.2.7.4" class="ltx_td ltx_align_left ltx_border_bb">0.69</td>
<td id="S3.T9.2.7.5" class="ltx_td ltx_align_left ltx_border_bb">0.66</td>
<td id="S3.T9.2.7.6" class="ltx_td ltx_align_left ltx_border_bb">0.71</td>
<td id="S3.T9.2.7.7" class="ltx_td ltx_align_left ltx_border_bb">0.27</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T9.3.1.1" class="ltx_text" style="font-size:90%;">Table 9</span>: </span><span id="S3.T9.4.2" class="ltx_text" style="font-size:90%;">Results for the state-of-the-art vision and language models on our dataset, evaluated with reference-based metrics ( ROUGE-L, METEOR, BERTscore, RefCLIP-S, and GPT-4) and reference-less metric (CLIP-S). Models or scores labeled with "~" are evaluated on 1,903 random samples. We only reported GPT-4V model and GPT-4 metric on samples due to the high cost of calling GPT-4V and GPT-4’s API. </span></figcaption>
</figure>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2311.15562/assets/Images/instruction_expert.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="684" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.3.2" class="ltx_text" style="font-size:90%;">Instructions for our domain expert annotation task. </span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III.4 </span>Qualitative Examples from Different Models</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We additionally provide a qualitative example from different models in Table <a href="#S3.T16" title="Table 16 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>, where the visual questions require significant reasoning. We think the visual question in Table <a href="#S3.T16" title="Table 16 ‣ III.4 Qualitative Examples from Different Models ‣ III Algorithm Benchmarking ‣ Fully Authentic Visual Question Answering Dataset from Online Communities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> is interesting as algorithms like AlphaGo or MuZero can surpass humans in chess games, but the vision and language models fail in this domain. For example, MiniGPT4, InstructBlip, and BLIP2 didn’t even understand the visual question. LLaVA directly copied the example provided in the context. mPLUG-Owl also directly copied the example provided in the context except omitted two moves, leading to an illegal move at round 8 (half move #15 Rh4). In contrast, GPT-4V gave an answer that on the surface-level seemed to make sense. Yet, GPT-4V violated the requirement of not capturing any pawn in the third round (half move #5 Bxf7+) as well as made an illegal move in round 11th (halfmove #21 gxf6). Of note, we verified all models’ answers with https://www.apronus.com/chess/pgnviewer/.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2311.15562/assets/Images/user_interface_example.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="594" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.3.2" class="ltx_text" style="font-size:90%;">Screen shot of the annotation task interface. Experts recorded their results in spreadsheets. </span></figcaption>
</figure>
<figure id="S3.SS4.1" class="ltx_table">
<figure id="S3.SS4.1.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:491.9pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.SS4.1.1.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:548.1pt;height:670.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.4pt,45.7pt) scale(0.88,0.88) ;">
<table id="S3.SS4.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.SS4.1.1.1.1.2" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="2"><span id="S3.SS4.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">A visual question where most of the models can answer correctly:</span></td>
</tr>
<tr id="S3.SS4.1.1.1.1.3" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Category</td>
<td id="S3.SS4.1.1.1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.1.1.1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.3.2.1.1" class="ltx_p" style="width:455.2pt;">Mechanics</span>
</span>
</td>
</tr>
<tr id="S3.SS4.1.1.1.1.1" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.1.2" class="ltx_td"></td>
<td id="S3.SS4.1.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.1.1.1.1" class="ltx_p" style="width:455.2pt;"><img src="/html/2311.15562/assets/Images/example_images/VQAinW_00045206.png" id="S3.SS4.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="314" height="236" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.SS4.1.1.1.1.4" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.4.1" class="ltx_td ltx_align_left">Question</td>
<td id="S3.SS4.1.1.1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.1.1.1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.4.2.1.1" class="ltx_p" style="width:455.2pt;">Tire sidewall damage, replace immediately?</span>
</span>
</td>
</tr>
<tr id="S3.SS4.1.1.1.1.5" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.5.1" class="ltx_td ltx_align_left">Context</td>
<td id="S3.SS4.1.1.1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.1.1.1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.5.2.1.1" class="ltx_p" style="width:455.2pt;">Somehow I was lucky enough to hit something when going downhill. And now my tire looks like in the picture. So should I still drive with this tire or get it replace immediately? And to replace it, is it safe to drive to the tire shop, which is a few miles away?</span>
</span>
</td>
</tr>
<tr id="S3.SS4.1.1.1.1.6" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.6.1" class="ltx_td ltx_align_left">Reference Answer</td>
<td id="S3.SS4.1.1.1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.1.1.1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.6.2.1.1" class="ltx_p" style="width:455.2pt;">Sidewall damage is a serious issue. Sidewall is the structural part of the tire, and damage to it can result in dangerous consequences. Replace your tire immediately. If you know how to put the spare tire on (and there is a spare tire in your car), please do it now, and drive only to the tire shop. Spare tires are not designed for extended drives, so don’t start driving around in the spare tire. If you have driven more than a few miles already with the tire before looking at the damage, you may have enough data to indicate that it may survive to the tire shop. I would however prefer the spare tire approach in your case. If there’s no spare tire, then that would be a difficult decision: to have your car towed with the broken tire off the ground, or to drive the car. You know more about the details in your case than I do, because you didn’t indicate how many miles exactly you drove with the tire after the incident.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.1.1.1.1.7" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t">GPT-4V (<span id="S3.SS4.1.1.1.1.7.1.1" class="ltx_text" style="color:#00FF00;">Correct</span>)</td>
<td id="S3.SS4.1.1.1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.1.1.1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.7.2.1.1" class="ltx_p" style="width:455.2pt;">This looks like a significant sidewall damage and driving on this tire can be dangerous. It is recommended to replace the tire immediately. If you have a spare tire, it would be safer to use the spare and drive to the tire shop to get a new tire. If you don’t have a spare, drive slowly and cautiously to the tire shop, and avoid highways and high speeds.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.1.1.1.1.8" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.8.1" class="ltx_td ltx_align_left ltx_border_t">mPLUG-Owl (<span id="S3.SS4.1.1.1.1.8.1.1" class="ltx_text" style="color:#0000FF;">Partially correct</span>)</td>
<td id="S3.SS4.1.1.1.1.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.1.1.1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.8.2.1.1" class="ltx_p" style="width:455.2pt;">Yes, it is advisable to replace the tire immediately if it has suffered significant damage. The sidewall of the tire is crucial for maintaining its structural integrity while driving. Damage to the sidewall can lead to air loss, which can increase the risk of a blowout, especially on high speeds or when making sharp turns. In such cases, driving with a damaged tire can be extremely dangerous and could result in a catastrophic accident. It is safer to replace the tire as soon as possible to ensure your safety on the road. <span id="S3.SS4.1.1.1.1.8.2.1.1.1" class="ltx_text" style="color:#FF0000;">To drive to the tire shop, it is usually safe to do so as long as the tire is not leaking air. However, it’s important to check for any visible signs of air leakage while driving and to ensure that the tire pressure is maintained at the recommended level. If you notice any signs of air leakage, pull over to a safe location and stop as soon as possible to check for any further issues.</span></span>
</span>
</td>
</tr>
<tr id="S3.SS4.1.1.1.1.9" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t">LLaVA (<span id="S3.SS4.1.1.1.1.9.1.1" class="ltx_text" style="color:#00FF00;">Correct</span>)</td>
<td id="S3.SS4.1.1.1.1.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.1.1.1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.9.2.1.1" class="ltx_p" style="width:455.2pt;">Given the severe sidewall damage to the tire, it is recommended to replace it immediately. While it is technically possible to drive with the damaged tire, there are risks associated with doing so. The damage could worsen, leading to a potential blowout, which could result in loss of control, accidents, or further tire damage. Additionally, driving with a damaged tire can impact the vehicle’s handling and braking capabilities. It is safer to replace the damaged tire with a new one, ensuring optimal road safety and vehicle performance. As for driving to the tire shop, it is advisable to do so if the tire shop is nearby and if there are no other more severe damages to the vehicle that require immediate attention. If in doubt, consult an expert or seek assistance from roadside services if available.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.1.1.1.1.10" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.10.1" class="ltx_td ltx_align_left ltx_border_t">MiniGPT-4 (<span id="S3.SS4.1.1.1.1.10.1.1" class="ltx_text" style="color:#00FF00;">Correct</span>)</td>
<td id="S3.SS4.1.1.1.1.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.1.1.1.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.10.2.1.1" class="ltx_p" style="width:455.2pt;">It is not recommended to drive with a damaged tire, as it could result in a blowout or loss of control of your vehicle. If the tire is severely damaged, it is important to replace it immediately, as it is not safe to continue driving on it. If the tire is only slightly damaged, it may still be safe to drive on it for a short distance, but it is still recommended to have it inspected and potentially replaced. It is safe to drive to the tire shop a few miles away, but it would be wise to make sure that the tire shop is open and able to serve you before making the trip. If you cannot replace the tire immediately, you should drive as little as possible and have the tire inspected as soon as possible.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.1.1.1.1.11" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.11.1" class="ltx_td ltx_align_left ltx_border_t">InstructBLIP (<span id="S3.SS4.1.1.1.1.11.1.1" class="ltx_text" style="color:#00FF00;">Correct</span>)</td>
<td id="S3.SS4.1.1.1.1.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.1.1.1.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.11.2.1.1" class="ltx_p" style="width:455.2pt;">replace immediately</span>
</span>
</td>
</tr>
<tr id="S3.SS4.1.1.1.1.12" class="ltx_tr">
<td id="S3.SS4.1.1.1.1.12.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">BLIP2 (<span id="S3.SS4.1.1.1.1.12.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.1.1.1.1.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.SS4.1.1.1.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.1.1.1.1.12.2.1.1" class="ltx_p" style="width:455.2pt;"><span id="S3.SS4.1.1.1.1.12.2.1.1.1" class="ltx_text" style="color:#FF0000;">yes, it is safe to drive to the tire shop</span></span>
</span>
</td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.T10" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T10.2.1.1" class="ltx_text" style="font-size:90%;">Table 10</span>: </span><span id="S3.T10.3.2" class="ltx_text" style="font-size:90%;">An example where most of the models can answer correctly. Models’ answers were verified and the errors were highlighted in red by a Mechanics Ph.D.</span></figcaption>
</figure>
</div>
</div>
</figure>
</figure>
<figure id="S3.SS4.2" class="ltx_table">
<figure id="S3.SS4.2.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:491.9pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.SS4.2.1.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:530.4pt;height:558.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.2pt,38.0pt) scale(0.88,0.88) ;">
<table id="S3.SS4.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.SS4.2.1.1.1.2" class="ltx_tr">
<td id="S3.SS4.2.1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="2"><span id="S3.SS4.2.1.1.1.2.1.1" class="ltx_text ltx_font_bold">A visual question needing specific domain knowledge:</span></td>
</tr>
<tr id="S3.SS4.2.1.1.1.3" class="ltx_tr">
<td id="S3.SS4.2.1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Category</td>
<td id="S3.SS4.2.1.1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.2.1.1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.2.1.1.1.3.2.1.1" class="ltx_p" style="width:455.2pt;">Economics</span>
</span>
</td>
</tr>
<tr id="S3.SS4.2.1.1.1.1" class="ltx_tr">
<td id="S3.SS4.2.1.1.1.1.2" class="ltx_td"></td>
<td id="S3.SS4.2.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.2.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.2.1.1.1.1.1.1.1" class="ltx_p" style="width:455.2pt;"><img src="/html/2311.15562/assets/Images/example_images/VQAinW_00012434.png" id="S3.SS4.2.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="396" height="393" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.SS4.2.1.1.1.4" class="ltx_tr">
<td id="S3.SS4.2.1.1.1.4.1" class="ltx_td ltx_align_left">Question</td>
<td id="S3.SS4.2.1.1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.2.1.1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.2.1.1.1.4.2.1.1" class="ltx_p" style="width:455.2pt;">Measuring income effect before knowing substitution effect</span>
</span>
</td>
</tr>
<tr id="S3.SS4.2.1.1.1.5" class="ltx_tr">
<td id="S3.SS4.2.1.1.1.5.1" class="ltx_td ltx_align_left">Context</td>
<td id="S3.SS4.2.1.1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.2.1.1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.2.1.1.1.5.2.1.1" class="ltx_p" style="width:455.2pt;">Suppose on the x axis we have shelter and on the y axis we have composite goods. Now, if the price of shelter increases, the optimal bundle changes from point A to point D. Standard textbook tells me to draw a line parallel to the new budget line B1, which is tangent to the indifference curve I0. In this way we can get the substitution effect. The income effect follows. My question is, can we reverse the two procedures and measure the income effect first by drawing a line parallel to the original budget line and tangent to the new indifference line? Thanks!</span>
</span>
</td>
</tr>
<tr id="S3.SS4.2.1.1.1.6" class="ltx_tr">
<td id="S3.SS4.2.1.1.1.6.1" class="ltx_td ltx_align_left">Reference Answer</td>
<td id="S3.SS4.2.1.1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.2.1.1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.2.1.1.1.6.2.1.1" class="ltx_p" style="width:455.2pt;">You can definitely do so! There are two versions of the income effect. The one that you are referring to is called the Equivalent variation. The two ways of looking at the problem were introduced by Sir John Richard Hicks. To see the impact of a price change in monetary terms, we ask how much money should have been taken before the price change to leave a consumer at the same utility level he attains after the price change. This is the same thing as you had asked: can we reverse the two procedures and measure the income effect first by drawing a line parallel to the original budget line and tangent to the new indifference line? I hope this has helped but if you wish to explore further, see the ’Consumer Surplus’ chapter in Hal Varian’s ’Intermediate Microeconomics’.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.2.1.1.1.7" class="ltx_tr">
<td id="S3.SS4.2.1.1.1.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">GPT-4V (<span id="S3.SS4.2.1.1.1.7.1.1" class="ltx_text" style="color:#0000FF;">Partially correct</span>)</td>
<td id="S3.SS4.2.1.1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.SS4.2.1.1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.2.1.1.1.7.2.1.1" class="ltx_p" style="width:455.2pt;">Yes, you can reverse the two procedures and measure the income effect first, but it is less intuitive and less accurate compared to measuring the substitution effect first. The reason for this is because the income effect is the difference between the old and new optimal bundles (A and D) after removing the substitution effect. Since the substitution effect is a result of changes in relative prices, in order to accurately measure the income effect, you would first need to find the new optimal bundle that would have been chosen if the relative prices had remained the same. In practice, this is difficult to do because you would have to make assumptions about how the individual’s preferences would have changed, and you would need to find the new optimal bundle on the new indifference curve. <span id="S3.SS4.2.1.1.1.7.2.1.1.1" class="ltx_text" style="color:#FF0000;">It would be much easier and more accurate to simply measure the substitution effect first</span>, and then subtract that from the total effect to find the income effect.</span>
</span>
</td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.T11" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T11.2.1.1" class="ltx_text" style="font-size:90%;">Table 11</span>: </span><span id="S3.T11.3.2" class="ltx_text" style="font-size:90%;">A visual question that requires specific domain knowledge in Economics and infographic understanding. Models’ answers are from GPT-4V. Errors are marked in red by an economics PhD student for reader convenience. </span></figcaption>
</figure>
</div>
</div>
</figure>
</figure>
<figure id="S3.SS4.3" class="ltx_table">
<figure id="S3.SS4.3.fig1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:491.9pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.SS4.3.fig1.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:548.7pt;height:514.7pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.4pt,35.0pt) scale(0.88,0.88) ;">
<table id="S3.SS4.3.fig1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.SS4.3.fig1.1.1.1" class="ltx_tr">
<td id="S3.SS4.3.fig1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="2"><span id="S3.SS4.3.fig1.1.1.1.1.1" class="ltx_text ltx_font_bold">A visual question needing specific domain knowledge:</span></td>
</tr>
<tr id="S3.SS4.3.fig1.1.1.2" class="ltx_tr">
<td id="S3.SS4.3.fig1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">mPLUG-Owl (<span id="S3.SS4.3.fig1.1.1.2.1.1" class="ltx_text" style="color:#0000FF;">Partially correct</span>)</td>
<td id="S3.SS4.3.fig1.1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.3.fig1.1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.3.fig1.1.1.2.2.1.1" class="ltx_p" style="width:455.2pt;">Yes, you can measure the income effect before knowing the substitution effect by drawing a line parallel to the original budget line and tangent to a new indifference curve. The income effect is the effect on consumption of a given good when the income of a consumer increases or decreases. To measure the income effect, we need to find the new indifference curve that represents consumption choices of consumers after their income has changed. The original budget line represents consumption choices of consumers before the change in income, while the new indifference curve represents consumption choices after the change in income. <span id="S3.SS4.3.fig1.1.1.2.2.1.1.1" class="ltx_text" style="color:#FF0000;">To find the new indifference curve, we can use the budget line equation: y = C(x, y) = w(y - t(x, y)) where x and y represent consumption of shelter and composite goods, respectively, and t(x, y) represents the marginal utility of composite goods. C(x, y) is the budget line, and w represents the consumer’s budget. The budget line is the line of points where the total utility is equal to a given consumer’s budget.</span> To measure the income effect, we need to find the point of intersection between the original budget line and the new indifference curve, which represents consumption choices after the change in income. <span id="S3.SS4.3.fig1.1.1.2.2.1.1.2" class="ltx_text" style="color:#FF0000;">The new indifference curve can be found by solving the budget equation using the new income level and the new marginal utility of composite goods.</span> To find the point of intersection between the original budget line and the new indifference curve, <span id="S3.SS4.3.fig1.1.1.2.2.1.1.3" class="ltx_text" style="color:#FF0000;">we need to solve the budget equation using the new income level and the new marginal utility of composite goods: w(y – t(x, y)) = y – t(x, y) Substituting the new marginal utility of composite goods into the budget equation, we get: w(y – t(x, y)) = y – t(x, y) = y – t((x1, y1), (x2, y2)) where (x1, y1) and (x2, y2) are the consumption points on the new indifference curve. We can then solve for the point of intersection between the original budget line and the new indifference curve by substituting x1 and y1 into the budget equation: w(y – t(x1, y1)) = y – t(x1, y1) = y.</span></span>
</span>
</td>
</tr>
<tr id="S3.SS4.3.fig1.1.1.3" class="ltx_tr">
<td id="S3.SS4.3.fig1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">LLaVA (<span id="S3.SS4.3.fig1.1.1.3.1.1" class="ltx_text" style="color:#0000FF;">Partially correct</span>)</td>
<td id="S3.SS4.3.fig1.1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.3.fig1.1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.3.fig1.1.1.3.2.1.1" class="ltx_p" style="width:455.2pt;">Yes, it is possible to measure the income effect first by drawing a line parallel to the original budget line and tangent to the new indifference curve. This can be done by following the steps you described in your question. The income effect represents the change in consumption due to a change in income, and it can be calculated by <span id="S3.SS4.3.fig1.1.1.3.2.1.1.1" class="ltx_text" style="color:#FF0000;">comparing the consumer’s initial position (point A) to the new optimal consumption point (point D)</span> after the change in the price of shelter. <span id="S3.SS4.3.fig1.1.1.3.2.1.1.2" class="ltx_text" style="color:#FF0000;">The substitution effect, on the other hand, is the effect of changing the relative price of shelter and composite goods on the consumer’s choice of consumption bundle. It can be derived from the difference between the tangent line parallel to the new budget line and the indifference curve, and the income effect.</span></span>
</span>
</td>
</tr>
<tr id="S3.SS4.3.fig1.1.1.4" class="ltx_tr">
<td id="S3.SS4.3.fig1.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t">MiniGPT-4(<span id="S3.SS4.3.fig1.1.1.4.1.1" class="ltx_text" style="color:#0000FF;">Partially correct</span>)</td>
<td id="S3.SS4.3.fig1.1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.3.fig1.1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.3.fig1.1.1.4.2.1.1" class="ltx_p" style="width:455.2pt;">Yes, you can certainly measure the income effect first by drawing a line parallel to the original budget line and tangent to the new indifference curve. This approach is commonly used in economics to measure the income effect of a change in the price of a good or service. The income effect is the change in the quantity demanded of a good or service due to a change in income. When income increases, the income effect causes the consumer to shift the demand curve to the right, leading to an increase in the quantity demanded of the good or service. Similarly, when income decreases, the income effect causes the demand curve to shift left, resulting in a decrease in the quantity demanded. To measure the income effect using a substitution effect approach, you would draw a line parallel to the original budget line (B1) that is tangent to the new indifference curve (I0’) that represents the consumer’s maximum utility point after the change in the price of the good or service. <span id="S3.SS4.3.fig1.1.1.4.2.1.1.1" class="ltx_text" style="color:#FF0000;">The point of tangency is denoted by the letter ’P’ on the new budget constraint. The ratio of the areas of the triangles PBC and PC defines the magnitude of the income effect, which can be measured in terms of the quantity demanded of the good or service</span>. Note that this approach is not the most intuitive way to measure income effect. It’s more common to draw a new budget constraint to measure the income effect. The new budget constraint after</span>
</span>
</td>
</tr>
<tr id="S3.SS4.3.fig1.1.1.5" class="ltx_tr">
<td id="S3.SS4.3.fig1.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t">InstructBLIP (<span id="S3.SS4.3.fig1.1.1.5.1.1" class="ltx_text" style="color:#0000FF;">Partially correct</span>)</td>
<td id="S3.SS4.3.fig1.1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.3.fig1.1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.3.fig1.1.1.5.2.1.1" class="ltx_p" style="width:455.2pt;">Yes, it is possible to reverse the two procedures and measure the income effect first by drawing a line parallel to the original budget line and tangent to the new indifference line.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.3.fig1.1.1.6" class="ltx_tr">
<td id="S3.SS4.3.fig1.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">BLIP2 (<span id="S3.SS4.3.fig1.1.1.6.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.3.fig1.1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.SS4.3.fig1.1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.3.fig1.1.1.6.2.1.1" class="ltx_p" style="width:455.2pt;"><span id="S3.SS4.3.fig1.1.1.6.2.1.1.1" class="ltx_text" style="color:#FF0000;">No, you can’t measure the income effect first by drawing a line parallel to the original budget line and tangent to the new indifference line.</span></span>
</span>
</td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.T12" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T12.2.1.1" class="ltx_text" style="font-size:90%;">Table 12</span>: </span><span id="S3.T12.3.2" class="ltx_text" style="font-size:90%;">A visual question that requires specific domain knowledge in Economics and infographic understanding. Models’ answers are from mPLUG-owl, LLaVA, MiniGPT-4, InstructBLIP, and BLIP2. Errors are marked in red by an economics Ph.D. student for reader convenience. </span></figcaption>
</figure>
</div>
</div>
</figure>
</figure>
<figure id="S3.T13" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S3.SS4.4.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:530.4pt;height:613.7pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.2pt,41.8pt) scale(0.88,0.88) ;">
<table id="S3.SS4.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.SS4.4.1.1.2" class="ltx_tr">
<td id="S3.SS4.4.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="2"><span id="S3.SS4.4.1.1.2.1.1" class="ltx_text ltx_font_bold">A visual question when the question and the context are misleading while the image contains true information:
:</span></td>
</tr>
<tr id="S3.SS4.4.1.1.3" class="ltx_tr">
<td id="S3.SS4.4.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Category</td>
<td id="S3.SS4.4.1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.4.1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.4.1.1.3.2.1.1" class="ltx_p" style="width:455.2pt;">Gardening &amp; Landscaping</span>
</span>
</td>
</tr>
<tr id="S3.SS4.4.1.1.1" class="ltx_tr">
<td id="S3.SS4.4.1.1.1.2" class="ltx_td"></td>
<td id="S3.SS4.4.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.4.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.4.1.1.1.1.1.1" class="ltx_p" style="width:455.2pt;"><img src="/html/2311.15562/assets/Images/example_images/mold.jpg" id="S3.SS4.4.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="314" height="177" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.SS4.4.1.1.4" class="ltx_tr">
<td id="S3.SS4.4.1.1.4.1" class="ltx_td ltx_align_left">Question</td>
<td id="S3.SS4.4.1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.4.1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.4.1.1.4.2.1.1" class="ltx_p" style="width:455.2pt;">How do I safely eradicate mold without hurting cactus seedlings?</span>
</span>
</td>
</tr>
<tr id="S3.SS4.4.1.1.5" class="ltx_tr">
<td id="S3.SS4.4.1.1.5.1" class="ltx_td ltx_align_left">Context</td>
<td id="S3.SS4.4.1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.4.1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.4.1.1.5.2.1.1" class="ltx_p" style="width:455.2pt;">Here are some of my seedlings, sown a little bit less than a month ago. As you can see, there’s mold starting to form. I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect from contamination. What would be an easy way to get rid of the mold without hurting the seedlings? Someone suggested to use hydrogen peroxide, but I’m not sure it’s safe for the seedlings. Ideas?</span>
</span>
</td>
</tr>
<tr id="S3.SS4.4.1.1.6" class="ltx_tr">
<td id="S3.SS4.4.1.1.6.1" class="ltx_td ltx_align_left">Reference Answer</td>
<td id="S3.SS4.4.1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.4.1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.4.1.1.6.2.1.1" class="ltx_p" style="width:455.2pt;">That isn’t mold, it’s moss/algae. You can tell (by the green) that it’s growing from energy obtained by photosynthesis, so it’s not a big threat competition-wise. It’s a sign that the mix is constantly moist on the surface, and also grows faster in high humidity. It will not damage the cactus seedlings, but here’s a tip. When cacti germinate, they grow very slowly/remain the same size for a long time. That’s because they’re developing a strong root system. A sign they are established is that they will begin to grow at a faster rate. This means the root system is growing deep, and you can allow the top 1/2 of mix to dry between waterings. This will keep moss from establishing. For now, you can use a toothpick and stir up the top layer if you want, to slow the moss, as described here: Is there anything wrong with allowing moss to grow around herbs? Or you can use a pair of tweezers and carefully remove the moss. In the long run, it really doesn’t matter. The moss won’t be a problem after the seedlings mature a little, and you don’t have to water so frequently. And by the way, those seedlings look great! I hope you have success with them.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.4.1.1.7" class="ltx_tr">
<td id="S3.SS4.4.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t">GPT-4V (<span id="S3.SS4.4.1.1.7.1.1" class="ltx_text" style="color:#0000FF;">Partially correct</span>)</td>
<td id="S3.SS4.4.1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.4.1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.4.1.1.7.2.1.1" class="ltx_p" style="width:455.2pt;">To get rid of mold without hurting your cactus seedlings, try the following methods: 1. Cinnamon: Cinnamon has antifungal properties and can be used as a natural fungicide. Simply sprinkle some ground cinnamon over the affected areas, and it should help control the mold growth. 2. Baking soda: Baking soda is also effective at controlling mold growth. Mix one tablespoon of baking soda with a quart of water, and spray the solution over the affected areas. 3. Vinegar: Vinegar also has antifungal properties and can be used as a natural fungicide. Mix one part vinegar with three parts water, and spray over affected areas. Some other tips to help prevent mold growth in the future include: - Limiting the amount of moisture in the soil - Increasing air circulation around the seedlings - Avoiding overwatering - Keeping soil temperature between 70-80 degrees Fahrenheit - Keeping relative humidity between 40-60%. Don’t use hydrogen peroxide as it can be harmful to the seedlings.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.4.1.1.8" class="ltx_tr">
<td id="S3.SS4.4.1.1.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">mPLUG-Owl (<span id="S3.SS4.4.1.1.8.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.4.1.1.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.SS4.4.1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.4.1.1.8.2.1.1" class="ltx_p" style="width:455.2pt;">It is essential to take steps to safely eradicate any mold growth on your cactus seedlings without harming them. One option is to use hydrogen peroxide. However, it is important to be cautious when using it on seedlings. Hydrogen peroxide is a strong oxidizing agent that can cause burns or damage to the cactus seedlings if used improperly. To safely use hydrogen peroxide to treat mold on the seedlings, follow these steps: 1. Use a dilute solution of hydrogen peroxide. The best dilution ratio is around 1 tablespoon of 3% hydrogen peroxide per gallon of water. 2. Pour the hydrogen peroxide solution into a shallow dish or container. 3. Soak the affected cactus seedlings in the solution for a few minutes. 4. Remove the seedlings from the solution and place them back in the pot with the Ziploc bag. 5. Seal the Ziploc bag, making sure to leave a small opening at the top to allow ventilation. By following these steps, you can ensure that the hydrogen peroxide solution will not come in direct contact with the cactus seedlings and will effectively eradicate any mold without harming the plants. It is essential to monitor the seedlings carefully and remove them from the hydrogen peroxide solution if any signs of injury or damage appear. If you are not confident in using hydrogen peroxide or have any concerns about its effectiveness, you may consider using a more gentle method, such as washing the seedlings with a gentle soap and water solution and allowing it to dry thoroughly.</span>
</span>
</td>
</tr>
</table>
</span></div>
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T13.3.1.1" class="ltx_text" style="font-size:90%;">Table 13</span>: </span><span id="S3.T13.4.2" class="ltx_text" style="font-size:90%;">A visual question when the question and context are misleading while the image contains true information. Model answers are from GPT-4V and mPLUG-Owl, all misled by the question and context. Models’ answers verified by a Ph.D. in Botany major. </span></figcaption>
</figure>
<figure id="S3.T14" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S3.T14.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:524.8pt;height:551.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.8pt,37.6pt) scale(0.88,0.88) ;">
<table id="S3.T14.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T14.1.1.1" class="ltx_tr">
<td id="S3.T14.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="2"><span id="S3.T14.1.1.1.1.1" class="ltx_text ltx_font_bold">A visual question when the question and context are misleading while the image contains true information:</span></td>
</tr>
<tr id="S3.T14.1.1.2" class="ltx_tr">
<td id="S3.T14.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">LLaVA (<span id="S3.T14.1.1.2.1.1" class="ltx_text" style="color:#0000FF;">Partially correct</span>)</td>
<td id="S3.T14.1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T14.1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T14.1.1.2.2.1.1" class="ltx_p" style="width:455.2pt;">To safely eradicate mold without hurting the cactus seedlings, you can follow these steps: 1. Identify the mold: Look closely at the mold’s appearance and determine if it’s harmful to cacti. Some molds can be beneficial, such as those that create the beautiful Cordyceps fungus, while others can be detrimental to the plant’s health. 2. Remove the mold: If the mold is detrimental, gently pick it off using a clean, dry cotton swab or a soft brush. Avoid using water or wetting the cotton swab, as this could spread the mold spores and affect the seedlings. 3. Improve air circulation and light: Ensure that your seedlings are receiving adequate light and air circulation. Mold thrives in dark, damp environments, so providing proper light and ventilation can help prevent mold growth. 4. Keep the seedlings well-spaced: Overcrowding can lead to mold, as it allows for insufficient air circulation. Keep the seedlings well-spaced and avoid overwatering to maintain a suitable environment for healthy growth. 5. Use hydrogen peroxide cautiously: If you must use hydrogen peroxide, dilute it to a low concentration (1-3%), and apply it only to the affected area, avoiding the seedlings. Remember that hydrogen peroxide can be harmful to some plants, so use it with caution and dilute it properly to minimize risk. Always prioritize the health and safety of your seedlings, and avoid using harsh chemicals or treatments that could harm them. Focus on maintaining a clean and well-lit environment to encourage healthy growth and prevent mold.</span>
</span>
</td>
</tr>
<tr id="S3.T14.1.1.3" class="ltx_tr">
<td id="S3.T14.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">MiniGPT-4 (<span id="S3.T14.1.1.3.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.T14.1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T14.1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T14.1.1.3.2.1.1" class="ltx_p" style="width:455.2pt;">It’s important to take mold in your cactus seedlings seriously and take measures to prevent its spread. While hydrogen peroxide (H2O2) is known for its antimicrobial properties, it can be harmful to plants, especially at high concentrations. I would not recommend using H2O2 to get rid of mold in your seedlings. Instead, you can use the following methods: 1. Water and sunlight: Water the affected area thoroughly and place it in direct sunlight. The sun’s UV rays can help to kill the mold spores. 2. Vinegar: Mix a solution of equal parts water and white vinegar in a spray bottle, and spray the mold off the seedlings. Vinegar is a natural fungicide. 3. Bleach: Mix a solution of 1 part bleach and 9 parts water and spray it onto the seedlings. Be careful not to overdose the bleach, as it can be harmful to plants and the environment. Remember, after treating the mold, to thoroughly clean and disinfect the affected area and surrounding areas to prevent the mold from re-appearing. Also keep the humidity and temperature of your grow area in check, to prevent mold from appearing again. Please let me know if you</span>
</span>
</td>
</tr>
<tr id="S3.T14.1.1.4" class="ltx_tr">
<td id="S3.T14.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t">InstructBLIP (<span id="S3.T14.1.1.4.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.T14.1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T14.1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T14.1.1.4.2.1.1" class="ltx_p" style="width:455.2pt;">The cactus seedlings in the image were sown a little bit less than a month ago. As you can see, there’s mold starting to form. I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination. As you can see, there’s mold starting to form. I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination. What would be an easy way to get rid of the mold without hurting the seedlings? Someone suggested to use hydrogen peroxide, but I’m not sure it’s safe for the seedlings. Ideas?</span>
</span>
</td>
</tr>
<tr id="S3.T14.1.1.5" class="ltx_tr">
<td id="S3.T14.1.1.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">BLIP2 (<span id="S3.T14.1.1.5.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.T14.1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T14.1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T14.1.1.5.2.1.1" class="ltx_p" style="width:455.2pt;">I’m using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination I am using the baggie method whereby the pot with the seeds is enclosed in a Ziploc bag, to maintain high humidity and protect against contamination.</span>
</span>
</td>
</tr>
</table>
</span></div>
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T14.3.1.1" class="ltx_text" style="font-size:90%;">Table 14</span>: </span><span id="S3.T14.4.2" class="ltx_text" style="font-size:90%;">A visual question when the question and context are misleading while the image contains true information. Model answers are from LLaVA, MiniGPT-4, InstructBLIP, and BLIP2, all misled by question and context. Models’ answers verified by a Ph.D. in Botany major. </span></figcaption>
</figure>
<figure id="S3.SS4.5" class="ltx_table">
<figure id="S3.SS4.5.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:491.9pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.SS4.5.1.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:520.5pt;height:620.2pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.5pt,42.2pt) scale(0.88,0.88) ;">
<table id="S3.SS4.5.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.SS4.5.1.1.1.2" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="2"><span id="S3.SS4.5.1.1.1.2.1.1" class="ltx_text ltx_font_bold">A visual question requires text recognition skills:</span></td>
</tr>
<tr id="S3.SS4.5.1.1.1.3" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Category</td>
<td id="S3.SS4.5.1.1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.5.1.1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.3.2.1.1" class="ltx_p" style="width:455.2pt;">Chinese Language</span>
</span>
</td>
</tr>
<tr id="S3.SS4.5.1.1.1.1" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.1.2" class="ltx_td"></td>
<td id="S3.SS4.5.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.5.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.1.1.1.1" class="ltx_p" style="width:455.2pt;"><img src="/html/2311.15562/assets/Images/example_images/VQAinW_00006699.png" id="S3.SS4.5.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="295" height="393" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.SS4.5.1.1.1.4" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.4.1" class="ltx_td ltx_align_left">Question</td>
<td id="S3.SS4.5.1.1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.5.1.1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.4.2.1.1" class="ltx_p" style="width:455.2pt;">What is the meaning of 心兴?</span>
</span>
</td>
</tr>
<tr id="S3.SS4.5.1.1.1.5" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.5.1" class="ltx_td ltx_align_left">Context</td>
<td id="S3.SS4.5.1.1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.5.1.1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.5.2.1.1" class="ltx_p" style="width:455.2pt;">What is the meaning of 心兴 in the picture below (towards bottom left in image)? I am guessing it means to think, but doesn’t 想 mean to think?</span>
</span>
</td>
</tr>
<tr id="S3.SS4.5.1.1.1.6" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.6.1" class="ltx_td ltx_align_left">Reference Answer</td>
<td id="S3.SS4.5.1.1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.5.1.1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.6.2.1.1" class="ltx_p" style="width:455.2pt;">That character is not「兴」(to rise &gt; excited, happy), it is「头」(head). 「心头」(literally heart + head) means mind - the picture is saying that you use your mind to think/reason.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.5.1.1.1.7" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t">GPT-4V (<span id="S3.SS4.5.1.1.1.7.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.5.1.1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.5.1.1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.7.2.1.1" class="ltx_p" style="width:455.2pt;">心兴 is a colloquial expression in Chinese. It roughly means "to have a happy mood or personally interested in something". It’s often used in the context of "心兴来了"  (to suddenly become interested or happy) or "心兴去了" (to suddenly lose interest or become unhappy). In the picture, the character is saying "我心兴来了"  which means "I am suddenly interested" or "I am suddenly in a good mood". As for your question about 想 , 想  does mean "to think", but it can also mean "to want" or "to wish". Chinese is a contextual language, so the meaning of a word or phrase can change depending on the context it is used in.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.5.1.1.1.8" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.8.1" class="ltx_td ltx_align_left ltx_border_t">mPLUG-Owl (<span id="S3.SS4.5.1.1.1.8.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.5.1.1.1.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.5.1.1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.8.2.1.1" class="ltx_p" style="width:455.2pt;">Yes, you are correct! The meaning of "心兴" is to be enthusiastic or have enthusiasm. It is often used to describe someone who is excited or passionate about something, such as a new project or idea. The meaning of the phrase "心兴" in the picture is to have enthusiasm for learning. It is a part of a Chinese character that is often used in children’s books and learning materials, such as the colorful picture you have provided. The image shows a young girl holding a book with this character on it, indicating that she is interested in learning and has an enthusiasm for reading and education.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.5.1.1.1.9" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t">LLaVA (<span id="S3.SS4.5.1.1.1.9.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.5.1.1.1.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.5.1.1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.9.2.1.1" class="ltx_p" style="width:455.2pt;">In this context, "心兴" likely means "thinking" or "intelligence" in Chinese. The image, which includes pictures of children with various expressions and a book with Chinese characters, suggests that the scene involves a learning activity or a discussion about the concepts covered in the book. The presence of "心兴" in the picture implies that it is a theme or an important idea related to the content being presented.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.5.1.1.1.10" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.10.1" class="ltx_td ltx_align_left ltx_border_t">MiniGPT-4 (<span id="S3.SS4.5.1.1.1.10.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.5.1.1.1.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.5.1.1.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.10.2.1.1" class="ltx_p" style="width:455.2pt;">"心兴" (xīn xìng)  in the picture is the Chinese word for "heartfelt" or "heartfelt" (a feeling of contentment or happiness). The character for "心" (xīn) means "heart" or "mind," while the character for "兴" (xìng) means "to feel happy or content." Therefore, "心兴" (xīn xìng)  can be translated as "feeling contented or happy." The character 想 (cāng)  in Chinese means to think or to ponder, so it’s not the same as "心兴" (xīn xìng) .</span>
</span>
</td>
</tr>
<tr id="S3.SS4.5.1.1.1.11" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.11.1" class="ltx_td ltx_align_left ltx_border_t">InstructBLIP (<span id="S3.SS4.5.1.1.1.11.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.5.1.1.1.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.5.1.1.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.11.2.1.1" class="ltx_p" style="width:455.2pt;">yes</span>
</span>
</td>
</tr>
<tr id="S3.SS4.5.1.1.1.12" class="ltx_tr">
<td id="S3.SS4.5.1.1.1.12.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">BLIP2 (<span id="S3.SS4.5.1.1.1.12.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.5.1.1.1.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.SS4.5.1.1.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.5.1.1.1.12.2.1.1" class="ltx_p" style="width:455.2pt;">,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,</span>
</span>
</td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.T15" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T15.2.1.1" class="ltx_text" style="font-size:90%;">Table 15</span>: </span><span id="S3.T15.3.2" class="ltx_text" style="font-size:90%;">A visual question related to the Chinese Language and requires text recognition skills. Models’ answers verified by a Chinese native speaker.</span></figcaption>
</figure>
</div>
</div>
</figure>
</figure>
<figure id="S3.SS4.6" class="ltx_table">
<figure id="S3.SS4.6.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:491.9pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.SS4.6.1.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:520.5pt;height:719.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.5pt,49.0pt) scale(0.88,0.88) ;">
<table id="S3.SS4.6.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.SS4.6.1.1.1.2" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="2"><span id="S3.SS4.6.1.1.1.2.1.1" class="ltx_text ltx_font_bold">A visual question requires multi-hop reasoning process:</span></td>
</tr>
<tr id="S3.SS4.6.1.1.1.3" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Category</td>
<td id="S3.SS4.6.1.1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.6.1.1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.3.2.1.1" class="ltx_p" style="width:455.2pt;">Chess</span>
</span>
</td>
</tr>
<tr id="S3.SS4.6.1.1.1.1" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.1.2" class="ltx_td"></td>
<td id="S3.SS4.6.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.6.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.1.1.1.1" class="ltx_p" style="width:455.2pt;"><img src="/html/2311.15562/assets/Images/example_images/VQAinW_00056374.png" id="S3.SS4.6.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="237" height="236" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.SS4.6.1.1.1.4" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.4.1" class="ltx_td ltx_align_left">Question</td>
<td id="S3.SS4.6.1.1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.6.1.1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.4.2.1.1" class="ltx_p" style="width:455.2pt;">Devouring Nobility</span>
</span>
</td>
</tr>
<tr id="S3.SS4.6.1.1.1.5" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.5.1" class="ltx_td ltx_align_left">Context</td>
<td id="S3.SS4.6.1.1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.6.1.1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.5.2.1.1" class="ltx_p" style="width:455.2pt;">Provide, with the minimum number of moves, a legal chess game where all pieces are captured except the sixteen pawns and the two kings. Example of final configuration: Which is done in 21 moves: 1. e4 Nc6 2. Bb5 e5 3. Bxc6 dxc6 4. Nf3 Bg4 5. h4 Bxf3 6. Rh3 Bxd1 7. Kxd1 f5 8. Rg3 Nf6 9. Rg4 fxg4 10. d4 Ba3 11. Bg5 Qd5 12. Bxf6 Qb3 13. bxa3 Rd8 14. Bxd8 Kf7 15. axb3 Rxd8 16. Nc3 Rd5 17. exd5 Kf6 18. Nb5 cxb5 19. Kd2 Kf5 20. Re1 Kf4 21. Re4+ Kxe4 Can you beat 21 moves?</span>
</span>
</td>
</tr>
<tr id="S3.SS4.6.1.1.1.6" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.6.1" class="ltx_td ltx_align_left">Reference Answer</td>
<td id="S3.SS4.6.1.1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.SS4.6.1.1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.6.2.1.1" class="ltx_p" style="width:455.2pt;">13 moves: 1. e3 e6 2. Na3 Bxa3 3. bxa3 Na6 4. Bxa6 bxa6 5. Rb1 Qf6 6. Rb8 Qa1 7. Rxc8+ Ke7 8. Ne2 Qxc1 9. O-O Qxd1 10. Rxg8 Qxe2 11. Rxh8 Qxf1+ 12. Kxf1 Re8 13. Rxe8+ Kxe8</span>
<span id="S3.SS4.6.1.1.1.6.2.1.2" class="ltx_p">12½: 1. e3 e6 2. Na3 Bxa3 3. bxa3 Na6 4. Bxa6 bxa6 5. Qf3 Qf6 6. Qxa8 Qxa1 7. Qxc8+ Ke7 8. Qxg8 Qxc1+ 9. Ke2 Qxg1 10. Qxh8 Qxh1 11. Qe8+ Kxe8 12. a4 Qe1+ 13. Kxe1 Try it online!</span>
</span>
</td>
</tr>
<tr id="S3.SS4.6.1.1.1.7" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t">GPT-4V (<span id="S3.SS4.6.1.1.1.7.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.6.1.1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.6.1.1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.7.2.1.1" class="ltx_p" style="width:455.2pt;">Yes, it can be done in 19 moves: 1. e4 Nc6 2. Bc4 e5 3. Bxf7+ Kxf7 4. Qh5+ g6 5. Qxe5 Nxe5 6. d4 Ng4 7. h4 Nh2 8. Rxh2 d5 9. Bg5 Qxg5 10. hxg5 Bf5 11. gxf6 Bc5 12. exf5 Bxd4 13. Rxh7+ Rxh7 14. fxg6+ Kxg6 15. Nf3 Rf8 16. Nxd4 Rxf6 17. Nd2 Rh1+ 18. Nxh1 Rf4 19. Ng3 Kg5 (Note: There may be even more efficient solutions, but this is the best I was able to come up with.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.6.1.1.1.8" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.8.1" class="ltx_td ltx_align_left ltx_border_t">mPLUG-Owl (<span id="S3.SS4.6.1.1.1.8.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.6.1.1.1.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.6.1.1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.8.2.1.1" class="ltx_p" style="width:455.2pt;">Yes, it is possible to beat the given game of chess with 19 moves. The game is in a drawish position, and it’s possible to force a win by sacrificing a piece and exploiting some of the tactics present in the game.</span>
<span id="S3.SS4.6.1.1.1.8.2.1.2" class="ltx_p">Here are the moves to beat the game:</span>
<span id="S3.SS4.6.1.1.1.8.2.1.3" class="ltx_p">1. e4 Nc6 2. Bb5 e5 3. Bxc6 dxc6 4. Nf3 Bg4 5. h4 Bxf3 6. Rh3 Bxd1 7. Kxd1 f5 8. Rh4 fxg4 9. d4 Ba3 10. Bg5 Qd5 11. Bxf6 Qb3 12. bxa3 Rd8 13. Bxd8 Kf7 14. axb3 Rxd8 15. Nc3 Rd5 16. exd5 Kf6 17. Nb5 cxb5 18. Kd2 Kf5 19. Re1 Kf4 (19. Re4+ Kxe4 would lead to a stalemate as white can capture the king with checkmate on the next turn. The move 19. Re1 Kf4 is therefore a better option to avoid stalemate).</span>
</span>
</td>
</tr>
<tr id="S3.SS4.6.1.1.1.9" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t">LLaVA (<span id="S3.SS4.6.1.1.1.9.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.6.1.1.1.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.6.1.1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.9.2.1.1" class="ltx_p" style="width:455.2pt;">I cannot play a game of chess directly with you, but I can provide you with a solution to the problem you have presented in the image. The image shows a chess board with a total of 16 pawns and two kings, with a total of 18 pieces on the board. In order to capture all the pieces except for the two kings and the 16 pawns, the following sequence of moves can be followed:</span>
<span id="S3.SS4.6.1.1.1.9.2.1.2" class="ltx_p">1. e4 Nc6 2. Bb5 e5 3. Bxc6 dxc6 4. Nf3 Bg4 5. h4 Bxf3 6. Rh3 Bxd1 7. Kxd1 f5 8. Rg3 Nf6 9. Rg4 fxg4 10. d4 Ba3 11. Bg5 Qd5 12. Bxf6 Qb3 13. bxa3 Rd8 14. Bxd8 Kf7 15. axb3 Rxd8 16. Nc3 Rd5 17. exd5 Kf6 18. Nb5 cxb5 19. Kd2 Kf5 20. Re1 Kf4 21. Re4+ Kxe4</span>
<span id="S3.SS4.6.1.1.1.9.2.1.3" class="ltx_p">This sequence of moves will capture all the pieces except for the two kings and the 16 pawns. The solution has been found using the minimax algorithm, which is an optimal algorithm for solving two-player games such as chess.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.6.1.1.1.10" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.10.1" class="ltx_td ltx_align_left ltx_border_t">MiniGPT-4 (<span id="S3.SS4.6.1.1.1.10.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.6.1.1.1.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.6.1.1.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.10.2.1.1" class="ltx_p" style="width:455.2pt;">As an AI language model, I can certainly analyze the position you provided and try to find a continuation that may lead to a win for White, but it’s important to note that I am not a chess engine and my analysis will not be as deep or as accurate as a specialized chess engine like Stockfish.</span>
<span id="S3.SS4.6.1.1.1.10.2.1.2" class="ltx_p">Given the position you provided, I can see that Black’s king is exposed and vulnerable to a quick checkmate. One move that may be try to force a win for White is 21. Rh8+. This move aims to put the king in check and force black king to move, and then exploit the exposed king with a series of moves like Rh6, Rh5, and Rh4, putting pressure on the king and threatening a checkmate. It is also possible to try to exploit the open position with a pawn storm with moves like e4 and e5, to put pressure on the black king and try to win the game.</span>
<span id="S3.SS4.6.1.1.1.10.2.1.3" class="ltx_p">It’s important to note that this is just one possible move and that a skilled chess player player could find other other other moves to try to to try to win the the game. Also, Stockfish or other chess engines will likely provide a more detailed analysis of the position and a better best continuation.</span>
</span>
</td>
</tr>
<tr id="S3.SS4.6.1.1.1.11" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.11.1" class="ltx_td ltx_align_left ltx_border_t">InstructBLIP (<span id="S3.SS4.6.1.1.1.11.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.6.1.1.1.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.SS4.6.1.1.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.11.2.1.1" class="ltx_p" style="width:455.2pt;">Chess</span>
</span>
</td>
</tr>
<tr id="S3.SS4.6.1.1.1.12" class="ltx_tr">
<td id="S3.SS4.6.1.1.1.12.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">BLIP2 (<span id="S3.SS4.6.1.1.1.12.1.1" class="ltx_text" style="color:#FF0000;">Incorrect</span>)</td>
<td id="S3.SS4.6.1.1.1.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.SS4.6.1.1.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.SS4.6.1.1.1.12.2.1.1" class="ltx_p" style="width:455.2pt;">bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 bxa3 b .</span>
</span>
</td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.T16" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T16.2.1.1" class="ltx_text" style="font-size:90%;">Table 16</span>: </span><span id="S3.T16.3.2" class="ltx_text" style="font-size:90%;">A visual question requires a significant reasoning process.</span></figcaption>
</figure>
</div>
</div>
</figure>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.15561" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.15562" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.15562">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.15562" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.15563" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 17:11:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
