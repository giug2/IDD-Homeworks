<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2209.05401] MaXM: Towards Multilingual Visual Question Answering</title><meta property="og:description" content="Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require a considerable amount of resources. In this pap…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MaXM: Towards Multilingual Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MaXM: Towards Multilingual Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2209.05401">

<!--Generated on Wed Feb 28 19:28:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MaXM: Towards Multilingual Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Soravit Changpinyo, Linting Xue, Michal Yarom, Ashish V. Thapliyal
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_bold">Idan Szpektor</span>, <span id="id3.2.id2" class="ltx_text ltx_font_bold">Julien Amelot</span>, <span id="id4.3.id3" class="ltx_text ltx_font_bold">Xi Chen</span>, <span id="id5.4.id4" class="ltx_text ltx_font_bold">Radu Soricut</span> 
<br class="ltx_break">Google Research
<br class="ltx_break"><a target="_blank" href="https://github.com/google-research-datasets/maxm" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/google-research-datasets/maxm</a>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require a considerable amount of resources. In this paper, we propose scalable solutions to multilingual visual question answering (mVQA), on both data and modeling fronts. We first propose a translation-based framework to mVQA data generation that requires much less human annotation efforts than the conventional approach of directly collection questions and answers. Then, we apply our framework to the multilingual captions in the Crossmodal-3600 dataset and develop an efficient annotation protocol to create <math id="id1.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="id1.1.m1.1a"><mi id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\mathrm{MaXM}</annotation></semantics></math>, a test-only VQA benchmark in 7 diverse languages. Finally, we develop a simple, lightweight, and effective approach as well as benchmark state-of-the-art English and multilingual VQA models. We hope that our benchmark encourages further research on mVQA.</p>
</div>
<figure id="S0.F1" class="ltx_figure">
<div id="S0.F1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:715.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(59.0pt,-97.4pt) scale(1.37421677383065,1.37421677383065) ;"><img src="/html/2209.05401/assets/x1.png" id="S0.F1.1.g1" class="ltx_graphics ltx_img_portrait" width="333" height="555" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S0.F1.3.1" class="ltx_text ltx_font_bold">Multilingual VQA Data in 7 languages.</span> The data is automatically generated from multilingual captions and then verified and adjusted by humans. From top to bottom: English (en), French (fr), Hindi (hi), Hebrew (iw), Romanian (ro), Thai (th), and Chinese (zh).</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA), the task of answering visual questions grounded in images, is key to human-machine interaction in the visual world. In particular, the natural language interface in VQA makes it easy for lay people to express their needs and benefit from its applications, including accessibility, education, and search.
Yet, VQA advances were mostly focused on English, therefore only applied to a privileged subset of human populations.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Arguably, the English language has dominated the field mainly because of the availability of English VQA benchmarks. These benchmarks are diverse, from general VQA <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib69" title="" class="ltx_ref">2016</a>); Kafle &amp; Kanan (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>); Krishna et al. (<a href="#bib.bib30" title="" class="ltx_ref">2017</a>); Antol et al. (<a href="#bib.bib5" title="" class="ltx_ref">2015</a>); Goyal et al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>); Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>, robust VQA <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>, compositional visual reasoning <cite class="ltx_cite ltx_citemacro_cite">Hudson &amp; Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>, for the blind and the visually-impaired <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>, scene-text understanding <cite class="ltx_cite ltx_citemacro_cite">Singh et al. (<a href="#bib.bib55" title="" class="ltx_ref">2019</a>); Biten et al. (<a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite>, to VQA that requires external, commonsense, or world knowledge <cite class="ltx_cite ltx_citemacro_cite">Marino et al. (<a href="#bib.bib41" title="" class="ltx_ref">2019</a>); Zellers et al. (<a href="#bib.bib68" title="" class="ltx_ref">2019</a>); Schwenk et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022</a>)</cite>. These benchmarks require considerable amount of resources to create, mostly by employing human annotators to laboriously collect and verify the questions and the answers for each image.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To extend VQA to all languages in the world, we must make data creation more automatic.
Building on recent work on automatic data creation for English VQA from captions <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>, in this paper we propose a translation-based framework for multilingual visual question answering (mVQA) data creation. Our framework automates much of the task of generating questions and answers, thus providing a scalable path to mVQA.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.4" class="ltx_p">We apply our framework to the generation of question-answer pairs from the multilingual captions of the recently-proposed Crossmodal-3600 dataset (<math id="S1.p4.1.m1.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="S1.p4.1.m1.1a"><mi id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><ci id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">\mathrm{XM3600}</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">Thapliyal et al. (<a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>. Combined with an efficient human annotation protocol, we construct <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="\mathrm{MAVERICS}" display="inline"><semantics id="S1.p4.2.m2.1a"><mi id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">MAVERICS</mi><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><ci id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">MAVERICS</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">\mathrm{MAVERICS}</annotation></semantics></math>-<math id="S1.p4.3.m3.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="S1.p4.3.m3.1a"><mi id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.1b"><ci id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.3.m3.1c">\mathrm{XM3600}</annotation></semantics></math> (<math id="S1.p4.4.m4.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S1.p4.4.m4.1a"><mi id="S1.p4.4.m4.1.1" xref="S1.p4.4.m4.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S1.p4.4.m4.1b"><ci id="S1.p4.4.m4.1.1.cmml" xref="S1.p4.4.m4.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.4.m4.1c">\mathrm{MaXM}</annotation></semantics></math>), a test benchmark for mVQA in 7 languages (see examples in Fig. <a href="#S0.F1" title="Figure 1 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Finally, we use this novel benchmark to drive progress in mVQA modeling and measure where we stand. We leverage advances in image modeling and multilingual modeling: ViT <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> and mT5 <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite> and propose a unified, extensible, open-ended mVQA model, called Simple MPT, which is competitive to state-of-the-art English VQA models that we adapt to apply in the mVQA setting (OFA <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">ofa</span></cite> and BLIP2 <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">blip2</span></cite>).
Overall, there exists a large room for improvement.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.4" class="ltx_p">In summary, our main contributions are
(i) a scalable translation-based framework for mVQA data generation based on captions (Sect. <a href="#S3" title="3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>);
(ii) an efficient annotation protocol, deriving a novel test benchmark called <math id="S1.p6.1.m1.1" class="ltx_Math" alttext="\mathrm{MAVERICS}" display="inline"><semantics id="S1.p6.1.m1.1a"><mi id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">MAVERICS</mi><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><ci id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">MAVERICS</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\mathrm{MAVERICS}</annotation></semantics></math>-<math id="S1.p6.2.m2.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="S1.p6.2.m2.1a"><mi id="S1.p6.2.m2.1.1" xref="S1.p6.2.m2.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="S1.p6.2.m2.1b"><ci id="S1.p6.2.m2.1.1.cmml" xref="S1.p6.2.m2.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">\mathrm{XM3600}</annotation></semantics></math> (<math id="S1.p6.3.m3.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S1.p6.3.m3.1a"><mi id="S1.p6.3.m3.1.1" xref="S1.p6.3.m3.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S1.p6.3.m3.1b"><ci id="S1.p6.3.m3.1.1.cmml" xref="S1.p6.3.m3.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.3.m3.1c">\mathrm{MaXM}</annotation></semantics></math>) in 7 diverse languages: English, French, Hindi, Hebrew, Romanian, Thai and Chinese (Sect. <a href="#S4" title="4 MaXM: Multilingual VQA Benchmark ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>);
(iii) simple and lightweight mVQA modeling (Sect. <a href="#S5.SS2" title="5.2 Models for Multilingual VQA ‣ 5 Evaluation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, Sect. <a href="#A3" title="Appendix C Simple MPT ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>) with strong performance;
(iv) benchmarking (adaptations of) the state-of-the-art VQA models on <math id="S1.p6.4.m4.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S1.p6.4.m4.1a"><mi id="S1.p6.4.m4.1.1" xref="S1.p6.4.m4.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S1.p6.4.m4.1b"><ci id="S1.p6.4.m4.1.1.cmml" xref="S1.p6.4.m4.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.4.m4.1c">\mathrm{MaXM}</annotation></semantics></math> (Sect. <a href="#S5.SS3" title="5.3 Results ‣ 5 Evaluation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>VQA and Multilingual Multimodal Benchmarks</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">English has been the primary language in which vision-and-language researchers study the VQA task, driven by the availability of data and benchmarks <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib69" title="" class="ltx_ref">2016</a>); Kafle &amp; Kanan (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>); Krishna et al. (<a href="#bib.bib30" title="" class="ltx_ref">2017</a>); Antol et al. (<a href="#bib.bib5" title="" class="ltx_ref">2015</a>); Goyal et al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>); Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>); Gurari et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>); Marino et al. (<a href="#bib.bib41" title="" class="ltx_ref">2019</a>); Singh et al. (<a href="#bib.bib55" title="" class="ltx_ref">2019</a>); Biten et al. (<a href="#bib.bib8" title="" class="ltx_ref">2019</a>); Sheng et al. (<a href="#bib.bib54" title="" class="ltx_ref">2021</a>); Li et al. (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>); Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>.
The only exception is xGQA <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>, an extension of the English GQA dataset <cite class="ltx_cite ltx_citemacro_cite">Hudson &amp; Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>. xGQA consists of human translations of the 12,578 English questions from 398 images in the balanced testdev split of GQA in 8 typologically diverse languages: English, German, Portuguese, Russian, Indonesian, Bengali, Korean, and Chinese. Besides the differences in the languages considered, our proposed approach to mVQA data creation complements xGQA (see Sect. <a href="#S4.SS4" title="4.4 Analysis and Discussion ‣ 4 MaXM: Multilingual VQA Benchmark ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.2" class="ltx_p">Beyond mVQA, training and evaluation data for multilingual multimodal models is limited. For a review of previous work, we refer the reader to the Image-Grounded
Language Understanding Evaluation (IGLUE) benchmark <cite class="ltx_cite ltx_citemacro_cite">Bugliarello et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>, where xGQA is a part of.
In general, early attempts often focus on Chinese <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib37" title="" class="ltx_ref">2019</a>); Wang et al. (<a href="#bib.bib62" title="" class="ltx_ref">2019</a>)</cite>, Japanese <cite class="ltx_cite ltx_citemacro_cite">Yoshikawa et al. (<a href="#bib.bib67" title="" class="ltx_ref">2017</a>); Aggarwal &amp; Kale (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> and several Indo-European languages (e.g., German, French, and Czech) <cite class="ltx_cite ltx_citemacro_cite">Elliott et al. (<a href="#bib.bib17" title="" class="ltx_ref">2016</a>, <a href="#bib.bib18" title="" class="ltx_ref">2017</a>); Barrault et al. (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>.
However, there is a recent effort toward a wider variety of both languages and tasks.
Examples include image retrieval <cite class="ltx_cite ltx_citemacro_cite">Aggarwal &amp; Kale (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> (also Russian, Korean, Turkish), visual natural language inference <cite class="ltx_cite ltx_citemacro_cite">Bugliarello et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite> (also Arabic), multilingual visual reasoning <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite> (also Indonesian, Swahili, Tamil, Turkish), and vision-and-language navigation <cite class="ltx_cite ltx_citemacro_cite">Ku et al. (<a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite> (also Hindi, Telugu).
Notably, Wikipedia Image Text (WIT) <cite class="ltx_cite ltx_citemacro_cite">Srinivasan et al. (<a href="#bib.bib56" title="" class="ltx_ref">2021</a>)</cite> provides a large-scale image-text dataset in 108 languages, automatically collected form Wikipedia, and Crossmodal-3600 (<math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\mathrm{XM3600}</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">Thapliyal et al. (<a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite> provides human-curated test-only image captions in 36 languages.
Our work builds on top of <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\mathrm{XM3600}</annotation></semantics></math>, and the 7 languages that we consider are typologically, genealogically, and geographically diverse.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>VQA Data Creation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Previous work on VQA data creation relies heavily on humans to create questions and answers <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib69" title="" class="ltx_ref">2016</a>); Krishna et al. (<a href="#bib.bib30" title="" class="ltx_ref">2017</a>); Goyal et al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>); Gurari et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>); Marino et al. (<a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite>. Some works attempt to automate this process. CLEVR <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib24" title="" class="ltx_ref">2017a</a>)</cite> uses a template-based approach, but it is based on synthetic images for which ground-truth annotations are available. GQA <cite class="ltx_cite ltx_citemacro_cite">Hudson &amp; Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> follows a similar approach but instead starts from Visual Genome scene graphs <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite>, which themselves require large annotation efforts.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">More relevant are works that rewrite image captions or video transcripts as question-answer pairs. COCOQA <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib48" title="" class="ltx_ref">2015</a>)</cite> uses a template-based approach that can only generate questions with one-word answers. WeaQA <cite class="ltx_cite ltx_citemacro_cite">Banerjee et al. (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> improves upon this with semantic role labeling, paraphrasing, and backtranslation.
Recently, <cite class="ltx_cite ltx_citemacro_citet">Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a href="#bib.bib66" title="" class="ltx_ref">2021</a>)</cite> leverage T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite> fine-tuned on question answering datasets, generating large-scale VQA datasets for images and videos, respectively. Our approach to mVQA data creation leverages <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mrow id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><msup id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml"><mi id="S2.SS2.p2.1.m1.1.1.2.2" xref="S2.SS2.p2.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S2.SS2.p2.1.m1.1.1.2.3" xref="S2.SS2.p2.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S2.SS2.p2.1.m1.1.1.1" xref="S2.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><times id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1.1"></times><apply id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.2.1.cmml" xref="S2.SS2.p2.1.m1.1.1.2">superscript</csymbol><ci id="S2.SS2.p2.1.m1.1.1.2.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S2.SS2.p2.1.m1.1.1.2.3.cmml" xref="S2.SS2.p2.1.m1.1.1.2.3">2</cn></apply><ci id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>, the approach in  <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> (Sect. <a href="#S3.SS1" title="3.1 Background: VQ²⁢A ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). To the best of our knowledge, besides xGQA, no other prior work on VQA data generation considered languages beyond English.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div id="S2.F2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:189.8pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.7pt,61.5pt) scale(0.606062889003503,0.606062889003503) ;"><img src="/html/2209.05401/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="611" height="333" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.3.1" class="ltx_text ltx_font_bold">Our approach to multilingual VQA data generation</span>, which is easy to scale, highly automatic and only requiring humans to modify “Almost Correct” questions or correct/expand answers (left) or filter out “Incorrect” questions(right). MT is short for automatic machine translation.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Multilingual VQA Data Creation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Like in many other machine learning tasks, the main bottleneck to mVQA is obtaining high-quality labeled data. The most popular data collection framework to English VQA is to ask a set of human annotators to come up with visual questions, and another set of annotator to answer them (Sect. <a href="#S2.SS2" title="2.2 VQA Data Creation ‣ 2 Related Work ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>). To scale VQA to all languages, we argue that mVQA data creation must significantly reduce its use of human annotation. To this end, we study the extension of an automatic English VQA data creation method called <span id="S3.p1.1.1" class="ltx_text ltx_font_bold">V</span>isual <span id="S3.p1.1.2" class="ltx_text ltx_font_bold">Q</span>uestion Generation with <span id="S3.p1.1.3" class="ltx_text ltx_font_bold">Q</span>uestion <span id="S3.p1.1.4" class="ltx_text ltx_font_bold">A</span>nswering validation, or <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><msup id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml"><mi id="S3.p1.1.m1.1.1.2.2" xref="S3.p1.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S3.p1.1.m1.1.1.2.3" xref="S3.p1.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><times id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></times><apply id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.2.1.cmml" xref="S3.p1.1.m1.1.1.2">superscript</csymbol><ci id="S3.p1.1.m1.1.1.2.2.cmml" xref="S3.p1.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S3.p1.1.m1.1.1.2.3.cmml" xref="S3.p1.1.m1.1.1.2.3">2</cn></apply><ci id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> for the purpose of mVQA data creation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Background: <math id="S3.SS1.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS1.1.m1.1b"><mrow id="S3.SS1.1.m1.1.1" xref="S3.SS1.1.m1.1.1.cmml"><msup id="S3.SS1.1.m1.1.1.2" xref="S3.SS1.1.m1.1.1.2.cmml"><mi id="S3.SS1.1.m1.1.1.2.2" xref="S3.SS1.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S3.SS1.1.m1.1.1.2.3" xref="S3.SS1.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS1.1.m1.1.1.1" xref="S3.SS1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS1.1.m1.1.1.3" xref="S3.SS1.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.1.m1.1c"><apply id="S3.SS1.1.m1.1.1.cmml" xref="S3.SS1.1.m1.1.1"><times id="S3.SS1.1.m1.1.1.1.cmml" xref="S3.SS1.1.m1.1.1.1"></times><apply id="S3.SS1.1.m1.1.1.2.cmml" xref="S3.SS1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.1.m1.1.1.2.1.cmml" xref="S3.SS1.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS1.1.m1.1.1.2.2.cmml" xref="S3.SS1.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S3.SS1.1.m1.1.1.2.3.cmml" xref="S3.SS1.1.m1.1.1.2.3">2</cn></apply><ci id="S3.SS1.1.m1.1.1.3.cmml" xref="S3.SS1.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.1.m1.1d">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><msup id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2.2" xref="S3.SS1.p1.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S3.SS1.p1.1.m1.1.1.2.3" xref="S3.SS1.p1.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><apply id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.2.3">2</cn></apply><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> approach leverages aligned image-text data sources that are available at scale <cite class="ltx_cite ltx_citemacro_cite">Ordonez et al. (<a href="#bib.bib42" title="" class="ltx_ref">2011</a>); Chen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2015</a>); Sharma et al. (<a href="#bib.bib52" title="" class="ltx_ref">2018</a>); Pont-Tuset et al. (<a href="#bib.bib44" title="" class="ltx_ref">2020</a>); Changpinyo et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>); Desai et al. (<a href="#bib.bib15" title="" class="ltx_ref">2021</a>); Schuhmann et al. (<a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite> and beyond English <cite class="ltx_cite ltx_citemacro_cite">Srinivasan et al. (<a href="#bib.bib56" title="" class="ltx_ref">2021</a>); Gu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>. It rewrites a declarative image caption into multiple interrogative question-answer pairs via three steps: (i) <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">Candidate Answer Extraction</em> extracts candidate answers based on syntactic and semantic analysis of an input caption, (ii) <em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">Question Generation</em> generates candidate questions for each candidate answer, (iii) <em id="S3.SS1.p1.1.3" class="ltx_emph ltx_font_italic">Answer Validation</em> filters candidate questions that do not pass a consistency check that involves automatically answering each question from the caption and comparing this answer to the original extracted answer <cite class="ltx_cite ltx_citemacro_cite">Alberti et al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>); Honovich et al. (<a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Each step in <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><msup id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2.2" xref="S3.SS1.p2.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S3.SS1.p2.1.m1.1.1.2.3" xref="S3.SS1.p2.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><times id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></times><apply id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.2.1.cmml" xref="S3.SS1.p2.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S3.SS1.p2.1.m1.1.1.2.3.cmml" xref="S3.SS1.p2.1.m1.1.1.2.3">2</cn></apply><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> is optimized for English; Step (i) uses English spaCy and both Step (ii) and Step (iii) leverage high-capacity English-pre-trained T5 models fine-tuned on English question answering datasets <cite class="ltx_cite ltx_citemacro_cite">Rajpurkar et al. (<a href="#bib.bib46" title="" class="ltx_ref">2016</a>, <a href="#bib.bib47" title="" class="ltx_ref">2018</a>); Kwiatkowski et al. (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Translation-based <math id="S3.SS2.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS2.1.m1.1b"><mrow id="S3.SS2.1.m1.1.1" xref="S3.SS2.1.m1.1.1.cmml"><msup id="S3.SS2.1.m1.1.1.2" xref="S3.SS2.1.m1.1.1.2.cmml"><mi id="S3.SS2.1.m1.1.1.2.2" xref="S3.SS2.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S3.SS2.1.m1.1.1.2.3" xref="S3.SS2.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.1.m1.1.1.1" xref="S3.SS2.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.1.m1.1.1.3" xref="S3.SS2.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.1.m1.1c"><apply id="S3.SS2.1.m1.1.1.cmml" xref="S3.SS2.1.m1.1.1"><times id="S3.SS2.1.m1.1.1.1.cmml" xref="S3.SS2.1.m1.1.1.1"></times><apply id="S3.SS2.1.m1.1.1.2.cmml" xref="S3.SS2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.1.m1.1.1.2.1.cmml" xref="S3.SS2.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS2.1.m1.1.1.2.2.cmml" xref="S3.SS2.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S3.SS2.1.m1.1.1.2.3.cmml" xref="S3.SS2.1.m1.1.1.2.3">2</cn></apply><ci id="S3.SS2.1.m1.1.1.3.cmml" xref="S3.SS2.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.1.m1.1d">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> (<math id="S3.SS2.2.m2.1" class="ltx_Math" alttext="\mathrm{TransVQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS2.2.m2.1b"><mrow id="S3.SS2.2.m2.1.1" xref="S3.SS2.2.m2.1.1.cmml"><msup id="S3.SS2.2.m2.1.1.2" xref="S3.SS2.2.m2.1.1.2.cmml"><mi id="S3.SS2.2.m2.1.1.2.2" xref="S3.SS2.2.m2.1.1.2.2.cmml">TransVQ</mi><mn id="S3.SS2.2.m2.1.1.2.3" xref="S3.SS2.2.m2.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.2.m2.1.1.1" xref="S3.SS2.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.2.m2.1.1.3" xref="S3.SS2.2.m2.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.2.m2.1c"><apply id="S3.SS2.2.m2.1.1.cmml" xref="S3.SS2.2.m2.1.1"><times id="S3.SS2.2.m2.1.1.1.cmml" xref="S3.SS2.2.m2.1.1.1"></times><apply id="S3.SS2.2.m2.1.1.2.cmml" xref="S3.SS2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.2.m2.1.1.2.1.cmml" xref="S3.SS2.2.m2.1.1.2">superscript</csymbol><ci id="S3.SS2.2.m2.1.1.2.2.cmml" xref="S3.SS2.2.m2.1.1.2.2">TransVQ</ci><cn type="integer" id="S3.SS2.2.m2.1.1.2.3.cmml" xref="S3.SS2.2.m2.1.1.2.3">2</cn></apply><ci id="S3.SS2.2.m2.1.1.3.cmml" xref="S3.SS2.2.m2.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.2.m2.1d">\mathrm{TransVQ}^{2}\!\mathrm{A}</annotation></semantics></math>)</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">Inspired by <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msup id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3">2</cn></apply><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>, our goal is to generate mVQA data at scale, leveraging multilingual image captions. Multilingualizing each step in <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><msup id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2.2" xref="S3.SS2.p1.2.m2.1.1.2.2.cmml">VQ</mi><mn id="S3.SS2.p1.2.m2.1.1.2.3" xref="S3.SS2.p1.2.m2.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><apply id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2">VQ</ci><cn type="integer" id="S3.SS2.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3">2</cn></apply><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> can be non-trivial and resource-intensive due to the heavy reliance of English tools, models, and data (Sect. <a href="#S3.SS1" title="3.1 Background: VQ²⁢A ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). To alleviate this, we propose a translation-based extension of <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><msup id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2.2" xref="S3.SS2.p1.3.m3.1.1.2.2.cmml">VQ</mi><mn id="S3.SS2.p1.3.m3.1.1.2.3" xref="S3.SS2.p1.3.m3.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><times id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></times><apply id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.1.2">superscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2.2">VQ</ci><cn type="integer" id="S3.SS2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3">2</cn></apply><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.19" class="ltx_p">Given an input caption <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">c</annotation></semantics></math> in any language, and a target language <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\langle\mathrm{lang}\rangle" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.2.2" xref="S3.SS2.p2.2.m2.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.2.2.1" xref="S3.SS2.p2.2.m2.1.2.1.1.cmml">⟨</mo><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">lang</mi><mo stretchy="false" id="S3.SS2.p2.2.m2.1.2.2.2" xref="S3.SS2.p2.2.m2.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.2.2"><csymbol cd="latexml" id="S3.SS2.p2.2.m2.1.2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">lang</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\langle\mathrm{lang}\rangle</annotation></semantics></math>, we want to generate question-answer pairs in <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\langle\mathrm{lang}\rangle" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.2.2" xref="S3.SS2.p2.3.m3.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.1.2.2.1" xref="S3.SS2.p2.3.m3.1.2.1.1.cmml">⟨</mo><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">lang</mi><mo stretchy="false" id="S3.SS2.p2.3.m3.1.2.2.2" xref="S3.SS2.p2.3.m3.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.2.2"><csymbol cd="latexml" id="S3.SS2.p2.3.m3.1.2.1.1.cmml" xref="S3.SS2.p2.3.m3.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">lang</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\langle\mathrm{lang}\rangle</annotation></semantics></math>. We propose Translation-based <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><msup id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2.2" xref="S3.SS2.p2.4.m4.1.1.2.2.cmml">VQ</mi><mn id="S3.SS2.p2.4.m4.1.1.2.3" xref="S3.SS2.p2.4.m4.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></times><apply id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2.2">VQ</ci><cn type="integer" id="S3.SS2.p2.4.m4.1.1.2.3.cmml" xref="S3.SS2.p2.4.m4.1.1.2.3">2</cn></apply><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> (<math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\mathrm{TransVQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><msup id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2.2" xref="S3.SS2.p2.5.m5.1.1.2.2.cmml">TransVQ</mi><mn id="S3.SS2.p2.5.m5.1.1.2.3" xref="S3.SS2.p2.5.m5.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><times id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></times><apply id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2.2">TransVQ</ci><cn type="integer" id="S3.SS2.p2.5.m5.1.1.2.3.cmml" xref="S3.SS2.p2.5.m5.1.1.2.3">2</cn></apply><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\mathrm{TransVQ}^{2}\!\mathrm{A}</annotation></semantics></math>), as follows:
<em id="S3.SS2.p2.19.2" class="ltx_emph ltx_font_italic"><span id="S3.SS2.p2.19.2.1" class="ltx_text ltx_font_bold">Step 1</span> Caption Translation</em>: Automatically translate a non-English caption <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">c</annotation></semantics></math> to English <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="c_{e}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><msub id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">c</mi><mi id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2">𝑐</ci><ci id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">c_{e}</annotation></semantics></math>.
<em id="S3.SS2.p2.8.1" class="ltx_emph ltx_font_italic"><span id="S3.SS2.p2.8.1.1" class="ltx_text ltx_font_bold">Step 2</span> Apply <math id="S3.SS2.p2.8.1.m1.1" class="ltx_Math" alttext="\mathbf{\mathrm{VQ}^{2}\!\mathrm{A}}" display="inline"><semantics id="S3.SS2.p2.8.1.m1.1a"><mrow id="S3.SS2.p2.8.1.m1.1.1" xref="S3.SS2.p2.8.1.m1.1.1.cmml"><msup id="S3.SS2.p2.8.1.m1.1.1.2" xref="S3.SS2.p2.8.1.m1.1.1.2.cmml"><mi id="S3.SS2.p2.8.1.m1.1.1.2.2" xref="S3.SS2.p2.8.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S3.SS2.p2.8.1.m1.1.1.2.3" xref="S3.SS2.p2.8.1.m1.1.1.2.3.cmml">𝟐</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.1.m1.1.1.1" xref="S3.SS2.p2.8.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p2.8.1.m1.1.1.3" xref="S3.SS2.p2.8.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.1.m1.1b"><apply id="S3.SS2.p2.8.1.m1.1.1.cmml" xref="S3.SS2.p2.8.1.m1.1.1"><times id="S3.SS2.p2.8.1.m1.1.1.1.cmml" xref="S3.SS2.p2.8.1.m1.1.1.1"></times><apply id="S3.SS2.p2.8.1.m1.1.1.2.cmml" xref="S3.SS2.p2.8.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.8.1.m1.1.1.2.1.cmml" xref="S3.SS2.p2.8.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.8.1.m1.1.1.2.2.cmml" xref="S3.SS2.p2.8.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S3.SS2.p2.8.1.m1.1.1.2.3.cmml" xref="S3.SS2.p2.8.1.m1.1.1.2.3">2</cn></apply><ci id="S3.SS2.p2.8.1.m1.1.1.3.cmml" xref="S3.SS2.p2.8.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.1.m1.1c">\mathbf{\mathrm{VQ}^{2}\!\mathrm{A}}</annotation></semantics></math></em>: Generate a set of English question-answer pairs <math id="S3.SS2.p2.9.m8.2" class="ltx_Math" alttext="\{q_{e},a_{e}\}" display="inline"><semantics id="S3.SS2.p2.9.m8.2a"><mrow id="S3.SS2.p2.9.m8.2.2.2" xref="S3.SS2.p2.9.m8.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p2.9.m8.2.2.2.3" xref="S3.SS2.p2.9.m8.2.2.3.cmml">{</mo><msub id="S3.SS2.p2.9.m8.1.1.1.1" xref="S3.SS2.p2.9.m8.1.1.1.1.cmml"><mi id="S3.SS2.p2.9.m8.1.1.1.1.2" xref="S3.SS2.p2.9.m8.1.1.1.1.2.cmml">q</mi><mi id="S3.SS2.p2.9.m8.1.1.1.1.3" xref="S3.SS2.p2.9.m8.1.1.1.1.3.cmml">e</mi></msub><mo id="S3.SS2.p2.9.m8.2.2.2.4" xref="S3.SS2.p2.9.m8.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.9.m8.2.2.2.2" xref="S3.SS2.p2.9.m8.2.2.2.2.cmml"><mi id="S3.SS2.p2.9.m8.2.2.2.2.2" xref="S3.SS2.p2.9.m8.2.2.2.2.2.cmml">a</mi><mi id="S3.SS2.p2.9.m8.2.2.2.2.3" xref="S3.SS2.p2.9.m8.2.2.2.2.3.cmml">e</mi></msub><mo stretchy="false" id="S3.SS2.p2.9.m8.2.2.2.5" xref="S3.SS2.p2.9.m8.2.2.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m8.2b"><set id="S3.SS2.p2.9.m8.2.2.3.cmml" xref="S3.SS2.p2.9.m8.2.2.2"><apply id="S3.SS2.p2.9.m8.1.1.1.1.cmml" xref="S3.SS2.p2.9.m8.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m8.1.1.1.1.1.cmml" xref="S3.SS2.p2.9.m8.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m8.1.1.1.1.2.cmml" xref="S3.SS2.p2.9.m8.1.1.1.1.2">𝑞</ci><ci id="S3.SS2.p2.9.m8.1.1.1.1.3.cmml" xref="S3.SS2.p2.9.m8.1.1.1.1.3">𝑒</ci></apply><apply id="S3.SS2.p2.9.m8.2.2.2.2.cmml" xref="S3.SS2.p2.9.m8.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m8.2.2.2.2.1.cmml" xref="S3.SS2.p2.9.m8.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.9.m8.2.2.2.2.2.cmml" xref="S3.SS2.p2.9.m8.2.2.2.2.2">𝑎</ci><ci id="S3.SS2.p2.9.m8.2.2.2.2.3.cmml" xref="S3.SS2.p2.9.m8.2.2.2.2.3">𝑒</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m8.2c">\{q_{e},a_{e}\}</annotation></semantics></math> from <math id="S3.SS2.p2.10.m9.1" class="ltx_Math" alttext="c_{e}" display="inline"><semantics id="S3.SS2.p2.10.m9.1a"><msub id="S3.SS2.p2.10.m9.1.1" xref="S3.SS2.p2.10.m9.1.1.cmml"><mi id="S3.SS2.p2.10.m9.1.1.2" xref="S3.SS2.p2.10.m9.1.1.2.cmml">c</mi><mi id="S3.SS2.p2.10.m9.1.1.3" xref="S3.SS2.p2.10.m9.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m9.1b"><apply id="S3.SS2.p2.10.m9.1.1.cmml" xref="S3.SS2.p2.10.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.10.m9.1.1.1.cmml" xref="S3.SS2.p2.10.m9.1.1">subscript</csymbol><ci id="S3.SS2.p2.10.m9.1.1.2.cmml" xref="S3.SS2.p2.10.m9.1.1.2">𝑐</ci><ci id="S3.SS2.p2.10.m9.1.1.3.cmml" xref="S3.SS2.p2.10.m9.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m9.1c">c_{e}</annotation></semantics></math>.
<em id="S3.SS2.p2.19.3" class="ltx_emph ltx_font_italic"><span id="S3.SS2.p2.19.3.1" class="ltx_text ltx_font_bold">Step 3</span> Question-Answer Translation</em>: Automatically translate all (<math id="S3.SS2.p2.11.m10.2" class="ltx_Math" alttext="q_{e},a_{e}" display="inline"><semantics id="S3.SS2.p2.11.m10.2a"><mrow id="S3.SS2.p2.11.m10.2.2.2" xref="S3.SS2.p2.11.m10.2.2.3.cmml"><msub id="S3.SS2.p2.11.m10.1.1.1.1" xref="S3.SS2.p2.11.m10.1.1.1.1.cmml"><mi id="S3.SS2.p2.11.m10.1.1.1.1.2" xref="S3.SS2.p2.11.m10.1.1.1.1.2.cmml">q</mi><mi id="S3.SS2.p2.11.m10.1.1.1.1.3" xref="S3.SS2.p2.11.m10.1.1.1.1.3.cmml">e</mi></msub><mo id="S3.SS2.p2.11.m10.2.2.2.3" xref="S3.SS2.p2.11.m10.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.11.m10.2.2.2.2" xref="S3.SS2.p2.11.m10.2.2.2.2.cmml"><mi id="S3.SS2.p2.11.m10.2.2.2.2.2" xref="S3.SS2.p2.11.m10.2.2.2.2.2.cmml">a</mi><mi id="S3.SS2.p2.11.m10.2.2.2.2.3" xref="S3.SS2.p2.11.m10.2.2.2.2.3.cmml">e</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m10.2b"><list id="S3.SS2.p2.11.m10.2.2.3.cmml" xref="S3.SS2.p2.11.m10.2.2.2"><apply id="S3.SS2.p2.11.m10.1.1.1.1.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.11.m10.1.1.1.1.1.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.11.m10.1.1.1.1.2.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.2">𝑞</ci><ci id="S3.SS2.p2.11.m10.1.1.1.1.3.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.3">𝑒</ci></apply><apply id="S3.SS2.p2.11.m10.2.2.2.2.cmml" xref="S3.SS2.p2.11.m10.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.11.m10.2.2.2.2.1.cmml" xref="S3.SS2.p2.11.m10.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.11.m10.2.2.2.2.2.cmml" xref="S3.SS2.p2.11.m10.2.2.2.2.2">𝑎</ci><ci id="S3.SS2.p2.11.m10.2.2.2.2.3.cmml" xref="S3.SS2.p2.11.m10.2.2.2.2.3">𝑒</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m10.2c">q_{e},a_{e}</annotation></semantics></math>) pairs to <math id="S3.SS2.p2.12.m11.1" class="ltx_Math" alttext="\langle\mathrm{lang}\rangle" display="inline"><semantics id="S3.SS2.p2.12.m11.1a"><mrow id="S3.SS2.p2.12.m11.1.2.2" xref="S3.SS2.p2.12.m11.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p2.12.m11.1.2.2.1" xref="S3.SS2.p2.12.m11.1.2.1.1.cmml">⟨</mo><mi id="S3.SS2.p2.12.m11.1.1" xref="S3.SS2.p2.12.m11.1.1.cmml">lang</mi><mo stretchy="false" id="S3.SS2.p2.12.m11.1.2.2.2" xref="S3.SS2.p2.12.m11.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.12.m11.1b"><apply id="S3.SS2.p2.12.m11.1.2.1.cmml" xref="S3.SS2.p2.12.m11.1.2.2"><csymbol cd="latexml" id="S3.SS2.p2.12.m11.1.2.1.1.cmml" xref="S3.SS2.p2.12.m11.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S3.SS2.p2.12.m11.1.1.cmml" xref="S3.SS2.p2.12.m11.1.1">lang</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.12.m11.1c">\langle\mathrm{lang}\rangle</annotation></semantics></math> (<math id="S3.SS2.p2.13.m12.2" class="ltx_Math" alttext="q,a" display="inline"><semantics id="S3.SS2.p2.13.m12.2a"><mrow id="S3.SS2.p2.13.m12.2.3.2" xref="S3.SS2.p2.13.m12.2.3.1.cmml"><mi id="S3.SS2.p2.13.m12.1.1" xref="S3.SS2.p2.13.m12.1.1.cmml">q</mi><mo id="S3.SS2.p2.13.m12.2.3.2.1" xref="S3.SS2.p2.13.m12.2.3.1.cmml">,</mo><mi id="S3.SS2.p2.13.m12.2.2" xref="S3.SS2.p2.13.m12.2.2.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.13.m12.2b"><list id="S3.SS2.p2.13.m12.2.3.1.cmml" xref="S3.SS2.p2.13.m12.2.3.2"><ci id="S3.SS2.p2.13.m12.1.1.cmml" xref="S3.SS2.p2.13.m12.1.1">𝑞</ci><ci id="S3.SS2.p2.13.m12.2.2.cmml" xref="S3.SS2.p2.13.m12.2.2">𝑎</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.13.m12.2c">q,a</annotation></semantics></math>).
<em id="S3.SS2.p2.19.4" class="ltx_emph ltx_font_italic"><span id="S3.SS2.p2.19.4.1" class="ltx_text ltx_font_bold">Step 4</span> Validation</em>: Filter (<math id="S3.SS2.p2.14.m13.2" class="ltx_Math" alttext="q,a" display="inline"><semantics id="S3.SS2.p2.14.m13.2a"><mrow id="S3.SS2.p2.14.m13.2.3.2" xref="S3.SS2.p2.14.m13.2.3.1.cmml"><mi id="S3.SS2.p2.14.m13.1.1" xref="S3.SS2.p2.14.m13.1.1.cmml">q</mi><mo id="S3.SS2.p2.14.m13.2.3.2.1" xref="S3.SS2.p2.14.m13.2.3.1.cmml">,</mo><mi id="S3.SS2.p2.14.m13.2.2" xref="S3.SS2.p2.14.m13.2.2.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.14.m13.2b"><list id="S3.SS2.p2.14.m13.2.3.1.cmml" xref="S3.SS2.p2.14.m13.2.3.2"><ci id="S3.SS2.p2.14.m13.1.1.cmml" xref="S3.SS2.p2.14.m13.1.1">𝑞</ci><ci id="S3.SS2.p2.14.m13.2.2.cmml" xref="S3.SS2.p2.14.m13.2.2">𝑎</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.14.m13.2c">q,a</annotation></semantics></math>) pairs<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Excluding answers to boolean questions.</span></span></span> in which <math id="S3.SS2.p2.15.m14.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS2.p2.15.m14.1a"><mi id="S3.SS2.p2.15.m14.1.1" xref="S3.SS2.p2.15.m14.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.15.m14.1b"><ci id="S3.SS2.p2.15.m14.1.1.cmml" xref="S3.SS2.p2.15.m14.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.15.m14.1c">a</annotation></semantics></math> does not appear in the original caption <math id="S3.SS2.p2.16.m15.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p2.16.m15.1a"><mi id="S3.SS2.p2.16.m15.1.1" xref="S3.SS2.p2.16.m15.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.16.m15.1b"><ci id="S3.SS2.p2.16.m15.1.1.cmml" xref="S3.SS2.p2.16.m15.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.16.m15.1c">c</annotation></semantics></math>, back-translating <math id="S3.SS2.p2.17.m16.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS2.p2.17.m16.1a"><mi id="S3.SS2.p2.17.m16.1.1" xref="S3.SS2.p2.17.m16.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.17.m16.1b"><ci id="S3.SS2.p2.17.m16.1.1.cmml" xref="S3.SS2.p2.17.m16.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.17.m16.1c">a</annotation></semantics></math> to <math id="S3.SS2.p2.18.m17.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p2.18.m17.1a"><mi id="S3.SS2.p2.18.m17.1.1" xref="S3.SS2.p2.18.m17.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.18.m17.1b"><ci id="S3.SS2.p2.18.m17.1.1.cmml" xref="S3.SS2.p2.18.m17.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.18.m17.1c">c</annotation></semantics></math>’s language if necessary. The upper part of Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 VQA Data Creation ‣ 2 Related Work ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> exemplifies <math id="S3.SS2.p2.19.m18.1" class="ltx_Math" alttext="\mathrm{TransVQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS2.p2.19.m18.1a"><mrow id="S3.SS2.p2.19.m18.1.1" xref="S3.SS2.p2.19.m18.1.1.cmml"><msup id="S3.SS2.p2.19.m18.1.1.2" xref="S3.SS2.p2.19.m18.1.1.2.cmml"><mi id="S3.SS2.p2.19.m18.1.1.2.2" xref="S3.SS2.p2.19.m18.1.1.2.2.cmml">TransVQ</mi><mn id="S3.SS2.p2.19.m18.1.1.2.3" xref="S3.SS2.p2.19.m18.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p2.19.m18.1.1.1" xref="S3.SS2.p2.19.m18.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p2.19.m18.1.1.3" xref="S3.SS2.p2.19.m18.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.19.m18.1b"><apply id="S3.SS2.p2.19.m18.1.1.cmml" xref="S3.SS2.p2.19.m18.1.1"><times id="S3.SS2.p2.19.m18.1.1.1.cmml" xref="S3.SS2.p2.19.m18.1.1.1"></times><apply id="S3.SS2.p2.19.m18.1.1.2.cmml" xref="S3.SS2.p2.19.m18.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.19.m18.1.1.2.1.cmml" xref="S3.SS2.p2.19.m18.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.19.m18.1.1.2.2.cmml" xref="S3.SS2.p2.19.m18.1.1.2.2">TransVQ</ci><cn type="integer" id="S3.SS2.p2.19.m18.1.1.2.3.cmml" xref="S3.SS2.p2.19.m18.1.1.2.3">2</cn></apply><ci id="S3.SS2.p2.19.m18.1.1.3.cmml" xref="S3.SS2.p2.19.m18.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.19.m18.1c">\mathrm{TransVQ}^{2}\!\mathrm{A}</annotation></semantics></math> using a Chinese caption from Crossmodal-3600 <cite class="ltx_cite ltx_citemacro_cite">Thapliyal et al. (<a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.4" class="ltx_p">We highlight that the approach we have described so far is fully automatic and applicable to a huge set of languages that are supported by automatic translation.
We note that the final validation is important due errors that could pile up during translation steps. This is especially acute in Step 3, since translating answers is harder due to the lack of disambiguating context in the short answers.
We also note that <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{TransVQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><msup id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2.2" xref="S3.SS2.p3.1.m1.1.1.2.2.cmml">TransVQ</mi><mn id="S3.SS2.p3.1.m1.1.1.2.3" xref="S3.SS2.p3.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><times id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></times><apply id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2.2">TransVQ</ci><cn type="integer" id="S3.SS2.p3.1.m1.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.1.1.2.3">2</cn></apply><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathrm{TransVQ}^{2}\!\mathrm{A}</annotation></semantics></math> can generate question/answer pairs in the target <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="\langle\mathrm{lang}\rangle" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.2.2" xref="S3.SS2.p3.2.m2.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p3.2.m2.1.2.2.1" xref="S3.SS2.p3.2.m2.1.2.1.1.cmml">⟨</mo><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">lang</mi><mo stretchy="false" id="S3.SS2.p3.2.m2.1.2.2.2" xref="S3.SS2.p3.2.m2.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.2.1.cmml" xref="S3.SS2.p3.2.m2.1.2.2"><csymbol cd="latexml" id="S3.SS2.p3.2.m2.1.2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">lang</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\langle\mathrm{lang}\rangle</annotation></semantics></math> from any caption. The output quality depends on the translation quality, e.g. the back-translation in step 4 from <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="\langle\mathrm{lang}\rangle" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.2.2" xref="S3.SS2.p3.3.m3.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p3.3.m3.1.2.2.1" xref="S3.SS2.p3.3.m3.1.2.1.1.cmml">⟨</mo><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">lang</mi><mo stretchy="false" id="S3.SS2.p3.3.m3.1.2.2.2" xref="S3.SS2.p3.3.m3.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.2.1.cmml" xref="S3.SS2.p3.3.m3.1.2.2"><csymbol cd="latexml" id="S3.SS2.p3.3.m3.1.2.1.1.cmml" xref="S3.SS2.p3.3.m3.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">lang</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\langle\mathrm{lang}\rangle</annotation></semantics></math> to c’s language. We use out-of-the-box translation tools in this work, and leave the exploration of better translation tailored for <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="\mathrm{TransVQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mrow id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><msup id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2.2" xref="S3.SS2.p3.4.m4.1.1.2.2.cmml">TransVQ</mi><mn id="S3.SS2.p3.4.m4.1.1.2.3" xref="S3.SS2.p3.4.m4.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p3.4.m4.1.1.1" xref="S3.SS2.p3.4.m4.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><times id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1"></times><apply id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.2.1.cmml" xref="S3.SS2.p3.4.m4.1.1.2">superscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.2.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2.2">TransVQ</ci><cn type="integer" id="S3.SS2.p3.4.m4.1.1.2.3.cmml" xref="S3.SS2.p3.4.m4.1.1.2.3">2</cn></apply><ci id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\mathrm{TransVQ}^{2}\!\mathrm{A}</annotation></semantics></math> for future work.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">In Sect. <a href="#S4" title="4 MaXM: Multilingual VQA Benchmark ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we employ human annotators to further clean and expand the generated data to create a high quality test benchmark.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Direct Question Generation (DirectQG)</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">One drawback of <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{TransVQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><msup id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2.2" xref="S3.SS3.p1.1.m1.1.1.2.2.cmml">TransVQ</mi><mn id="S3.SS3.p1.1.m1.1.1.2.3" xref="S3.SS3.p1.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><apply id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2.2">TransVQ</ci><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.2.3">2</cn></apply><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\mathrm{TransVQ}^{2}\!\mathrm{A}</annotation></semantics></math> is the low coverage of particular types of answers, such as “no”. This is because the captions generally do not indicate the absence of objects or properties (e.g., <em id="S3.SS3.p1.2.1" class="ltx_emph ltx_font_italic">“There is no dog”</em>, <em id="S3.SS3.p1.2.2" class="ltx_emph ltx_font_italic">“The dog is not white”</em>).
To mitigate this bias, we train a multilingual question generator that takes in an answer and a caption in a target language and generates relevant questions in the same language. We use the model to generate questions for “yes”, “no”, or “none” as answers in each target language, as a complement to <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\mathrm{TransVQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><msup id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2.2" xref="S3.SS3.p1.2.m2.1.1.2.2.cmml">TransVQ</mi><mn id="S3.SS3.p1.2.m2.1.1.2.3" xref="S3.SS3.p1.2.m2.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><times id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></times><apply id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.2">superscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2">TransVQ</ci><cn type="integer" id="S3.SS3.p1.2.m2.1.1.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3">2</cn></apply><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\mathrm{TransVQ}^{2}\!\mathrm{A}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">Concretely, we fine-tuned mT5-XXL <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite> on large-scale translated COCO Captions <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite> and its corresponding VQA data <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><msup id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2.2" xref="S3.SS3.p2.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S3.SS3.p2.1.m1.1.1.2.3" xref="S3.SS3.p2.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><apply id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.2.1.cmml" xref="S3.SS3.p2.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S3.SS3.p2.1.m1.1.1.2.3.cmml" xref="S3.SS3.p2.1.m1.1.1.2.3">2</cn></apply><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-COCO  <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>.
For validation, we used the subset of generated multilingual VQA data in Sect. <a href="#S3.SS2" title="3.2 Translation-based VQ²⁢A (TransVQ²⁢A) ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, with <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mo id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><csymbol cd="latexml" id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\sim</annotation></semantics></math>300 golden examples for each language. The best checkpoint was selected based on ROUGE-L scores.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div id="S3.F3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:322.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(7.0pt,-5.2pt) scale(1.03339174053022,1.03339174053022) ;"><img src="/html/2209.05401/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="444" height="333" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.3.1" class="ltx_text ltx_font_bold">The diversity of multilingual captions in <math id="S3.F3.3.1.m1.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="S3.F3.3.1.m1.1b"><mi id="S3.F3.3.1.m1.1.1" xref="S3.F3.3.1.m1.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="S3.F3.3.1.m1.1c"><ci id="S3.F3.3.1.m1.1.1.cmml" xref="S3.F3.3.1.m1.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.1.m1.1d">\mathrm{XM3600}</annotation></semantics></math>.</span> We show the captions (their English translations) from 4 languages for the images of a snow cannon (left) and xiao long bao (right).</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">en</td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">fr</td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r">hi</td>
<td id="S3.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_r">iw</td>
<td id="S3.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_r">ro</td>
<td id="S3.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_r">th</td>
<td id="S3.T1.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center">zh</td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Captions</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7200</td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8562</td>
<td id="S3.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8503</td>
<td id="S3.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7200</td>
<td id="S3.T1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7123</td>
<td id="S3.T1.1.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7200</td>
<td id="S3.T1.1.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">7174</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">English QAs</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">373248</td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">499900</td>
<td id="S3.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">520080</td>
<td id="S3.T1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">544268</td>
<td id="S3.T1.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">516604</td>
<td id="S3.T1.1.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">415180</td>
<td id="S3.T1.1.3.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">524252</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Validated</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">264930</td>
<td id="S3.T1.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">343621</td>
<td id="S3.T1.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">346948</td>
<td id="S3.T1.1.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">375629</td>
<td id="S3.T1.1.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">346887</td>
<td id="S3.T1.1.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">286024</td>
<td id="S3.T1.1.4.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">362304</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_left ltx_border_r">English QAs</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">(71.0%)</td>
<td id="S3.T1.1.5.3" class="ltx_td ltx_align_center ltx_border_r">(68.7%)</td>
<td id="S3.T1.1.5.4" class="ltx_td ltx_align_center ltx_border_r">(66.7%)</td>
<td id="S3.T1.1.5.5" class="ltx_td ltx_align_center ltx_border_r">(69.0%)</td>
<td id="S3.T1.1.5.6" class="ltx_td ltx_align_center ltx_border_r">(67.1%)</td>
<td id="S3.T1.1.5.7" class="ltx_td ltx_align_center ltx_border_r">(68.9%)</td>
<td id="S3.T1.1.5.8" class="ltx_td ltx_nopad_r ltx_align_center">(69.1%)</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Validated</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">264724</td>
<td id="S3.T1.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">122644</td>
<td id="S3.T1.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">153465</td>
<td id="S3.T1.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">128613</td>
<td id="S3.T1.1.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">121221</td>
<td id="S3.T1.1.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95531</td>
<td id="S3.T1.1.6.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">182095</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Multilingual QAs</td>
<td id="S3.T1.1.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">(99.92%)</td>
<td id="S3.T1.1.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">(33.85%)</td>
<td id="S3.T1.1.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">(53.67%)</td>
<td id="S3.T1.1.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">(37.08%)</td>
<td id="S3.T1.1.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">(32.27%)</td>
<td id="S3.T1.1.7.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">(27.53%)</td>
<td id="S3.T1.1.7.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">(52.99%)</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S3.T1.3.1" class="ltx_text ltx_font_italic">Number of Instances (% of Previous Stage)</span> of automatically-generated question-answer (QA) pairs based on Crossmodal-3600 captions. Validated English pairs are w.r.t the QG-QA consistency filter. Validated multilingual pairs are w.r.t the caption-answer consistency filter.
</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_r">Label</td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_r">Question</td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_center">Answer</td>
</tr>
<tr id="S3.T2.1.2" class="ltx_tr">
<td id="S3.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.1.2.1.1" class="ltx_text ltx_font_italic">Correct</span></td>
<td id="S3.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Makes sense AND is relevant to the image.</td>
<td id="S3.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">Satisfies the question’s intent wrt the image.</td>
</tr>
<tr id="S3.T2.1.3" class="ltx_tr">
<td id="S3.T2.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.1.3.1.1" class="ltx_text ltx_font_italic">Almost Correct</span></td>
<td id="S3.T2.1.3.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2">Correct but its surface form can be improved (syntactic errors or awkward/uncommon usages.)</td>
</tr>
<tr id="S3.T2.1.4" class="ltx_tr">
<td id="S3.T2.1.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.1.4.1.1" class="ltx_text ltx_font_italic">Incorrect</span></td>
<td id="S3.T2.1.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" colspan="2">NOT Correct.</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Definition of <span id="S3.T2.5.1" class="ltx_text ltx_font_italic">Correct</span>, <span id="S3.T2.6.2" class="ltx_text ltx_font_italic">Almost Correct</span> and <span id="S3.T2.7.3" class="ltx_text ltx_font_italic">Incorrect</span> labels for questions and answers in our annotation protocol.</figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">en</td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">fr</td>
<td id="S3.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">hi</td>
<td id="S3.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">iw</td>
<td id="S3.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ro</td>
<td id="S3.T3.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">th</td>
<td id="S3.T3.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">zh</td>
</tr>
<tr id="S3.T3.1.2" class="ltx_tr">
<td id="S3.T3.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"># of questions evaluated</td>
<td id="S3.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">377</td>
<td id="S3.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">389</td>
<td id="S3.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">400</td>
<td id="S3.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">365</td>
<td id="S3.T3.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">440</td>
<td id="S3.T3.1.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">401</td>
<td id="S3.T3.1.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">391</td>
</tr>
<tr id="S3.T3.1.3" class="ltx_tr">
<td id="S3.T3.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">% Correct</td>
<td id="S3.T3.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.6%</td>
<td id="S3.T3.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.8%</td>
<td id="S3.T3.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.5%</td>
<td id="S3.T3.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.6%</td>
<td id="S3.T3.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.1%</td>
<td id="S3.T3.1.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.4%</td>
<td id="S3.T3.1.3.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">51.9%</td>
</tr>
<tr id="S3.T3.1.4" class="ltx_tr">
<td id="S3.T3.1.4.1" class="ltx_td ltx_align_left ltx_border_r">% Almost Correct</td>
<td id="S3.T3.1.4.2" class="ltx_td ltx_align_center ltx_border_r">17.5%</td>
<td id="S3.T3.1.4.3" class="ltx_td ltx_align_center ltx_border_r">12.3%</td>
<td id="S3.T3.1.4.4" class="ltx_td ltx_align_center ltx_border_r">9.0%</td>
<td id="S3.T3.1.4.5" class="ltx_td ltx_align_center ltx_border_r">22.5%</td>
<td id="S3.T3.1.4.6" class="ltx_td ltx_align_center ltx_border_r">9.3%</td>
<td id="S3.T3.1.4.7" class="ltx_td ltx_align_center ltx_border_r">27.9%</td>
<td id="S3.T3.1.4.8" class="ltx_td ltx_nopad_r ltx_align_center">25.1%</td>
</tr>
<tr id="S3.T3.1.5" class="ltx_tr">
<td id="S3.T3.1.5.1" class="ltx_td ltx_align_left ltx_border_r">% Incorrect</td>
<td id="S3.T3.1.5.2" class="ltx_td ltx_align_center ltx_border_r">19.9%</td>
<td id="S3.T3.1.5.3" class="ltx_td ltx_align_center ltx_border_r">21.9%</td>
<td id="S3.T3.1.5.4" class="ltx_td ltx_align_center ltx_border_r">24.50%</td>
<td id="S3.T3.1.5.5" class="ltx_td ltx_align_center ltx_border_r">15.9%</td>
<td id="S3.T3.1.5.6" class="ltx_td ltx_align_center ltx_border_r">31.6%</td>
<td id="S3.T3.1.5.7" class="ltx_td ltx_align_center ltx_border_r">24.7%</td>
<td id="S3.T3.1.5.8" class="ltx_td ltx_nopad_r ltx_align_center">23.02%</td>
</tr>
<tr id="S3.T3.1.6" class="ltx_tr">
<td id="S3.T3.1.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"># of answers evaluated</td>
<td id="S3.T3.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">302</td>
<td id="S3.T3.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">304</td>
<td id="S3.T3.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">302</td>
<td id="S3.T3.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">307</td>
<td id="S3.T3.1.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">301</td>
<td id="S3.T3.1.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">302</td>
<td id="S3.T3.1.6.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">301</td>
</tr>
<tr id="S3.T3.1.7" class="ltx_tr">
<td id="S3.T3.1.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">% Correct</td>
<td id="S3.T3.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.2%</td>
<td id="S3.T3.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.0%</td>
<td id="S3.T3.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.8%</td>
<td id="S3.T3.1.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.9%</td>
<td id="S3.T3.1.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.2%</td>
<td id="S3.T3.1.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.2%</td>
<td id="S3.T3.1.7.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">81.9%</td>
</tr>
<tr id="S3.T3.1.8" class="ltx_tr">
<td id="S3.T3.1.8.1" class="ltx_td ltx_align_left ltx_border_r">% Almost Correct</td>
<td id="S3.T3.1.8.2" class="ltx_td ltx_align_center ltx_border_r">26.5%</td>
<td id="S3.T3.1.8.3" class="ltx_td ltx_align_center ltx_border_r">24.0%</td>
<td id="S3.T3.1.8.4" class="ltx_td ltx_align_center ltx_border_r">17.9%</td>
<td id="S3.T3.1.8.5" class="ltx_td ltx_align_center ltx_border_r">24.1%</td>
<td id="S3.T3.1.8.6" class="ltx_td ltx_align_center ltx_border_r">16.9%</td>
<td id="S3.T3.1.8.7" class="ltx_td ltx_align_center ltx_border_r">7.9%</td>
<td id="S3.T3.1.8.8" class="ltx_td ltx_nopad_r ltx_align_center">9.2%</td>
</tr>
<tr id="S3.T3.1.9" class="ltx_tr">
<td id="S3.T3.1.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">% Incorrect</td>
<td id="S3.T3.1.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">7.3%</td>
<td id="S3.T3.1.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">3.9%</td>
<td id="S3.T3.1.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">4.3%</td>
<td id="S3.T3.1.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">2.0%</td>
<td id="S3.T3.1.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">7.0%</td>
<td id="S3.T3.1.9.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">9.9%</td>
<td id="S3.T3.1.9.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">8.9%</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S3.T3.3.1" class="ltx_text ltx_font_bold">Human evaluation</span> of the generated questions and answers.
</figcaption>
</figure>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.3.1" class="ltx_tr">
<td id="S3.T4.3.1.1" class="ltx_td ltx_align_center ltx_border_r">Question</td>
<td id="S3.T4.3.1.2" class="ltx_td ltx_align_center" colspan="7">Percentage</td>
</tr>
<tr id="S3.T4.3.2" class="ltx_tr">
<td id="S3.T4.3.2.1" class="ltx_td ltx_align_center ltx_border_r">Prefix</td>
<td id="S3.T4.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">en</td>
<td id="S3.T4.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">fr</td>
<td id="S3.T4.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">hi</td>
<td id="S3.T4.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">iw</td>
<td id="S3.T4.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ro</td>
<td id="S3.T4.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">th</td>
<td id="S3.T4.3.2.8" class="ltx_td ltx_align_center ltx_border_t">zh</td>
</tr>
<tr id="S3.T4.3.3" class="ltx_tr">
<td id="S3.T4.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><em id="S3.T4.3.3.1.1" class="ltx_emph ltx_font_italic">“is”</em></td>
<td id="S3.T4.3.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">22.8</td>
<td id="S3.T4.3.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">21.2</td>
<td id="S3.T4.3.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">21.4</td>
<td id="S3.T4.3.3.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">17.8</td>
<td id="S3.T4.3.3.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">19.5</td>
<td id="S3.T4.3.3.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">16.2</td>
<td id="S3.T4.3.3.8" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">20.2</td>
</tr>
<tr id="S3.T4.3.4" class="ltx_tr">
<td id="S3.T4.3.4.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.4.1.1" class="ltx_emph ltx_font_italic">“what is”</em></td>
<td id="S3.T4.3.4.2" class="ltx_td ltx_align_right ltx_border_r">15.8</td>
<td id="S3.T4.3.4.3" class="ltx_td ltx_align_right ltx_border_r">11.3</td>
<td id="S3.T4.3.4.4" class="ltx_td ltx_align_right ltx_border_r">16.0</td>
<td id="S3.T4.3.4.5" class="ltx_td ltx_align_right ltx_border_r">15.2</td>
<td id="S3.T4.3.4.6" class="ltx_td ltx_align_right ltx_border_r">13.5</td>
<td id="S3.T4.3.4.7" class="ltx_td ltx_align_right ltx_border_r">11.6</td>
<td id="S3.T4.3.4.8" class="ltx_td ltx_nopad_r ltx_align_right">13.7</td>
</tr>
<tr id="S3.T4.3.5" class="ltx_tr">
<td id="S3.T4.3.5.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.5.1.1" class="ltx_emph ltx_font_italic">“how many”</em></td>
<td id="S3.T4.3.5.2" class="ltx_td ltx_align_right ltx_border_r">15.1</td>
<td id="S3.T4.3.5.3" class="ltx_td ltx_align_right ltx_border_r">11.3</td>
<td id="S3.T4.3.5.4" class="ltx_td ltx_align_right ltx_border_r">13.9</td>
<td id="S3.T4.3.5.5" class="ltx_td ltx_align_right ltx_border_r">10.2</td>
<td id="S3.T4.3.5.6" class="ltx_td ltx_align_right ltx_border_r">14.1</td>
<td id="S3.T4.3.5.7" class="ltx_td ltx_align_right ltx_border_r">15.2</td>
<td id="S3.T4.3.5.8" class="ltx_td ltx_nopad_r ltx_align_right">12.1</td>
</tr>
<tr id="S3.T4.3.6" class="ltx_tr">
<td id="S3.T4.3.6.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.6.1.1" class="ltx_emph ltx_font_italic">“where”</em></td>
<td id="S3.T4.3.6.2" class="ltx_td ltx_align_right ltx_border_r">6.7</td>
<td id="S3.T4.3.6.3" class="ltx_td ltx_align_right ltx_border_r">9.2</td>
<td id="S3.T4.3.6.4" class="ltx_td ltx_align_right ltx_border_r">7.5</td>
<td id="S3.T4.3.6.5" class="ltx_td ltx_align_right ltx_border_r">8.6</td>
<td id="S3.T4.3.6.6" class="ltx_td ltx_align_right ltx_border_r">6.6</td>
<td id="S3.T4.3.6.7" class="ltx_td ltx_align_right ltx_border_r">9.3</td>
<td id="S3.T4.3.6.8" class="ltx_td ltx_nopad_r ltx_align_right">5.9</td>
</tr>
<tr id="S3.T4.3.7" class="ltx_tr">
<td id="S3.T4.3.7.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.7.1.1" class="ltx_emph ltx_font_italic">“what kind”</em></td>
<td id="S3.T4.3.7.2" class="ltx_td ltx_align_right ltx_border_r">6.0</td>
<td id="S3.T4.3.7.3" class="ltx_td ltx_align_right ltx_border_r">7.2</td>
<td id="S3.T4.3.7.4" class="ltx_td ltx_align_right ltx_border_r">1.4</td>
<td id="S3.T4.3.7.5" class="ltx_td ltx_align_right ltx_border_r">3.8</td>
<td id="S3.T4.3.7.6" class="ltx_td ltx_align_right ltx_border_r">6.6</td>
<td id="S3.T4.3.7.7" class="ltx_td ltx_align_right ltx_border_r">4.0</td>
<td id="S3.T4.3.7.8" class="ltx_td ltx_nopad_r ltx_align_right">3.6</td>
</tr>
<tr id="S3.T4.3.8" class="ltx_tr">
<td id="S3.T4.3.8.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.8.1.1" class="ltx_emph ltx_font_italic">“what are”</em></td>
<td id="S3.T4.3.8.2" class="ltx_td ltx_align_right ltx_border_r">3.4</td>
<td id="S3.T4.3.8.3" class="ltx_td ltx_align_right ltx_border_r">1.4</td>
<td id="S3.T4.3.8.4" class="ltx_td ltx_align_right ltx_border_r">1.0</td>
<td id="S3.T4.3.8.5" class="ltx_td ltx_align_right ltx_border_r">2.2</td>
<td id="S3.T4.3.8.6" class="ltx_td ltx_align_right ltx_border_r">2.4</td>
<td id="S3.T4.3.8.7" class="ltx_td ltx_align_right ltx_border_r">2.3</td>
<td id="S3.T4.3.8.8" class="ltx_td ltx_nopad_r ltx_align_right">2.3</td>
</tr>
<tr id="S3.T4.3.9" class="ltx_tr">
<td id="S3.T4.3.9.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.9.1.1" class="ltx_emph ltx_font_italic">“who”</em></td>
<td id="S3.T4.3.9.2" class="ltx_td ltx_align_right ltx_border_r">3.4</td>
<td id="S3.T4.3.9.3" class="ltx_td ltx_align_right ltx_border_r">3.1</td>
<td id="S3.T4.3.9.4" class="ltx_td ltx_align_right ltx_border_r">9.2</td>
<td id="S3.T4.3.9.5" class="ltx_td ltx_align_right ltx_border_r">2.5</td>
<td id="S3.T4.3.9.6" class="ltx_td ltx_align_right ltx_border_r">1.5</td>
<td id="S3.T4.3.9.7" class="ltx_td ltx_align_right ltx_border_r">2.6</td>
<td id="S3.T4.3.9.8" class="ltx_td ltx_nopad_r ltx_align_right">2.3</td>
</tr>
<tr id="S3.T4.3.10" class="ltx_tr">
<td id="S3.T4.3.10.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.10.1.1" class="ltx_emph ltx_font_italic">“are”</em></td>
<td id="S3.T4.3.10.2" class="ltx_td ltx_align_right ltx_border_r">3.4</td>
<td id="S3.T4.3.10.3" class="ltx_td ltx_align_right ltx_border_r">0.7</td>
<td id="S3.T4.3.10.4" class="ltx_td ltx_align_right ltx_border_r">4.1</td>
<td id="S3.T4.3.10.5" class="ltx_td ltx_align_right ltx_border_r">3.8</td>
<td id="S3.T4.3.10.6" class="ltx_td ltx_align_right ltx_border_r">3.6</td>
<td id="S3.T4.3.10.7" class="ltx_td ltx_align_right ltx_border_r">3.3</td>
<td id="S3.T4.3.10.8" class="ltx_td ltx_nopad_r ltx_align_right">2.0</td>
</tr>
<tr id="S3.T4.3.11" class="ltx_tr">
<td id="S3.T4.3.11.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.11.1.1" class="ltx_emph ltx_font_italic">“what color”</em></td>
<td id="S3.T4.3.11.2" class="ltx_td ltx_align_right ltx_border_r">3.4</td>
<td id="S3.T4.3.11.3" class="ltx_td ltx_align_right ltx_border_r">7.2</td>
<td id="S3.T4.3.11.4" class="ltx_td ltx_align_right ltx_border_r">5.1</td>
<td id="S3.T4.3.11.5" class="ltx_td ltx_align_right ltx_border_r">9.2</td>
<td id="S3.T4.3.11.6" class="ltx_td ltx_align_right ltx_border_r">6.9</td>
<td id="S3.T4.3.11.7" class="ltx_td ltx_align_right ltx_border_r">8.6</td>
<td id="S3.T4.3.11.8" class="ltx_td ltx_nopad_r ltx_align_right">7.5</td>
</tr>
<tr id="S3.T4.3.12" class="ltx_tr">
<td id="S3.T4.3.12.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.12.1.1" class="ltx_emph ltx_font_italic">“a”</em></td>
<td id="S3.T4.3.12.2" class="ltx_td ltx_align_right ltx_border_r">3.0</td>
<td id="S3.T4.3.12.3" class="ltx_td ltx_align_right ltx_border_r">3.1</td>
<td id="S3.T4.3.12.4" class="ltx_td ltx_align_right ltx_border_r">1.4</td>
<td id="S3.T4.3.12.5" class="ltx_td ltx_align_right ltx_border_r">3.5</td>
<td id="S3.T4.3.12.6" class="ltx_td ltx_align_right ltx_border_r">2.1</td>
<td id="S3.T4.3.12.7" class="ltx_td ltx_align_right ltx_border_r">3.0</td>
<td id="S3.T4.3.12.8" class="ltx_td ltx_nopad_r ltx_align_right">2.9</td>
</tr>
<tr id="S3.T4.3.13" class="ltx_tr">
<td id="S3.T4.3.13.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.13.1.1" class="ltx_emph ltx_font_italic">“what type”</em></td>
<td id="S3.T4.3.13.2" class="ltx_td ltx_align_right ltx_border_r">2.7</td>
<td id="S3.T4.3.13.3" class="ltx_td ltx_align_right ltx_border_r">2.4</td>
<td id="S3.T4.3.13.4" class="ltx_td ltx_align_right ltx_border_r">0.3</td>
<td id="S3.T4.3.13.5" class="ltx_td ltx_align_right ltx_border_r">0.3</td>
<td id="S3.T4.3.13.6" class="ltx_td ltx_align_right ltx_border_r">2.1</td>
<td id="S3.T4.3.13.7" class="ltx_td ltx_align_right ltx_border_r">1.0</td>
<td id="S3.T4.3.13.8" class="ltx_td ltx_nopad_r ltx_align_right">4.6</td>
</tr>
<tr id="S3.T4.3.14" class="ltx_tr">
<td id="S3.T4.3.14.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.14.1.1" class="ltx_emph ltx_font_italic">“what was”</em></td>
<td id="S3.T4.3.14.2" class="ltx_td ltx_align_right ltx_border_r">1.0</td>
<td id="S3.T4.3.14.3" class="ltx_td ltx_align_right ltx_border_r">0.3</td>
<td id="S3.T4.3.14.4" class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td id="S3.T4.3.14.5" class="ltx_td ltx_align_right ltx_border_r">0.6</td>
<td id="S3.T4.3.14.6" class="ltx_td ltx_align_right ltx_border_r">1.2</td>
<td id="S3.T4.3.14.7" class="ltx_td ltx_align_right ltx_border_r">3.3</td>
<td id="S3.T4.3.14.8" class="ltx_td ltx_nopad_r ltx_align_right">0.7</td>
</tr>
<tr id="S3.T4.3.15" class="ltx_tr">
<td id="S3.T4.3.15.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.15.1.1" class="ltx_emph ltx_font_italic">“do”</em></td>
<td id="S3.T4.3.15.2" class="ltx_td ltx_align_right ltx_border_r">0.7</td>
<td id="S3.T4.3.15.3" class="ltx_td ltx_align_right ltx_border_r">0.3</td>
<td id="S3.T4.3.15.4" class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td id="S3.T4.3.15.5" class="ltx_td ltx_align_right ltx_border_r">0.3</td>
<td id="S3.T4.3.15.6" class="ltx_td ltx_align_right ltx_border_r">0.6</td>
<td id="S3.T4.3.15.7" class="ltx_td ltx_align_right ltx_border_r">0.7</td>
<td id="S3.T4.3.15.8" class="ltx_td ltx_nopad_r ltx_align_right">2.3</td>
</tr>
<tr id="S3.T4.3.16" class="ltx_tr">
<td id="S3.T4.3.16.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.16.1.1" class="ltx_emph ltx_font_italic">“in”</em></td>
<td id="S3.T4.3.16.2" class="ltx_td ltx_align_right ltx_border_r">0.7</td>
<td id="S3.T4.3.16.3" class="ltx_td ltx_align_right ltx_border_r">1.7</td>
<td id="S3.T4.3.16.4" class="ltx_td ltx_align_right ltx_border_r">0.3</td>
<td id="S3.T4.3.16.5" class="ltx_td ltx_align_right ltx_border_r">0.3</td>
<td id="S3.T4.3.16.6" class="ltx_td ltx_align_right ltx_border_r">2.4</td>
<td id="S3.T4.3.16.7" class="ltx_td ltx_align_right ltx_border_r">1.7</td>
<td id="S3.T4.3.16.8" class="ltx_td ltx_nopad_r ltx_align_right">2.0</td>
</tr>
<tr id="S3.T4.3.17" class="ltx_tr">
<td id="S3.T4.3.17.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.17.1.1" class="ltx_emph ltx_font_italic">“besides”</em></td>
<td id="S3.T4.3.17.2" class="ltx_td ltx_align_right ltx_border_r">0.3</td>
<td id="S3.T4.3.17.3" class="ltx_td ltx_align_right ltx_border_r">1.0</td>
<td id="S3.T4.3.17.4" class="ltx_td ltx_align_right ltx_border_r">4.4</td>
<td id="S3.T4.3.17.5" class="ltx_td ltx_align_right ltx_border_r">3.2</td>
<td id="S3.T4.3.17.6" class="ltx_td ltx_align_right ltx_border_r">1.2</td>
<td id="S3.T4.3.17.7" class="ltx_td ltx_align_right ltx_border_r">0.3</td>
<td id="S3.T4.3.17.8" class="ltx_td ltx_nopad_r ltx_align_right">2.3</td>
</tr>
<tr id="S3.T4.3.18" class="ltx_tr">
<td id="S3.T4.3.18.1" class="ltx_td ltx_align_left ltx_border_r"><em id="S3.T4.3.18.1.1" class="ltx_emph ltx_font_italic">“does”</em></td>
<td id="S3.T4.3.18.2" class="ltx_td ltx_align_right ltx_border_r">0.3</td>
<td id="S3.T4.3.18.3" class="ltx_td ltx_align_right ltx_border_r">2.7</td>
<td id="S3.T4.3.18.4" class="ltx_td ltx_align_right ltx_border_r">1.7</td>
<td id="S3.T4.3.18.5" class="ltx_td ltx_align_right ltx_border_r">2.5</td>
<td id="S3.T4.3.18.6" class="ltx_td ltx_align_right ltx_border_r">1.5</td>
<td id="S3.T4.3.18.7" class="ltx_td ltx_align_right ltx_border_r">3.3</td>
<td id="S3.T4.3.18.8" class="ltx_td ltx_nopad_r ltx_align_right">0.7</td>
</tr>
<tr id="S3.T4.3.19" class="ltx_tr">
<td id="S3.T4.3.19.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">other</td>
<td id="S3.T4.3.19.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r">11.4</td>
<td id="S3.T4.3.19.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r">16.0</td>
<td id="S3.T4.3.19.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r">12.2</td>
<td id="S3.T4.3.19.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r">15.9</td>
<td id="S3.T4.3.19.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_r">14.1</td>
<td id="S3.T4.3.19.7" class="ltx_td ltx_align_right ltx_border_b ltx_border_r">13.6</td>
<td id="S3.T4.3.19.8" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_b">15.3</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S3.T4.5.1" class="ltx_text ltx_font_bold">The distribution of question types</span> in <math id="S3.T4.2.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S3.T4.2.m1.1b"><mi id="S3.T4.2.m1.1.1" xref="S3.T4.2.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S3.T4.2.m1.1c"><ci id="S3.T4.2.m1.1.1.cmml" xref="S3.T4.2.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.m1.1d">\mathrm{MaXM}</annotation></semantics></math> across languages. Approximated by their corresponding English question prefixes.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>MaXM: Multilingual VQA Benchmark</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">In this section, we leverage the approach we presented in Sect. <a href="#S3" title="3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for creating a multilingual VQA test-only benchmark. We next describe our data sources, how candidate data was generated, human annotation protocol, and an analysis and a discussion of our benchmark. Following the naming convention in <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>, we call our benchmark MAVERICS-XM3600, or <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\mathrm{MaXM}</annotation></semantics></math> in short. We will release <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\mathrm{MaXM}</annotation></semantics></math> to foster research on mVQA.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data Sources</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Language Selection</span>. We chose 7 languages
that are 1) typologically diverse, 2) genealogically diverse, and 3) geographically diverse: English (en), French (fr), Hindi (hi), Hebrew (iw), Romanian (ro), Thai (th), and Chinese (zh).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Image and Caption Selection</span>. We chose a subset of the images in Crossmodal-3600 (<math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\mathrm{XM3600}</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">Thapliyal et al. (<a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>, in which high-quality multilingual image captions are available. For each language, 100 validation and test images of Open Images <cite class="ltx_cite ltx_citemacro_cite">Krasin et al. (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>); Kuznetsova et al. (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> that were taken in the region(s) in which those languages were spoken were selected.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Our image selection criteria cover a wide range of visual concepts in different cultural contexts, making the constructed VQA examples diverse and specific to the languages of the captions related to each image.
For example, in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.3 Direct Question Generation (DirectQG) ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, unlike French and Romanian speakers, Hebrew and Thai speakers are less likely to know what a snow cannon is. On the other hand, Thai and Chinese speakers are more likely to understand what xiao long bao is, whereas in French or Hindi it could be referred to as dim-sum ravioli or Chinese dim sum.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Another benefit of <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mi id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><ci id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\mathrm{XM3600}</annotation></semantics></math> is that the Open Images images are out-of-domain with respect to most widely-used VQA benchmarks <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib48" title="" class="ltx_ref">2015</a>); Antol et al. (<a href="#bib.bib5" title="" class="ltx_ref">2015</a>); Zhu et al. (<a href="#bib.bib69" title="" class="ltx_ref">2016</a>); Krishna et al. (<a href="#bib.bib30" title="" class="ltx_ref">2017</a>); Goyal et al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>); Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>); Hudson &amp; Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>); Marino et al. (<a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite>, which are often based on MS-COCO images <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib39" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Large-Scale mVQA Data Creation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We apply our approach described in Sect. <a href="#S3" title="3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> to the <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\mathrm{XM3600}</annotation></semantics></math> captions to generate a large number of question-answer pairs for each language.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.2" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">TransVQ<sup id="S4.SS2.p2.1.1.1" class="ltx_sup"><span id="S4.SS2.p2.1.1.1.1" class="ltx_text ltx_font_medium">2</span></sup>A</span>.
Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Direct Question Generation (DirectQG) ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reports the number of question-answer pairs at different stages in our pipeline. Overall, we are able to generate a large number of question-answer pairs in all languages. We found that, across languages, approximately 30% of (translated) English question-answer pairs are filtered out due to <math id="S4.SS2.p2.2.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S4.SS2.p2.2.m1.1a"><mrow id="S4.SS2.p2.2.m1.1.1" xref="S4.SS2.p2.2.m1.1.1.cmml"><msup id="S4.SS2.p2.2.m1.1.1.2" xref="S4.SS2.p2.2.m1.1.1.2.cmml"><mi id="S4.SS2.p2.2.m1.1.1.2.2" xref="S4.SS2.p2.2.m1.1.1.2.2.cmml">VQ</mi><mn id="S4.SS2.p2.2.m1.1.1.2.3" xref="S4.SS2.p2.2.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m1.1.1.1" xref="S4.SS2.p2.2.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.SS2.p2.2.m1.1.1.3" xref="S4.SS2.p2.2.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m1.1b"><apply id="S4.SS2.p2.2.m1.1.1.cmml" xref="S4.SS2.p2.2.m1.1.1"><times id="S4.SS2.p2.2.m1.1.1.1.cmml" xref="S4.SS2.p2.2.m1.1.1.1"></times><apply id="S4.SS2.p2.2.m1.1.1.2.cmml" xref="S4.SS2.p2.2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m1.1.1.2.1.cmml" xref="S4.SS2.p2.2.m1.1.1.2">superscript</csymbol><ci id="S4.SS2.p2.2.m1.1.1.2.2.cmml" xref="S4.SS2.p2.2.m1.1.1.2.2">VQ</ci><cn type="integer" id="S4.SS2.p2.2.m1.1.1.2.3.cmml" xref="S4.SS2.p2.2.m1.1.1.2.3">2</cn></apply><ci id="S4.SS2.p2.2.m1.1.1.3.cmml" xref="S4.SS2.p2.2.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> validation.
In contrast, different percentages of translated answers across languages are filtered out based on the caption-answer consistency validation. A main reason for this is the quality of question-answer translation. For instance, 68% of questions with “alb" (masculine “white” in Romanian) are filtered out because they are not translated to the correct feminine form “alba” w.r.t the corresponding object in the question.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.3" class="ltx_p"><span id="S4.SS2.p3.3.1" class="ltx_text ltx_font_bold">DirectQG</span>.
We augment the <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{TransVQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><msup id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2.2" xref="S4.SS2.p3.1.m1.1.1.2.2.cmml">TransVQ</mi><mn id="S4.SS2.p3.1.m1.1.1.2.3" xref="S4.SS2.p3.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><apply id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.2.1.cmml" xref="S4.SS2.p3.1.m1.1.1.2">superscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.2.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2.2">TransVQ</ci><cn type="integer" id="S4.SS2.p3.1.m1.1.1.2.3.cmml" xref="S4.SS2.p3.1.m1.1.1.2.3">2</cn></apply><ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathrm{TransVQ}^{2}\!\mathrm{A}</annotation></semantics></math> questions with additional candidate questions generated by <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="\mathrm{TransVQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><msup id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml"><mi id="S4.SS2.p3.2.m2.1.1.2.2" xref="S4.SS2.p3.2.m2.1.1.2.2.cmml">TransVQ</mi><mn id="S4.SS2.p3.2.m2.1.1.2.3" xref="S4.SS2.p3.2.m2.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S4.SS2.p3.2.m2.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><times id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1"></times><apply id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.2.m2.1.1.2.1.cmml" xref="S4.SS2.p3.2.m2.1.1.2">superscript</csymbol><ci id="S4.SS2.p3.2.m2.1.1.2.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2.2">TransVQ</ci><cn type="integer" id="S4.SS2.p3.2.m2.1.1.2.3.cmml" xref="S4.SS2.p3.2.m2.1.1.2.3">2</cn></apply><ci id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\mathrm{TransVQ}^{2}\!\mathrm{A}</annotation></semantics></math> (Sect. <a href="#S3.SS3" title="3.3 Direct Question Generation (DirectQG) ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>), using the <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mi id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><ci id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">\mathrm{XM3600}</annotation></semantics></math> captions paired with “yes”, “no”, or “none” in their corresponding language as input.

</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Human Annotation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We employed native speakers for each of the selected 7 languages to annotate and create our benchmark.
We designed an annotation protocol to balance efficiency and accuracy. In particular, we keep human-in-the-loop brief and only when an automated model straggles in a task, e.g., correcting translation artifacts, expanding answers, identifying sensitive questions. Furthermore, our protocol promotes quick discarding of examples, when the question does not make sense. We provide more details next and also in Appendix <a href="#A2" title="Appendix B Human Verification and Modification ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Question and Answer Validation</span>.
We define a 3-way rating system of <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">Correct</span>, <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_italic">Almost Correct</span> and <span id="S4.SS3.p2.1.4" class="ltx_text ltx_font_italic">Incorrect</span> for both the questions and the answers (see Table <a href="#S3.T2" title="Table 2 ‣ 3.3 Direct Question Generation (DirectQG) ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
<span id="S4.SS3.p2.1.5" class="ltx_text ltx_font_italic">Correct</span> questions are kept unchanged, <span id="S4.SS3.p2.1.6" class="ltx_text ltx_font_italic">Almost Correct</span> questions are manually rewritten, and <span id="S4.SS3.p2.1.7" class="ltx_text ltx_font_italic">Incorrect</span> questions are discarded. Given <span id="S4.SS3.p2.1.8" class="ltx_text ltx_font_italic">Correct</span> and <span id="S4.SS3.p2.1.9" class="ltx_text ltx_font_italic">Almost Correct</span> questions, an annotator rates the answer and corrects it in the cases of both <span id="S4.SS3.p2.1.10" class="ltx_text ltx_font_italic">Almost Correct</span> and <span id="S4.SS3.p2.1.11" class="ltx_text ltx_font_italic">Incorrect</span>.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Direct Question Generation (DirectQG) ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> reports label distribution for questions and answers randomly-sampled from those generated by <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{TransVQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><msup id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2.2" xref="S4.SS3.p3.1.m1.1.1.2.2.cmml">TransVQ</mi><mn id="S4.SS3.p3.1.m1.1.1.2.3" xref="S4.SS3.p3.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><times id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1"></times><apply id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p3.1.m1.1.1.2.1.cmml" xref="S4.SS3.p3.1.m1.1.1.2">superscript</csymbol><ci id="S4.SS3.p3.1.m1.1.1.2.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2.2">TransVQ</ci><cn type="integer" id="S4.SS3.p3.1.m1.1.1.2.3.cmml" xref="S4.SS3.p3.1.m1.1.1.2.3">2</cn></apply><ci id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\mathrm{TransVQ}^{2}\!\mathrm{A}</annotation></semantics></math><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For pairs generated by DirectQG, we did not perform exhaustive verification; we only asked the raters to annotate <span id="footnote2.1" class="ltx_text ltx_font_italic">Correct</span> and <span id="footnote2.2" class="ltx_text ltx_font_italic">Almost Correct</span> questions related to “no” answers.</span></span></span>. Across languages, we observe at least 75% <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_italic">Correct</span> or <span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_italic">Almost Correct</span> questions and, given these questions, at least 90% <span id="S4.SS3.p3.1.3" class="ltx_text ltx_font_italic">Correct</span> or <span id="S4.SS3.p3.1.4" class="ltx_text ltx_font_italic">Almost Correct</span> answers. This highlights the effectiveness of our approach.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Answer Expansion and Standardization</span>.
We split the generated questions into 4 categories: boolean, numeric, color, and others. We then asked the annotators to perform standardization of boolean, numeric, and color questions based on each language’s guideline. For the rest of the questions, we tasked another set of at least 2 annotators per language to expand the answers to these questions with as many additionally correct (but not overly lengthy) answers as they can.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">Additional Filtering</span>.
Our raters performed another round of verification, filtering out examples with “ambiguous” and “responsible-AI-sensitive” questions and/or with inappropriate image content. The raters also labeled “Collection” questions that are likely to lead to long answers that are difficult to evaluate, such as for “What is on the table?” when there are multiple items, without filter them out.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.F4.1" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:244.5pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-151.8pt,94.9pt) scale(0.562496160791221,0.562496160791221) ;"><img src="/html/2209.05401/assets/figure/what.png" id="S4.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="960" height="600" alt="Refer to caption">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.F4.2" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:84.1pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-169.1pt,36.2pt) scale(0.535710636618626,0.535710636618626) ;"><img src="/html/2209.05401/assets/x4.png" id="S4.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="777" height="166" alt="Refer to caption">
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Top answer cloud is for <em id="S4.F4.8.1" class="ltx_emph ltx_font_italic">“What”</em> questions (excluding <em id="S4.F4.9.2" class="ltx_emph ltx_font_italic">“What color”</em>). Bottom answer clouds from left to right are for <em id="S4.F4.10.3" class="ltx_emph ltx_font_italic">“What color”</em>, Boolean <em id="S4.F4.11.4" class="ltx_emph ltx_font_italic">“Is/Are/Was/Were/Do/Does/Did”</em>, and <em id="S4.F4.12.5" class="ltx_emph ltx_font_italic">“How many”</em> questions, respectively.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Analysis and Discussion</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p"><span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">Size and Question Type and Answer Distributions</span>.
<math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\mathrm{MaXM}</annotation></semantics></math> v1 includes 2,142 questions in 7 languages: English (298), French (293), Hindi (294), Hebrew (315), Romanian (333), Thai (302), and Chinese (307).</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.2" class="ltx_p">Table <a href="#S3.T4" title="Table 4 ‣ 3.3 Direct Question Generation (DirectQG) ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows a breakdown of question types from <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><ci id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\mathrm{MaXM}</annotation></semantics></math>. Since question prefixes in some languages are not indicative of question types (e.g., Thai does not always begin the "What" questions with the Thai "What"), we estimate a question’s type using the prefix of its <span id="S4.SS4.p2.2.1" class="ltx_text ltx_font_bold">English</span> version before translation. We observe diverse types and a high degree of linguistic variations.
Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.3 Human Annotation ‣ 4 MaXM: Multilingual VQA Benchmark ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents word clouds for answers for selected question types: <em id="S4.SS4.p2.2.2" class="ltx_emph ltx_font_italic">"What"</em>, <em id="S4.SS4.p2.2.3" class="ltx_emph ltx_font_italic">"What color"</em>, Boolean, and <em id="S4.SS4.p2.2.4" class="ltx_emph ltx_font_italic">"How many"</em>, to further illustrate the diverse answers within <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mi id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><ci id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">\mathrm{MaXM}</annotation></semantics></math> for each question type.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Comparison to xGQA</span>.
In terms of settings, one difference is the languages of the answers; xGQA operates in the “cross-lingual" setting where the input question can be non-English but the output answer is always English. While this simplifies the evaluation process, we argue that the “multilingual" setting with non-English answers considered in <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mi id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><ci id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\mathrm{MaXM}</annotation></semantics></math> is more practical.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Another difference is the definition of the zero-shot setting; xGQA refers to unseen languages (not images) whereas our setting is more general, referring to both unseen images and languages. Finally, the type of translated data and how it is used for training are different; we only consider zero-shot setting and always use machine-translated questions for training, while xGQA considers both zero-shot and few-shot settings with human-translated questions involved only in the few-shot case.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.2" class="ltx_p">In terms of the datasets, xGQA inherits the characteristics of GQA, whose questions are restricted in style (e.g., generated by a probabilistic template-based question engine) and in the skills required (e.g., reasoning-based with multi-step inference of object attributes and relationships) <cite class="ltx_cite ltx_citemacro_cite">Hudson &amp; Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>. In contrast, <math id="S4.SS4.p5.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S4.SS4.p5.1.m1.1a"><mi id="S4.SS4.p5.1.m1.1.1" xref="S4.SS4.p5.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.1.m1.1b"><ci id="S4.SS4.p5.1.m1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.1.m1.1c">\mathrm{MaXM}</annotation></semantics></math>’s questions are more general. Additionally, xGQA considers the same set of questions for all languages, whereas <math id="S4.SS4.p5.2.m2.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S4.SS4.p5.2.m2.1a"><mi id="S4.SS4.p5.2.m2.1.1" xref="S4.SS4.p5.2.m2.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.2.m2.1b"><ci id="S4.SS4.p5.2.m2.1.1.cmml" xref="S4.SS4.p5.2.m2.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.2.m2.1c">\mathrm{MaXM}</annotation></semantics></math> considers different sets of questions guided by the captions in each language.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.3.1" class="ltx_tr">
<td id="S4.T5.3.1.1" class="ltx_td ltx_align_center ltx_border_r" colspan="3">Model</td>
<td id="S4.T5.3.1.2" class="ltx_td ltx_align_center" colspan="7">Language</td>
</tr>
<tr id="S4.T5.3.2" class="ltx_tr">
<td id="S4.T5.3.2.1" class="ltx_td ltx_border_r" colspan="3"></td>
<td id="S4.T5.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">en</td>
<td id="S4.T5.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">fr</td>
<td id="S4.T5.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">hi</td>
<td id="S4.T5.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">iw</td>
<td id="S4.T5.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ro</td>
<td id="S4.T5.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">th</td>
<td id="S4.T5.3.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">zh</td>
</tr>
<tr id="S4.T5.3.3" class="ltx_tr">
<td id="S4.T5.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.3.1.1" class="ltx_text"><em id="S4.T5.3.3.1.1.1" class="ltx_emph ltx_font_italic">Translate-Test</em></span></td>
<td id="S4.T5.3.3.2" class="ltx_td ltx_align_center ltx_border_t">OFA <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">ofa</span></cite>
</td>
<td id="S4.T5.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[470M]</td>
<td id="S4.T5.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.6</td>
<td id="S4.T5.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.3</td>
<td id="S4.T5.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.5</td>
<td id="S4.T5.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.7</td>
<td id="S4.T5.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.4</td>
<td id="S4.T5.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.5</td>
<td id="S4.T5.3.3.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">17.6</td>
</tr>
<tr id="S4.T5.3.4" class="ltx_tr">
<td id="S4.T5.3.4.1" class="ltx_td ltx_align_center">BLIP2 <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">blip2</span></cite>
</td>
<td id="S4.T5.3.4.2" class="ltx_td ltx_align_center ltx_border_r">[11B]</td>
<td id="S4.T5.3.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.3.4.3.1" class="ltx_text ltx_font_italic">48.7</span></td>
<td id="S4.T5.3.4.4" class="ltx_td ltx_align_center ltx_border_r">20.1</td>
<td id="S4.T5.3.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.3.4.5.1" class="ltx_text ltx_font_italic">63.3</span></td>
<td id="S4.T5.3.4.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.3.4.6.1" class="ltx_text ltx_font_italic">49.2</span></td>
<td id="S4.T5.3.4.7" class="ltx_td ltx_align_center ltx_border_r">39.0</td>
<td id="S4.T5.3.4.8" class="ltx_td ltx_align_center ltx_border_r">49.0</td>
<td id="S4.T5.3.4.9" class="ltx_td ltx_nopad_r ltx_align_center">28.0</td>
</tr>
<tr id="S4.T5.3.5" class="ltx_tr">
<td id="S4.T5.3.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.5.1.1" class="ltx_text"><em id="S4.T5.3.5.1.1.1" class="ltx_emph ltx_font_italic">Translate-Train</em></span></td>
<td id="S4.T5.3.5.2" class="ltx_td ltx_align_center ltx_border_t">Simple MPT (ours)</td>
<td id="S4.T5.3.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[1.5B]</td>
<td id="S4.T5.3.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.6</td>
<td id="S4.T5.3.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.3.5.5.1" class="ltx_text ltx_font_italic">36.2</span></td>
<td id="S4.T5.3.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">55.1</td>
<td id="S4.T5.3.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.6</td>
<td id="S4.T5.3.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.3.5.8.1" class="ltx_text ltx_font_italic">42.3</span></td>
<td id="S4.T5.3.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.3.5.9.1" class="ltx_text ltx_font_italic">50.0</span></td>
<td id="S4.T5.3.5.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T5.3.5.10.1" class="ltx_text ltx_font_italic">30.3</span></td>
</tr>
<tr id="S4.T5.3.6" class="ltx_tr">
<td id="S4.T5.3.6.1" class="ltx_td ltx_align_center ltx_border_bb">PaLI <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">pali2</span></cite>
</td>
<td id="S4.T5.3.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">[17B]</td>
<td id="S4.T5.3.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.3.6.3.1" class="ltx_text ltx_font_bold">56.4</span></td>
<td id="S4.T5.3.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.3.6.4.1" class="ltx_text ltx_font_bold">46.4</span></td>
<td id="S4.T5.3.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.3.6.5.1" class="ltx_text ltx_font_bold">67.3</span></td>
<td id="S4.T5.3.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.3.6.6.1" class="ltx_text ltx_font_bold">60.0</span></td>
<td id="S4.T5.3.6.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.3.6.7.1" class="ltx_text ltx_font_bold">57.4</span></td>
<td id="S4.T5.3.6.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.3.6.8.1" class="ltx_text ltx_font_bold">65.6</span></td>
<td id="S4.T5.3.6.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T5.3.6.9.1" class="ltx_text ltx_font_bold">46.9</span></td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="S4.T5.2.1" class="ltx_text ltx_font_bold">Benchmarking VQA models on <math id="S4.T5.2.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S4.T5.2.1.m1.1b"><mi id="S4.T5.2.1.m1.1.1" xref="S4.T5.2.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S4.T5.2.1.m1.1c"><ci id="S4.T5.2.1.m1.1.1.cmml" xref="S4.T5.2.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.1.m1.1d">\mathrm{MaXM}</annotation></semantics></math></span>. Accuracy (%) of OFA-Large (OFA), BLIP2, our lightweight Simple MPT, and PaLI-17B (PaLI), with approximate parameter count in brackets. All are finetuned on VQA2.0, English-only for <em id="S4.T5.7.2" class="ltx_emph ltx_font_italic">Translate-Train</em> and 13 languages for Translate-Test. Best results are <span id="S4.T5.8.3" class="ltx_text ltx_font_bold">bold</span>. Second best <span id="S4.T5.9.4" class="ltx_text ltx_font_italic">italized</span>.</figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.2.2.3" class="ltx_tr">
<td id="S4.T6.2.2.3.1" class="ltx_td ltx_align_center ltx_border_r" colspan="2">Model</td>
<td id="S4.T6.2.2.3.2" class="ltx_td ltx_align_center ltx_border_r">Training</td>
<td id="S4.T6.2.2.3.3" class="ltx_td ltx_align_center" colspan="7">Language</td>
</tr>
<tr id="S4.T6.2.2.4" class="ltx_tr">
<td id="S4.T6.2.2.4.1" class="ltx_td ltx_border_r" colspan="2"></td>
<td id="S4.T6.2.2.4.2" class="ltx_td ltx_align_center ltx_border_r">Dataset</td>
<td id="S4.T6.2.2.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">en</td>
<td id="S4.T6.2.2.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">fr</td>
<td id="S4.T6.2.2.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">hi</td>
<td id="S4.T6.2.2.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">iw</td>
<td id="S4.T6.2.2.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ro</td>
<td id="S4.T6.2.2.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">th</td>
<td id="S4.T6.2.2.4.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">zh</td>
</tr>
<tr id="S4.T6.2.2.5" class="ltx_tr">
<td id="S4.T6.2.2.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T6.2.2.5.1.1" class="ltx_text"><em id="S4.T6.2.2.5.1.1.1" class="ltx_emph ltx_font_italic">Translate-Train</em></span></td>
<td id="S4.T6.2.2.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Single-Language</td>
<td id="S4.T6.2.2.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">VQA2.0</td>
<td id="S4.T6.2.2.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37.6</td>
<td id="S4.T6.2.2.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.8</td>
<td id="S4.T6.2.2.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.7</td>
<td id="S4.T6.2.2.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.6</td>
<td id="S4.T6.2.2.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.0</td>
<td id="S4.T6.2.2.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.0</td>
<td id="S4.T6.2.2.5.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">29.0</td>
</tr>
<tr id="S4.T6.2.2.6" class="ltx_tr">
<td id="S4.T6.2.2.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Simple MPT</td>
<td id="S4.T6.2.2.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">VQA2.0</td>
<td id="S4.T6.2.2.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.6</td>
<td id="S4.T6.2.2.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.2</td>
<td id="S4.T6.2.2.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">55.1</td>
<td id="S4.T6.2.2.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.6</td>
<td id="S4.T6.2.2.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">42.3</td>
<td id="S4.T6.2.2.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.0</td>
<td id="S4.T6.2.2.6.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">30.3</td>
</tr>
<tr id="S4.T6.1.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">Simple MPT</td>
<td id="S4.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S4.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S4.T6.1.1.1.1.m1.1a"><mrow id="S4.T6.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.cmml"><msup id="S4.T6.1.1.1.1.m1.1.1.2" xref="S4.T6.1.1.1.1.m1.1.1.2.cmml"><mi id="S4.T6.1.1.1.1.m1.1.1.2.2" xref="S4.T6.1.1.1.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S4.T6.1.1.1.1.m1.1.1.2.3" xref="S4.T6.1.1.1.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S4.T6.1.1.1.1.m1.1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.T6.1.1.1.1.m1.1.1.3" xref="S4.T6.1.1.1.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.m1.1b"><apply id="S4.T6.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1"><times id="S4.T6.1.1.1.1.m1.1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1.1"></times><apply id="S4.T6.1.1.1.1.m1.1.1.2.cmml" xref="S4.T6.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T6.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1.2">superscript</csymbol><ci id="S4.T6.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T6.1.1.1.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S4.T6.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T6.1.1.1.1.m1.1.1.2.3">2</cn></apply><ci id="S4.T6.1.1.1.1.m1.1.1.3.cmml" xref="S4.T6.1.1.1.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-COCO</td>
<td id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">48.0</span></td>
<td id="S4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">43.3</span></td>
<td id="S4.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.1.1.1.5.1" class="ltx_text ltx_font_bold">56.8</span></td>
<td id="S4.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.1.1.1.6.1" class="ltx_text ltx_font_bold">42.2</span></td>
<td id="S4.T6.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.1.1.1.7.1" class="ltx_text ltx_font_bold">45.6</span></td>
<td id="S4.T6.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.1.1.1.8.1" class="ltx_text ltx_font_bold">52.3</span></td>
<td id="S4.T6.1.1.1.9" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.1.1.1.9.1" class="ltx_text ltx_font_bold">34.5</span></td>
</tr>
<tr id="S4.T6.2.2.2" class="ltx_tr">
<td id="S4.T6.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Simple MPT</td>
<td id="S4.T6.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S4.T6.2.2.2.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S4.T6.2.2.2.1.m1.1a"><mrow id="S4.T6.2.2.2.1.m1.1.1" xref="S4.T6.2.2.2.1.m1.1.1.cmml"><msup id="S4.T6.2.2.2.1.m1.1.1.2" xref="S4.T6.2.2.2.1.m1.1.1.2.cmml"><mi id="S4.T6.2.2.2.1.m1.1.1.2.2" xref="S4.T6.2.2.2.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S4.T6.2.2.2.1.m1.1.1.2.3" xref="S4.T6.2.2.2.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S4.T6.2.2.2.1.m1.1.1.1" xref="S4.T6.2.2.2.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.T6.2.2.2.1.m1.1.1.3" xref="S4.T6.2.2.2.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.1.m1.1b"><apply id="S4.T6.2.2.2.1.m1.1.1.cmml" xref="S4.T6.2.2.2.1.m1.1.1"><times id="S4.T6.2.2.2.1.m1.1.1.1.cmml" xref="S4.T6.2.2.2.1.m1.1.1.1"></times><apply id="S4.T6.2.2.2.1.m1.1.1.2.cmml" xref="S4.T6.2.2.2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T6.2.2.2.1.m1.1.1.2.1.cmml" xref="S4.T6.2.2.2.1.m1.1.1.2">superscript</csymbol><ci id="S4.T6.2.2.2.1.m1.1.1.2.2.cmml" xref="S4.T6.2.2.2.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S4.T6.2.2.2.1.m1.1.1.2.3.cmml" xref="S4.T6.2.2.2.1.m1.1.1.2.3">2</cn></apply><ci id="S4.T6.2.2.2.1.m1.1.1.3.cmml" xref="S4.T6.2.2.2.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-CC3M</td>
<td id="S4.T6.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">38.3</td>
<td id="S4.T6.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">34.1</td>
<td id="S4.T6.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">45.6</td>
<td id="S4.T6.2.2.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">34.9</td>
<td id="S4.T6.2.2.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">36.9</td>
<td id="S4.T6.2.2.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">45.0</td>
<td id="S4.T6.2.2.2.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">29.0</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span><span id="S4.T6.4.1" class="ltx_text ltx_font_bold">Effect of Training Data Sources</span>. Accuracy of Single-Language baselines (MPT architecture) and Accuracy (%) of MPT models trained on different training datasets.</figcaption>
</figure>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T7.5.1" class="ltx_tr">
<td id="S4.T7.5.1.1" class="ltx_td ltx_align_center ltx_border_r">Metric</td>
<td id="S4.T7.5.1.2" class="ltx_td ltx_align_center ltx_border_r" colspan="2">Model</td>
<td id="S4.T7.5.1.3" class="ltx_td ltx_align_center" colspan="7">Language</td>
</tr>
<tr id="S4.T7.5.2" class="ltx_tr">
<td id="S4.T7.5.2.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T7.5.2.2" class="ltx_td ltx_border_r" colspan="2"></td>
<td id="S4.T7.5.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">en</td>
<td id="S4.T7.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">fr</td>
<td id="S4.T7.5.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">hi</td>
<td id="S4.T7.5.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">iw</td>
<td id="S4.T7.5.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ro</td>
<td id="S4.T7.5.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">th</td>
<td id="S4.T7.5.2.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">zh</td>
</tr>
<tr id="S4.T7.5.3" class="ltx_tr">
<td id="S4.T7.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T7.5.3.1.1" class="ltx_text">Accuracy</span></td>
<td id="S4.T7.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><em id="S4.T7.5.3.2.1" class="ltx_emph ltx_font_italic">Translate-Train</em></td>
<td id="S4.T7.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Simple MPT</td>
<td id="S4.T7.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.6</td>
<td id="S4.T7.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.5.3.5.1" class="ltx_text ltx_font_bold">36.2</span></td>
<td id="S4.T7.5.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">55.1</td>
<td id="S4.T7.5.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.6</td>
<td id="S4.T7.5.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.5.3.8.1" class="ltx_text ltx_font_bold">42.3</span></td>
<td id="S4.T7.5.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.5.3.9.1" class="ltx_text ltx_font_bold">50.0</span></td>
<td id="S4.T7.5.3.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T7.5.3.10.1" class="ltx_text ltx_font_bold">30.3</span></td>
</tr>
<tr id="S4.T7.5.4" class="ltx_tr">
<td id="S4.T7.5.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T7.5.4.1.1" class="ltx_text"><em id="S4.T7.5.4.1.1.1" class="ltx_emph ltx_font_italic">Translate-Test</em></span></td>
<td id="S4.T7.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OFA</td>
<td id="S4.T7.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.6</td>
<td id="S4.T7.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.3</td>
<td id="S4.T7.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.5</td>
<td id="S4.T7.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.7</td>
<td id="S4.T7.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.4</td>
<td id="S4.T7.5.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.5</td>
<td id="S4.T7.5.4.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">17.6</td>
</tr>
<tr id="S4.T7.5.5" class="ltx_tr">
<td id="S4.T7.5.5.1" class="ltx_td ltx_align_center ltx_border_r">BLIP2</td>
<td id="S4.T7.5.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.5.5.2.1" class="ltx_text ltx_font_bold">48.7</span></td>
<td id="S4.T7.5.5.3" class="ltx_td ltx_align_center ltx_border_r">20.1</td>
<td id="S4.T7.5.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.5.5.4.1" class="ltx_text ltx_font_bold">63.3</span></td>
<td id="S4.T7.5.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.5.5.5.1" class="ltx_text ltx_font_bold">49.2</span></td>
<td id="S4.T7.5.5.6" class="ltx_td ltx_align_center ltx_border_r">39.0</td>
<td id="S4.T7.5.5.7" class="ltx_td ltx_align_center ltx_border_r">49.0</td>
<td id="S4.T7.5.5.8" class="ltx_td ltx_nopad_r ltx_align_center">28.0</td>
</tr>
<tr id="S4.T7.5.6" class="ltx_tr">
<td id="S4.T7.5.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T7.5.6.1.1" class="ltx_text">CIDEr</span></td>
<td id="S4.T7.5.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><em id="S4.T7.5.6.2.1" class="ltx_emph ltx_font_italic">Translate-Train</em></td>
<td id="S4.T7.5.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Simple MPT</td>
<td id="S4.T7.5.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">91.5</td>
<td id="S4.T7.5.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.5.6.5.1" class="ltx_text ltx_font_bold">102.0</span></td>
<td id="S4.T7.5.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.0</td>
<td id="S4.T7.5.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.6</td>
<td id="S4.T7.5.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.5.6.8.1" class="ltx_text ltx_font_bold">89.6</span></td>
<td id="S4.T7.5.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.7</td>
<td id="S4.T7.5.6.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T7.5.6.10.1" class="ltx_text ltx_font_bold">68.4</span></td>
</tr>
<tr id="S4.T7.5.7" class="ltx_tr">
<td id="S4.T7.5.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T7.5.7.1.1" class="ltx_text"><em id="S4.T7.5.7.1.1.1" class="ltx_emph ltx_font_italic">Translate-Test</em></span></td>
<td id="S4.T7.5.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OFA</td>
<td id="S4.T7.5.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.3</td>
<td id="S4.T7.5.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.4</td>
<td id="S4.T7.5.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.3</td>
<td id="S4.T7.5.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.6</td>
<td id="S4.T7.5.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.4</td>
<td id="S4.T7.5.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.2</td>
<td id="S4.T7.5.7.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">51.1</td>
</tr>
<tr id="S4.T7.5.8" class="ltx_tr">
<td id="S4.T7.5.8.1" class="ltx_td ltx_align_center ltx_border_r">BLIP2</td>
<td id="S4.T7.5.8.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.5.8.2.1" class="ltx_text ltx_font_bold">121.8</span></td>
<td id="S4.T7.5.8.3" class="ltx_td ltx_align_center ltx_border_r">67.6</td>
<td id="S4.T7.5.8.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.5.8.4.1" class="ltx_text ltx_font_bold">88.1</span></td>
<td id="S4.T7.5.8.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.5.8.5.1" class="ltx_text ltx_font_bold">93.8</span></td>
<td id="S4.T7.5.8.6" class="ltx_td ltx_align_center ltx_border_r">79.1</td>
<td id="S4.T7.5.8.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.5.8.7.1" class="ltx_text ltx_font_bold">91.5</span></td>
<td id="S4.T7.5.8.8" class="ltx_td ltx_nopad_r ltx_align_center">65.7</td>
</tr>
<tr id="S4.T7.5.9" class="ltx_tr">
<td id="S4.T7.5.9.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T7.5.9.1.1" class="ltx_text">ROUGE-L</span></td>
<td id="S4.T7.5.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><em id="S4.T7.5.9.2.1" class="ltx_emph ltx_font_italic">Translate-Train</em></td>
<td id="S4.T7.5.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Simple MPT</td>
<td id="S4.T7.5.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.0</td>
<td id="S4.T7.5.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.5.9.5.1" class="ltx_text ltx_font_bold">47.9</span></td>
<td id="S4.T7.5.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">57.9</td>
<td id="S4.T7.5.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">42.8</td>
<td id="S4.T7.5.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.5.9.8.1" class="ltx_text ltx_font_bold">49.4</span></td>
<td id="S4.T7.5.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.5.9.9.1" class="ltx_text ltx_font_bold">57.7</span></td>
<td id="S4.T7.5.9.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">37.7</td>
</tr>
<tr id="S4.T7.5.10" class="ltx_tr">
<td id="S4.T7.5.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T7.5.10.1.1" class="ltx_text"><em id="S4.T7.5.10.1.1.1" class="ltx_emph ltx_font_italic">Translate-Test</em></span></td>
<td id="S4.T7.5.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OFA</td>
<td id="S4.T7.5.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.5</td>
<td id="S4.T7.5.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.2</td>
<td id="S4.T7.5.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.0</td>
<td id="S4.T7.5.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.8</td>
<td id="S4.T7.5.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.3</td>
<td id="S4.T7.5.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.0</td>
<td id="S4.T7.5.10.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">31.4</td>
</tr>
<tr id="S4.T7.5.11" class="ltx_tr">
<td id="S4.T7.5.11.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">BLIP2</td>
<td id="S4.T7.5.11.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T7.5.11.2.1" class="ltx_text ltx_font_bold">62.6</span></td>
<td id="S4.T7.5.11.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">32.6</td>
<td id="S4.T7.5.11.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T7.5.11.4.1" class="ltx_text ltx_font_bold">67.2</span></td>
<td id="S4.T7.5.11.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T7.5.11.5.1" class="ltx_text ltx_font_bold">52.6</span></td>
<td id="S4.T7.5.11.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">47.4</td>
<td id="S4.T7.5.11.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">53.6</td>
<td id="S4.T7.5.11.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T7.5.11.8.1" class="ltx_text ltx_font_bold">39.9</span></td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span><span id="S4.T7.8.1" class="ltx_text ltx_font_bold">Results on Soft Metrics</span>. Accuracy (%), CIDEr (<math id="S4.T7.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T7.3.m1.1b"><mo id="S4.T7.3.m1.1.1" xref="S4.T7.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T7.3.m1.1c"><times id="S4.T7.3.m1.1.1.cmml" xref="S4.T7.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.m1.1d">\times</annotation></semantics></math> 100), and ROUGE-L (<math id="S4.T7.4.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T7.4.m2.1b"><mo id="S4.T7.4.m2.1.1" xref="S4.T7.4.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T7.4.m2.1c"><times id="S4.T7.4.m2.1.1.cmml" xref="S4.T7.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.m2.1d">\times</annotation></semantics></math> 100) of Simple MPT, OFA, and BLIP2. All are finetuned on VQA2.0. Best results are <span id="S4.T7.9.2" class="ltx_text ltx_font_bold">bold</span>.</figcaption>
</figure>
<figure id="S4.T8" class="ltx_table">
<table id="S4.T8.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T8.5.1" class="ltx_tr">
<td id="S4.T8.5.1.1" class="ltx_td ltx_align_center ltx_border_r">Metric</td>
<td id="S4.T8.5.1.2" class="ltx_td ltx_align_center ltx_border_r" colspan="2">Model</td>
<td id="S4.T8.5.1.3" class="ltx_td ltx_align_center" colspan="7">Language</td>
</tr>
<tr id="S4.T8.5.2" class="ltx_tr">
<td id="S4.T8.5.2.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T8.5.2.2" class="ltx_td ltx_border_r" colspan="2"></td>
<td id="S4.T8.5.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">en</td>
<td id="S4.T8.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">fr</td>
<td id="S4.T8.5.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">hi</td>
<td id="S4.T8.5.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">iw</td>
<td id="S4.T8.5.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ro</td>
<td id="S4.T8.5.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">th</td>
<td id="S4.T8.5.2.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">zh</td>
</tr>
<tr id="S4.T8.5.3" class="ltx_tr">
<td id="S4.T8.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T8.5.3.1.1" class="ltx_text">Accuracy</span></td>
<td id="S4.T8.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T8.5.3.2.1" class="ltx_text"><em id="S4.T8.5.3.2.1.1" class="ltx_emph ltx_font_italic">Translate-Test</em></span></td>
<td id="S4.T8.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OFA</td>
<td id="S4.T8.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.6</td>
<td id="S4.T8.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.3</td>
<td id="S4.T8.5.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.5</td>
<td id="S4.T8.5.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.7</td>
<td id="S4.T8.5.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.4</td>
<td id="S4.T8.5.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.5</td>
<td id="S4.T8.5.3.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">17.6</td>
</tr>
<tr id="S4.T8.5.4" class="ltx_tr">
<td id="S4.T8.5.4.1" class="ltx_td ltx_align_center ltx_border_r">BLIP2</td>
<td id="S4.T8.5.4.2" class="ltx_td ltx_align_center ltx_border_r">48.7</td>
<td id="S4.T8.5.4.3" class="ltx_td ltx_align_center ltx_border_r">20.1</td>
<td id="S4.T8.5.4.4" class="ltx_td ltx_align_center ltx_border_r">63.3</td>
<td id="S4.T8.5.4.5" class="ltx_td ltx_align_center ltx_border_r">49.2</td>
<td id="S4.T8.5.4.6" class="ltx_td ltx_align_center ltx_border_r">39.0</td>
<td id="S4.T8.5.4.7" class="ltx_td ltx_align_center ltx_border_r">49.0</td>
<td id="S4.T8.5.4.8" class="ltx_td ltx_nopad_r ltx_align_center">28.0</td>
</tr>
<tr id="S4.T8.5.5" class="ltx_tr">
<td id="S4.T8.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T8.5.5.1.1" class="ltx_text">No adapt</span></td>
<td id="S4.T8.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OFA</td>
<td id="S4.T8.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.6</td>
<td id="S4.T8.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.3</td>
<td id="S4.T8.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.3</td>
<td id="S4.T8.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.3</td>
<td id="S4.T8.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.8</td>
<td id="S4.T8.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0</td>
<td id="S4.T8.5.5.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.0</td>
</tr>
<tr id="S4.T8.5.6" class="ltx_tr">
<td id="S4.T8.5.6.1" class="ltx_td ltx_align_center ltx_border_r">BLIP2</td>
<td id="S4.T8.5.6.2" class="ltx_td ltx_align_center ltx_border_r">48.7</td>
<td id="S4.T8.5.6.3" class="ltx_td ltx_align_center ltx_border_r">9.2</td>
<td id="S4.T8.5.6.4" class="ltx_td ltx_align_center ltx_border_r">0.3</td>
<td id="S4.T8.5.6.5" class="ltx_td ltx_align_center ltx_border_r">1.9</td>
<td id="S4.T8.5.6.6" class="ltx_td ltx_align_center ltx_border_r">3.9</td>
<td id="S4.T8.5.6.7" class="ltx_td ltx_align_center ltx_border_r">7.0</td>
<td id="S4.T8.5.6.8" class="ltx_td ltx_nopad_r ltx_align_center">2.3</td>
</tr>
<tr id="S4.T8.5.7" class="ltx_tr">
<td id="S4.T8.5.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T8.5.7.1.1" class="ltx_text">CIDEr</span></td>
<td id="S4.T8.5.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T8.5.7.2.1" class="ltx_text"><em id="S4.T8.5.7.2.1.1" class="ltx_emph ltx_font_italic">Translate-Test</em></span></td>
<td id="S4.T8.5.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OFA</td>
<td id="S4.T8.5.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.3</td>
<td id="S4.T8.5.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.4</td>
<td id="S4.T8.5.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.3</td>
<td id="S4.T8.5.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.6</td>
<td id="S4.T8.5.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.4</td>
<td id="S4.T8.5.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.2</td>
<td id="S4.T8.5.7.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">51.1</td>
</tr>
<tr id="S4.T8.5.8" class="ltx_tr">
<td id="S4.T8.5.8.1" class="ltx_td ltx_align_center ltx_border_r">BLIP2</td>
<td id="S4.T8.5.8.2" class="ltx_td ltx_align_center ltx_border_r">121.8</td>
<td id="S4.T8.5.8.3" class="ltx_td ltx_align_center ltx_border_r">67.6</td>
<td id="S4.T8.5.8.4" class="ltx_td ltx_align_center ltx_border_r">88.1</td>
<td id="S4.T8.5.8.5" class="ltx_td ltx_align_center ltx_border_r">93.8</td>
<td id="S4.T8.5.8.6" class="ltx_td ltx_align_center ltx_border_r">79.1</td>
<td id="S4.T8.5.8.7" class="ltx_td ltx_align_center ltx_border_r">91.5</td>
<td id="S4.T8.5.8.8" class="ltx_td ltx_nopad_r ltx_align_center">65.7</td>
</tr>
<tr id="S4.T8.5.9" class="ltx_tr">
<td id="S4.T8.5.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T8.5.9.1.1" class="ltx_text">No adapt</span></td>
<td id="S4.T8.5.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OFA</td>
<td id="S4.T8.5.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.3</td>
<td id="S4.T8.5.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.8</td>
<td id="S4.T8.5.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.7</td>
<td id="S4.T8.5.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.8</td>
<td id="S4.T8.5.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.6</td>
<td id="S4.T8.5.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.8</td>
<td id="S4.T8.5.9.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">3.6</td>
</tr>
<tr id="S4.T8.5.10" class="ltx_tr">
<td id="S4.T8.5.10.1" class="ltx_td ltx_align_center ltx_border_r">BLIP2</td>
<td id="S4.T8.5.10.2" class="ltx_td ltx_align_center ltx_border_r">121.8</td>
<td id="S4.T8.5.10.3" class="ltx_td ltx_align_center ltx_border_r">28.0</td>
<td id="S4.T8.5.10.4" class="ltx_td ltx_align_center ltx_border_r">5.1</td>
<td id="S4.T8.5.10.5" class="ltx_td ltx_align_center ltx_border_r">4.0</td>
<td id="S4.T8.5.10.6" class="ltx_td ltx_align_center ltx_border_r">12.1</td>
<td id="S4.T8.5.10.7" class="ltx_td ltx_align_center ltx_border_r">16.0</td>
<td id="S4.T8.5.10.8" class="ltx_td ltx_nopad_r ltx_align_center">8.2</td>
</tr>
<tr id="S4.T8.5.11" class="ltx_tr">
<td id="S4.T8.5.11.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T8.5.11.1.1" class="ltx_text">ROUGE-L</span></td>
<td id="S4.T8.5.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T8.5.11.2.1" class="ltx_text"><em id="S4.T8.5.11.2.1.1" class="ltx_emph ltx_font_italic">Translate-Test</em></span></td>
<td id="S4.T8.5.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OFA</td>
<td id="S4.T8.5.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.5</td>
<td id="S4.T8.5.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.2</td>
<td id="S4.T8.5.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.0</td>
<td id="S4.T8.5.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.8</td>
<td id="S4.T8.5.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.3</td>
<td id="S4.T8.5.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.0</td>
<td id="S4.T8.5.11.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">31.4</td>
</tr>
<tr id="S4.T8.5.12" class="ltx_tr">
<td id="S4.T8.5.12.1" class="ltx_td ltx_align_center ltx_border_r">BLIP2</td>
<td id="S4.T8.5.12.2" class="ltx_td ltx_align_center ltx_border_r">62.6</td>
<td id="S4.T8.5.12.3" class="ltx_td ltx_align_center ltx_border_r">32.6</td>
<td id="S4.T8.5.12.4" class="ltx_td ltx_align_center ltx_border_r">67.2</td>
<td id="S4.T8.5.12.5" class="ltx_td ltx_align_center ltx_border_r">52.6</td>
<td id="S4.T8.5.12.6" class="ltx_td ltx_align_center ltx_border_r">47.4</td>
<td id="S4.T8.5.12.7" class="ltx_td ltx_align_center ltx_border_r">53.6</td>
<td id="S4.T8.5.12.8" class="ltx_td ltx_nopad_r ltx_align_center">39.9</td>
</tr>
<tr id="S4.T8.5.13" class="ltx_tr">
<td id="S4.T8.5.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T8.5.13.1.1" class="ltx_text">No adapt</span></td>
<td id="S4.T8.5.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OFA</td>
<td id="S4.T8.5.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.5</td>
<td id="S4.T8.5.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.6</td>
<td id="S4.T8.5.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.4</td>
<td id="S4.T8.5.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.8</td>
<td id="S4.T8.5.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.9</td>
<td id="S4.T8.5.13.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.3</td>
<td id="S4.T8.5.13.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">6.6</td>
</tr>
<tr id="S4.T8.5.14" class="ltx_tr">
<td id="S4.T8.5.14.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">BLIP2</td>
<td id="S4.T8.5.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">62.6</td>
<td id="S4.T8.5.14.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">12.5</td>
<td id="S4.T8.5.14.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">6.2</td>
<td id="S4.T8.5.14.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">2.9</td>
<td id="S4.T8.5.14.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">5.4</td>
<td id="S4.T8.5.14.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">10.8</td>
<td id="S4.T8.5.14.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">7.3</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span><span id="S4.T8.7.1" class="ltx_text ltx_font_bold">Results without adaptation for Translate-Test</span>. Accuracy (%), CIDEr (<math id="S4.T8.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T8.3.m1.1b"><mo id="S4.T8.3.m1.1.1" xref="S4.T8.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T8.3.m1.1c"><times id="S4.T8.3.m1.1.1.cmml" xref="S4.T8.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.3.m1.1d">\times</annotation></semantics></math> 100), and ROUGE-L (<math id="S4.T8.4.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T8.4.m2.1b"><mo id="S4.T8.4.m2.1.1" xref="S4.T8.4.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T8.4.m2.1c"><times id="S4.T8.4.m2.1.1.cmml" xref="S4.T8.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.4.m2.1d">\times</annotation></semantics></math> 100) of OFA and BLIP2 with Translate-Test and without (No adapt).</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation Protocol</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Evaluation Metrics</span>. We use Exact Match <em id="S5.SS1.p1.1.2" class="ltx_emph ltx_font_italic">Accuracy</em> as the main evaluation measure for <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\mathrm{MaXM}</annotation></semantics></math>, following previous work on VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib5" title="" class="ltx_ref">2015</a>); Goyal et al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>); Gurari et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>. We deem the answer as correct if it matches any of the ground-truth answers. To assess the degree of strictness of this measure, we also consider <em id="S5.SS1.p1.1.3" class="ltx_emph ltx_font_italic">soft</em> text similarity metrics <em id="S5.SS1.p1.1.4" class="ltx_emph ltx_font_italic">CIDEr</em> <cite class="ltx_cite ltx_citemacro_cite">Vedantam et al. (<a href="#bib.bib58" title="" class="ltx_ref">2015</a>)</cite> and <em id="S5.SS1.p1.1.5" class="ltx_emph ltx_font_italic">ROUGE-L</em> <cite class="ltx_cite ltx_citemacro_cite">Lin (<a href="#bib.bib38" title="" class="ltx_ref">2004</a>)</cite> in our experiments, where we treat each of the ground-truth answers equally as one of the references (as if each of them was answered by an annotator).</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.2" class="ltx_p"><span id="S5.SS1.p2.2.1" class="ltx_text ltx_font_bold">Training Data</span>. MaXM is a test-only benchmark; it cannot be used for training. We designate VQA2.0 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> and its translations as the default training data source for our benchmark, due to its popularity and quality, similarly to the use of COCO-Captions <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite> for the nocaps benchmark <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nocaps</span></cite> in the image captioning task. Nevertheless, we allow free use of existing VQA resources for training as long as the corresponding training images do not overlap with MaXM images. In our experiments, we also consider <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mrow id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><msup id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml"><mi id="S5.SS1.p2.1.m1.1.1.2.2" xref="S5.SS1.p2.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S5.SS1.p2.1.m1.1.1.2.3" xref="S5.SS1.p2.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S5.SS1.p2.1.m1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><times id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1"></times><apply id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.1.1.2.1.cmml" xref="S5.SS1.p2.1.m1.1.1.2">superscript</csymbol><ci id="S5.SS1.p2.1.m1.1.1.2.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S5.SS1.p2.1.m1.1.1.2.3.cmml" xref="S5.SS1.p2.1.m1.1.1.2.3">2</cn></apply><ci id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-COCO and <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mrow id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><msup id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml"><mi id="S5.SS1.p2.2.m2.1.1.2.2" xref="S5.SS1.p2.2.m2.1.1.2.2.cmml">VQ</mi><mn id="S5.SS1.p2.2.m2.1.1.2.3" xref="S5.SS1.p2.2.m2.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S5.SS1.p2.2.m2.1.1.1" xref="S5.SS1.p2.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><times id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1.1"></times><apply id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.2.1.cmml" xref="S5.SS1.p2.2.m2.1.1.2">superscript</csymbol><ci id="S5.SS1.p2.2.m2.1.1.2.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2.2">VQ</ci><cn type="integer" id="S5.SS1.p2.2.m2.1.1.2.3.cmml" xref="S5.SS1.p2.2.m2.1.1.2.3">2</cn></apply><ci id="S5.SS1.p2.2.m2.1.1.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-CC3M <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> to assess the effect of text domain gap.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Models for Multilingual VQA</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Inspired by approaches to multilingual NLP research, we consider two main families of models for mVQA that adapt existing source English VQA datasets to target languages: <em id="S5.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Translate-Test</em> and <em id="S5.SS2.p1.1.2" class="ltx_emph ltx_font_italic">Translate-Train</em>. Translate-Test leaves the training data and the model as-is, but translates the test VQA data to the the source language English, apply the model, and then translate it back to the target language. On the other hand, <em id="S5.SS2.p1.1.3" class="ltx_emph ltx_font_italic">Translate-Train</em> translates the English VQA data to a target language, trains a model on this pseudo VQA data (i.e., their translations), and directly apply the trained model to the test data.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Translate-Test</span>. We consider two open-source state-of-the-art VQA models: <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_bold">OFA-Large</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">ofa</span></cite> and BLIP2 <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">blip2</span></cite>. Neither of them are designed for mVQA.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Translate-Train</span>. We include the results from the state-of-the-art multilingual vision-and-language model <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_bold">PaLI-17B</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">pali2</span></cite>, which pretrains on diverse VQA datasets in 35 languages <cite class="ltx_cite ltx_citemacro_cite">Thapliyal et al. (<a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite> among other datasets, and then finetune on VQA2.0 in 13 languages: en, bn, de, fr, hi, id, iw, ko, pt, ro, ru, th, zh. Further, we implement a lightweight version of PaLI, called Simple Multi-Language Prompted Training, <span id="S5.SS2.p3.1.3" class="ltx_text ltx_font_bold">Simple MPT</span>, with a much smaller model and without vision-and-language pre-training. <span id="S5.SS2.p3.1.4" class="ltx_text ltx_font_bold">Simple MPT</span> is trained on the data in 13 languages in a multi-task fashion. Details can be found in Appendix <a href="#A3" title="Appendix C Simple MPT ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Results</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p"><span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_bold">Main Results</span>. Table <a href="#S4.T5" title="Table 5 ‣ 4.4 Analysis and Discussion ‣ 4 MaXM: Multilingual VQA Benchmark ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> benchmarks our proposed Simple MPT and state-of-the-art VQA models on <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\mathrm{MaXM}</annotation></semantics></math>. We observe that PaLI-17B performs best on all languages. This can be attributed to both the fact that PaLI is the strongest English VQA model and the fact that it was designed to be multilingual, leveraging pre-training image-text corpus in 105 languages. This result suggests it can be beneficial to design and develop multilingual VQA models from day one.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Surprisingly, our proposed Simple MPT model is a strong baseline even though it is much smaller than PaLI and does not leverage multilingual pre-training data. While its English performance is on par with OFA and much worse than BLIP2, its multilingual performance excels, outperforming OFA in all languages and underperforms BLIP2 only for Hindi and Hebrew.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Overall, our result suggests that <em id="S5.SS3.p3.1.1" class="ltx_emph ltx_font_italic">Translate-Train</em> may be a superior approach to mVQA to <em id="S5.SS3.p3.1.2" class="ltx_emph ltx_font_italic">Translate-Test</em>. We note, however, that in our early experiments, we find that Translate-Train is inferior to Translate-Test as an adaptation approach for <em id="S5.SS3.p3.1.3" class="ltx_emph ltx_font_italic">English</em> VQA models. For instance, the answer of finetuned BLIP2 to the French question <em id="S5.SS3.p3.1.4" class="ltx_emph ltx_font_italic">“Outre les fleurs roses, quelle autre couleur y avait-il dans le jardin?”</em> (<em id="S5.SS3.p3.1.5" class="ltx_emph ltx_font_italic">“Besides pink flowers, what other color was there in the garden?”</em>) is <em id="S5.SS3.p3.1.6" class="ltx_emph ltx_font_italic">“pink”</em> while the correct answer is <em id="S5.SS3.p3.1.7" class="ltx_emph ltx_font_italic">“blanc”</em> (<em id="S5.SS3.p3.1.8" class="ltx_emph ltx_font_italic">“white”</em>) — wrong both in terms of language and semantics. It is not immediately obvious how to adapt English VQA models with, for example, vocab and tokenizers that overfit the English language. This again suggests that the design of these multimodal models would benefit from having multilinguality in mind from the start.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para ltx_noindent">
<p id="S5.SS3.p4.5" class="ltx_p"><span id="S5.SS3.p4.5.1" class="ltx_text ltx_font_bold">Single-Language vs. Multi-Language Training, Different Training Datasets</span>. In Table <a href="#S4.T6" title="Table 6 ‣ 4.4 Analysis and Discussion ‣ 4 MaXM: Multilingual VQA Benchmark ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, our Simple MPT model performs similarly or better than each of the Single-Language baselines. This suggests that modern models are capable of learning from related languages. We also find that translated COCO is overall the best training data source. We attribute this to (i) the fact that <math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><mrow id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml"><msup id="S5.SS3.p4.1.m1.1.1.2" xref="S5.SS3.p4.1.m1.1.1.2.cmml"><mi id="S5.SS3.p4.1.m1.1.1.2.2" xref="S5.SS3.p4.1.m1.1.1.2.2.cmml">VQ</mi><mn id="S5.SS3.p4.1.m1.1.1.2.3" xref="S5.SS3.p4.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S5.SS3.p4.1.m1.1.1.1" xref="S5.SS3.p4.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS3.p4.1.m1.1.1.3" xref="S5.SS3.p4.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><apply id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1"><times id="S5.SS3.p4.1.m1.1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1.1"></times><apply id="S5.SS3.p4.1.m1.1.1.2.cmml" xref="S5.SS3.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p4.1.m1.1.1.2.1.cmml" xref="S5.SS3.p4.1.m1.1.1.2">superscript</csymbol><ci id="S5.SS3.p4.1.m1.1.1.2.2.cmml" xref="S5.SS3.p4.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="S5.SS3.p4.1.m1.1.1.2.3.cmml" xref="S5.SS3.p4.1.m1.1.1.2.3">2</cn></apply><ci id="S5.SS3.p4.1.m1.1.1.3.cmml" xref="S5.SS3.p4.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> was used to generate <math id="S5.SS3.p4.2.m2.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S5.SS3.p4.2.m2.1a"><mrow id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml"><msup id="S5.SS3.p4.2.m2.1.1.2" xref="S5.SS3.p4.2.m2.1.1.2.cmml"><mi id="S5.SS3.p4.2.m2.1.1.2.2" xref="S5.SS3.p4.2.m2.1.1.2.2.cmml">VQ</mi><mn id="S5.SS3.p4.2.m2.1.1.2.3" xref="S5.SS3.p4.2.m2.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S5.SS3.p4.2.m2.1.1.1" xref="S5.SS3.p4.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS3.p4.2.m2.1.1.3" xref="S5.SS3.p4.2.m2.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><apply id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1"><times id="S5.SS3.p4.2.m2.1.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1.1"></times><apply id="S5.SS3.p4.2.m2.1.1.2.cmml" xref="S5.SS3.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p4.2.m2.1.1.2.1.cmml" xref="S5.SS3.p4.2.m2.1.1.2">superscript</csymbol><ci id="S5.SS3.p4.2.m2.1.1.2.2.cmml" xref="S5.SS3.p4.2.m2.1.1.2.2">VQ</ci><cn type="integer" id="S5.SS3.p4.2.m2.1.1.2.3.cmml" xref="S5.SS3.p4.2.m2.1.1.2.3">2</cn></apply><ci id="S5.SS3.p4.2.m2.1.1.3.cmml" xref="S5.SS3.p4.2.m2.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-COCO, and (ii) <math id="S5.SS3.p4.3.m3.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S5.SS3.p4.3.m3.1a"><mrow id="S5.SS3.p4.3.m3.1.1" xref="S5.SS3.p4.3.m3.1.1.cmml"><msup id="S5.SS3.p4.3.m3.1.1.2" xref="S5.SS3.p4.3.m3.1.1.2.cmml"><mi id="S5.SS3.p4.3.m3.1.1.2.2" xref="S5.SS3.p4.3.m3.1.1.2.2.cmml">VQ</mi><mn id="S5.SS3.p4.3.m3.1.1.2.3" xref="S5.SS3.p4.3.m3.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S5.SS3.p4.3.m3.1.1.1" xref="S5.SS3.p4.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS3.p4.3.m3.1.1.3" xref="S5.SS3.p4.3.m3.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.3.m3.1b"><apply id="S5.SS3.p4.3.m3.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1"><times id="S5.SS3.p4.3.m3.1.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1.1"></times><apply id="S5.SS3.p4.3.m3.1.1.2.cmml" xref="S5.SS3.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p4.3.m3.1.1.2.1.cmml" xref="S5.SS3.p4.3.m3.1.1.2">superscript</csymbol><ci id="S5.SS3.p4.3.m3.1.1.2.2.cmml" xref="S5.SS3.p4.3.m3.1.1.2.2">VQ</ci><cn type="integer" id="S5.SS3.p4.3.m3.1.1.2.3.cmml" xref="S5.SS3.p4.3.m3.1.1.2.3">2</cn></apply><ci id="S5.SS3.p4.3.m3.1.1.3.cmml" xref="S5.SS3.p4.3.m3.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.3.m3.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-COCO is generally more robust in the cross-dataset setting <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>. However, <math id="S5.SS3.p4.4.m4.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S5.SS3.p4.4.m4.1a"><mrow id="S5.SS3.p4.4.m4.1.1" xref="S5.SS3.p4.4.m4.1.1.cmml"><msup id="S5.SS3.p4.4.m4.1.1.2" xref="S5.SS3.p4.4.m4.1.1.2.cmml"><mi id="S5.SS3.p4.4.m4.1.1.2.2" xref="S5.SS3.p4.4.m4.1.1.2.2.cmml">VQ</mi><mn id="S5.SS3.p4.4.m4.1.1.2.3" xref="S5.SS3.p4.4.m4.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S5.SS3.p4.4.m4.1.1.1" xref="S5.SS3.p4.4.m4.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS3.p4.4.m4.1.1.3" xref="S5.SS3.p4.4.m4.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.4.m4.1b"><apply id="S5.SS3.p4.4.m4.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1"><times id="S5.SS3.p4.4.m4.1.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1.1"></times><apply id="S5.SS3.p4.4.m4.1.1.2.cmml" xref="S5.SS3.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p4.4.m4.1.1.2.1.cmml" xref="S5.SS3.p4.4.m4.1.1.2">superscript</csymbol><ci id="S5.SS3.p4.4.m4.1.1.2.2.cmml" xref="S5.SS3.p4.4.m4.1.1.2.2">VQ</ci><cn type="integer" id="S5.SS3.p4.4.m4.1.1.2.3.cmml" xref="S5.SS3.p4.4.m4.1.1.2.3">2</cn></apply><ci id="S5.SS3.p4.4.m4.1.1.3.cmml" xref="S5.SS3.p4.4.m4.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.4.m4.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-CC3M is unable to outperform VQA2.0 despite (i); applying <math id="S5.SS3.p4.5.m5.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="S5.SS3.p4.5.m5.1a"><mrow id="S5.SS3.p4.5.m5.1.1" xref="S5.SS3.p4.5.m5.1.1.cmml"><msup id="S5.SS3.p4.5.m5.1.1.2" xref="S5.SS3.p4.5.m5.1.1.2.cmml"><mi id="S5.SS3.p4.5.m5.1.1.2.2" xref="S5.SS3.p4.5.m5.1.1.2.2.cmml">VQ</mi><mn id="S5.SS3.p4.5.m5.1.1.2.3" xref="S5.SS3.p4.5.m5.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S5.SS3.p4.5.m5.1.1.1" xref="S5.SS3.p4.5.m5.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS3.p4.5.m5.1.1.3" xref="S5.SS3.p4.5.m5.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.5.m5.1b"><apply id="S5.SS3.p4.5.m5.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1"><times id="S5.SS3.p4.5.m5.1.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1.1"></times><apply id="S5.SS3.p4.5.m5.1.1.2.cmml" xref="S5.SS3.p4.5.m5.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p4.5.m5.1.1.2.1.cmml" xref="S5.SS3.p4.5.m5.1.1.2">superscript</csymbol><ci id="S5.SS3.p4.5.m5.1.1.2.2.cmml" xref="S5.SS3.p4.5.m5.1.1.2.2">VQ</ci><cn type="integer" id="S5.SS3.p4.5.m5.1.1.2.3.cmml" xref="S5.SS3.p4.5.m5.1.1.2.3">2</cn></apply><ci id="S5.SS3.p4.5.m5.1.1.3.cmml" xref="S5.SS3.p4.5.m5.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.5.m5.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> to the noisy alt-texts in CC3M <cite class="ltx_cite ltx_citemacro_cite">Sharma et al. (<a href="#bib.bib52" title="" class="ltx_ref">2018</a>)</cite> is prone to errors that would only be exacerbated by automatic MT.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para ltx_noindent">
<p id="S5.SS3.p5.1" class="ltx_p"><span id="S5.SS3.p5.1.1" class="ltx_text ltx_font_bold">Less Strict Metrics</span>. In Table <a href="#S4.T7" title="Table 7 ‣ 4.4 Analysis and Discussion ‣ 4 MaXM: Multilingual VQA Benchmark ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> We observe generally consistent results when using CIDEr and ROUGE-L instead of the stricter Accuracy, except for Thai and Chinese, where the gaps in Accuracy are small to begin with.</p>
</div>
<div id="S5.SS3.p6" class="ltx_para ltx_noindent">
<p id="S5.SS3.p6.1" class="ltx_p"><span id="S5.SS3.p6.1.1" class="ltx_text ltx_font_bold">No Adaptation via Translate-Test</span>. Can existing English VQA models <em id="S5.SS3.p6.1.2" class="ltx_emph ltx_font_italic">work</em> out of the box? In Table <a href="#S4.T8" title="Table 8 ‣ 4.4 Analysis and Discussion ‣ 4 MaXM: Multilingual VQA Benchmark ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we find that the answer is no. Expectedly, the models perform well on French, which is closer to English than other languages are.</p>
</div>
<div id="S5.SS3.p7" class="ltx_para ltx_noindent">
<p id="S5.SS3.p7.1" class="ltx_p"><span id="S5.SS3.p7.1.1" class="ltx_text ltx_font_bold">Simple MPT on xGQA</span>. Can our modeling approach be extended to the cross-lingual setting in xGQA <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>? We report this result in Appendix <a href="#A4" title="Appendix D Additional Results ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We take initial steps toward multilingual VQA by proposing scalable solutions on both data creation and modeling fronts. We create a multilingual VQA benchmark in 7 diverse languages to drive modeling progress on multilingual VQA. We establish strong unified and open-ended VQA models that work well on 13 languages as well as benchmark state-of-the-art models. For future work, we would like to expand native-language question generation that is done in a limited scope and have single one for all target answers.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgments</span>. We would like to thank Jialin Wu, Kenton Lee, Tomer Levinboim, Nan Ding, Sarah Laszlo, Doron Kukliansky, and Tania Bedrax-Weiss for their feedback and discussion. Word clouds are generated from <a target="_blank" href="https://www.jasondavies.com/wordcloud/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.jasondavies.com/wordcloud/</a>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aggarwal &amp; Kale (2020)</span>
<span class="ltx_bibblock">
Aggarwal, P. and Kale, A.

</span>
<span class="ltx_bibblock">Towards zero-shot cross-lingual image retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.05107</em>, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2018)</span>
<span class="ltx_bibblock">
Agrawal, A., Batra, D., Parikh, D., and Kembhavi, A.

</span>
<span class="ltx_bibblock">Don’t just assume; look and answer: Overcoming priors for visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc,
K., Mensch, A., Millican, K., Reynolds, M., et al.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.14198</em>, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alberti et al. (2019)</span>
<span class="ltx_bibblock">
Alberti, C., Andor, D., Pitler, E., Devlin, J., and Collins, M.

</span>
<span class="ltx_bibblock">Synthetic QA corpora generation with roundtrip consistency.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C.,
and Parikh, D.

</span>
<span class="ltx_bibblock">VQA: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee et al. (2021)</span>
<span class="ltx_bibblock">
Banerjee, P., Gokhale, T., Yang, Y., and Baral, C.

</span>
<span class="ltx_bibblock">WeaQA: Weak supervision via captions for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Findings of ACL-IJCNLP</em>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2018)</span>
<span class="ltx_bibblock">
Barrault, L., Bougares, F., Specia, L., Lala, C., Elliott, D., and Frank, S.

</span>
<span class="ltx_bibblock">Findings of the third shared task on multimodal machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Conference on Machine Translation:
Shared Task Papers</em>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biten et al. (2019)</span>
<span class="ltx_bibblock">
Biten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E.,
Jawahar, C., and Karatzas, D.

</span>
<span class="ltx_bibblock">Scene text visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bradbury et al. (2018)</span>
<span class="ltx_bibblock">
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin,
D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and
Zhang, Q.

</span>
<span class="ltx_bibblock">JAX: composable transformations of Python+NumPy programs,
2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://github.com/google/jax" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/google/jax</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bugliarello et al. (2022)</span>
<span class="ltx_bibblock">
Bugliarello, E., Liu, F., Pfeiffer, J., Reddy, S., Elliott, D., Ponti, E. M.,
and Vulić, I.

</span>
<span class="ltx_bibblock">IGLUE: A benchmark for transfer learning across modalities, tasks,
and languages.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo et al. (2021)</span>
<span class="ltx_bibblock">
Changpinyo, S., Sharma, P., Ding, N., and Soricut, R.

</span>
<span class="ltx_bibblock">Conceptual 12M: Pushing web-scale image-text pre-training to
recognize long-tail visual concepts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo et al. (2022)</span>
<span class="ltx_bibblock">
Changpinyo, S., Kukliansky, D., Szpektor, I., Chen, X., Ding, N., and Soricut,
R.

</span>
<span class="ltx_bibblock">All you may need for VQA are image captions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2015)</span>
<span class="ltx_bibblock">
Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollár, P., and
Zitnick, C. L.

</span>
<span class="ltx_bibblock">Microsoft COCO Captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1504.00325</em>, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2021)</span>
<span class="ltx_bibblock">
Cho, J., Lei, J., Tan, H., and Bansal, M.

</span>
<span class="ltx_bibblock">Unifying vision-and-language tasks via text generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai et al. (2021)</span>
<span class="ltx_bibblock">
Desai, K., Kaul, G., Aysola, Z., and Johnson, J.

</span>
<span class="ltx_bibblock">RedCaps: Web-curated image-text data created by the people, for the
people.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
Uszkoreit, J., and Houlsby, N.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al. (2016)</span>
<span class="ltx_bibblock">
Elliott, D., Frank, S., Sima’an, K., and Specia, L.

</span>
<span class="ltx_bibblock">Multi30K: Multilingual English-German image descriptions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Workshop on Vision and Language,
ACL</em>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al. (2017)</span>
<span class="ltx_bibblock">
Elliott, D., Frank, S., Barrault, L., Bougares, F., and Specia, L.

</span>
<span class="ltx_bibblock">Findings of the second shared task on multimodal machine translation
and multilingual image description.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Second Conference on Machine
Translation</em>, Sep 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of image
understanding in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2022)</span>
<span class="ltx_bibblock">
Gu, J., Meng, X., Lu, G., Hou, L., Niu, M., Xu, H., Liang, X., Zhang, W.,
Jiang, X., and Xu, C.

</span>
<span class="ltx_bibblock">Wukong: 100 million large-scale chinese cross-modal pre-training
dataset and a foundation framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.06767</em>, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. (2018)</span>
<span class="ltx_bibblock">
Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and
Bigham, J. P.

</span>
<span class="ltx_bibblock">VizWiz Grand Challenge: Answering visual questions from blind
people.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et al. (2021)</span>
<span class="ltx_bibblock">
Honovich, O., Choshen, L., Aharoni, R., Neeman, E., Szpektor, I., and Abend, O.

</span>
<span class="ltx_bibblock"><math id="bib.bib22.1.m1.1" class="ltx_Math" alttext="Q^{2}" display="inline"><semantics id="bib.bib22.1.m1.1a"><msup id="bib.bib22.1.m1.1.1" xref="bib.bib22.1.m1.1.1.cmml"><mi id="bib.bib22.1.m1.1.1.2" xref="bib.bib22.1.m1.1.1.2.cmml">Q</mi><mn id="bib.bib22.1.m1.1.1.3" xref="bib.bib22.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib22.1.m1.1b"><apply id="bib.bib22.1.m1.1.1.cmml" xref="bib.bib22.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib22.1.m1.1.1.1.cmml" xref="bib.bib22.1.m1.1.1">superscript</csymbol><ci id="bib.bib22.1.m1.1.1.2.cmml" xref="bib.bib22.1.m1.1.1.2">𝑄</ci><cn type="integer" id="bib.bib22.1.m1.1.1.3.cmml" xref="bib.bib22.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.1.m1.1c">Q^{2}</annotation></semantics></math>: Evaluating factual consistency in knowledge-grounded
dialogues via question generation and question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.2.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson &amp; Manning (2019)</span>
<span class="ltx_bibblock">
Hudson, D. A. and Manning, C. D.

</span>
<span class="ltx_bibblock">GQA: A new dataset for real-world visual reasoning and
compositional question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017a)</span>
<span class="ltx_bibblock">
Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick,
C., and Girshick, R.

</span>
<span class="ltx_bibblock">CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017a.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017b)</span>
<span class="ltx_bibblock">
Johnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y., Chen, Z., Thorat, N.,
Viégas, F., Wattenberg, M., Corrado, G., Hughes, M., and Dean, J.

</span>
<span class="ltx_bibblock">Google’s multilingual neural machine translation system: Enabling
zero-shot translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">TACL</em>, 5:339–351, 2017b.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle &amp; Kanan (2017)</span>
<span class="ltx_bibblock">
Kafle, K. and Kanan, C.

</span>
<span class="ltx_bibblock">An analysis of visual question answering algorithms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy &amp; Fei-Fei (2015)</span>
<span class="ltx_bibblock">
Karpathy, A. and Fei-Fei, L.

</span>
<span class="ltx_bibblock">Deep visual-semantic alignments for generating image descriptions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2015.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krasin et al. (2017)</span>
<span class="ltx_bibblock">
Krasin, I., Duerig, T., Alldrin, N., Ferrari, V., Abu-El-Haija, S., Kuznetsova,
A., Rom, H., Uijlings, J., Popov, S., Kamali, S., Malloci, M., Pont-Tuset,
J., Veit, A., Belongie, S., Gomes, V., Gupta, A., Sun, C., Chechik, G., Cai,
D., Feng, Z., Narayanan, D., and Murphy, K.

</span>
<span class="ltx_bibblock">OpenImages: A public dataset for large-scale multi-label and
multi-class image classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Dataset available from https://g.co/dataset/openimages</em>, 2017.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreutzer et al. (2022)</span>
<span class="ltx_bibblock">
Kreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh,
N., Tapo, A., Subramani, N., Sokolov, A., Sikasote, C., Setyawan, M., Sarin,
S., Samb, S., Sagot, B., Rivera, C., Rios, A., Papadimitriou, I., Osei, S.,
Suarez, P. O., Orife, I., Ogueji, K., Rubungo, A. N., Nguyen, T. Q.,
Müller, M., Müller, A., Muhammad, S. H., Muhammad, N., Mnyakeni, A.,
Mirzakhalov, J., Matangira, T., Leong, C., Lawson, N., Kudugunta, S.,
Jernite, Y., Jenny, M., Firat, O., Dossou, B. F. P., Dlamini, S., de Silva,
N., Çabuk Ballı, S., Biderman, S., Battisti, A., Baruwa, A., Bapna,
A., Baljekar, P., Azime, I. A., Awokoya, A., Ataman, D., Ahia, O., Ahia, O.,
Agrawal, S., and Adeyemi, M.

</span>
<span class="ltx_bibblock">Quality at a glance: An audit of web-crawled multilingual datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
10:50–72, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1162/tacl_a_00447</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.tacl-1.4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.tacl-1.4</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2017)</span>
<span class="ltx_bibblock">
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis, Y., Li, L.-J., Shamma, D. A., Bernstein, M., and Fei-Fei, L.

</span>
<span class="ltx_bibblock">Visual Genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IJCV</em>, 123(1):32–73, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ku et al. (2020)</span>
<span class="ltx_bibblock">
Ku, A., Anderson, P., Patel, R., Ie, E., and Baldridge, J.

</span>
<span class="ltx_bibblock">Room-Across-Room: Multilingual vision-and-language navigation with
dense spatiotemporal grounding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuznetsova et al. (2020)</span>
<span class="ltx_bibblock">
Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J. R. R., Krasin, I.,
Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Duerig, T., and
Ferrari, V.

</span>
<span class="ltx_bibblock">The open images dataset V4: unified image classification, object
detection, and visual relationship detection at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IJCV</em>, 128(7):1956–1981, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti,
C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., , Toutanova, K., Jones,
L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov,
S.

</span>
<span class="ltx_bibblock">Natural Questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">TACL</em>, 7:453–466, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2021)</span>
<span class="ltx_bibblock">
Lee, H., Scialom, T., Yoon, S., Dernoncourt, F., and Jung, K.

</span>
<span class="ltx_bibblock">QACE: Asking questions to evaluate an image caption.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP</em>, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lembersky et al. (2012)</span>
<span class="ltx_bibblock">
Lembersky, G., Ordan, N., and Wintner, S.

</span>
<span class="ltx_bibblock">Language models for machine translation: Original vs. translated
texts.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Computational Linguistics</em>, 38(4):799–825,
2012.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Li, L., Lei, J., Gan, Z., and Liu, J.

</span>
<span class="ltx_bibblock">Adversarial VQA: A new benchmark for evaluating the robustness of
vqa models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Li, X., Xu, C., Wang, X., Lan, W., Jia, Z., Yang, G., and Xu, J.

</span>
<span class="ltx_bibblock">COCO-CN for cross-lingual image tagging, captioning, and retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>, 21(9):2347–2360, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Lin, C.-Y.

</span>
<span class="ltx_bibblock">ROUGE: A package for automatic evaluation of summaries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Text Summarization Branches Out</em>, 2004.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J.,
Perona, P., Ramanan, D., Zitnick, C. L., and Dollár, P.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2014.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Liu, F., Bugliarello, E., Ponti, E. M., Reddy, S., Collier, N., and Elliott, D.

</span>
<span class="ltx_bibblock">Visually grounded reasoning across languages and cultures.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. (2019)</span>
<span class="ltx_bibblock">
Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R.

</span>
<span class="ltx_bibblock">OK-VQA: A visual question answering benchmark requiring external
knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ordonez et al. (2011)</span>
<span class="ltx_bibblock">
Ordonez, V., Kulkarni, G., and Berg, T.

</span>
<span class="ltx_bibblock">Im2Text: Describing images using 1 million captioned photographs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, 2011.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2022)</span>
<span class="ltx_bibblock">
Pfeiffer, J., Geigle, G., Kamath, A., Steitz, J.-M. O., Roth, S., Vulić,
I., and Gurevych, I.

</span>
<span class="ltx_bibblock">xGQA: Cross-lingual visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Findings of ACL</em>, 2022.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pont-Tuset et al. (2020)</span>
<span class="ltx_bibblock">
Pont-Tuset, J., Uijlings, J., Changpinyo, S., Soricut, R., and Ferrari, V.

</span>
<span class="ltx_bibblock">Connecting vision and language with localized narratives.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2020.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
Y., Li, W., and Liu, P. J.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">JMLR</em>, 2020.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. (2016)</span>
<span class="ltx_bibblock">
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.

</span>
<span class="ltx_bibblock">SQuAD: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2016.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. (2018)</span>
<span class="ltx_bibblock">
Rajpurkar, P., Jia, R., and Liang, P.

</span>
<span class="ltx_bibblock">Know what you don’t know: Unanswerable questions for SQuAD.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2018.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Ren, M., Kiros, R., and Zemel, R.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, 2015.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2022)</span>
<span class="ltx_bibblock">
Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z.,
Chaffin, A., Stiegler, A., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker,
U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta,
D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X.,
Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A.,
Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Scao, T. L., Biderman, S.,
Gao, L., Wolf, T., and Rush, A. M.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. (2021)</span>
<span class="ltx_bibblock">
Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A.,
Coombes, T., Jitsev, J., and Komatsuzaki, A.

</span>
<span class="ltx_bibblock">LAION-400M: Open dataset of CLIP-filtered 400 million image-text
pairs.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.02114</em>, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk et al. (2022)</span>
<span class="ltx_bibblock">
Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R.

</span>
<span class="ltx_bibblock">A-OKVQA: A benchmark for visual question answering using world
knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.01718</em>, 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2018)</span>
<span class="ltx_bibblock">
Sharma, P., Ding, N., Goodman, S., and Soricut, R.

</span>
<span class="ltx_bibblock">Conceptual Captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2018.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer &amp; Stern (2018)</span>
<span class="ltx_bibblock">
Shazeer, N. and Stern, M.

</span>
<span class="ltx_bibblock">Adafactor: Adaptive learning rates with sublinear memory cost.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2018.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al. (2021)</span>
<span class="ltx_bibblock">
Sheng, S., Singh, A., Goswami, V., Magana, J. A. L., Galuba, W., Parikh, D.,
and Kiela, D.

</span>
<span class="ltx_bibblock">Human-adversarial visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2021.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2019)</span>
<span class="ltx_bibblock">
Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D.,
and Rohrbach, M.

</span>
<span class="ltx_bibblock">Towards VQA models that can read.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srinivasan et al. (2021)</span>
<span class="ltx_bibblock">
Srinivasan, K., Raman, K., Chen, J., Bendersky, M., and Najork, M.

</span>
<span class="ltx_bibblock">WIT: Wikipedia-based image text dataset for multimodal multilingual
machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">SIGIR</em>, 2021.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thapliyal et al. (2022)</span>
<span class="ltx_bibblock">
Thapliyal, A. V., Pont-Tuset, J., Chen, X., and Soricut, R.

</span>
<span class="ltx_bibblock">Crossmodal-3600: A massively multilingual multimodal evaluation
dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.12522</em>, 2022.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vedantam et al. (2015)</span>
<span class="ltx_bibblock">
Vedantam, R., Lawrence Zitnick, C., and Parikh, D.

</span>
<span class="ltx_bibblock">CIDEr: Consensus-based image description evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2015.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Volansky et al. (2013)</span>
<span class="ltx_bibblock">
Volansky, V., Ordan, N., and Wintner, S.

</span>
<span class="ltx_bibblock">On the features of translationese.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Digital Scholarship in the Humanities</em>, 30(1):98–118, 2013.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Wang, A., Cho, K., and Lewis, M.

</span>
<span class="ltx_bibblock">Asking and answering questions to evaluate the factual consistency of
summaries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022a)</span>
<span class="ltx_bibblock">
Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., and
Wang, L.

</span>
<span class="ltx_bibblock">GIT: A generative image-to-text transformer for vision and
language.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.14100</em>, 2022a.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Wang, X., Wu, J., Chen, J., Li, L., Wang, Y.-F., and Wang, W. Y.

</span>
<span class="ltx_bibblock">VaTeX: A large-scale, high-quality multilingual dataset for
video-and-language research.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022b)</span>
<span class="ltx_bibblock">
Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., and Cao, Y.

</span>
<span class="ltx_bibblock">SimVLM: Simple visual language model pretraining with weak
supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2022b.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai,
A. M., and Le, Q. V.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2022.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2021)</span>
<span class="ltx_bibblock">
Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua,
A., and Raffel, C.

</span>
<span class="ltx_bibblock">mT5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 2021.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2021)</span>
<span class="ltx_bibblock">
Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C.

</span>
<span class="ltx_bibblock">Just ask: Learning to answer questions from millions of narrated
videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoshikawa et al. (2017)</span>
<span class="ltx_bibblock">
Yoshikawa, Y., Shigeto, Y., and Takeuchi, A.

</span>
<span class="ltx_bibblock">STAIR captions: Constructing a large-scale Japanese image caption
dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2017.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2019)</span>
<span class="ltx_bibblock">
Zellers, R., Bisk, Y., Farhadi, A., and Choi, Y.

</span>
<span class="ltx_bibblock">From recognition to cognition: Visual commonsense reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2016)</span>
<span class="ltx_bibblock">
Zhu, Y., Groth, O., Bernstein, M., and Li, F.-F.

</span>
<span class="ltx_bibblock">Visual7W: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2016.

</span>
</li>
</ul>
</section>
<figure id="A0.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A0.F5.1" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:114.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-319.4pt,84.4pt) scale(0.40431266347824,0.40431266347824) ;"><img src="/html/2209.05401/assets/figure/anno_general.png" id="A0.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="1484" height="392" alt="Refer to caption">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A0.F5.2" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:432.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-321.6pt,320.7pt) scale(0.402684559954311,0.402684559954311) ;"><img src="/html/2209.05401/assets/figure/anno_question.png" id="A0.F5.2.g1" class="ltx_graphics ltx_img_square" width="1490" height="1486" alt="Refer to caption">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A0.F5.3" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:241.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-240.7pt,133.8pt) scale(0.473933642965915,0.473933642965915) ;"><img src="/html/2209.05401/assets/figure/anno_rai.png" id="A0.F5.3.g1" class="ltx_graphics ltx_img_landscape" width="1266" height="704" alt="Refer to caption">
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="A0.F5.5.1" class="ltx_text ltx_font_bold">Detailed Instructions on Question Annotation</span></figcaption>
</figure>
<figure id="A0.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A0.F6.1" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:310.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-322.3pt,230.7pt) scale(0.402144766806521,0.402144766806521) ;"><img src="/html/2209.05401/assets/figure/anno_answer.png" id="A0.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="1492" height="1068" alt="Refer to caption">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A0.F6.2" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:296.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-242.8pt,166.1pt) scale(0.471698108509793,0.471698108509793) ;"><img src="/html/2209.05401/assets/figure/anno_answer_standardize.png" id="A0.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="1272" height="870" alt="Refer to caption">
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="A0.F6.4.1" class="ltx_text ltx_font_bold">Detailed Instructions on Answer Annotation
as well as Answer Expansion and Standardization.</span></figcaption>
</figure>
<figure id="A0.F7" class="ltx_figure">
<div id="A0.F7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:607.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(26.5pt,-43.7pt) scale(1.16809262364543,1.16809262364543) ;"><img src="/html/2209.05401/assets/x5.png" id="A0.F7.1.g1" class="ltx_graphics ltx_img_portrait" width="333" height="555" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="A0.F7.3.1" class="ltx_text ltx_font_bold">Additional MaXM examples.</span></figcaption>
</figure>
<figure id="A0.F8" class="ltx_figure">
<div id="A0.F8.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.3pt;height:712.3pt;vertical-align:-356.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-99.2pt,82.3pt) scale(0.683853131544502,0.683853131544502) ;"><img src="/html/2209.05401/assets/x6.png" id="A0.F8.1.g1" class="ltx_graphics ltx_img_portrait" width="333" height="555" alt="Refer to caption"><img src="/html/2209.05401/assets/x7.png" id="A0.F8.2.g2" class="ltx_graphics ltx_img_portrait" width="333" height="555" alt="Refer to caption"><img src="/html/2209.05401/assets/x8.png" id="A0.F8.3.g3" class="ltx_graphics ltx_img_portrait" width="333" height="555" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="A0.F8.5.1" class="ltx_text ltx_font_bold">Additional examples on our approach to multilingual VQA data generation</span>. Green, yellow, and red texts correspond to “Correct”, “Almost Correct”, and “Incorrect,” respectively.</figcaption>
</figure>
<figure id="A0.F9" class="ltx_figure">
<div id="A0.F9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:607.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(26.5pt,-43.7pt) scale(1.16809262364543,1.16809262364543) ;"><img src="/html/2209.05401/assets/x9.png" id="A0.F9.1.g1" class="ltx_graphics ltx_img_portrait" width="333" height="555" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="A0.F9.3.1" class="ltx_text ltx_font_bold">Examples of Collection questions.</span></figcaption>
</figure>
<figure id="A0.F10" class="ltx_figure">
<div id="A0.F10.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:607.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(26.5pt,-43.7pt) scale(1.16809262364543,1.16809262364543) ;"><img src="/html/2209.05401/assets/x10.png" id="A0.F10.1.g1" class="ltx_graphics ltx_img_portrait" width="333" height="555" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span id="A0.F10.3.1" class="ltx_text ltx_font_bold">Examples of Ambiguous questions</span> that we flagged and filtered out.</figcaption>
</figure>
<figure id="A0.F11" class="ltx_figure">
<div id="A0.F11.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:607.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(26.5pt,-43.7pt) scale(1.16809262364543,1.16809262364543) ;"><img src="/html/2209.05401/assets/x11.png" id="A0.F11.1.g1" class="ltx_graphics ltx_img_portrait" width="333" height="555" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span id="A0.F11.3.1" class="ltx_text ltx_font_bold">Examples of Responsible-AI-sensitive questions</span> that we flagged and filtered out. Faces are hidden.</figcaption>
</figure>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Considerations and Limitations</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Our dataset is intended to be used for research-only purposes.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">Our pipeline takes in an image caption as input. Image captions may have mistakes and biases, which could be further amplified by machine learning models used by our approach. In particular, we use generative models for automatic question generation and machine translation that may create outputs with incorrect or nonfactual contents or outputs with Translationese artifacts. We have mitigated this manually via human in the loop and automatically via the caption-answer consistency check (cf., Sect. <a href="#S3.SS2" title="3.2 Translation-based VQ²⁢A (TransVQ²⁢A) ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Note that the English <math id="A1.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="A1.p2.1.m1.1a"><mrow id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml"><msup id="A1.p2.1.m1.1.1.2" xref="A1.p2.1.m1.1.1.2.cmml"><mi id="A1.p2.1.m1.1.1.2.2" xref="A1.p2.1.m1.1.1.2.2.cmml">VQ</mi><mn id="A1.p2.1.m1.1.1.2.3" xref="A1.p2.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="A1.p2.1.m1.1.1.1" xref="A1.p2.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="A1.p2.1.m1.1.1.3" xref="A1.p2.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b"><apply id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1"><times id="A1.p2.1.m1.1.1.1.cmml" xref="A1.p2.1.m1.1.1.1"></times><apply id="A1.p2.1.m1.1.1.2.cmml" xref="A1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.p2.1.m1.1.1.2.1.cmml" xref="A1.p2.1.m1.1.1.2">superscript</csymbol><ci id="A1.p2.1.m1.1.1.2.2.cmml" xref="A1.p2.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="A1.p2.1.m1.1.1.2.3.cmml" xref="A1.p2.1.m1.1.1.2.3">2</cn></apply><ci id="A1.p2.1.m1.1.1.3.cmml" xref="A1.p2.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> that we leverage in our pipeline also has similar filtering using the round-trip consistency check via question answering. Together these significantly improve the correctness and fluency of our pipeline. In addition, we explicitly mark examples that can be considered Responsible-AI-sensitive, but not necessarily incorrect; see Sect. <a href="#A2" title="Appendix B Human Verification and Modification ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> for details and examples.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">Another type of biases is the low coverage of particular types of answers, resulting from the image captions not mentioning the absences of objects or properties. We have also taken a step toward mitigating this. See Sect. <a href="#S3.SS3" title="3.3 Direct Question Generation (DirectQG) ‣ 3 Multilingual VQA Data Creation ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<div id="A1.p4" class="ltx_para">
<p id="A1.p4.1" class="ltx_p">Finally, we select a diverse set of languages, alleviating typological, genealogical, and geographical language biases presented in the VQA research community.</p>
</div>
<div id="A1.p5" class="ltx_para">
<p id="A1.p5.3" class="ltx_p">We mainly use Crossmodal-3600 (<math id="A1.p5.1.m1.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="A1.p5.1.m1.1a"><mi id="A1.p5.1.m1.1.1" xref="A1.p5.1.m1.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="A1.p5.1.m1.1b"><ci id="A1.p5.1.m1.1.1.cmml" xref="A1.p5.1.m1.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.1.m1.1c">\mathrm{XM3600}</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">Thapliyal et al. (<a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>. Open Images <cite class="ltx_cite ltx_citemacro_cite">Krasin et al. (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>); Kuznetsova et al. (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> and the multilingual captions in <math id="A1.p5.2.m2.1" class="ltx_Math" alttext="\mathrm{XM3600}" display="inline"><semantics id="A1.p5.2.m2.1a"><mi id="A1.p5.2.m2.1.1" xref="A1.p5.2.m2.1.1.cmml">XM3600</mi><annotation-xml encoding="MathML-Content" id="A1.p5.2.m2.1b"><ci id="A1.p5.2.m2.1.1.cmml" xref="A1.p5.2.m2.1.1">XM3600</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.2.m2.1c">\mathrm{XM3600}</annotation></semantics></math> are human-curated and cleaned, which mitigates the risks that <math id="A1.p5.3.m3.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="A1.p5.3.m3.1a"><mi id="A1.p5.3.m3.1.1" xref="A1.p5.3.m3.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="A1.p5.3.m3.1b"><ci id="A1.p5.3.m3.1.1.cmml" xref="A1.p5.3.m3.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.3.m3.1c">\mathrm{MaXM}</annotation></semantics></math> would contain information that names or uniquely identifies individual people or offensive content.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Human Verification and Modification</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Annotation Guideline</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">We provide our general instructions and detailed instructions on question annotation in Fig. <a href="#A0.F5" title="Figure 5 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where we explicitly ask the annotators to be wary of Responsible-AI-sensitive questions. Fig. <a href="#A0.F6" title="Figure 6 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> provides detailed instructions on
answer annotation and on answer expansion and standardization.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Additional Examples</h3>

<div id="A2.SS2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS2.p1.1" class="ltx_p"><span id="A2.SS2.p1.1.1" class="ltx_text ltx_font_bold">Additional Examples</span>.
Fig. <a href="#A0.F7" title="Figure 7 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> provides additional examples to the ones in Fig. <a href="#S0.F1" title="Figure 1 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Again, we highlight the richness and diversity of our questions. For instance, it requires recognizing a cross under occlusion (French), a type of vegetables (Hindi), the Arabic language (Hebrew), and a type of flowers (Romanian). Some of these examples are specific to particular languages; it would be difficult for other language speakers to answer the Hebrew example (or the Chinese example in Fig. <a href="#S0.F1" title="Figure 1 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which requires OCR).</p>
</div>
<div id="A2.SS2.p2" class="ltx_para">
<p id="A2.SS2.p2.1" class="ltx_p">We also highlight the richness of our candidate answers. For the “where” question in Thai, 10 answers count as correct. Similarly, the Romanian example in Fig. <a href="#S0.F1" title="Figure 1 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides multiple diverse surface forms for “coffee with cream.”</p>
</div>
<div id="A2.SS2.p3" class="ltx_para">
<p id="A2.SS2.p3.1" class="ltx_p">Fig. <a href="#A0.F8" title="Figure 8 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> additional examples to the Chinese one in Fig <a href="#S2.F2" title="Figure 2 ‣ 2.2 VQA Data Creation ‣ 2 Related Work ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. These examples showcase the efficiency of our annotation process. They also provide concrete examples of “Almost Correct.” For instance, in the middle example, the Thai translation of “What leaves are in the photo?” is not <em id="A2.SS2.p3.1.1" class="ltx_emph ltx_font_italic">neutral</em> because it contains an Honorific particle <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://en.wikipedia.org/wiki/Thai_honorifics" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/wiki/Thai_honorifics</a></span></span></span>; it ends with “khá” which signifies a sign of respect to the addressee and indicates that the sex of the speaker is female. Finally, these examples provide a glimpse of sources of errors. For instance, it is <math id="A2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="A2.SS2.p3.1.m1.1a"><mrow id="A2.SS2.p3.1.m1.1.1" xref="A2.SS2.p3.1.m1.1.1.cmml"><msup id="A2.SS2.p3.1.m1.1.1.2" xref="A2.SS2.p3.1.m1.1.1.2.cmml"><mi id="A2.SS2.p3.1.m1.1.1.2.2" xref="A2.SS2.p3.1.m1.1.1.2.2.cmml">VQ</mi><mn id="A2.SS2.p3.1.m1.1.1.2.3" xref="A2.SS2.p3.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="A2.SS2.p3.1.m1.1.1.1" xref="A2.SS2.p3.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="A2.SS2.p3.1.m1.1.1.3" xref="A2.SS2.p3.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.1.m1.1b"><apply id="A2.SS2.p3.1.m1.1.1.cmml" xref="A2.SS2.p3.1.m1.1.1"><times id="A2.SS2.p3.1.m1.1.1.1.cmml" xref="A2.SS2.p3.1.m1.1.1.1"></times><apply id="A2.SS2.p3.1.m1.1.1.2.cmml" xref="A2.SS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.SS2.p3.1.m1.1.1.2.1.cmml" xref="A2.SS2.p3.1.m1.1.1.2">superscript</csymbol><ci id="A2.SS2.p3.1.m1.1.1.2.2.cmml" xref="A2.SS2.p3.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="A2.SS2.p3.1.m1.1.1.2.3.cmml" xref="A2.SS2.p3.1.m1.1.1.2.3">2</cn></apply><ci id="A2.SS2.p3.1.m1.1.1.3.cmml" xref="A2.SS2.p3.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math> that hallucinates “in the video” in the Hindi example on the right.</p>
</div>
<div id="A2.SS2.p4" class="ltx_para ltx_noindent">
<p id="A2.SS2.p4.1" class="ltx_p"><span id="A2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Collection Examples</span>.
Fig. <a href="#A0.F10" title="Figure 10 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> provides examples of “Collection” questions. We keep these questions as we believe they are useful in practice and as a way to encourage the community to work on better automatic evaluation metrics for this type of questions.</p>
</div>
<div id="A2.SS2.p5" class="ltx_para ltx_noindent">
<p id="A2.SS2.p5.1" class="ltx_p"><span id="A2.SS2.p5.1.1" class="ltx_text ltx_font_bold">Ambiguous Examples</span>.
Fig. <a href="#A0.F9" title="Figure 9 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> provides examples of “Ambiguous” questions that we filter out. Reasons include object being too small (English) or irregular (Chinese), determining sizes being subjective (French), and not enough context (Hebrew, Romanian). “What kind/What type” questions are particularly difficult to answer and tend to be ambiguous.</p>
</div>
<div id="A2.SS2.p6" class="ltx_para ltx_noindent">
<p id="A2.SS2.p6.1" class="ltx_p"><span id="A2.SS2.p6.1.1" class="ltx_text ltx_font_bold">Responsible-AI-sensitive Examples</span>.
Fig. <a href="#A0.F11" title="Figure 11 ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> provides examples of Responsible-AI-sensitive questions that we filter out. These cases are often associated with directly asking for the information about or describing a particular gender or race, or involving an incorrect assumption about such protected attributes (e.g., girl vs. woman in the Hebrew example).</p>
</div>
<figure id="A2.F12" class="ltx_figure">
<div id="A2.F12.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:384.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(89.4pt,-88.0pt) scale(1.84543254655707,1.84543254655707) ;"><img src="/html/2209.05401/assets/x12.png" id="A2.F12.1.g1" class="ltx_graphics ltx_img_square" width="222" height="222" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><span id="A2.F12.3.1" class="ltx_text ltx_font_bold">Our Simple MPT model</span> used in our experiments. We leverage ViT <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> and mT5 <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite> and train them together end-to-end.</figcaption>
</figure>
<figure id="A2.T9" class="ltx_table">
<table id="A2.T9.3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A2.T9.3.3.4" class="ltx_tr">
<td id="A2.T9.3.3.4.1" class="ltx_td ltx_border_r"></td>
<td id="A2.T9.3.3.4.2" class="ltx_td ltx_align_center ltx_border_r">Finetuning</td>
<td id="A2.T9.3.3.4.3" class="ltx_td ltx_align_center" colspan="8">Question Language</td>
</tr>
<tr id="A2.T9.3.3.5" class="ltx_tr">
<td id="A2.T9.3.3.5.1" class="ltx_td ltx_align_center ltx_border_r">Model</td>
<td id="A2.T9.3.3.5.2" class="ltx_td ltx_align_center ltx_border_r">Dataset</td>
<td id="A2.T9.3.3.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">en</td>
<td id="A2.T9.3.3.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">bn</td>
<td id="A2.T9.3.3.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">de</td>
<td id="A2.T9.3.3.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">id</td>
<td id="A2.T9.3.3.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ko</td>
<td id="A2.T9.3.3.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">pt</td>
<td id="A2.T9.3.3.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ru</td>
<td id="A2.T9.3.3.5.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">zh</td>
</tr>
<tr id="A2.T9.3.3.6" class="ltx_tr">
<td id="A2.T9.3.3.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">M3P <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A2.T9.3.3.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">GQA</td>
<td id="A2.T9.3.3.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A2.T9.3.3.6.3.1" class="ltx_text ltx_font_bold">58.4</span></td>
<td id="A2.T9.3.3.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.6</td>
<td id="A2.T9.3.3.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.8</td>
<td id="A2.T9.3.3.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.7</td>
<td id="A2.T9.3.3.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.7</td>
<td id="A2.T9.3.3.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.7</td>
<td id="A2.T9.3.3.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.3</td>
<td id="A2.T9.3.3.6.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">19.7</td>
</tr>
<tr id="A2.T9.1.1.1" class="ltx_tr">
<td id="A2.T9.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r">mBERT<math id="A2.T9.1.1.1.1.m1.1" class="ltx_Math" alttext="{}^{\textrm{Ada}}" display="inline"><semantics id="A2.T9.1.1.1.1.m1.1a"><msup id="A2.T9.1.1.1.1.m1.1.1" xref="A2.T9.1.1.1.1.m1.1.1.cmml"><mi id="A2.T9.1.1.1.1.m1.1.1a" xref="A2.T9.1.1.1.1.m1.1.1.cmml"></mi><mtext id="A2.T9.1.1.1.1.m1.1.1.1" xref="A2.T9.1.1.1.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="A2.T9.1.1.1.1.m1.1b"><apply id="A2.T9.1.1.1.1.m1.1.1.cmml" xref="A2.T9.1.1.1.1.m1.1.1"><ci id="A2.T9.1.1.1.1.m1.1.1.1a.cmml" xref="A2.T9.1.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T9.1.1.1.1.m1.1.1.1.cmml" xref="A2.T9.1.1.1.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.1.1.1.1.m1.1c">{}^{\textrm{Ada}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A2.T9.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">GQA</td>
<td id="A2.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r"><em id="A2.T9.1.1.1.3.1" class="ltx_emph ltx_font_italic">56.3</em></td>
<td id="A2.T9.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r">13.4</td>
<td id="A2.T9.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r">32.4</td>
<td id="A2.T9.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r">19.8</td>
<td id="A2.T9.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r">19.9</td>
<td id="A2.T9.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r">31.5</td>
<td id="A2.T9.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r">25.5</td>
<td id="A2.T9.1.1.1.10" class="ltx_td ltx_nopad_r ltx_align_center">26.2</td>
</tr>
<tr id="A2.T9.3.3.7" class="ltx_tr">
<td id="A2.T9.3.3.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Single-Language</td>
<td id="A2.T9.3.3.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">VQA2.0</td>
<td id="A2.T9.3.3.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">43.1</td>
<td id="A2.T9.3.3.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">37.9</td>
<td id="A2.T9.3.3.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">39.6</td>
<td id="A2.T9.3.3.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A2.T9.3.3.7.6.1" class="ltx_text ltx_font_bold">40.4</span></td>
<td id="A2.T9.3.3.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><em id="A2.T9.3.3.7.7.1" class="ltx_emph ltx_font_italic">38.9</em></td>
<td id="A2.T9.3.3.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><em id="A2.T9.3.3.7.8.1" class="ltx_emph ltx_font_italic">40.3</em></td>
<td id="A2.T9.3.3.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">39.3</td>
<td id="A2.T9.3.3.7.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><em id="A2.T9.3.3.7.10.1" class="ltx_emph ltx_font_italic">39.7</em></td>
</tr>
<tr id="A2.T9.3.3.8" class="ltx_tr">
<td id="A2.T9.3.3.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Simple MPT</td>
<td id="A2.T9.3.3.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">VQA2.0</td>
<td id="A2.T9.3.3.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.5</td>
<td id="A2.T9.3.3.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><em id="A2.T9.3.3.8.4.1" class="ltx_emph ltx_font_italic">38.6</em></td>
<td id="A2.T9.3.3.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><em id="A2.T9.3.3.8.5.1" class="ltx_emph ltx_font_italic">40.5</em></td>
<td id="A2.T9.3.3.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39.5</td>
<td id="A2.T9.3.3.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.7</td>
<td id="A2.T9.3.3.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39.8</td>
<td id="A2.T9.3.3.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><em id="A2.T9.3.3.8.9.1" class="ltx_emph ltx_font_italic">39.5</em></td>
<td id="A2.T9.3.3.8.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">39.5</td>
</tr>
<tr id="A2.T9.2.2.2" class="ltx_tr">
<td id="A2.T9.2.2.2.2" class="ltx_td ltx_align_left ltx_border_r">Simple MPT</td>
<td id="A2.T9.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="A2.T9.2.2.2.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="A2.T9.2.2.2.1.m1.1a"><mrow id="A2.T9.2.2.2.1.m1.1.1" xref="A2.T9.2.2.2.1.m1.1.1.cmml"><msup id="A2.T9.2.2.2.1.m1.1.1.2" xref="A2.T9.2.2.2.1.m1.1.1.2.cmml"><mi id="A2.T9.2.2.2.1.m1.1.1.2.2" xref="A2.T9.2.2.2.1.m1.1.1.2.2.cmml">VQ</mi><mn id="A2.T9.2.2.2.1.m1.1.1.2.3" xref="A2.T9.2.2.2.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="A2.T9.2.2.2.1.m1.1.1.1" xref="A2.T9.2.2.2.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="A2.T9.2.2.2.1.m1.1.1.3" xref="A2.T9.2.2.2.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T9.2.2.2.1.m1.1b"><apply id="A2.T9.2.2.2.1.m1.1.1.cmml" xref="A2.T9.2.2.2.1.m1.1.1"><times id="A2.T9.2.2.2.1.m1.1.1.1.cmml" xref="A2.T9.2.2.2.1.m1.1.1.1"></times><apply id="A2.T9.2.2.2.1.m1.1.1.2.cmml" xref="A2.T9.2.2.2.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T9.2.2.2.1.m1.1.1.2.1.cmml" xref="A2.T9.2.2.2.1.m1.1.1.2">superscript</csymbol><ci id="A2.T9.2.2.2.1.m1.1.1.2.2.cmml" xref="A2.T9.2.2.2.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="A2.T9.2.2.2.1.m1.1.1.2.3.cmml" xref="A2.T9.2.2.2.1.m1.1.1.2.3">2</cn></apply><ci id="A2.T9.2.2.2.1.m1.1.1.3.cmml" xref="A2.T9.2.2.2.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.2.2.2.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-COCO</td>
<td id="A2.T9.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">36.6</td>
<td id="A2.T9.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r">34.3</td>
<td id="A2.T9.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r">36.1</td>
<td id="A2.T9.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r">35.5</td>
<td id="A2.T9.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r">35.1</td>
<td id="A2.T9.2.2.2.8" class="ltx_td ltx_align_center ltx_border_r">34.6</td>
<td id="A2.T9.2.2.2.9" class="ltx_td ltx_align_center ltx_border_r">34.5</td>
<td id="A2.T9.2.2.2.10" class="ltx_td ltx_nopad_r ltx_align_center">35.4</td>
</tr>
<tr id="A2.T9.3.3.3" class="ltx_tr">
<td id="A2.T9.3.3.3.2" class="ltx_td ltx_align_left ltx_border_r">Simple MPT</td>
<td id="A2.T9.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="A2.T9.3.3.3.1.m1.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="A2.T9.3.3.3.1.m1.1a"><mrow id="A2.T9.3.3.3.1.m1.1.1" xref="A2.T9.3.3.3.1.m1.1.1.cmml"><msup id="A2.T9.3.3.3.1.m1.1.1.2" xref="A2.T9.3.3.3.1.m1.1.1.2.cmml"><mi id="A2.T9.3.3.3.1.m1.1.1.2.2" xref="A2.T9.3.3.3.1.m1.1.1.2.2.cmml">VQ</mi><mn id="A2.T9.3.3.3.1.m1.1.1.2.3" xref="A2.T9.3.3.3.1.m1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="A2.T9.3.3.3.1.m1.1.1.1" xref="A2.T9.3.3.3.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="A2.T9.3.3.3.1.m1.1.1.3" xref="A2.T9.3.3.3.1.m1.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T9.3.3.3.1.m1.1b"><apply id="A2.T9.3.3.3.1.m1.1.1.cmml" xref="A2.T9.3.3.3.1.m1.1.1"><times id="A2.T9.3.3.3.1.m1.1.1.1.cmml" xref="A2.T9.3.3.3.1.m1.1.1.1"></times><apply id="A2.T9.3.3.3.1.m1.1.1.2.cmml" xref="A2.T9.3.3.3.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T9.3.3.3.1.m1.1.1.2.1.cmml" xref="A2.T9.3.3.3.1.m1.1.1.2">superscript</csymbol><ci id="A2.T9.3.3.3.1.m1.1.1.2.2.cmml" xref="A2.T9.3.3.3.1.m1.1.1.2.2">VQ</ci><cn type="integer" id="A2.T9.3.3.3.1.m1.1.1.2.3.cmml" xref="A2.T9.3.3.3.1.m1.1.1.2.3">2</cn></apply><ci id="A2.T9.3.3.3.1.m1.1.1.3.cmml" xref="A2.T9.3.3.3.1.m1.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.3.3.3.1.m1.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-CC3M</td>
<td id="A2.T9.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">34.0</td>
<td id="A2.T9.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r">30.9</td>
<td id="A2.T9.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r">33.3</td>
<td id="A2.T9.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r">33.2</td>
<td id="A2.T9.3.3.3.7" class="ltx_td ltx_align_center ltx_border_r">32.5</td>
<td id="A2.T9.3.3.3.8" class="ltx_td ltx_align_center ltx_border_r">32.1</td>
<td id="A2.T9.3.3.3.9" class="ltx_td ltx_align_center ltx_border_r">32.0</td>
<td id="A2.T9.3.3.3.10" class="ltx_td ltx_nopad_r ltx_align_center">32.7</td>
</tr>
<tr id="A2.T9.3.3.9" class="ltx_tr">
<td id="A2.T9.3.3.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">PaLI <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">pali2</span></cite>
</td>
<td id="A2.T9.3.3.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">VQA2.0</td>
<td id="A2.T9.3.3.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">54.2</td>
<td id="A2.T9.3.3.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="A2.T9.3.3.9.4.1" class="ltx_text ltx_font_bold">50.0</span></td>
<td id="A2.T9.3.3.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="A2.T9.3.3.9.5.1" class="ltx_text ltx_font_bold">52.2</span></td>
<td id="A2.T9.3.3.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="A2.T9.3.3.9.6.1" class="ltx_text ltx_font_bold">50.6</span></td>
<td id="A2.T9.3.3.9.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="A2.T9.3.3.9.7.1" class="ltx_text ltx_font_bold">50.4</span></td>
<td id="A2.T9.3.3.9.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="A2.T9.3.3.9.8.1" class="ltx_text ltx_font_bold">51.3</span></td>
<td id="A2.T9.3.3.9.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="A2.T9.3.3.9.9.1" class="ltx_text ltx_font_bold">50.3</span></td>
<td id="A2.T9.3.3.9.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t"><span id="A2.T9.3.3.9.10.1" class="ltx_text ltx_font_bold">50.6</span></td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span><span id="A2.T9.7.1" class="ltx_text ltx_font_bold">Zero-Shot Results on xGQA</span>. Accuracy (%) of our Simple MPT models trained on different training datasets as well as our Single-Language baselines and the baselines from <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>. Best results are <span id="A2.T9.8.2" class="ltx_text ltx_font_bold">bold</span>. Second best <em id="A2.T9.9.3" class="ltx_emph ltx_font_italic">italicized</em>.</figcaption>
</figure>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Simple MPT</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">In this section, we describe <span id="A3.p1.1.1" class="ltx_text ltx_font_bold">Simple MPT</span>, a lightweight model for mVQA in detail.</p>
</div>
<div id="A3.p2" class="ltx_para ltx_noindent">
<p id="A3.p2.1" class="ltx_p"><span id="A3.p2.1.1" class="ltx_text ltx_font_bold">Design</span>. Much of the previous work on VQA is built for English.
Further, VQA is often formulated as <em id="A3.p2.1.2" class="ltx_emph ltx_font_italic">vocab-based VQA</em>, a classification task into a pre-defined space of top (English) answer vocabulary; see, e.g., <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib5" title="" class="ltx_ref">2015</a>); Goyal et al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>. The main drawback of this approach is its inability to deal with rare answers through language compositionality. Recent work considers VQA as generation <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>); Wang et al. (<a href="#bib.bib63" title="" class="ltx_ref">2022b</a>); Alayrac et al. (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>); Wang et al. (<a href="#bib.bib61" title="" class="ltx_ref">2022a</a>)</cite>, capable of <em id="A3.p2.1.3" class="ltx_emph ltx_font_italic">open-ended VQA</em>. We adopt this as a scalable and flexible modeling approach to mVQA as the language coverage increases. In particular, we propose a <em id="A3.p2.1.4" class="ltx_emph ltx_font_italic">single</em> open-ended VQA model for multiple languages. Our proposed formulation is more desirable than existing ones since it takes advantage of both compositionality in individual languages and the relationship among related languages. To this end, we first describe an encoder-decoder architecture for VQA in the open-ended generation setting. Then, we describe how we train this model for multiple languages. This is summarized in Fig. <a href="#A2.F12" title="Figure 12 ‣ B.2 Additional Examples ‣ Appendix B Human Verification and Modification ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<div id="A3.p3" class="ltx_para ltx_noindent">
<p id="A3.p3.1" class="ltx_p"><span id="A3.p3.1.1" class="ltx_text ltx_font_bold">Open-Ended VQA</span>.
Our starting architecture is mT5 <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite>, a multilingual variant of T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>. mT5 is an encoder-decoder transformer-based architecture, pre-trained on a Common Crawl-based dataset covering 101 languages. This allows us to leverage multilingual language understanding (for the questions) and generation (for the answers) from the get-go. To adapt mT5 to the VQA task, we prepend patch embeddings from the image to the question tokens. In particular, we encode the image pixels using Vision Transformers (ViT) <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. We use ViT-L16 and mT5-Large in all of our experiments. Both mT5 and ViT are trained together in an end-to-end fashion to predict the target answer for each image-question pair, using the standard cross-entropy loss.</p>
</div>
<div id="A3.p4" class="ltx_para ltx_noindent">
<p id="A3.p4.8" class="ltx_p"><span id="A3.p4.8.1" class="ltx_text ltx_font_bold">Multi-Language Prompted Training</span>.
We resort to multi-task prompted/instruction training  <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>); Wei et al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite>, where a task corresponds to VQA for a particular language. For the input question <math id="A3.p4.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A3.p4.1.m1.1a"><mo stretchy="false" id="A3.p4.1.m1.1.1" xref="A3.p4.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A3.p4.1.m1.1b"><ci id="A3.p4.1.m1.1.1.cmml" xref="A3.p4.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.1.m1.1c">\langle</annotation></semantics></math>question<math id="A3.p4.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A3.p4.2.m2.1a"><mo stretchy="false" id="A3.p4.2.m2.1.1" xref="A3.p4.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A3.p4.2.m2.1b"><ci id="A3.p4.2.m2.1.1.cmml" xref="A3.p4.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.2.m2.1c">\rangle</annotation></semantics></math> in language <math id="A3.p4.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A3.p4.3.m3.1a"><mo stretchy="false" id="A3.p4.3.m3.1.1" xref="A3.p4.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A3.p4.3.m3.1b"><ci id="A3.p4.3.m3.1.1.cmml" xref="A3.p4.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.3.m3.1c">\langle</annotation></semantics></math>lang<math id="A3.p4.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A3.p4.4.m4.1a"><mo stretchy="false" id="A3.p4.4.m4.1.1" xref="A3.p4.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A3.p4.4.m4.1b"><ci id="A3.p4.4.m4.1.1.cmml" xref="A3.p4.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.4.m4.1c">\rangle</annotation></semantics></math>, we construct the prompt “Answer in <math id="A3.p4.5.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A3.p4.5.m5.1a"><mo stretchy="false" id="A3.p4.5.m5.1.1" xref="A3.p4.5.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A3.p4.5.m5.1b"><ci id="A3.p4.5.m5.1.1.cmml" xref="A3.p4.5.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.5.m5.1c">\langle</annotation></semantics></math>lang<math id="A3.p4.6.m6.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A3.p4.6.m6.1a"><mo stretchy="false" id="A3.p4.6.m6.1.1" xref="A3.p4.6.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A3.p4.6.m6.1b"><ci id="A3.p4.6.m6.1.1.cmml" xref="A3.p4.6.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.6.m6.1c">\rangle</annotation></semantics></math>: <math id="A3.p4.7.m7.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A3.p4.7.m7.1a"><mo stretchy="false" id="A3.p4.7.m7.1.1" xref="A3.p4.7.m7.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A3.p4.7.m7.1b"><ci id="A3.p4.7.m7.1.1.cmml" xref="A3.p4.7.m7.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.7.m7.1c">\langle</annotation></semantics></math>question<math id="A3.p4.8.m8.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A3.p4.8.m8.1a"><mo stretchy="false" id="A3.p4.8.m8.1.1" xref="A3.p4.8.m8.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A3.p4.8.m8.1b"><ci id="A3.p4.8.m8.1.1.cmml" xref="A3.p4.8.m8.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.8.m8.1c">\rangle</annotation></semantics></math>” and use it as the text input to our model, similar to a modification to the input in Google’s Multilingual Neural Machine Translation System <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib25" title="" class="ltx_ref">2017b</a>)</cite>. Such a design for multi-task learning makes extending VQA to multiple languages simple; as data for additional languages become available, one can simply add them to the pool without the need for architecture changes.</p>
</div>
<div id="A3.p5" class="ltx_para ltx_noindent">
<p id="A3.p5.2" class="ltx_p"><span id="A3.p5.2.1" class="ltx_text ltx_font_bold">Implementation Details</span>. We use the Flax implementation <cite class="ltx_cite ltx_citemacro_cite">Bradbury et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>. For training both our 2<math id="A3.p5.1.m1.1" class="ltx_Math" alttext="\langle\mathrm{lang}\rangle" display="inline"><semantics id="A3.p5.1.m1.1a"><mrow id="A3.p5.1.m1.1.2.2" xref="A3.p5.1.m1.1.2.1.cmml"><mo stretchy="false" id="A3.p5.1.m1.1.2.2.1" xref="A3.p5.1.m1.1.2.1.1.cmml">⟨</mo><mi id="A3.p5.1.m1.1.1" xref="A3.p5.1.m1.1.1.cmml">lang</mi><mo stretchy="false" id="A3.p5.1.m1.1.2.2.2" xref="A3.p5.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.p5.1.m1.1b"><apply id="A3.p5.1.m1.1.2.1.cmml" xref="A3.p5.1.m1.1.2.2"><csymbol cd="latexml" id="A3.p5.1.m1.1.2.1.1.cmml" xref="A3.p5.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="A3.p5.1.m1.1.1.cmml" xref="A3.p5.1.m1.1.1">lang</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p5.1.m1.1c">\langle\mathrm{lang}\rangle</annotation></semantics></math> and 2en models, we use Adafactor <cite class="ltx_cite ltx_citemacro_cite">Shazeer &amp; Stern (<a href="#bib.bib53" title="" class="ltx_ref">2018</a>)</cite> with a <math id="A3.p5.2.m2.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="A3.p5.2.m2.1a"><msub id="A3.p5.2.m2.1.1" xref="A3.p5.2.m2.1.1.cmml"><mi id="A3.p5.2.m2.1.1.2" xref="A3.p5.2.m2.1.1.2.cmml">β</mi><mn id="A3.p5.2.m2.1.1.3" xref="A3.p5.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A3.p5.2.m2.1b"><apply id="A3.p5.2.m2.1.1.cmml" xref="A3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="A3.p5.2.m2.1.1.1.cmml" xref="A3.p5.2.m2.1.1">subscript</csymbol><ci id="A3.p5.2.m2.1.1.2.cmml" xref="A3.p5.2.m2.1.1.2">𝛽</ci><cn type="integer" id="A3.p5.2.m2.1.1.3.cmml" xref="A3.p5.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p5.2.m2.1c">\beta_{1}</annotation></semantics></math> of 0 and a second-moment exponential decay of 0.8. We use a linear warmup of 1K steps with a peak learning of learning rate of 1e-3 and inverse square-root decay. We set the ViT dropout rate to 0 and the mT5 dropout rate to 0.1. We train each model with data parallelism using 16 Cloud TPU Pods<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://cloud.google.com/tpu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cloud.google.com/tpu</a></span></span></span>, each with a batch size of 512, for 100K steps. We use standard image resolution of 224x224. We use the maximum input length of 24 and the target output length of 8.</p>
</div>
<div id="A3.p6" class="ltx_para">
<p id="A3.p6.3" class="ltx_p">We consider three datasets for training Simple MPT, all are translations of existing large-scale English VQA datasets to the 13 languages covered by <math id="A3.p6.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="A3.p6.1.m1.1a"><mi id="A3.p6.1.m1.1.1" xref="A3.p6.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="A3.p6.1.m1.1b"><ci id="A3.p6.1.m1.1.1.cmml" xref="A3.p6.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p6.1.m1.1c">\mathrm{MaXM}</annotation></semantics></math> and xGQA. We use the Karpathy training split <cite class="ltx_cite ltx_citemacro_cite">Karpathy &amp; Fei-Fei (<a href="#bib.bib27" title="" class="ltx_ref">2015</a>)</cite> for VQA2.0 and <math id="A3.p6.2.m2.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="A3.p6.2.m2.1a"><mrow id="A3.p6.2.m2.1.1" xref="A3.p6.2.m2.1.1.cmml"><msup id="A3.p6.2.m2.1.1.2" xref="A3.p6.2.m2.1.1.2.cmml"><mi id="A3.p6.2.m2.1.1.2.2" xref="A3.p6.2.m2.1.1.2.2.cmml">VQ</mi><mn id="A3.p6.2.m2.1.1.2.3" xref="A3.p6.2.m2.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="A3.p6.2.m2.1.1.1" xref="A3.p6.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="A3.p6.2.m2.1.1.3" xref="A3.p6.2.m2.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.p6.2.m2.1b"><apply id="A3.p6.2.m2.1.1.cmml" xref="A3.p6.2.m2.1.1"><times id="A3.p6.2.m2.1.1.1.cmml" xref="A3.p6.2.m2.1.1.1"></times><apply id="A3.p6.2.m2.1.1.2.cmml" xref="A3.p6.2.m2.1.1.2"><csymbol cd="ambiguous" id="A3.p6.2.m2.1.1.2.1.cmml" xref="A3.p6.2.m2.1.1.2">superscript</csymbol><ci id="A3.p6.2.m2.1.1.2.2.cmml" xref="A3.p6.2.m2.1.1.2.2">VQ</ci><cn type="integer" id="A3.p6.2.m2.1.1.2.3.cmml" xref="A3.p6.2.m2.1.1.2.3">2</cn></apply><ci id="A3.p6.2.m2.1.1.3.cmml" xref="A3.p6.2.m2.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p6.2.m2.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-COCO and the standard training split for <math id="A3.p6.3.m3.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="A3.p6.3.m3.1a"><mrow id="A3.p6.3.m3.1.1" xref="A3.p6.3.m3.1.1.cmml"><msup id="A3.p6.3.m3.1.1.2" xref="A3.p6.3.m3.1.1.2.cmml"><mi id="A3.p6.3.m3.1.1.2.2" xref="A3.p6.3.m3.1.1.2.2.cmml">VQ</mi><mn id="A3.p6.3.m3.1.1.2.3" xref="A3.p6.3.m3.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="A3.p6.3.m3.1.1.1" xref="A3.p6.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="A3.p6.3.m3.1.1.3" xref="A3.p6.3.m3.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.p6.3.m3.1b"><apply id="A3.p6.3.m3.1.1.cmml" xref="A3.p6.3.m3.1.1"><times id="A3.p6.3.m3.1.1.1.cmml" xref="A3.p6.3.m3.1.1.1"></times><apply id="A3.p6.3.m3.1.1.2.cmml" xref="A3.p6.3.m3.1.1.2"><csymbol cd="ambiguous" id="A3.p6.3.m3.1.1.2.1.cmml" xref="A3.p6.3.m3.1.1.2">superscript</csymbol><ci id="A3.p6.3.m3.1.1.2.2.cmml" xref="A3.p6.3.m3.1.1.2.2">VQ</ci><cn type="integer" id="A3.p6.3.m3.1.1.2.3.cmml" xref="A3.p6.3.m3.1.1.2.3">2</cn></apply><ci id="A3.p6.3.m3.1.1.3.cmml" xref="A3.p6.3.m3.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p6.3.m3.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-CC3M.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Results</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">Our Simple MPT in the main paper predicts the answer in the same language as the question. Here, we explore if our Simple MPT can also be useful for the cross-lingual setting in xGQA <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>, where the model always predicts the answer in English.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.9" class="ltx_p">Similar to <math id="A4.p2.1.m1.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="A4.p2.1.m1.1a"><mi id="A4.p2.1.m1.1.1" xref="A4.p2.1.m1.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="A4.p2.1.m1.1b"><ci id="A4.p2.1.m1.1.1.cmml" xref="A4.p2.1.m1.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.1.m1.1c">\mathrm{MaXM}</annotation></semantics></math>, xGQA is a test-only benchmark, the testdev split of 12,578 question-answer pairs per language from 398 images in 8 languages (en,bn,de,id,ko,pt,ru,zh). To evaluate Simple MPT on this dataset, we use the setting in the main paper: training on VQA2.0, <math id="A4.p2.2.m2.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="A4.p2.2.m2.1a"><mrow id="A4.p2.2.m2.1.1" xref="A4.p2.2.m2.1.1.cmml"><msup id="A4.p2.2.m2.1.1.2" xref="A4.p2.2.m2.1.1.2.cmml"><mi id="A4.p2.2.m2.1.1.2.2" xref="A4.p2.2.m2.1.1.2.2.cmml">VQ</mi><mn id="A4.p2.2.m2.1.1.2.3" xref="A4.p2.2.m2.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="A4.p2.2.m2.1.1.1" xref="A4.p2.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="A4.p2.2.m2.1.1.3" xref="A4.p2.2.m2.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.p2.2.m2.1b"><apply id="A4.p2.2.m2.1.1.cmml" xref="A4.p2.2.m2.1.1"><times id="A4.p2.2.m2.1.1.1.cmml" xref="A4.p2.2.m2.1.1.1"></times><apply id="A4.p2.2.m2.1.1.2.cmml" xref="A4.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="A4.p2.2.m2.1.1.2.1.cmml" xref="A4.p2.2.m2.1.1.2">superscript</csymbol><ci id="A4.p2.2.m2.1.1.2.2.cmml" xref="A4.p2.2.m2.1.1.2.2">VQ</ci><cn type="integer" id="A4.p2.2.m2.1.1.2.3.cmml" xref="A4.p2.2.m2.1.1.2.3">2</cn></apply><ci id="A4.p2.2.m2.1.1.3.cmml" xref="A4.p2.2.m2.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.2.m2.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-COCO, and <math id="A4.p2.3.m3.1" class="ltx_Math" alttext="\mathrm{VQ}^{2}\!\mathrm{A}" display="inline"><semantics id="A4.p2.3.m3.1a"><mrow id="A4.p2.3.m3.1.1" xref="A4.p2.3.m3.1.1.cmml"><msup id="A4.p2.3.m3.1.1.2" xref="A4.p2.3.m3.1.1.2.cmml"><mi id="A4.p2.3.m3.1.1.2.2" xref="A4.p2.3.m3.1.1.2.2.cmml">VQ</mi><mn id="A4.p2.3.m3.1.1.2.3" xref="A4.p2.3.m3.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="A4.p2.3.m3.1.1.1" xref="A4.p2.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="A4.p2.3.m3.1.1.3" xref="A4.p2.3.m3.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.p2.3.m3.1b"><apply id="A4.p2.3.m3.1.1.cmml" xref="A4.p2.3.m3.1.1"><times id="A4.p2.3.m3.1.1.1.cmml" xref="A4.p2.3.m3.1.1.1"></times><apply id="A4.p2.3.m3.1.1.2.cmml" xref="A4.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="A4.p2.3.m3.1.1.2.1.cmml" xref="A4.p2.3.m3.1.1.2">superscript</csymbol><ci id="A4.p2.3.m3.1.1.2.2.cmml" xref="A4.p2.3.m3.1.1.2.2">VQ</ci><cn type="integer" id="A4.p2.3.m3.1.1.2.3.cmml" xref="A4.p2.3.m3.1.1.2.3">2</cn></apply><ci id="A4.p2.3.m3.1.1.3.cmml" xref="A4.p2.3.m3.1.1.3">A</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.3.m3.1c">\mathrm{VQ}^{2}\!\mathrm{A}</annotation></semantics></math>-COCO but do not translate the training answers. We also use the prompt “Answer in en: <math id="A4.p2.4.m4.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A4.p2.4.m4.1a"><mo stretchy="false" id="A4.p2.4.m4.1.1" xref="A4.p2.4.m4.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A4.p2.4.m4.1b"><ci id="A4.p2.4.m4.1.1.cmml" xref="A4.p2.4.m4.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.4.m4.1c">\langle</annotation></semantics></math>question<math id="A4.p2.5.m5.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A4.p2.5.m5.1a"><mo stretchy="false" id="A4.p2.5.m5.1.1" xref="A4.p2.5.m5.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A4.p2.5.m5.1b"><ci id="A4.p2.5.m5.1.1.cmml" xref="A4.p2.5.m5.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.5.m5.1c">\rangle</annotation></semantics></math>” instead of “Answer in <math id="A4.p2.6.m6.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A4.p2.6.m6.1a"><mo stretchy="false" id="A4.p2.6.m6.1.1" xref="A4.p2.6.m6.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A4.p2.6.m6.1b"><ci id="A4.p2.6.m6.1.1.cmml" xref="A4.p2.6.m6.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.6.m6.1c">\langle</annotation></semantics></math>lang<math id="A4.p2.7.m7.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A4.p2.7.m7.1a"><mo stretchy="false" id="A4.p2.7.m7.1.1" xref="A4.p2.7.m7.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A4.p2.7.m7.1b"><ci id="A4.p2.7.m7.1.1.cmml" xref="A4.p2.7.m7.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.7.m7.1c">\rangle</annotation></semantics></math>: <math id="A4.p2.8.m8.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A4.p2.8.m8.1a"><mo stretchy="false" id="A4.p2.8.m8.1.1" xref="A4.p2.8.m8.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A4.p2.8.m8.1b"><ci id="A4.p2.8.m8.1.1.cmml" xref="A4.p2.8.m8.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.8.m8.1c">\langle</annotation></semantics></math>question<math id="A4.p2.9.m9.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A4.p2.9.m9.1a"><mo stretchy="false" id="A4.p2.9.m9.1.1" xref="A4.p2.9.m9.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A4.p2.9.m9.1b"><ci id="A4.p2.9.m9.1.1.cmml" xref="A4.p2.9.m9.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.9.m9.1c">\rangle</annotation></semantics></math>”.</p>
</div>
<div id="A4.p3" class="ltx_para">
<p id="A4.p3.4" class="ltx_p">Table <a href="#A2.T9" title="Table 9 ‣ B.2 Additional Examples ‣ Appendix B Human Verification and Modification ‣ MaXM: Towards Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> reports the results. Our baselines are M3P and mBERT<math id="A4.p3.1.m1.1" class="ltx_Math" alttext="{}^{\textrm{Ada}}" display="inline"><semantics id="A4.p3.1.m1.1a"><msup id="A4.p3.1.m1.1.1" xref="A4.p3.1.m1.1.1.cmml"><mi id="A4.p3.1.m1.1.1a" xref="A4.p3.1.m1.1.1.cmml"></mi><mtext id="A4.p3.1.m1.1.1.1" xref="A4.p3.1.m1.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="A4.p3.1.m1.1b"><apply id="A4.p3.1.m1.1.1.cmml" xref="A4.p3.1.m1.1.1"><ci id="A4.p3.1.m1.1.1.1a.cmml" xref="A4.p3.1.m1.1.1.1"><mtext mathsize="70%" id="A4.p3.1.m1.1.1.1.cmml" xref="A4.p3.1.m1.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.1.m1.1c">{}^{\textrm{Ada}}</annotation></semantics></math> from <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>. Note that both M3P and mBERT<math id="A4.p3.2.m2.1" class="ltx_Math" alttext="{}^{\textrm{Ada}}" display="inline"><semantics id="A4.p3.2.m2.1a"><msup id="A4.p3.2.m2.1.1" xref="A4.p3.2.m2.1.1.cmml"><mi id="A4.p3.2.m2.1.1a" xref="A4.p3.2.m2.1.1.cmml"></mi><mtext id="A4.p3.2.m2.1.1.1" xref="A4.p3.2.m2.1.1.1a.cmml">Ada</mtext></msup><annotation-xml encoding="MathML-Content" id="A4.p3.2.m2.1b"><apply id="A4.p3.2.m2.1.1.cmml" xref="A4.p3.2.m2.1.1"><ci id="A4.p3.2.m2.1.1.1a.cmml" xref="A4.p3.2.m2.1.1.1"><mtext mathsize="70%" id="A4.p3.2.m2.1.1.1.cmml" xref="A4.p3.2.m2.1.1.1">Ada</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.2.m2.1c">{}^{\textrm{Ada}}</annotation></semantics></math> have access to the (English) GQA training data <cite class="ltx_cite ltx_citemacro_cite">Hudson &amp; Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>, where our model does not. On the other hand, they do not use translated data as in our case.
We outperform <em id="A4.p3.4.1" class="ltx_emph ltx_font_italic">multilingual</em> zero-shot baselines on all non-English languages, without access to English GQA labeled data. This further confirms that our unified approach to mVQA is effective. In addition, unlike on <math id="A4.p3.3.m3.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="A4.p3.3.m3.1a"><mi id="A4.p3.3.m3.1.1" xref="A4.p3.3.m3.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="A4.p3.3.m3.1b"><ci id="A4.p3.3.m3.1.1.cmml" xref="A4.p3.3.m3.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.3.m3.1c">\mathrm{MaXM}</annotation></semantics></math>, VQA2.0 is the best pre-training data source. We attribute this to the fact that VQA2.0 and xGQA share COCO images <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib39" title="" class="ltx_ref">2014</a>)</cite>. This highlights the utility of <math id="A4.p3.4.m4.1" class="ltx_Math" alttext="\mathrm{MaXM}" display="inline"><semantics id="A4.p3.4.m4.1a"><mi id="A4.p3.4.m4.1.1" xref="A4.p3.4.m4.1.1.cmml">MaXM</mi><annotation-xml encoding="MathML-Content" id="A4.p3.4.m4.1b"><ci id="A4.p3.4.m4.1.1.cmml" xref="A4.p3.4.m4.1.1">MaXM</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.4.m4.1c">\mathrm{MaXM}</annotation></semantics></math> as additional out-of-domain test-only VQA evaluation data.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2209.05400" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2209.05401" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2209.05401">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2209.05401" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2209.05402" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 19:28:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
