<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LEMON: Localized Editing with Mesh Optimization and Neural Shaders</title>
<!--Generated on Wed Sep 18 14:30:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.12024v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S1" title="In LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S2" title="In LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S3" title="In LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S3.SS1" title="In 3 Method ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S3.SS2" title="In 3 Method ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>LEMON</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4" title="In LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4.SS1" title="In 4 Experiments ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4.SS2" title="In 4 Experiments ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Qualitative Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4.SS3" title="In 4 Experiments ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Quantitative Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4.SS4" title="In 4 Experiments ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S5" title="In LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S6" title="In LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Acknowledgements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S7" title="In LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S7.SS1" title="In 7 Appendix ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Additional Qualitative Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S7.SS2" title="In 7 Appendix ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Code and Video</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">LEMON: Localized Editing with Mesh Optimization and Neural Shaders</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Furkan Mert Algan  Umut Yazgan  Driton Salihu  Cem Eteke  Eckehard Steinbach
<br class="ltx_break"/>Technical University of Munich
<br class="ltx_break"/>Chair of Media Technology and Munich Institute of Robotics and Machine Intelligence (MIRMI)

<br class="ltx_break"/>School of Computation, Information and Technology
Department of Computer Engineering

<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1" style="font-size:90%;">{fmert.algan, umut.yazgan, driton.salihu, cem.eteke, eckehard.steinbach}@tum.de </span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">In practical use cases, polygonal mesh editing can be faster than generating new ones, but it can still be challenging and time-consuming for users. Existing solutions for this problem tend to focus on a single task, either geometry or novel view synthesis, which often leads to disjointed results between the mesh and view. In this work, we propose LEMON, a mesh editing pipeline that combines neural deferred shading with localized mesh optimization. Our approach begins by identifying the most important vertices in the mesh for editing, utilizing a segmentation model to focus on these key regions. Given multi-view images of an object, we optimize a neural shader and a polygonal mesh while extracting the normal map and the rendered image from each view. By using these outputs as conditioning data, we edit the input images with a text-to-image diffusion model and iteratively update our dataset while deforming the mesh. This process results in a polygonal mesh that is edited according to the given text instruction, preserving the geometric characteristics of the initial mesh while focusing on the most significant areas. We evaluate our pipeline using the DTU dataset, demonstrating that it generates finely-edited meshes more rapidly than the current state-of-the-art methods. We include our code and additional results in the supplementary material.</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">{strip}</span><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="396" id="p1.g1" src="x1.png" width="789"/>
</div>
<figure class="ltx_figure" id="S0.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.3.2" style="font-size:90%;">We propose LEMON, a polygonal mesh editing method that takes multi-view images and user-provided text instructions as input and edits the mesh while preserving the geometric characteristics of the original mesh. Our method localizes accordingly to given instruction only changes the important parts of the mesh and provides a neural shader for the novel view.</span></figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">3D object representations can now be obtained with relative ease through neural rendering methods such as NeRFs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib19" title="">19</a>]</cite> and Gaussian Splatting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib13" title="">13</a>]</cite>. By using multi-view pictures from calibrated cameras and optimizing a neural network, 3D object representations can be generated efficiently. Yet, if we want to edit a representation while retaining its unique characteristics instead of generating a new one, manual editing is often the only viable option.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Although manual editing methods offer greater control to the user, this approach can be cumbersome and time-consuming because the user has to process each piece of data themselves. Instruct-NeRF2NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib10" title="">10</a>]</cite> and more recently GaussianEditor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib5" title="">5</a>]</cite> deliver impressive results in editing 3D scenes from multi-view images based on user-provided text prompts, but it also carries the inherent limitations of novel view methods. A significant drawback is the challenge of extracting meshes efficiently, represent scenes as continuous volumetric functions,and Gaussian Splatting represents them using Gaussian distributions. This limits their use in many cases where polygonal meshes are needed, as it can be challenging to maintain the characteristics of the edited object while incorporating new details.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We propose LEMON, a localized mesh editing method that deforms polygonal meshes based on given text instructions while preserving 3D consistency. Given multi-view images of the mesh, we use CLIPSeg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib17" title="">17</a>]</cite> to generate a vertex scores to determine important vertices in the given context. Afterwards, we use normal maps and shaded images of the mesh obtained through neural deferred shading <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib29" title="">29</a>]</cite> as conditions of the ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib31" title="">31</a>]</cite> to achieve 3D consistent images. Based on vertex scores, we mask edited images to localize our modifications. We iteratively update our multi-view image dataset with modified pictures and deform meshes based on these edits and vertex scores. Through this process, we ensure that the resulting meshes reflect the text-based instructions while maintaining the initial 3D structure.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Neural Rendering:</span> As deep learning techniques gain prominence in both computer vision and graphics, neural rendering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib28" title="">28</a>]</cite> plays an important role in 3D reconstruction. One of the most popular techniques, neural radiance fields(NeRF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib19" title="">19</a>]</cite>, are a type of volumetric scene representation based on a continuous volumetric function parameterized by a multilayer perceptron(MLP). NeRF can produce photorealistic renderings but is computationally expensive and slow. Additionally, most NeRF methods focus on view synthesis and rely on other techniques for surface extraction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib12" title="">12</a>]</cite>, which often results in less accurate meshes. In recent years, Gaussian Splatting  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib13" title="">13</a>]</cite> has emerged as an efficient alternative, offering faster rendering by representing scenes with a collection of Gaussian kernels, though it also struggles with precise surface reconstruction. Recent works, such as  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib9" title="">9</a>]</cite>, show that it is possible to extract meshes from Gaussians. However, rather than being direct polygonal mesh editing, it is more of a post-processing step that adds extra time to the editing process. By contrast, we use neural deferred shading(NDS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib29" title="">29</a>]</cite>, which integrates traditional mesh deformation with a neural shading pipeline, enabling more precise and detailed 3D reconstructions from multi-view images.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Diffusion Models:</span> Recent breakthroughs in diffusion models provide a more flexible approach to creating images from text or other conditioning data. Denoising Diffusion Implicit Models (DDIM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib26" title="">26</a>]</cite> iteratively remove noise from an initial noisy input to generate samples, while Contrastive Language–Image Pre-training (CLIP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib23" title="">23</a>]</cite> guides image generation based on text prompts. Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib24" title="">24</a>]</cite> is known for its high-quality outputs while InstructPix2Pix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib3" title="">3</a>]</cite> extends this concept by using text prompts to edit existing images. However, expressing complex layouts through text prompts alone can be challenging in text-to-image models. We choose ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib31" title="">31</a>]</cite> because it addresses this issue by incorporating spatial conditioning controls, such as normal maps, which helps maintain the geometric characteristics of the object.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Mesh Editing:</span> Despite the extensive research in mesh generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib16" title="">16</a>]</cite>, there are relatively few studies focusing on 3D model editing, and these typically do not involve polygonal mesh editing. Instruct-NeRF2NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib10" title="">10</a>]</cite> introduced a text-based editing technique for NeRF scenes, where an image-conditioned diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib3" title="">3</a>]</cite> generates new images based on a NeRF representation of a scene and the images used to construct it, which are then used to iteratively update the training dataset, guiding the NeRF to converge to the edited version. However, this technique relies on an implicit representation rather than directly manipulating the surface geometry, which limits its applicability to polygonal mesh editing. GaussianEditor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib5" title="">5</a>]</cite> follows a similar logic as Instruct-NeRF2NeRF) but applies it to Gaussian splatting, enabling text-based editing of Gaussian representations. Yet, for precise mesh extraction and reconstruction, additional pipelines like SuGaR  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib9" title="">9</a>]</cite> are necessary to efficiently convert Gaussian splats into accurate 3D meshes. Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib1" title="">1</a>]</cite>, TextDeformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib8" title="">8</a>]</cite> is capable of creating global deformations on an input mesh based on a text prompt. However direct manipulation of meshes through Jacobians is computationally costly and they do not provide any color information about the edited mesh.
In contrast, our method deforms a polygonal mesh using a fast neural shader combined with a diffusion model, giving the deformable mesh additional context about color and specularity.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="512" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.4.2.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.2.1" style="font-size:90%;">Pipeline of LEMON: We complement a multi-view mesh reconstruction model with a text-to-image model using localized features. After gathering vertex scores in our pre-processing step, we begin our editing process. Every <math alttext="d" class="ltx_Math" display="inline" id="S2.F2.2.1.m1.1"><semantics id="S2.F2.2.1.m1.1b"><mi id="S2.F2.2.1.m1.1.1" xref="S2.F2.2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.F2.2.1.m1.1c"><ci id="S2.F2.2.1.m1.1.1.cmml" xref="S2.F2.2.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.2.1.m1.1d">d</annotation><annotation encoding="application/x-llamapun" id="S2.F2.2.1.m1.1e">italic_d</annotation></semantics></math> iterations new images are generated by the ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib31" title="">31</a>]</cite>, based on the prompt. The initial noise calculation of diffusion model is derived from a weighted sum of input images and rendered images, while it is conditioned on rendered normals and images of the mesh. The generated images are masked, and the masked regions are overlaid onto the original images, creating modified versions that are then used to update the dataset. Using vertex scores as a mask on the mesh, we update only the subset of vertices that is relevant to the prompt. By continuously updating the dataset with edited images, we deform the mesh to align with the user’s request.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.8">We propose LEMON, a fast and lightweight method for editing polygonal meshes by combining neural deferred shading and text-to-image models. We take a set of multi-view <math alttext="m" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_m</annotation></semantics></math> images, <math alttext="\mathcal{I}=\{I^{1},\ldots,I^{m}\}" class="ltx_Math" display="inline" id="S3.p1.2.m2.3"><semantics id="S3.p1.2.m2.3a"><mrow id="S3.p1.2.m2.3.3" xref="S3.p1.2.m2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.2.m2.3.3.4" xref="S3.p1.2.m2.3.3.4.cmml">ℐ</mi><mo id="S3.p1.2.m2.3.3.3" xref="S3.p1.2.m2.3.3.3.cmml">=</mo><mrow id="S3.p1.2.m2.3.3.2.2" xref="S3.p1.2.m2.3.3.2.3.cmml"><mo id="S3.p1.2.m2.3.3.2.2.3" stretchy="false" xref="S3.p1.2.m2.3.3.2.3.cmml">{</mo><msup id="S3.p1.2.m2.2.2.1.1.1" xref="S3.p1.2.m2.2.2.1.1.1.cmml"><mi id="S3.p1.2.m2.2.2.1.1.1.2" xref="S3.p1.2.m2.2.2.1.1.1.2.cmml">I</mi><mn id="S3.p1.2.m2.2.2.1.1.1.3" xref="S3.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msup><mo id="S3.p1.2.m2.3.3.2.2.4" xref="S3.p1.2.m2.3.3.2.3.cmml">,</mo><mi id="S3.p1.2.m2.1.1" mathvariant="normal" xref="S3.p1.2.m2.1.1.cmml">…</mi><mo id="S3.p1.2.m2.3.3.2.2.5" xref="S3.p1.2.m2.3.3.2.3.cmml">,</mo><msup id="S3.p1.2.m2.3.3.2.2.2" xref="S3.p1.2.m2.3.3.2.2.2.cmml"><mi id="S3.p1.2.m2.3.3.2.2.2.2" xref="S3.p1.2.m2.3.3.2.2.2.2.cmml">I</mi><mi id="S3.p1.2.m2.3.3.2.2.2.3" xref="S3.p1.2.m2.3.3.2.2.2.3.cmml">m</mi></msup><mo id="S3.p1.2.m2.3.3.2.2.6" stretchy="false" xref="S3.p1.2.m2.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.3b"><apply id="S3.p1.2.m2.3.3.cmml" xref="S3.p1.2.m2.3.3"><eq id="S3.p1.2.m2.3.3.3.cmml" xref="S3.p1.2.m2.3.3.3"></eq><ci id="S3.p1.2.m2.3.3.4.cmml" xref="S3.p1.2.m2.3.3.4">ℐ</ci><set id="S3.p1.2.m2.3.3.2.3.cmml" xref="S3.p1.2.m2.3.3.2.2"><apply id="S3.p1.2.m2.2.2.1.1.1.cmml" xref="S3.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.2.2.1.1.1.1.cmml" xref="S3.p1.2.m2.2.2.1.1.1">superscript</csymbol><ci id="S3.p1.2.m2.2.2.1.1.1.2.cmml" xref="S3.p1.2.m2.2.2.1.1.1.2">𝐼</ci><cn id="S3.p1.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S3.p1.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">…</ci><apply id="S3.p1.2.m2.3.3.2.2.2.cmml" xref="S3.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p1.2.m2.3.3.2.2.2.1.cmml" xref="S3.p1.2.m2.3.3.2.2.2">superscript</csymbol><ci id="S3.p1.2.m2.3.3.2.2.2.2.cmml" xref="S3.p1.2.m2.3.3.2.2.2.2">𝐼</ci><ci id="S3.p1.2.m2.3.3.2.2.2.3.cmml" xref="S3.p1.2.m2.3.3.2.2.2.3">𝑚</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.3c">\mathcal{I}=\{I^{1},\ldots,I^{m}\}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.3d">caligraphic_I = { italic_I start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , italic_I start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }</annotation></semantics></math>, from calibrated cameras, along with corresponding masks of the interested zone, <math alttext="\mathcal{M}=\{M^{1},\ldots,M^{m}\}" class="ltx_Math" display="inline" id="S3.p1.3.m3.3"><semantics id="S3.p1.3.m3.3a"><mrow id="S3.p1.3.m3.3.3" xref="S3.p1.3.m3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.3.m3.3.3.4" xref="S3.p1.3.m3.3.3.4.cmml">ℳ</mi><mo id="S3.p1.3.m3.3.3.3" xref="S3.p1.3.m3.3.3.3.cmml">=</mo><mrow id="S3.p1.3.m3.3.3.2.2" xref="S3.p1.3.m3.3.3.2.3.cmml"><mo id="S3.p1.3.m3.3.3.2.2.3" stretchy="false" xref="S3.p1.3.m3.3.3.2.3.cmml">{</mo><msup id="S3.p1.3.m3.2.2.1.1.1" xref="S3.p1.3.m3.2.2.1.1.1.cmml"><mi id="S3.p1.3.m3.2.2.1.1.1.2" xref="S3.p1.3.m3.2.2.1.1.1.2.cmml">M</mi><mn id="S3.p1.3.m3.2.2.1.1.1.3" xref="S3.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msup><mo id="S3.p1.3.m3.3.3.2.2.4" xref="S3.p1.3.m3.3.3.2.3.cmml">,</mo><mi id="S3.p1.3.m3.1.1" mathvariant="normal" xref="S3.p1.3.m3.1.1.cmml">…</mi><mo id="S3.p1.3.m3.3.3.2.2.5" xref="S3.p1.3.m3.3.3.2.3.cmml">,</mo><msup id="S3.p1.3.m3.3.3.2.2.2" xref="S3.p1.3.m3.3.3.2.2.2.cmml"><mi id="S3.p1.3.m3.3.3.2.2.2.2" xref="S3.p1.3.m3.3.3.2.2.2.2.cmml">M</mi><mi id="S3.p1.3.m3.3.3.2.2.2.3" xref="S3.p1.3.m3.3.3.2.2.2.3.cmml">m</mi></msup><mo id="S3.p1.3.m3.3.3.2.2.6" stretchy="false" xref="S3.p1.3.m3.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.3b"><apply id="S3.p1.3.m3.3.3.cmml" xref="S3.p1.3.m3.3.3"><eq id="S3.p1.3.m3.3.3.3.cmml" xref="S3.p1.3.m3.3.3.3"></eq><ci id="S3.p1.3.m3.3.3.4.cmml" xref="S3.p1.3.m3.3.3.4">ℳ</ci><set id="S3.p1.3.m3.3.3.2.3.cmml" xref="S3.p1.3.m3.3.3.2.2"><apply id="S3.p1.3.m3.2.2.1.1.1.cmml" xref="S3.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.p1.3.m3.2.2.1.1.1">superscript</csymbol><ci id="S3.p1.3.m3.2.2.1.1.1.2.cmml" xref="S3.p1.3.m3.2.2.1.1.1.2">𝑀</ci><cn id="S3.p1.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S3.p1.3.m3.2.2.1.1.1.3">1</cn></apply><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">…</ci><apply id="S3.p1.3.m3.3.3.2.2.2.cmml" xref="S3.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p1.3.m3.3.3.2.2.2.1.cmml" xref="S3.p1.3.m3.3.3.2.2.2">superscript</csymbol><ci id="S3.p1.3.m3.3.3.2.2.2.2.cmml" xref="S3.p1.3.m3.3.3.2.2.2.2">𝑀</ci><ci id="S3.p1.3.m3.3.3.2.2.2.3.cmml" xref="S3.p1.3.m3.3.3.2.2.2.3">𝑚</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.3c">\mathcal{M}=\{M^{1},\ldots,M^{m}\}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.3d">caligraphic_M = { italic_M start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , italic_M start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }</annotation></semantics></math>, and the camera parameters, as input. As in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib29" title="">29</a>]</cite>, we represent the surfaces as a triangle mesh, <math alttext="\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{F})" class="ltx_Math" display="inline" id="S3.p1.4.m4.3"><semantics id="S3.p1.4.m4.3a"><mrow id="S3.p1.4.m4.3.4" xref="S3.p1.4.m4.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.4.m4.3.4.2" xref="S3.p1.4.m4.3.4.2.cmml">𝒢</mi><mo id="S3.p1.4.m4.3.4.1" xref="S3.p1.4.m4.3.4.1.cmml">=</mo><mrow id="S3.p1.4.m4.3.4.3.2" xref="S3.p1.4.m4.3.4.3.1.cmml"><mo id="S3.p1.4.m4.3.4.3.2.1" stretchy="false" xref="S3.p1.4.m4.3.4.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">𝒱</mi><mo id="S3.p1.4.m4.3.4.3.2.2" xref="S3.p1.4.m4.3.4.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.p1.4.m4.2.2" xref="S3.p1.4.m4.2.2.cmml">ℰ</mi><mo id="S3.p1.4.m4.3.4.3.2.3" xref="S3.p1.4.m4.3.4.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.p1.4.m4.3.3" xref="S3.p1.4.m4.3.3.cmml">ℱ</mi><mo id="S3.p1.4.m4.3.4.3.2.4" stretchy="false" xref="S3.p1.4.m4.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.3b"><apply id="S3.p1.4.m4.3.4.cmml" xref="S3.p1.4.m4.3.4"><eq id="S3.p1.4.m4.3.4.1.cmml" xref="S3.p1.4.m4.3.4.1"></eq><ci id="S3.p1.4.m4.3.4.2.cmml" xref="S3.p1.4.m4.3.4.2">𝒢</ci><vector id="S3.p1.4.m4.3.4.3.1.cmml" xref="S3.p1.4.m4.3.4.3.2"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">𝒱</ci><ci id="S3.p1.4.m4.2.2.cmml" xref="S3.p1.4.m4.2.2">ℰ</ci><ci id="S3.p1.4.m4.3.3.cmml" xref="S3.p1.4.m4.3.3">ℱ</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.3c">\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{F})</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.3d">caligraphic_G = ( caligraphic_V , caligraphic_E , caligraphic_F )</annotation></semantics></math>, consisting of vertices <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S3.p1.5.m5.1"><semantics id="S3.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.1d">caligraphic_V</annotation></semantics></math>, edges <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.p1.6.m6.1"><semantics id="S3.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.1d">caligraphic_E</annotation></semantics></math>, and faces <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S3.p1.7.m7.1"><semantics id="S3.p1.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><ci id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m7.1d">caligraphic_F</annotation></semantics></math>. Additionally, we take a text prompt <math alttext="T" class="ltx_Math" display="inline" id="S3.p1.8.m8.1"><semantics id="S3.p1.8.m8.1a"><mi id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><ci id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.p1.8.m8.1d">italic_T</annotation></semantics></math> for the editing instruction. The output of our method is an edited version of the initial mesh and a neural shader, based on the user-provided instruction. In this section, we first provide background information on the pipelines we’ve used; then we explain how our method integrates them.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Background</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Neural Deferred Shading:</span> NDS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib29" title="">29</a>]</cite> is an analysis-by-synthesis mesh reconstruction method that optimizes a mesh and a neural shader simultaneously by using calibrated images and their corresponding masks as its input. If the initial mesh is not provided the optimization process begins with a mesh that is derived from the masks and resembles a visual hull <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib15" title="">15</a>]</cite>. Starting with a coarsely triangulated mesh, its resolution is gradually increased as optimization proceeds. In each upsampling iteration the surface is remeshed reducing the average edge length by half <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In the first step, the mesh is rasterized using a differentiable renderer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib14" title="">14</a>]</cite>, which provides triangle indices and barycentric coordinates for each pixel. By interpolating this information, a geometry buffer (g-buffer) is generated, containing per-pixel positions, normals, and mask information. In second step g-buffer is processed by a learned shader, a MLP which resulting in an RGB color:</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{\theta}(x,n,{\omega}_{o})\in[0,1]^{3}" class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.cmml"><msub id="S3.E1.m1.5.5.1.3" xref="S3.E1.m1.5.5.1.3.cmml"><mi id="S3.E1.m1.5.5.1.3.2" xref="S3.E1.m1.5.5.1.3.2.cmml">f</mi><mi id="S3.E1.m1.5.5.1.3.3" xref="S3.E1.m1.5.5.1.3.3.cmml">θ</mi></msub><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.2.cmml"><mo id="S3.E1.m1.5.5.1.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">x</mi><mo id="S3.E1.m1.5.5.1.1.1.3" xref="S3.E1.m1.5.5.1.1.2.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">n</mi><mo id="S3.E1.m1.5.5.1.1.1.4" xref="S3.E1.m1.5.5.1.1.2.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">ω</mi><mi id="S3.E1.m1.5.5.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.3.cmml">o</mi></msub><mo id="S3.E1.m1.5.5.1.1.1.5" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.2" xref="S3.E1.m1.5.5.2.cmml">∈</mo><msup id="S3.E1.m1.5.5.3" xref="S3.E1.m1.5.5.3.cmml"><mrow id="S3.E1.m1.5.5.3.2.2" xref="S3.E1.m1.5.5.3.2.1.cmml"><mo id="S3.E1.m1.5.5.3.2.2.1" stretchy="false" xref="S3.E1.m1.5.5.3.2.1.cmml">[</mo><mn id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">0</mn><mo id="S3.E1.m1.5.5.3.2.2.2" xref="S3.E1.m1.5.5.3.2.1.cmml">,</mo><mn id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">1</mn><mo id="S3.E1.m1.5.5.3.2.2.3" stretchy="false" xref="S3.E1.m1.5.5.3.2.1.cmml">]</mo></mrow><mn id="S3.E1.m1.5.5.3.3" xref="S3.E1.m1.5.5.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><in id="S3.E1.m1.5.5.2.cmml" xref="S3.E1.m1.5.5.2"></in><apply id="S3.E1.m1.5.5.1.cmml" xref="S3.E1.m1.5.5.1"><times id="S3.E1.m1.5.5.1.2.cmml" xref="S3.E1.m1.5.5.1.2"></times><apply id="S3.E1.m1.5.5.1.3.cmml" xref="S3.E1.m1.5.5.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.3.1.cmml" xref="S3.E1.m1.5.5.1.3">subscript</csymbol><ci id="S3.E1.m1.5.5.1.3.2.cmml" xref="S3.E1.m1.5.5.1.3.2">𝑓</ci><ci id="S3.E1.m1.5.5.1.3.3.cmml" xref="S3.E1.m1.5.5.1.3.3">𝜃</ci></apply><vector id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑥</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑛</ci><apply id="S3.E1.m1.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2">𝜔</ci><ci id="S3.E1.m1.5.5.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3">𝑜</ci></apply></vector></apply><apply id="S3.E1.m1.5.5.3.cmml" xref="S3.E1.m1.5.5.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.3.1.cmml" xref="S3.E1.m1.5.5.3">superscript</csymbol><interval closure="closed" id="S3.E1.m1.5.5.3.2.1.cmml" xref="S3.E1.m1.5.5.3.2.2"><cn id="S3.E1.m1.3.3.cmml" type="integer" xref="S3.E1.m1.3.3">0</cn><cn id="S3.E1.m1.4.4.cmml" type="integer" xref="S3.E1.m1.4.4">1</cn></interval><cn id="S3.E1.m1.5.5.3.3.cmml" type="integer" xref="S3.E1.m1.5.5.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">f_{\theta}(x,n,{\omega}_{o})\in[0,1]^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x , italic_n , italic_ω start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) ∈ [ 0 , 1 ] start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.4">where <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_θ</annotation></semantics></math> represents the learnable parameters, <math alttext="x\in{\mathbb{R}}^{3}" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.1"><semantics id="S3.SS1.p4.2.m2.1a"><mrow id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">x</mi><mo id="S3.SS1.p4.2.m2.1.1.1" xref="S3.SS1.p4.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml"><mi id="S3.SS1.p4.2.m2.1.1.3.2" xref="S3.SS1.p4.2.m2.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS1.p4.2.m2.1.1.3.3" xref="S3.SS1.p4.2.m2.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><in id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1.1"></in><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">𝑥</ci><apply id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.3.1.cmml" xref="S3.SS1.p4.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.3.2.cmml" xref="S3.SS1.p4.2.m2.1.1.3.2">ℝ</ci><cn id="S3.SS1.p4.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS1.p4.2.m2.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">x\in{\mathbb{R}}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.1d">italic_x ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> is the position, <math alttext="n\in{\mathbb{R}}^{3}" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3.1"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">n</mi><mo id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.2" xref="S3.SS1.p4.3.m3.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS1.p4.3.m3.1.1.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><in id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1"></in><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">𝑛</ci><apply id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.2">ℝ</ci><cn id="S3.SS1.p4.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS1.p4.3.m3.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">n\in{\mathbb{R}}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m3.1d">italic_n ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the normal and <math alttext="\omega_{o}\in{\mathbb{R}}^{3}" class="ltx_Math" display="inline" id="S3.SS1.p4.4.m4.1"><semantics id="S3.SS1.p4.4.m4.1a"><mrow id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><msub id="S3.SS1.p4.4.m4.1.1.2" xref="S3.SS1.p4.4.m4.1.1.2.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2.2" xref="S3.SS1.p4.4.m4.1.1.2.2.cmml">ω</mi><mi id="S3.SS1.p4.4.m4.1.1.2.3" xref="S3.SS1.p4.4.m4.1.1.2.3.cmml">o</mi></msub><mo id="S3.SS1.p4.4.m4.1.1.1" xref="S3.SS1.p4.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3.cmml"><mi id="S3.SS1.p4.4.m4.1.1.3.2" xref="S3.SS1.p4.4.m4.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS1.p4.4.m4.1.1.3.3" xref="S3.SS1.p4.4.m4.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><in id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1.1"></in><apply id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.2.1.cmml" xref="S3.SS1.p4.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.2.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2.2">𝜔</ci><ci id="S3.SS1.p4.4.m4.1.1.2.3.cmml" xref="S3.SS1.p4.4.m4.1.1.2.3">𝑜</ci></apply><apply id="S3.SS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.3.1.cmml" xref="S3.SS1.p4.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.3.2.cmml" xref="S3.SS1.p4.4.m4.1.1.3.2">ℝ</ci><cn id="S3.SS1.p4.4.m4.1.1.3.3.cmml" type="integer" xref="S3.SS1.p4.4.m4.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\omega_{o}\in{\mathbb{R}}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.4.m4.1d">italic_ω start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> is the view direction relative to the center of the camera.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">The neural shader and the initial mesh is optimized based on an objective function that balances the rendered appearance of mesh and geometric characteristics of the mesh:</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\underset{V,\theta}{\arg\min}L_{\text{appearance }}(\mathcal{G},\theta;%
\mathcal{I},\mathcal{M})+L_{\text{geometry }}(\mathcal{G})" class="ltx_Math" display="block" id="S3.E2.m1.7"><semantics id="S3.E2.m1.7a"><mrow id="S3.E2.m1.7.8" xref="S3.E2.m1.7.8.cmml"><mrow id="S3.E2.m1.7.8.2" xref="S3.E2.m1.7.8.2.cmml"><munder accentunder="true" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mrow id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml"><mi id="S3.E2.m1.2.2.3.1" xref="S3.E2.m1.2.2.3.1.cmml">arg</mi><mo id="S3.E2.m1.2.2.3a" lspace="0.167em" xref="S3.E2.m1.2.2.3.cmml">⁡</mo><mi id="S3.E2.m1.2.2.3.2" xref="S3.E2.m1.2.2.3.2.cmml">min</mi></mrow><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">V</mi><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">θ</mi></mrow></munder><mo id="S3.E2.m1.7.8.2.1" xref="S3.E2.m1.7.8.2.1.cmml">⁢</mo><msub id="S3.E2.m1.7.8.2.2" xref="S3.E2.m1.7.8.2.2.cmml"><mi id="S3.E2.m1.7.8.2.2.2" xref="S3.E2.m1.7.8.2.2.2.cmml">L</mi><mtext id="S3.E2.m1.7.8.2.2.3" xref="S3.E2.m1.7.8.2.2.3a.cmml">appearance </mtext></msub><mo id="S3.E2.m1.7.8.2.1a" xref="S3.E2.m1.7.8.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.7.8.2.3.2" xref="S3.E2.m1.7.8.2.3.1.cmml"><mo id="S3.E2.m1.7.8.2.3.2.1" stretchy="false" xref="S3.E2.m1.7.8.2.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">𝒢</mi><mo id="S3.E2.m1.7.8.2.3.2.2" xref="S3.E2.m1.7.8.2.3.1.cmml">,</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">θ</mi><mo id="S3.E2.m1.7.8.2.3.2.3" xref="S3.E2.m1.7.8.2.3.1.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">ℐ</mi><mo id="S3.E2.m1.7.8.2.3.2.4" xref="S3.E2.m1.7.8.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml">ℳ</mi><mo id="S3.E2.m1.7.8.2.3.2.5" stretchy="false" xref="S3.E2.m1.7.8.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.7.8.1" xref="S3.E2.m1.7.8.1.cmml">+</mo><mrow id="S3.E2.m1.7.8.3" xref="S3.E2.m1.7.8.3.cmml"><msub id="S3.E2.m1.7.8.3.2" xref="S3.E2.m1.7.8.3.2.cmml"><mi id="S3.E2.m1.7.8.3.2.2" xref="S3.E2.m1.7.8.3.2.2.cmml">L</mi><mtext id="S3.E2.m1.7.8.3.2.3" xref="S3.E2.m1.7.8.3.2.3a.cmml">geometry </mtext></msub><mo id="S3.E2.m1.7.8.3.1" xref="S3.E2.m1.7.8.3.1.cmml">⁢</mo><mrow id="S3.E2.m1.7.8.3.3.2" xref="S3.E2.m1.7.8.3.cmml"><mo id="S3.E2.m1.7.8.3.3.2.1" stretchy="false" xref="S3.E2.m1.7.8.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.7.7" xref="S3.E2.m1.7.7.cmml">𝒢</mi><mo id="S3.E2.m1.7.8.3.3.2.2" stretchy="false" xref="S3.E2.m1.7.8.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.7b"><apply id="S3.E2.m1.7.8.cmml" xref="S3.E2.m1.7.8"><plus id="S3.E2.m1.7.8.1.cmml" xref="S3.E2.m1.7.8.1"></plus><apply id="S3.E2.m1.7.8.2.cmml" xref="S3.E2.m1.7.8.2"><times id="S3.E2.m1.7.8.2.1.cmml" xref="S3.E2.m1.7.8.2.1"></times><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑉</ci><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">𝜃</ci></list><apply id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"><arg id="S3.E2.m1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.3.1"></arg><min id="S3.E2.m1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.3.2"></min></apply></apply><apply id="S3.E2.m1.7.8.2.2.cmml" xref="S3.E2.m1.7.8.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.7.8.2.2.1.cmml" xref="S3.E2.m1.7.8.2.2">subscript</csymbol><ci id="S3.E2.m1.7.8.2.2.2.cmml" xref="S3.E2.m1.7.8.2.2.2">𝐿</ci><ci id="S3.E2.m1.7.8.2.2.3a.cmml" xref="S3.E2.m1.7.8.2.2.3"><mtext id="S3.E2.m1.7.8.2.2.3.cmml" mathsize="70%" xref="S3.E2.m1.7.8.2.2.3">appearance </mtext></ci></apply><vector id="S3.E2.m1.7.8.2.3.1.cmml" xref="S3.E2.m1.7.8.2.3.2"><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝒢</ci><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝜃</ci><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">ℐ</ci><ci id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6">ℳ</ci></vector></apply><apply id="S3.E2.m1.7.8.3.cmml" xref="S3.E2.m1.7.8.3"><times id="S3.E2.m1.7.8.3.1.cmml" xref="S3.E2.m1.7.8.3.1"></times><apply id="S3.E2.m1.7.8.3.2.cmml" xref="S3.E2.m1.7.8.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.7.8.3.2.1.cmml" xref="S3.E2.m1.7.8.3.2">subscript</csymbol><ci id="S3.E2.m1.7.8.3.2.2.cmml" xref="S3.E2.m1.7.8.3.2.2">𝐿</ci><ci id="S3.E2.m1.7.8.3.2.3a.cmml" xref="S3.E2.m1.7.8.3.2.3"><mtext id="S3.E2.m1.7.8.3.2.3.cmml" mathsize="70%" xref="S3.E2.m1.7.8.3.2.3">geometry </mtext></ci></apply><ci id="S3.E2.m1.7.7.cmml" xref="S3.E2.m1.7.7">𝒢</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.7c">\underset{V,\theta}{\arg\min}L_{\text{appearance }}(\mathcal{G},\theta;%
\mathcal{I},\mathcal{M})+L_{\text{geometry }}(\mathcal{G})</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.7d">start_UNDERACCENT italic_V , italic_θ end_UNDERACCENT start_ARG roman_arg roman_min end_ARG italic_L start_POSTSUBSCRIPT appearance end_POSTSUBSCRIPT ( caligraphic_G , italic_θ ; caligraphic_I , caligraphic_M ) + italic_L start_POSTSUBSCRIPT geometry end_POSTSUBSCRIPT ( caligraphic_G )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.6">where <math alttext="L_{\text{appearance }}" class="ltx_Math" display="inline" id="S3.SS1.p7.1.m1.1"><semantics id="S3.SS1.p7.1.m1.1a"><msub id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml"><mi id="S3.SS1.p7.1.m1.1.1.2" xref="S3.SS1.p7.1.m1.1.1.2.cmml">L</mi><mtext id="S3.SS1.p7.1.m1.1.1.3" xref="S3.SS1.p7.1.m1.1.1.3a.cmml">appearance </mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><apply id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.1.m1.1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p7.1.m1.1.1.2.cmml" xref="S3.SS1.p7.1.m1.1.1.2">𝐿</ci><ci id="S3.SS1.p7.1.m1.1.1.3a.cmml" xref="S3.SS1.p7.1.m1.1.1.3"><mtext id="S3.SS1.p7.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p7.1.m1.1.1.3">appearance </mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">L_{\text{appearance }}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.1.m1.1d">italic_L start_POSTSUBSCRIPT appearance end_POSTSUBSCRIPT</annotation></semantics></math> consists of a shading loss which computes distance between rendered image <math alttext="\mathcal{\tilde{I}}" class="ltx_Math" display="inline" id="S3.SS1.p7.2.m2.1"><semantics id="S3.SS1.p7.2.m2.1a"><mover accent="true" id="S3.SS1.p7.2.m2.1.1" xref="S3.SS1.p7.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p7.2.m2.1.1.2" xref="S3.SS1.p7.2.m2.1.1.2.cmml">ℐ</mi><mo id="S3.SS1.p7.2.m2.1.1.1" xref="S3.SS1.p7.2.m2.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m2.1b"><apply id="S3.SS1.p7.2.m2.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1"><ci id="S3.SS1.p7.2.m2.1.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1.1">~</ci><ci id="S3.SS1.p7.2.m2.1.1.2.cmml" xref="S3.SS1.p7.2.m2.1.1.2">ℐ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m2.1c">\mathcal{\tilde{I}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.2.m2.1d">over~ start_ARG caligraphic_I end_ARG</annotation></semantics></math> and input image <math alttext="\mathcal{I}" class="ltx_Math" display="inline" id="S3.SS1.p7.3.m3.1"><semantics id="S3.SS1.p7.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p7.3.m3.1.1" xref="S3.SS1.p7.3.m3.1.1.cmml">ℐ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.3.m3.1b"><ci id="S3.SS1.p7.3.m3.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1">ℐ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.3.m3.1c">\mathcal{I}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.3.m3.1d">caligraphic_I</annotation></semantics></math> and a mask loss, which computes distance between rendered mask <math alttext="\mathcal{\tilde{M}}" class="ltx_Math" display="inline" id="S3.SS1.p7.4.m4.1"><semantics id="S3.SS1.p7.4.m4.1a"><mover accent="true" id="S3.SS1.p7.4.m4.1.1" xref="S3.SS1.p7.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p7.4.m4.1.1.2" xref="S3.SS1.p7.4.m4.1.1.2.cmml">ℳ</mi><mo id="S3.SS1.p7.4.m4.1.1.1" xref="S3.SS1.p7.4.m4.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.4.m4.1b"><apply id="S3.SS1.p7.4.m4.1.1.cmml" xref="S3.SS1.p7.4.m4.1.1"><ci id="S3.SS1.p7.4.m4.1.1.1.cmml" xref="S3.SS1.p7.4.m4.1.1.1">~</ci><ci id="S3.SS1.p7.4.m4.1.1.2.cmml" xref="S3.SS1.p7.4.m4.1.1.2">ℳ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.4.m4.1c">\mathcal{\tilde{M}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.4.m4.1d">over~ start_ARG caligraphic_M end_ARG</annotation></semantics></math> and input mask <math alttext="\mathcal{{M}}" class="ltx_Math" display="inline" id="S3.SS1.p7.5.m5.1"><semantics id="S3.SS1.p7.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p7.5.m5.1.1" xref="S3.SS1.p7.5.m5.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.5.m5.1b"><ci id="S3.SS1.p7.5.m5.1.1.cmml" xref="S3.SS1.p7.5.m5.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.5.m5.1c">\mathcal{{M}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.5.m5.1d">caligraphic_M</annotation></semantics></math>. <math alttext="L_{\text{geometry }}" class="ltx_Math" display="inline" id="S3.SS1.p7.6.m6.1"><semantics id="S3.SS1.p7.6.m6.1a"><msub id="S3.SS1.p7.6.m6.1.1" xref="S3.SS1.p7.6.m6.1.1.cmml"><mi id="S3.SS1.p7.6.m6.1.1.2" xref="S3.SS1.p7.6.m6.1.1.2.cmml">L</mi><mtext id="S3.SS1.p7.6.m6.1.1.3" xref="S3.SS1.p7.6.m6.1.1.3a.cmml">geometry </mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.6.m6.1b"><apply id="S3.SS1.p7.6.m6.1.1.cmml" xref="S3.SS1.p7.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.6.m6.1.1.1.cmml" xref="S3.SS1.p7.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p7.6.m6.1.1.2.cmml" xref="S3.SS1.p7.6.m6.1.1.2">𝐿</ci><ci id="S3.SS1.p7.6.m6.1.1.3a.cmml" xref="S3.SS1.p7.6.m6.1.1.3"><mtext id="S3.SS1.p7.6.m6.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p7.6.m6.1.1.3">geometry </mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.6.m6.1c">L_{\text{geometry }}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.6.m6.1d">italic_L start_POSTSUBSCRIPT geometry end_POSTSUBSCRIPT</annotation></semantics></math> ensures to avoid undesired vertex configurations while deforming the mesh by minimizing the distance between a vertex and the average position of the neighbors and computing cosine similarity between neighboring face normals. Because mesh deformation and shader optimization happen simultaneously, NDS can preserve the geometric characteristics of opaque objects, even with varying materials and illumination, making it an ideal candidate for use in mesh editing.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p8.1.1">ControlNet:</span> Denoising diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib6" title="">6</a>]</cite> generate data by incrementally removing noise, with U-Net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib25" title="">25</a>]</cite> providing both local and global context during the denoising process. Text-to-image diffusion models like Instruct-Pix2Pix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib3" title="">3</a>]</cite> produce high-quality images from text-based instructions, encoding text into latent vectors using pretrained language models like CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib23" title="">23</a>]</cite>. However, these models often lack control over specific conditions and rely on broad assumptions about user preferences. Moreover, training new models can be time-consuming and burdensome, particularly with large datasets.</p>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.1">ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib31" title="">31</a>]</cite> freezes the original parameters of a diffusion model and creates a trainable copy of its encoding layers, which are then trained on specific conditions. This method maintains the original diffusion model’s quality and functionality, while allowing for more control through defined conditions. In our case, extra conditions like normal maps provide valuable supervision on geometry of the mesh, which is why we chose ControlNet as our diffusion model in our mesh editing pipeline.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>LEMON</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Starting with an initial polygonal mesh (along with a corresponding dataset of calibrated images and their camera parameters), our method combines a diffusion model with a neural deferred shading pipeline to deform the mesh based on given textual instruction. If the initial mesh is not provided, it is created using standard neural deferred shading pipeline. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib29" title="">29</a>]</cite></p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.2">We closely follow Instruct-NeRF2NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib10" title="">10</a>]</cite>’s editing process albeit with some differences. First we perform a pre-processing step to determine important regions of the mesh based on given instruction. During our editing process, we store another set of ground truth multi-view images, which we denote as <math alttext="\mathcal{{I}}^{v}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msup id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">ℐ</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">v</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ℐ</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathcal{{I}}^{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">caligraphic_I start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT</annotation></semantics></math>. After a certain number of iterations <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_d</annotation></semantics></math> we update our training dataset by replacing images with modified ones. Sometime later our mesh deforms into the desired edited version. Overview of our pipeline is given in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Vertex and Image Scores:</span> For the first step of our pre-processing, we generate segmentation masks for every calibrated image using the provided text prompt. To keep it simple for the user and ensure compatibility with our image editing process using CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib23" title="">23</a>]</cite>, we have selected CLIPSeg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib17" title="">17</a>]</cite> to generate segmentation scores. These scores highlight the regions that are relevant to the given prompt as seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S3.F3" title="Figure 3 ‣ 3.2 LEMON ‣ 3 Method ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">To assign these scores to the vertices, we use NvDiffrast  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib14" title="">14</a>]</cite> to render the mesh from given viewpoints and project segmentation scores onto the mesh surface. Each vertex’s score is then calculated by averaging the projected scores from all viewpoints. This aggregated scoring system makes the segmentation scores more accurate based on the geometry, effectively representing each vertex’s importance in depicting the given prompt’s feature distribution across the 3D mesh. After calculating the segmentation scores for each vertex, we project these scores back onto the image viewpoints. This process ensures that the segmentation scores are accurately reflected in the 2D image space.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.5">In order to provide the user with more control over how much the mesh changes based on the context, the threshold <math alttext="\tau\in[0,1]" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.2"><semantics id="S3.SS2.p5.1.m1.2a"><mrow id="S3.SS2.p5.1.m1.2.3" xref="S3.SS2.p5.1.m1.2.3.cmml"><mi id="S3.SS2.p5.1.m1.2.3.2" xref="S3.SS2.p5.1.m1.2.3.2.cmml">τ</mi><mo id="S3.SS2.p5.1.m1.2.3.1" xref="S3.SS2.p5.1.m1.2.3.1.cmml">∈</mo><mrow id="S3.SS2.p5.1.m1.2.3.3.2" xref="S3.SS2.p5.1.m1.2.3.3.1.cmml"><mo id="S3.SS2.p5.1.m1.2.3.3.2.1" stretchy="false" xref="S3.SS2.p5.1.m1.2.3.3.1.cmml">[</mo><mn id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.p5.1.m1.2.3.3.2.2" xref="S3.SS2.p5.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS2.p5.1.m1.2.2" xref="S3.SS2.p5.1.m1.2.2.cmml">1</mn><mo id="S3.SS2.p5.1.m1.2.3.3.2.3" stretchy="false" xref="S3.SS2.p5.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.2b"><apply id="S3.SS2.p5.1.m1.2.3.cmml" xref="S3.SS2.p5.1.m1.2.3"><in id="S3.SS2.p5.1.m1.2.3.1.cmml" xref="S3.SS2.p5.1.m1.2.3.1"></in><ci id="S3.SS2.p5.1.m1.2.3.2.cmml" xref="S3.SS2.p5.1.m1.2.3.2">𝜏</ci><interval closure="closed" id="S3.SS2.p5.1.m1.2.3.3.1.cmml" xref="S3.SS2.p5.1.m1.2.3.3.2"><cn id="S3.SS2.p5.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p5.1.m1.1.1">0</cn><cn id="S3.SS2.p5.1.m1.2.2.cmml" type="integer" xref="S3.SS2.p5.1.m1.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.2c">\tau\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.2d">italic_τ ∈ [ 0 , 1 ]</annotation></semantics></math> is also taken as an input. By keeping scores larger than <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">italic_τ</annotation></semantics></math>, we gather new image masks <math alttext="\mathcal{\widetilde{M}}=\{\widetilde{M}^{1},\ldots,\widetilde{M}^{m}\}" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m3.3"><semantics id="S3.SS2.p5.3.m3.3a"><mrow id="S3.SS2.p5.3.m3.3.3" xref="S3.SS2.p5.3.m3.3.3.cmml"><mover accent="true" id="S3.SS2.p5.3.m3.3.3.4" xref="S3.SS2.p5.3.m3.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.3.m3.3.3.4.2" xref="S3.SS2.p5.3.m3.3.3.4.2.cmml">ℳ</mi><mo id="S3.SS2.p5.3.m3.3.3.4.1" xref="S3.SS2.p5.3.m3.3.3.4.1.cmml">~</mo></mover><mo id="S3.SS2.p5.3.m3.3.3.3" xref="S3.SS2.p5.3.m3.3.3.3.cmml">=</mo><mrow id="S3.SS2.p5.3.m3.3.3.2.2" xref="S3.SS2.p5.3.m3.3.3.2.3.cmml"><mo id="S3.SS2.p5.3.m3.3.3.2.2.3" stretchy="false" xref="S3.SS2.p5.3.m3.3.3.2.3.cmml">{</mo><msup id="S3.SS2.p5.3.m3.2.2.1.1.1" xref="S3.SS2.p5.3.m3.2.2.1.1.1.cmml"><mover accent="true" id="S3.SS2.p5.3.m3.2.2.1.1.1.2" xref="S3.SS2.p5.3.m3.2.2.1.1.1.2.cmml"><mi id="S3.SS2.p5.3.m3.2.2.1.1.1.2.2" xref="S3.SS2.p5.3.m3.2.2.1.1.1.2.2.cmml">M</mi><mo id="S3.SS2.p5.3.m3.2.2.1.1.1.2.1" xref="S3.SS2.p5.3.m3.2.2.1.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p5.3.m3.2.2.1.1.1.3" xref="S3.SS2.p5.3.m3.2.2.1.1.1.3.cmml">1</mn></msup><mo id="S3.SS2.p5.3.m3.3.3.2.2.4" xref="S3.SS2.p5.3.m3.3.3.2.3.cmml">,</mo><mi id="S3.SS2.p5.3.m3.1.1" mathvariant="normal" xref="S3.SS2.p5.3.m3.1.1.cmml">…</mi><mo id="S3.SS2.p5.3.m3.3.3.2.2.5" xref="S3.SS2.p5.3.m3.3.3.2.3.cmml">,</mo><msup id="S3.SS2.p5.3.m3.3.3.2.2.2" xref="S3.SS2.p5.3.m3.3.3.2.2.2.cmml"><mover accent="true" id="S3.SS2.p5.3.m3.3.3.2.2.2.2" xref="S3.SS2.p5.3.m3.3.3.2.2.2.2.cmml"><mi id="S3.SS2.p5.3.m3.3.3.2.2.2.2.2" xref="S3.SS2.p5.3.m3.3.3.2.2.2.2.2.cmml">M</mi><mo id="S3.SS2.p5.3.m3.3.3.2.2.2.2.1" xref="S3.SS2.p5.3.m3.3.3.2.2.2.2.1.cmml">~</mo></mover><mi id="S3.SS2.p5.3.m3.3.3.2.2.2.3" xref="S3.SS2.p5.3.m3.3.3.2.2.2.3.cmml">m</mi></msup><mo id="S3.SS2.p5.3.m3.3.3.2.2.6" stretchy="false" xref="S3.SS2.p5.3.m3.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.3b"><apply id="S3.SS2.p5.3.m3.3.3.cmml" xref="S3.SS2.p5.3.m3.3.3"><eq id="S3.SS2.p5.3.m3.3.3.3.cmml" xref="S3.SS2.p5.3.m3.3.3.3"></eq><apply id="S3.SS2.p5.3.m3.3.3.4.cmml" xref="S3.SS2.p5.3.m3.3.3.4"><ci id="S3.SS2.p5.3.m3.3.3.4.1.cmml" xref="S3.SS2.p5.3.m3.3.3.4.1">~</ci><ci id="S3.SS2.p5.3.m3.3.3.4.2.cmml" xref="S3.SS2.p5.3.m3.3.3.4.2">ℳ</ci></apply><set id="S3.SS2.p5.3.m3.3.3.2.3.cmml" xref="S3.SS2.p5.3.m3.3.3.2.2"><apply id="S3.SS2.p5.3.m3.2.2.1.1.1.cmml" xref="S3.SS2.p5.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS2.p5.3.m3.2.2.1.1.1">superscript</csymbol><apply id="S3.SS2.p5.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS2.p5.3.m3.2.2.1.1.1.2"><ci id="S3.SS2.p5.3.m3.2.2.1.1.1.2.1.cmml" xref="S3.SS2.p5.3.m3.2.2.1.1.1.2.1">~</ci><ci id="S3.SS2.p5.3.m3.2.2.1.1.1.2.2.cmml" xref="S3.SS2.p5.3.m3.2.2.1.1.1.2.2">𝑀</ci></apply><cn id="S3.SS2.p5.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS2.p5.3.m3.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">…</ci><apply id="S3.SS2.p5.3.m3.3.3.2.2.2.cmml" xref="S3.SS2.p5.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.3.3.2.2.2.1.cmml" xref="S3.SS2.p5.3.m3.3.3.2.2.2">superscript</csymbol><apply id="S3.SS2.p5.3.m3.3.3.2.2.2.2.cmml" xref="S3.SS2.p5.3.m3.3.3.2.2.2.2"><ci id="S3.SS2.p5.3.m3.3.3.2.2.2.2.1.cmml" xref="S3.SS2.p5.3.m3.3.3.2.2.2.2.1">~</ci><ci id="S3.SS2.p5.3.m3.3.3.2.2.2.2.2.cmml" xref="S3.SS2.p5.3.m3.3.3.2.2.2.2.2">𝑀</ci></apply><ci id="S3.SS2.p5.3.m3.3.3.2.2.2.3.cmml" xref="S3.SS2.p5.3.m3.3.3.2.2.2.3">𝑚</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.3c">\mathcal{\widetilde{M}}=\{\widetilde{M}^{1},\ldots,\widetilde{M}^{m}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.3.m3.3d">over~ start_ARG caligraphic_M end_ARG = { over~ start_ARG italic_M end_ARG start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , over~ start_ARG italic_M end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }</annotation></semantics></math> and a subset of vertices <math alttext="\mathcal{\widetilde{V}}\subset\mathcal{V}" class="ltx_Math" display="inline" id="S3.SS2.p5.4.m4.1"><semantics id="S3.SS2.p5.4.m4.1a"><mrow id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml"><mover accent="true" id="S3.SS2.p5.4.m4.1.1.2" xref="S3.SS2.p5.4.m4.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.4.m4.1.1.2.2" xref="S3.SS2.p5.4.m4.1.1.2.2.cmml">𝒱</mi><mo id="S3.SS2.p5.4.m4.1.1.2.1" xref="S3.SS2.p5.4.m4.1.1.2.1.cmml">~</mo></mover><mo id="S3.SS2.p5.4.m4.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.cmml">⊂</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.4.m4.1.1.3" xref="S3.SS2.p5.4.m4.1.1.3.cmml">𝒱</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1"><subset id="S3.SS2.p5.4.m4.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1"></subset><apply id="S3.SS2.p5.4.m4.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1.2"><ci id="S3.SS2.p5.4.m4.1.1.2.1.cmml" xref="S3.SS2.p5.4.m4.1.1.2.1">~</ci><ci id="S3.SS2.p5.4.m4.1.1.2.2.cmml" xref="S3.SS2.p5.4.m4.1.1.2.2">𝒱</ci></apply><ci id="S3.SS2.p5.4.m4.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3">𝒱</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">\mathcal{\widetilde{V}}\subset\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.4.m4.1d">over~ start_ARG caligraphic_V end_ARG ⊂ caligraphic_V</annotation></semantics></math> whose scores are higher than <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS2.p5.5.m5.1"><semantics id="S3.SS2.p5.5.m5.1a"><mi id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><ci id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.5.m5.1d">italic_τ</annotation></semantics></math>. We optimize only this subset while editing the mesh. We are doing these processes to ensure that changes occur only in the most important regions, localized according to the context of the instruction.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="S3.F3.g1" src="x3.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Image and vertex scoring process. Using CLIPSeg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib17" title="">17</a>]</cite> we segment most important parts of the mesh given instruction</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.4"><span class="ltx_text ltx_font_bold" id="S3.SS2.p6.4.1">Editing Image:</span> For the editing process, we use a pre-trained variant of ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib31" title="">31</a>]</cite> with two modules, one fine-tuned on Instruct-Pix2Pix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib3" title="">3</a>]</cite> images and the other on normal maps. For calculation of the initial noise <math alttext="z_{0}" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.1"><semantics id="S3.SS2.p6.1.m1.1a"><msub id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml"><mi id="S3.SS2.p6.1.m1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.2.cmml">z</mi><mn id="S3.SS2.p6.1.m1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.2">𝑧</ci><cn id="S3.SS2.p6.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p6.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">z_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.1d">italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, Instruct-NeRF2NeRF follows the approach of SDEdit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib18" title="">18</a>]</cite> where the rendered image of the current global 3D model plays a role in the output of the diffusion model. Instead of using only the rendered image, we use a weighted sum of the rendered image <math alttext="\mathcal{\tilde{I}}^{v}" class="ltx_Math" display="inline" id="S3.SS2.p6.2.m2.1"><semantics id="S3.SS2.p6.2.m2.1a"><msup id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p6.2.m2.1.1.2" xref="S3.SS2.p6.2.m2.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p6.2.m2.1.1.2.2" xref="S3.SS2.p6.2.m2.1.1.2.2.cmml">ℐ</mi><mo id="S3.SS2.p6.2.m2.1.1.2.1" xref="S3.SS2.p6.2.m2.1.1.2.1.cmml">~</mo></mover><mi id="S3.SS2.p6.2.m2.1.1.3" xref="S3.SS2.p6.2.m2.1.1.3.cmml">v</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><apply id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.2.m2.1.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p6.2.m2.1.1.2.cmml" xref="S3.SS2.p6.2.m2.1.1.2"><ci id="S3.SS2.p6.2.m2.1.1.2.1.cmml" xref="S3.SS2.p6.2.m2.1.1.2.1">~</ci><ci id="S3.SS2.p6.2.m2.1.1.2.2.cmml" xref="S3.SS2.p6.2.m2.1.1.2.2">ℐ</ci></apply><ci id="S3.SS2.p6.2.m2.1.1.3.cmml" xref="S3.SS2.p6.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">\mathcal{\tilde{I}}^{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.2.m2.1d">over~ start_ARG caligraphic_I end_ARG start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT</annotation></semantics></math> and the original input image <math alttext="\mathcal{{I}}^{v}" class="ltx_Math" display="inline" id="S3.SS2.p6.3.m3.1"><semantics id="S3.SS2.p6.3.m3.1a"><msup id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p6.3.m3.1.1.2" xref="S3.SS2.p6.3.m3.1.1.2.cmml">ℐ</mi><mi id="S3.SS2.p6.3.m3.1.1.3" xref="S3.SS2.p6.3.m3.1.1.3.cmml">v</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><apply id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.p6.3.m3.1.1.2.cmml" xref="S3.SS2.p6.3.m3.1.1.2">ℐ</ci><ci id="S3.SS2.p6.3.m3.1.1.3.cmml" xref="S3.SS2.p6.3.m3.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">\mathcal{{I}}^{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.3.m3.1d">caligraphic_I start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT</annotation></semantics></math> from a given viewpoint <math alttext="v" class="ltx_Math" display="inline" id="S3.SS2.p6.4.m4.1"><semantics id="S3.SS2.p6.4.m4.1a"><mi id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><ci id="S3.SS2.p6.4.m4.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">v</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.4.m4.1d">italic_v</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z_{0}=\sqrt{\hat{\alpha_{0}}}(\lambda\mathcal{E}(\mathcal{\tilde{I}}^{v})+(1-%
\lambda)\mathcal{E}(\mathcal{{I}}^{v}))+\sqrt{1-\hat{\alpha_{0}}}\epsilon" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">z</mi><mn id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">0</mn></msub><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msqrt id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml"><msub id="S3.E3.m1.1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.1.3.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.3.2.2.2" xref="S3.E3.m1.1.1.1.1.3.2.2.2.cmml">α</mi><mn id="S3.E3.m1.1.1.1.1.3.2.2.3" xref="S3.E3.m1.1.1.1.1.3.2.2.3.cmml">0</mn></msub><mo id="S3.E3.m1.1.1.1.1.3.2.1" xref="S3.E3.m1.1.1.1.1.3.2.1.cmml">^</mo></mover></msqrt><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml">λ</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.cmml">ℰ</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.2a" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ℐ</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">~</mo></mover><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">v</mi></msup><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.1.1.1.4.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mn id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.2.cmml">1</mn><mo id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.1.cmml">−</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.3.cmml">λ</mi></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.3.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.1.1.1.3.4" xref="S3.E3.m1.1.1.1.1.1.1.1.3.4.cmml">ℰ</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.3.3a" xref="S3.E3.m1.1.1.1.1.1.1.1.3.3.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.3.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.cmml">(</mo><msup id="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.2.cmml">ℐ</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.3.cmml">v</mi></msup><mo id="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><msqrt id="S3.E3.m1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.3.2.cmml"><mrow id="S3.E3.m1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.3.2.2.cmml"><mn id="S3.E3.m1.1.1.1.3.2.2.2" xref="S3.E3.m1.1.1.1.3.2.2.2.cmml">1</mn><mo id="S3.E3.m1.1.1.1.3.2.2.1" xref="S3.E3.m1.1.1.1.3.2.2.1.cmml">−</mo><mover accent="true" id="S3.E3.m1.1.1.1.3.2.2.3" xref="S3.E3.m1.1.1.1.3.2.2.3.cmml"><msub id="S3.E3.m1.1.1.1.3.2.2.3.2" xref="S3.E3.m1.1.1.1.3.2.2.3.2.cmml"><mi id="S3.E3.m1.1.1.1.3.2.2.3.2.2" xref="S3.E3.m1.1.1.1.3.2.2.3.2.2.cmml">α</mi><mn id="S3.E3.m1.1.1.1.3.2.2.3.2.3" xref="S3.E3.m1.1.1.1.3.2.2.3.2.3.cmml">0</mn></msub><mo id="S3.E3.m1.1.1.1.3.2.2.3.1" xref="S3.E3.m1.1.1.1.3.2.2.3.1.cmml">^</mo></mover></mrow></msqrt><mo id="S3.E3.m1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.3.3.cmml">ϵ</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝑧</ci><cn id="S3.E3.m1.1.1.3.3.cmml" type="integer" xref="S3.E3.m1.1.1.3.3">0</cn></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><plus id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></plus><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></times><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><root id="S3.E3.m1.1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.3"></root><apply id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2"><ci id="S3.E3.m1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2.1">^</ci><apply id="S3.E3.m1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2.2">𝛼</ci><cn id="S3.E3.m1.1.1.1.1.3.2.2.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.3.2.2.3">0</cn></apply></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><plus id="S3.E3.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.4"></plus><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">𝜆</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4">ℰ</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.1">~</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.2">ℐ</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3">𝑣</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3"><times id="S3.E3.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.3"></times><apply id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1"><minus id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.1"></minus><cn id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.2.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.2">1</cn><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.3">𝜆</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.4">ℰ</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.1">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.2">ℐ</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.1.1.3">𝑣</ci></apply></apply></apply></apply><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><times id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3.1"></times><apply id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2"><root id="S3.E3.m1.1.1.1.3.2a.cmml" xref="S3.E3.m1.1.1.1.3.2"></root><apply id="S3.E3.m1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2"><minus id="S3.E3.m1.1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.1.3.2.2.1"></minus><cn id="S3.E3.m1.1.1.1.3.2.2.2.cmml" type="integer" xref="S3.E3.m1.1.1.1.3.2.2.2">1</cn><apply id="S3.E3.m1.1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.1.1.1.3.2.2.3"><ci id="S3.E3.m1.1.1.1.3.2.2.3.1.cmml" xref="S3.E3.m1.1.1.1.3.2.2.3.1">^</ci><apply id="S3.E3.m1.1.1.1.3.2.2.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.2.2.3.2.1.cmml" xref="S3.E3.m1.1.1.1.3.2.2.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.3.2.2.3.2.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2.3.2.2">𝛼</ci><cn id="S3.E3.m1.1.1.1.3.2.2.3.2.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.3.2.2.3.2.3">0</cn></apply></apply></apply></apply><ci id="S3.E3.m1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3">italic-ϵ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">z_{0}=\sqrt{\hat{\alpha_{0}}}(\lambda\mathcal{E}(\mathcal{\tilde{I}}^{v})+(1-%
\lambda)\mathcal{E}(\mathcal{{I}}^{v}))+\sqrt{1-\hat{\alpha_{0}}}\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = square-root start_ARG over^ start_ARG italic_α start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG end_ARG ( italic_λ caligraphic_E ( over~ start_ARG caligraphic_I end_ARG start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ) + ( 1 - italic_λ ) caligraphic_E ( caligraphic_I start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ) ) + square-root start_ARG 1 - over^ start_ARG italic_α start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG end_ARG italic_ϵ</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.7">where <math alttext="\epsilon\sim\mathcal{N}(0,1)" class="ltx_Math" display="inline" id="S3.SS2.p8.1.m1.2"><semantics id="S3.SS2.p8.1.m1.2a"><mrow id="S3.SS2.p8.1.m1.2.3" xref="S3.SS2.p8.1.m1.2.3.cmml"><mi id="S3.SS2.p8.1.m1.2.3.2" xref="S3.SS2.p8.1.m1.2.3.2.cmml">ϵ</mi><mo id="S3.SS2.p8.1.m1.2.3.1" xref="S3.SS2.p8.1.m1.2.3.1.cmml">∼</mo><mrow id="S3.SS2.p8.1.m1.2.3.3" xref="S3.SS2.p8.1.m1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.1.m1.2.3.3.2" xref="S3.SS2.p8.1.m1.2.3.3.2.cmml">𝒩</mi><mo id="S3.SS2.p8.1.m1.2.3.3.1" xref="S3.SS2.p8.1.m1.2.3.3.1.cmml">⁢</mo><mrow id="S3.SS2.p8.1.m1.2.3.3.3.2" xref="S3.SS2.p8.1.m1.2.3.3.3.1.cmml"><mo id="S3.SS2.p8.1.m1.2.3.3.3.2.1" stretchy="false" xref="S3.SS2.p8.1.m1.2.3.3.3.1.cmml">(</mo><mn id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.p8.1.m1.2.3.3.3.2.2" xref="S3.SS2.p8.1.m1.2.3.3.3.1.cmml">,</mo><mn id="S3.SS2.p8.1.m1.2.2" xref="S3.SS2.p8.1.m1.2.2.cmml">1</mn><mo id="S3.SS2.p8.1.m1.2.3.3.3.2.3" stretchy="false" xref="S3.SS2.p8.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.2b"><apply id="S3.SS2.p8.1.m1.2.3.cmml" xref="S3.SS2.p8.1.m1.2.3"><csymbol cd="latexml" id="S3.SS2.p8.1.m1.2.3.1.cmml" xref="S3.SS2.p8.1.m1.2.3.1">similar-to</csymbol><ci id="S3.SS2.p8.1.m1.2.3.2.cmml" xref="S3.SS2.p8.1.m1.2.3.2">italic-ϵ</ci><apply id="S3.SS2.p8.1.m1.2.3.3.cmml" xref="S3.SS2.p8.1.m1.2.3.3"><times id="S3.SS2.p8.1.m1.2.3.3.1.cmml" xref="S3.SS2.p8.1.m1.2.3.3.1"></times><ci id="S3.SS2.p8.1.m1.2.3.3.2.cmml" xref="S3.SS2.p8.1.m1.2.3.3.2">𝒩</ci><interval closure="open" id="S3.SS2.p8.1.m1.2.3.3.3.1.cmml" xref="S3.SS2.p8.1.m1.2.3.3.3.2"><cn id="S3.SS2.p8.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p8.1.m1.1.1">0</cn><cn id="S3.SS2.p8.1.m1.2.2.cmml" type="integer" xref="S3.SS2.p8.1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.2c">\epsilon\sim\mathcal{N}(0,1)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.1.m1.2d">italic_ϵ ∼ caligraphic_N ( 0 , 1 )</annotation></semantics></math>, <math alttext="\hat{\alpha_{0}}" class="ltx_Math" display="inline" id="S3.SS2.p8.2.m2.1"><semantics id="S3.SS2.p8.2.m2.1a"><mover accent="true" id="S3.SS2.p8.2.m2.1.1" xref="S3.SS2.p8.2.m2.1.1.cmml"><msub id="S3.SS2.p8.2.m2.1.1.2" xref="S3.SS2.p8.2.m2.1.1.2.cmml"><mi id="S3.SS2.p8.2.m2.1.1.2.2" xref="S3.SS2.p8.2.m2.1.1.2.2.cmml">α</mi><mn id="S3.SS2.p8.2.m2.1.1.2.3" xref="S3.SS2.p8.2.m2.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS2.p8.2.m2.1.1.1" xref="S3.SS2.p8.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m2.1b"><apply id="S3.SS2.p8.2.m2.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1"><ci id="S3.SS2.p8.2.m2.1.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1.1">^</ci><apply id="S3.SS2.p8.2.m2.1.1.2.cmml" xref="S3.SS2.p8.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p8.2.m2.1.1.2.1.cmml" xref="S3.SS2.p8.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p8.2.m2.1.1.2.2.cmml" xref="S3.SS2.p8.2.m2.1.1.2.2">𝛼</ci><cn id="S3.SS2.p8.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS2.p8.2.m2.1.1.2.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m2.1c">\hat{\alpha_{0}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.2.m2.1d">over^ start_ARG italic_α start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> is the noise scheduling factor at timestep 0 and <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS2.p8.3.m3.1"><semantics id="S3.SS2.p8.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.3.m3.1.1" xref="S3.SS2.p8.3.m3.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.3.m3.1b"><ci id="S3.SS2.p8.3.m3.1.1.cmml" xref="S3.SS2.p8.3.m3.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.3.m3.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.3.m3.1d">caligraphic_E</annotation></semantics></math> is the CLIP image encoder. Hyperparameter <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p8.4.m4.1"><semantics id="S3.SS2.p8.4.m4.1a"><mi id="S3.SS2.p8.4.m4.1.1" xref="S3.SS2.p8.4.m4.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.4.m4.1b"><ci id="S3.SS2.p8.4.m4.1.1.cmml" xref="S3.SS2.p8.4.m4.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.4.m4.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.4.m4.1d">italic_λ</annotation></semantics></math> is the latent weight used to control the proportion of <math alttext="\mathcal{\tilde{I}}^{v}" class="ltx_Math" display="inline" id="S3.SS2.p8.5.m5.1"><semantics id="S3.SS2.p8.5.m5.1a"><msup id="S3.SS2.p8.5.m5.1.1" xref="S3.SS2.p8.5.m5.1.1.cmml"><mover accent="true" id="S3.SS2.p8.5.m5.1.1.2" xref="S3.SS2.p8.5.m5.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.5.m5.1.1.2.2" xref="S3.SS2.p8.5.m5.1.1.2.2.cmml">ℐ</mi><mo id="S3.SS2.p8.5.m5.1.1.2.1" xref="S3.SS2.p8.5.m5.1.1.2.1.cmml">~</mo></mover><mi id="S3.SS2.p8.5.m5.1.1.3" xref="S3.SS2.p8.5.m5.1.1.3.cmml">v</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.5.m5.1b"><apply id="S3.SS2.p8.5.m5.1.1.cmml" xref="S3.SS2.p8.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.5.m5.1.1.1.cmml" xref="S3.SS2.p8.5.m5.1.1">superscript</csymbol><apply id="S3.SS2.p8.5.m5.1.1.2.cmml" xref="S3.SS2.p8.5.m5.1.1.2"><ci id="S3.SS2.p8.5.m5.1.1.2.1.cmml" xref="S3.SS2.p8.5.m5.1.1.2.1">~</ci><ci id="S3.SS2.p8.5.m5.1.1.2.2.cmml" xref="S3.SS2.p8.5.m5.1.1.2.2">ℐ</ci></apply><ci id="S3.SS2.p8.5.m5.1.1.3.cmml" xref="S3.SS2.p8.5.m5.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.5.m5.1c">\mathcal{\tilde{I}}^{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.5.m5.1d">over~ start_ARG caligraphic_I end_ARG start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{{I}}^{v}" class="ltx_Math" display="inline" id="S3.SS2.p8.6.m6.1"><semantics id="S3.SS2.p8.6.m6.1a"><msup id="S3.SS2.p8.6.m6.1.1" xref="S3.SS2.p8.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.6.m6.1.1.2" xref="S3.SS2.p8.6.m6.1.1.2.cmml">ℐ</mi><mi id="S3.SS2.p8.6.m6.1.1.3" xref="S3.SS2.p8.6.m6.1.1.3.cmml">v</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.6.m6.1b"><apply id="S3.SS2.p8.6.m6.1.1.cmml" xref="S3.SS2.p8.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.6.m6.1.1.1.cmml" xref="S3.SS2.p8.6.m6.1.1">superscript</csymbol><ci id="S3.SS2.p8.6.m6.1.1.2.cmml" xref="S3.SS2.p8.6.m6.1.1.2">ℐ</ci><ci id="S3.SS2.p8.6.m6.1.1.3.cmml" xref="S3.SS2.p8.6.m6.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.6.m6.1c">\mathcal{{I}}^{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.6.m6.1d">caligraphic_I start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT</annotation></semantics></math> latents. In the case of <math alttext="\lambda=1" class="ltx_Math" display="inline" id="S3.SS2.p8.7.m7.1"><semantics id="S3.SS2.p8.7.m7.1a"><mrow id="S3.SS2.p8.7.m7.1.1" xref="S3.SS2.p8.7.m7.1.1.cmml"><mi id="S3.SS2.p8.7.m7.1.1.2" xref="S3.SS2.p8.7.m7.1.1.2.cmml">λ</mi><mo id="S3.SS2.p8.7.m7.1.1.1" xref="S3.SS2.p8.7.m7.1.1.1.cmml">=</mo><mn id="S3.SS2.p8.7.m7.1.1.3" xref="S3.SS2.p8.7.m7.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.7.m7.1b"><apply id="S3.SS2.p8.7.m7.1.1.cmml" xref="S3.SS2.p8.7.m7.1.1"><eq id="S3.SS2.p8.7.m7.1.1.1.cmml" xref="S3.SS2.p8.7.m7.1.1.1"></eq><ci id="S3.SS2.p8.7.m7.1.1.2.cmml" xref="S3.SS2.p8.7.m7.1.1.2">𝜆</ci><cn id="S3.SS2.p8.7.m7.1.1.3.cmml" type="integer" xref="S3.SS2.p8.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.7.m7.1c">\lambda=1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.7.m7.1d">italic_λ = 1</annotation></semantics></math>, the initial noise calculation is the same as Instruct-NeRF2NeRF. Since our focus is on editing rather than mesh generation, we want our edited mesh to be constrained on the initial input pictures as well. However in order to be restrained not too much on initial mesh we also add rendered images to add some variance to the input noise. This allows the diffusion model to not diverge too much to the dark and bright images as seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S3.F4" title="Figure 4 ‣ 3.2 LEMON ‣ 3 Method ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.7">As conditions for the diffusion model, we include the encoded text prompt <math alttext="c_{T}" class="ltx_Math" display="inline" id="S3.SS2.p9.1.m1.1"><semantics id="S3.SS2.p9.1.m1.1a"><msub id="S3.SS2.p9.1.m1.1.1" xref="S3.SS2.p9.1.m1.1.1.cmml"><mi id="S3.SS2.p9.1.m1.1.1.2" xref="S3.SS2.p9.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS2.p9.1.m1.1.1.3" xref="S3.SS2.p9.1.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.1.m1.1b"><apply id="S3.SS2.p9.1.m1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.1.m1.1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p9.1.m1.1.1.2.cmml" xref="S3.SS2.p9.1.m1.1.1.2">𝑐</ci><ci id="S3.SS2.p9.1.m1.1.1.3.cmml" xref="S3.SS2.p9.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.1.m1.1c">c_{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.1.m1.1d">italic_c start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math>, generated by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib23" title="">23</a>]</cite> to guide our pipeline for editing. For the normal map module, we provide the normal map <math alttext="n_{i}" class="ltx_Math" display="inline" id="S3.SS2.p9.2.m2.1"><semantics id="S3.SS2.p9.2.m2.1a"><msub id="S3.SS2.p9.2.m2.1.1" xref="S3.SS2.p9.2.m2.1.1.cmml"><mi id="S3.SS2.p9.2.m2.1.1.2" xref="S3.SS2.p9.2.m2.1.1.2.cmml">n</mi><mi id="S3.SS2.p9.2.m2.1.1.3" xref="S3.SS2.p9.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.2.m2.1b"><apply id="S3.SS2.p9.2.m2.1.1.cmml" xref="S3.SS2.p9.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.2.m2.1.1.1.cmml" xref="S3.SS2.p9.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p9.2.m2.1.1.2.cmml" xref="S3.SS2.p9.2.m2.1.1.2">𝑛</ci><ci id="S3.SS2.p9.2.m2.1.1.3.cmml" xref="S3.SS2.p9.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.2.m2.1c">n_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.2.m2.1d">italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> of the g-buffer from a given viewpoint <math alttext="v" class="ltx_Math" display="inline" id="S3.SS2.p9.3.m3.1"><semantics id="S3.SS2.p9.3.m3.1a"><mi id="S3.SS2.p9.3.m3.1.1" xref="S3.SS2.p9.3.m3.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.3.m3.1b"><ci id="S3.SS2.p9.3.m3.1.1.cmml" xref="S3.SS2.p9.3.m3.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.3.m3.1c">v</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.3.m3.1d">italic_v</annotation></semantics></math> as conditioning input. This leads to generated images that are more geometrically consistent, directly influenced by the optimized mesh <math alttext="\mathcal{G}" class="ltx_Math" display="inline" id="S3.SS2.p9.4.m4.1"><semantics id="S3.SS2.p9.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p9.4.m4.1.1" xref="S3.SS2.p9.4.m4.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.4.m4.1b"><ci id="S3.SS2.p9.4.m4.1.1.cmml" xref="S3.SS2.p9.4.m4.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.4.m4.1c">\mathcal{G}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.4.m4.1d">caligraphic_G</annotation></semantics></math>. Instead of using ground-truth images for conditions as in Instruct-NeRF2NeRF, our Instruct-Pix2Pix module uses rendered images <math alttext="\mathcal{\tilde{I}}^{v}" class="ltx_Math" display="inline" id="S3.SS2.p9.5.m5.1"><semantics id="S3.SS2.p9.5.m5.1a"><msup id="S3.SS2.p9.5.m5.1.1" xref="S3.SS2.p9.5.m5.1.1.cmml"><mover accent="true" id="S3.SS2.p9.5.m5.1.1.2" xref="S3.SS2.p9.5.m5.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p9.5.m5.1.1.2.2" xref="S3.SS2.p9.5.m5.1.1.2.2.cmml">ℐ</mi><mo id="S3.SS2.p9.5.m5.1.1.2.1" xref="S3.SS2.p9.5.m5.1.1.2.1.cmml">~</mo></mover><mi id="S3.SS2.p9.5.m5.1.1.3" xref="S3.SS2.p9.5.m5.1.1.3.cmml">v</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.5.m5.1b"><apply id="S3.SS2.p9.5.m5.1.1.cmml" xref="S3.SS2.p9.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.5.m5.1.1.1.cmml" xref="S3.SS2.p9.5.m5.1.1">superscript</csymbol><apply id="S3.SS2.p9.5.m5.1.1.2.cmml" xref="S3.SS2.p9.5.m5.1.1.2"><ci id="S3.SS2.p9.5.m5.1.1.2.1.cmml" xref="S3.SS2.p9.5.m5.1.1.2.1">~</ci><ci id="S3.SS2.p9.5.m5.1.1.2.2.cmml" xref="S3.SS2.p9.5.m5.1.1.2.2">ℐ</ci></apply><ci id="S3.SS2.p9.5.m5.1.1.3.cmml" xref="S3.SS2.p9.5.m5.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.5.m5.1c">\mathcal{\tilde{I}}^{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.5.m5.1d">over~ start_ARG caligraphic_I end_ARG start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT</annotation></semantics></math> generated by <math alttext="f_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p9.6.m6.1"><semantics id="S3.SS2.p9.6.m6.1a"><msub id="S3.SS2.p9.6.m6.1.1" xref="S3.SS2.p9.6.m6.1.1.cmml"><mi id="S3.SS2.p9.6.m6.1.1.2" xref="S3.SS2.p9.6.m6.1.1.2.cmml">f</mi><mi id="S3.SS2.p9.6.m6.1.1.3" xref="S3.SS2.p9.6.m6.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.6.m6.1b"><apply id="S3.SS2.p9.6.m6.1.1.cmml" xref="S3.SS2.p9.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.6.m6.1.1.1.cmml" xref="S3.SS2.p9.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p9.6.m6.1.1.2.cmml" xref="S3.SS2.p9.6.m6.1.1.2">𝑓</ci><ci id="S3.SS2.p9.6.m6.1.1.3.cmml" xref="S3.SS2.p9.6.m6.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.6.m6.1c">f_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.6.m6.1d">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> as its conditioning input. This results in generated images with greater variability, directly influenced by the optimized neural shader <math alttext="f_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p9.7.m7.1"><semantics id="S3.SS2.p9.7.m7.1a"><msub id="S3.SS2.p9.7.m7.1.1" xref="S3.SS2.p9.7.m7.1.1.cmml"><mi id="S3.SS2.p9.7.m7.1.1.2" xref="S3.SS2.p9.7.m7.1.1.2.cmml">f</mi><mi id="S3.SS2.p9.7.m7.1.1.3" xref="S3.SS2.p9.7.m7.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.7.m7.1b"><apply id="S3.SS2.p9.7.m7.1.1.cmml" xref="S3.SS2.p9.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.7.m7.1.1.1.cmml" xref="S3.SS2.p9.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p9.7.m7.1.1.2.cmml" xref="S3.SS2.p9.7.m7.1.1.2">𝑓</ci><ci id="S3.SS2.p9.7.m7.1.1.3.cmml" xref="S3.SS2.p9.7.m7.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.7.m7.1c">f_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.7.m7.1d">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>. In a sense our neural deferred shading pipeline and our conditioning inputs optimize each other, resulting in more consistent image generation.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="193" id="S3.F4.g1" src="x4.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.8.4.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.6.3" style="font-size:90%;">The effect of the latent weight hyperparameter <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.F4.4.1.m1.1"><semantics id="S3.F4.4.1.m1.1b"><mi id="S3.F4.4.1.m1.1.1" xref="S3.F4.4.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.F4.4.1.m1.1c"><ci id="S3.F4.4.1.m1.1.1.cmml" xref="S3.F4.4.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.4.1.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.F4.4.1.m1.1e">italic_λ</annotation></semantics></math> on editing of the skull object from  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib11" title="">11</a>]</cite> for the ”Turn it into Batman” prompt. Top of the skull has very bright shading, but the prompt requires the object to be darker. When <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.F4.5.2.m2.1"><semantics id="S3.F4.5.2.m2.1b"><mi id="S3.F4.5.2.m2.1.1" xref="S3.F4.5.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.F4.5.2.m2.1c"><ci id="S3.F4.5.2.m2.1.1.cmml" xref="S3.F4.5.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.5.2.m2.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.F4.5.2.m2.1e">italic_λ</annotation></semantics></math> is set to 0, only the ground truth image is used for the initial noise calculation, resulting in the lines in the skull to stay. If <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.F4.6.3.m3.1"><semantics id="S3.F4.6.3.m3.1b"><mi id="S3.F4.6.3.m3.1.1" xref="S3.F4.6.3.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.F4.6.3.m3.1c"><ci id="S3.F4.6.3.m3.1.1.cmml" xref="S3.F4.6.3.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.6.3.m3.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.F4.6.3.m3.1e">italic_λ</annotation></semantics></math> is too high, the rendered image may diverge to darker tones, leading to unintended edits.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p10">
<p class="ltx_p" id="S3.SS2.p10.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p10.1.1">Optimizing Process:</span> We adapt the iterative dataset update of Instruct-NeRF2NeRF to our neural deferred shading pipeline. After the generation of the initial mesh, every <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.p10.1.m1.1"><semantics id="S3.SS2.p10.1.m1.1a"><mi id="S3.SS2.p10.1.m1.1.1" xref="S3.SS2.p10.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.1.m1.1b"><ci id="S3.SS2.p10.1.m1.1.1.cmml" xref="S3.SS2.p10.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p10.1.m1.1d">italic_d</annotation></semantics></math> iteration we create a modified input image using our diffusion model and change the input image of the current iteration with the modified image. We gradually optimize vertices of the mesh and neural shader based on new input images. The equation below demonstrates the image update process.</p>
</div>
<div class="ltx_para" id="S3.SS2.p11">
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{{{I}}}_{i+1}^{v}\leftarrow U_{\theta}\left(\mathcal{I}_{0}^{v},%
\mathcal{{\tilde{I}}}_{i}^{v},t;\mathcal{{\tilde{I}}}_{i}^{v},{n_{i}}^{v},c_{T%
}\right)\odot{\widetilde{M}^{v}}+\mathcal{I}_{0}^{v}\odot(1-{\widetilde{M}^{v}})" class="ltx_Math" display="block" id="S3.E4.m1.7"><semantics id="S3.E4.m1.7a"><mrow id="S3.E4.m1.7.7" xref="S3.E4.m1.7.7.cmml"><msubsup id="S3.E4.m1.7.7.8" xref="S3.E4.m1.7.7.8.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.7.7.8.2.2" xref="S3.E4.m1.7.7.8.2.2.cmml">ℐ</mi><mrow id="S3.E4.m1.7.7.8.2.3" xref="S3.E4.m1.7.7.8.2.3.cmml"><mi id="S3.E4.m1.7.7.8.2.3.2" xref="S3.E4.m1.7.7.8.2.3.2.cmml">i</mi><mo id="S3.E4.m1.7.7.8.2.3.1" xref="S3.E4.m1.7.7.8.2.3.1.cmml">+</mo><mn id="S3.E4.m1.7.7.8.2.3.3" xref="S3.E4.m1.7.7.8.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.7.7.8.3" xref="S3.E4.m1.7.7.8.3.cmml">v</mi></msubsup><mo id="S3.E4.m1.7.7.7" stretchy="false" xref="S3.E4.m1.7.7.7.cmml">←</mo><mrow id="S3.E4.m1.7.7.6" xref="S3.E4.m1.7.7.6.cmml"><mrow id="S3.E4.m1.6.6.5.5" xref="S3.E4.m1.6.6.5.5.cmml"><mrow id="S3.E4.m1.6.6.5.5.5" xref="S3.E4.m1.6.6.5.5.5.cmml"><msub id="S3.E4.m1.6.6.5.5.5.7" xref="S3.E4.m1.6.6.5.5.5.7.cmml"><mi id="S3.E4.m1.6.6.5.5.5.7.2" xref="S3.E4.m1.6.6.5.5.5.7.2.cmml">U</mi><mi id="S3.E4.m1.6.6.5.5.5.7.3" xref="S3.E4.m1.6.6.5.5.5.7.3.cmml">θ</mi></msub><mo id="S3.E4.m1.6.6.5.5.5.6" xref="S3.E4.m1.6.6.5.5.5.6.cmml">⁢</mo><mrow id="S3.E4.m1.6.6.5.5.5.5.5" xref="S3.E4.m1.6.6.5.5.5.5.6.cmml"><mo id="S3.E4.m1.6.6.5.5.5.5.5.6" xref="S3.E4.m1.6.6.5.5.5.5.6.cmml">(</mo><msubsup id="S3.E4.m1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.cmml">ℐ</mi><mn id="S3.E4.m1.2.2.1.1.1.1.1.1.2.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.3.cmml">0</mn><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.3.cmml">v</mi></msubsup><mo id="S3.E4.m1.6.6.5.5.5.5.5.7" xref="S3.E4.m1.6.6.5.5.5.5.6.cmml">,</mo><msubsup id="S3.E4.m1.3.3.2.2.2.2.2.2" xref="S3.E4.m1.3.3.2.2.2.2.2.2.cmml"><mover accent="true" id="S3.E4.m1.3.3.2.2.2.2.2.2.2.2" xref="S3.E4.m1.3.3.2.2.2.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.3.3.2.2.2.2.2.2.2.2.2" xref="S3.E4.m1.3.3.2.2.2.2.2.2.2.2.2.cmml">ℐ</mi><mo id="S3.E4.m1.3.3.2.2.2.2.2.2.2.2.1" xref="S3.E4.m1.3.3.2.2.2.2.2.2.2.2.1.cmml">~</mo></mover><mi id="S3.E4.m1.3.3.2.2.2.2.2.2.2.3" xref="S3.E4.m1.3.3.2.2.2.2.2.2.2.3.cmml">i</mi><mi id="S3.E4.m1.3.3.2.2.2.2.2.2.3" xref="S3.E4.m1.3.3.2.2.2.2.2.2.3.cmml">v</mi></msubsup><mo id="S3.E4.m1.6.6.5.5.5.5.5.8" xref="S3.E4.m1.6.6.5.5.5.5.6.cmml">,</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">t</mi><mo id="S3.E4.m1.6.6.5.5.5.5.5.9" xref="S3.E4.m1.6.6.5.5.5.5.6.cmml">;</mo><msubsup id="S3.E4.m1.4.4.3.3.3.3.3.3" xref="S3.E4.m1.4.4.3.3.3.3.3.3.cmml"><mover accent="true" id="S3.E4.m1.4.4.3.3.3.3.3.3.2.2" xref="S3.E4.m1.4.4.3.3.3.3.3.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.4.4.3.3.3.3.3.3.2.2.2" xref="S3.E4.m1.4.4.3.3.3.3.3.3.2.2.2.cmml">ℐ</mi><mo id="S3.E4.m1.4.4.3.3.3.3.3.3.2.2.1" xref="S3.E4.m1.4.4.3.3.3.3.3.3.2.2.1.cmml">~</mo></mover><mi id="S3.E4.m1.4.4.3.3.3.3.3.3.2.3" xref="S3.E4.m1.4.4.3.3.3.3.3.3.2.3.cmml">i</mi><mi id="S3.E4.m1.4.4.3.3.3.3.3.3.3" xref="S3.E4.m1.4.4.3.3.3.3.3.3.3.cmml">v</mi></msubsup><mo id="S3.E4.m1.6.6.5.5.5.5.5.10" xref="S3.E4.m1.6.6.5.5.5.5.6.cmml">,</mo><mmultiscripts id="S3.E4.m1.5.5.4.4.4.4.4.4" xref="S3.E4.m1.5.5.4.4.4.4.4.4.cmml"><mi id="S3.E4.m1.5.5.4.4.4.4.4.4.2.2" xref="S3.E4.m1.5.5.4.4.4.4.4.4.2.2.cmml">n</mi><mi id="S3.E4.m1.5.5.4.4.4.4.4.4.2.3" xref="S3.E4.m1.5.5.4.4.4.4.4.4.2.3.cmml">i</mi><mrow id="S3.E4.m1.5.5.4.4.4.4.4.4a" xref="S3.E4.m1.5.5.4.4.4.4.4.4.cmml"></mrow><mrow id="S3.E4.m1.5.5.4.4.4.4.4.4b" xref="S3.E4.m1.5.5.4.4.4.4.4.4.cmml"></mrow><mi id="S3.E4.m1.5.5.4.4.4.4.4.4.3" xref="S3.E4.m1.5.5.4.4.4.4.4.4.3.cmml">v</mi></mmultiscripts><mo id="S3.E4.m1.6.6.5.5.5.5.5.11" xref="S3.E4.m1.6.6.5.5.5.5.6.cmml">,</mo><msub id="S3.E4.m1.6.6.5.5.5.5.5.5" xref="S3.E4.m1.6.6.5.5.5.5.5.5.cmml"><mi id="S3.E4.m1.6.6.5.5.5.5.5.5.2" xref="S3.E4.m1.6.6.5.5.5.5.5.5.2.cmml">c</mi><mi id="S3.E4.m1.6.6.5.5.5.5.5.5.3" xref="S3.E4.m1.6.6.5.5.5.5.5.5.3.cmml">T</mi></msub><mo id="S3.E4.m1.6.6.5.5.5.5.5.12" rspace="0.055em" xref="S3.E4.m1.6.6.5.5.5.5.6.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.6.6.5.5.6" rspace="0.222em" xref="S3.E4.m1.6.6.5.5.6.cmml">⊙</mo><msup id="S3.E4.m1.6.6.5.5.7" xref="S3.E4.m1.6.6.5.5.7.cmml"><mover accent="true" id="S3.E4.m1.6.6.5.5.7.2" xref="S3.E4.m1.6.6.5.5.7.2.cmml"><mi id="S3.E4.m1.6.6.5.5.7.2.2" xref="S3.E4.m1.6.6.5.5.7.2.2.cmml">M</mi><mo id="S3.E4.m1.6.6.5.5.7.2.1" xref="S3.E4.m1.6.6.5.5.7.2.1.cmml">~</mo></mover><mi id="S3.E4.m1.6.6.5.5.7.3" xref="S3.E4.m1.6.6.5.5.7.3.cmml">v</mi></msup></mrow><mo id="S3.E4.m1.7.7.6.7" xref="S3.E4.m1.7.7.6.7.cmml">+</mo><mrow id="S3.E4.m1.7.7.6.6" xref="S3.E4.m1.7.7.6.6.cmml"><msubsup id="S3.E4.m1.7.7.6.6.3" xref="S3.E4.m1.7.7.6.6.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.7.7.6.6.3.2.2" xref="S3.E4.m1.7.7.6.6.3.2.2.cmml">ℐ</mi><mn id="S3.E4.m1.7.7.6.6.3.2.3" xref="S3.E4.m1.7.7.6.6.3.2.3.cmml">0</mn><mi id="S3.E4.m1.7.7.6.6.3.3" xref="S3.E4.m1.7.7.6.6.3.3.cmml">v</mi></msubsup><mo id="S3.E4.m1.7.7.6.6.2" lspace="0.222em" rspace="0.222em" xref="S3.E4.m1.7.7.6.6.2.cmml">⊙</mo><mrow id="S3.E4.m1.7.7.6.6.1.1" xref="S3.E4.m1.7.7.6.6.1.1.1.cmml"><mo id="S3.E4.m1.7.7.6.6.1.1.2" stretchy="false" xref="S3.E4.m1.7.7.6.6.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.7.7.6.6.1.1.1" xref="S3.E4.m1.7.7.6.6.1.1.1.cmml"><mn id="S3.E4.m1.7.7.6.6.1.1.1.2" xref="S3.E4.m1.7.7.6.6.1.1.1.2.cmml">1</mn><mo id="S3.E4.m1.7.7.6.6.1.1.1.1" xref="S3.E4.m1.7.7.6.6.1.1.1.1.cmml">−</mo><msup id="S3.E4.m1.7.7.6.6.1.1.1.3" xref="S3.E4.m1.7.7.6.6.1.1.1.3.cmml"><mover accent="true" id="S3.E4.m1.7.7.6.6.1.1.1.3.2" xref="S3.E4.m1.7.7.6.6.1.1.1.3.2.cmml"><mi id="S3.E4.m1.7.7.6.6.1.1.1.3.2.2" xref="S3.E4.m1.7.7.6.6.1.1.1.3.2.2.cmml">M</mi><mo id="S3.E4.m1.7.7.6.6.1.1.1.3.2.1" xref="S3.E4.m1.7.7.6.6.1.1.1.3.2.1.cmml">~</mo></mover><mi id="S3.E4.m1.7.7.6.6.1.1.1.3.3" xref="S3.E4.m1.7.7.6.6.1.1.1.3.3.cmml">v</mi></msup></mrow><mo id="S3.E4.m1.7.7.6.6.1.1.3" stretchy="false" xref="S3.E4.m1.7.7.6.6.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.7b"><apply id="S3.E4.m1.7.7.cmml" xref="S3.E4.m1.7.7"><ci id="S3.E4.m1.7.7.7.cmml" xref="S3.E4.m1.7.7.7">←</ci><apply id="S3.E4.m1.7.7.8.cmml" xref="S3.E4.m1.7.7.8"><csymbol cd="ambiguous" id="S3.E4.m1.7.7.8.1.cmml" xref="S3.E4.m1.7.7.8">superscript</csymbol><apply id="S3.E4.m1.7.7.8.2.cmml" xref="S3.E4.m1.7.7.8"><csymbol cd="ambiguous" id="S3.E4.m1.7.7.8.2.1.cmml" xref="S3.E4.m1.7.7.8">subscript</csymbol><ci id="S3.E4.m1.7.7.8.2.2.cmml" xref="S3.E4.m1.7.7.8.2.2">ℐ</ci><apply id="S3.E4.m1.7.7.8.2.3.cmml" xref="S3.E4.m1.7.7.8.2.3"><plus id="S3.E4.m1.7.7.8.2.3.1.cmml" xref="S3.E4.m1.7.7.8.2.3.1"></plus><ci id="S3.E4.m1.7.7.8.2.3.2.cmml" xref="S3.E4.m1.7.7.8.2.3.2">𝑖</ci><cn id="S3.E4.m1.7.7.8.2.3.3.cmml" type="integer" xref="S3.E4.m1.7.7.8.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.7.7.8.3.cmml" xref="S3.E4.m1.7.7.8.3">𝑣</ci></apply><apply id="S3.E4.m1.7.7.6.cmml" xref="S3.E4.m1.7.7.6"><plus id="S3.E4.m1.7.7.6.7.cmml" xref="S3.E4.m1.7.7.6.7"></plus><apply id="S3.E4.m1.6.6.5.5.cmml" xref="S3.E4.m1.6.6.5.5"><csymbol cd="latexml" id="S3.E4.m1.6.6.5.5.6.cmml" xref="S3.E4.m1.6.6.5.5.6">direct-product</csymbol><apply id="S3.E4.m1.6.6.5.5.5.cmml" xref="S3.E4.m1.6.6.5.5.5"><times id="S3.E4.m1.6.6.5.5.5.6.cmml" xref="S3.E4.m1.6.6.5.5.5.6"></times><apply id="S3.E4.m1.6.6.5.5.5.7.cmml" xref="S3.E4.m1.6.6.5.5.5.7"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.5.5.5.7.1.cmml" xref="S3.E4.m1.6.6.5.5.5.7">subscript</csymbol><ci id="S3.E4.m1.6.6.5.5.5.7.2.cmml" xref="S3.E4.m1.6.6.5.5.5.7.2">𝑈</ci><ci id="S3.E4.m1.6.6.5.5.5.7.3.cmml" xref="S3.E4.m1.6.6.5.5.5.7.3">𝜃</ci></apply><vector id="S3.E4.m1.6.6.5.5.5.5.6.cmml" xref="S3.E4.m1.6.6.5.5.5.5.5"><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2">ℐ</ci><cn id="S3.E4.m1.2.2.1.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.3">0</cn></apply><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.3">𝑣</ci></apply><apply id="S3.E4.m1.3.3.2.2.2.2.2.2.cmml" xref="S3.E4.m1.3.3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.3.3.2.2.2.2.2.2">superscript</csymbol><apply id="S3.E4.m1.3.3.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.3.3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.3.3.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E4.m1.3.3.2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.3.3.2.2.2.2.2.2.2.2"><ci id="S3.E4.m1.3.3.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.3.3.2.2.2.2.2.2.2.2.1">~</ci><ci id="S3.E4.m1.3.3.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.3.3.2.2.2.2.2.2.2.2.2">ℐ</ci></apply><ci id="S3.E4.m1.3.3.2.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.3.3.2.2.2.2.2.2.2.3">𝑖</ci></apply><ci id="S3.E4.m1.3.3.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.3.3.2.2.2.2.2.2.3">𝑣</ci></apply><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝑡</ci><apply id="S3.E4.m1.4.4.3.3.3.3.3.3.cmml" xref="S3.E4.m1.4.4.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.3.3.3.3.3.3.1.cmml" xref="S3.E4.m1.4.4.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E4.m1.4.4.3.3.3.3.3.3.2.cmml" xref="S3.E4.m1.4.4.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.3.3.3.3.3.3.2.1.cmml" xref="S3.E4.m1.4.4.3.3.3.3.3.3">subscript</csymbol><apply id="S3.E4.m1.4.4.3.3.3.3.3.3.2.2.cmml" xref="S3.E4.m1.4.4.3.3.3.3.3.3.2.2"><ci id="S3.E4.m1.4.4.3.3.3.3.3.3.2.2.1.cmml" xref="S3.E4.m1.4.4.3.3.3.3.3.3.2.2.1">~</ci><ci id="S3.E4.m1.4.4.3.3.3.3.3.3.2.2.2.cmml" xref="S3.E4.m1.4.4.3.3.3.3.3.3.2.2.2">ℐ</ci></apply><ci id="S3.E4.m1.4.4.3.3.3.3.3.3.2.3.cmml" xref="S3.E4.m1.4.4.3.3.3.3.3.3.2.3">𝑖</ci></apply><ci id="S3.E4.m1.4.4.3.3.3.3.3.3.3.cmml" xref="S3.E4.m1.4.4.3.3.3.3.3.3.3">𝑣</ci></apply><apply id="S3.E4.m1.5.5.4.4.4.4.4.4.cmml" xref="S3.E4.m1.5.5.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.4.4.4.4.4.4.1.cmml" xref="S3.E4.m1.5.5.4.4.4.4.4.4">superscript</csymbol><apply id="S3.E4.m1.5.5.4.4.4.4.4.4.2.cmml" xref="S3.E4.m1.5.5.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.4.4.4.4.4.4.2.1.cmml" xref="S3.E4.m1.5.5.4.4.4.4.4.4">subscript</csymbol><ci id="S3.E4.m1.5.5.4.4.4.4.4.4.2.2.cmml" xref="S3.E4.m1.5.5.4.4.4.4.4.4.2.2">𝑛</ci><ci id="S3.E4.m1.5.5.4.4.4.4.4.4.2.3.cmml" xref="S3.E4.m1.5.5.4.4.4.4.4.4.2.3">𝑖</ci></apply><ci id="S3.E4.m1.5.5.4.4.4.4.4.4.3.cmml" xref="S3.E4.m1.5.5.4.4.4.4.4.4.3">𝑣</ci></apply><apply id="S3.E4.m1.6.6.5.5.5.5.5.5.cmml" xref="S3.E4.m1.6.6.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.5.5.5.5.5.5.1.cmml" xref="S3.E4.m1.6.6.5.5.5.5.5.5">subscript</csymbol><ci id="S3.E4.m1.6.6.5.5.5.5.5.5.2.cmml" xref="S3.E4.m1.6.6.5.5.5.5.5.5.2">𝑐</ci><ci id="S3.E4.m1.6.6.5.5.5.5.5.5.3.cmml" xref="S3.E4.m1.6.6.5.5.5.5.5.5.3">𝑇</ci></apply></vector></apply><apply id="S3.E4.m1.6.6.5.5.7.cmml" xref="S3.E4.m1.6.6.5.5.7"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.5.5.7.1.cmml" xref="S3.E4.m1.6.6.5.5.7">superscript</csymbol><apply id="S3.E4.m1.6.6.5.5.7.2.cmml" xref="S3.E4.m1.6.6.5.5.7.2"><ci id="S3.E4.m1.6.6.5.5.7.2.1.cmml" xref="S3.E4.m1.6.6.5.5.7.2.1">~</ci><ci id="S3.E4.m1.6.6.5.5.7.2.2.cmml" xref="S3.E4.m1.6.6.5.5.7.2.2">𝑀</ci></apply><ci id="S3.E4.m1.6.6.5.5.7.3.cmml" xref="S3.E4.m1.6.6.5.5.7.3">𝑣</ci></apply></apply><apply id="S3.E4.m1.7.7.6.6.cmml" xref="S3.E4.m1.7.7.6.6"><csymbol cd="latexml" id="S3.E4.m1.7.7.6.6.2.cmml" xref="S3.E4.m1.7.7.6.6.2">direct-product</csymbol><apply id="S3.E4.m1.7.7.6.6.3.cmml" xref="S3.E4.m1.7.7.6.6.3"><csymbol cd="ambiguous" id="S3.E4.m1.7.7.6.6.3.1.cmml" xref="S3.E4.m1.7.7.6.6.3">superscript</csymbol><apply id="S3.E4.m1.7.7.6.6.3.2.cmml" xref="S3.E4.m1.7.7.6.6.3"><csymbol cd="ambiguous" id="S3.E4.m1.7.7.6.6.3.2.1.cmml" xref="S3.E4.m1.7.7.6.6.3">subscript</csymbol><ci id="S3.E4.m1.7.7.6.6.3.2.2.cmml" xref="S3.E4.m1.7.7.6.6.3.2.2">ℐ</ci><cn id="S3.E4.m1.7.7.6.6.3.2.3.cmml" type="integer" xref="S3.E4.m1.7.7.6.6.3.2.3">0</cn></apply><ci id="S3.E4.m1.7.7.6.6.3.3.cmml" xref="S3.E4.m1.7.7.6.6.3.3">𝑣</ci></apply><apply id="S3.E4.m1.7.7.6.6.1.1.1.cmml" xref="S3.E4.m1.7.7.6.6.1.1"><minus id="S3.E4.m1.7.7.6.6.1.1.1.1.cmml" xref="S3.E4.m1.7.7.6.6.1.1.1.1"></minus><cn id="S3.E4.m1.7.7.6.6.1.1.1.2.cmml" type="integer" xref="S3.E4.m1.7.7.6.6.1.1.1.2">1</cn><apply id="S3.E4.m1.7.7.6.6.1.1.1.3.cmml" xref="S3.E4.m1.7.7.6.6.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.7.7.6.6.1.1.1.3.1.cmml" xref="S3.E4.m1.7.7.6.6.1.1.1.3">superscript</csymbol><apply id="S3.E4.m1.7.7.6.6.1.1.1.3.2.cmml" xref="S3.E4.m1.7.7.6.6.1.1.1.3.2"><ci id="S3.E4.m1.7.7.6.6.1.1.1.3.2.1.cmml" xref="S3.E4.m1.7.7.6.6.1.1.1.3.2.1">~</ci><ci id="S3.E4.m1.7.7.6.6.1.1.1.3.2.2.cmml" xref="S3.E4.m1.7.7.6.6.1.1.1.3.2.2">𝑀</ci></apply><ci id="S3.E4.m1.7.7.6.6.1.1.1.3.3.cmml" xref="S3.E4.m1.7.7.6.6.1.1.1.3.3">𝑣</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.7c">\mathcal{{{I}}}_{i+1}^{v}\leftarrow U_{\theta}\left(\mathcal{I}_{0}^{v},%
\mathcal{{\tilde{I}}}_{i}^{v},t;\mathcal{{\tilde{I}}}_{i}^{v},{n_{i}}^{v},c_{T%
}\right)\odot{\widetilde{M}^{v}}+\mathcal{I}_{0}^{v}\odot(1-{\widetilde{M}^{v}})</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.7d">caligraphic_I start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ← italic_U start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( caligraphic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT , over~ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT , italic_t ; over~ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT , italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT , italic_c start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ⊙ over~ start_ARG italic_M end_ARG start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT + caligraphic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ⊙ ( 1 - over~ start_ARG italic_M end_ARG start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p12">
<p class="ltx_p" id="S3.SS2.p12.6">where <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p12.1.m1.1"><semantics id="S3.SS2.p12.1.m1.1a"><mi id="S3.SS2.p12.1.m1.1.1" xref="S3.SS2.p12.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.1.m1.1b"><ci id="S3.SS2.p12.1.m1.1.1.cmml" xref="S3.SS2.p12.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p12.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p12.1.m1.1d">italic_t</annotation></semantics></math> is the noise level, randomly selected between <math alttext="[t_{min},t_{max}]" class="ltx_Math" display="inline" id="S3.SS2.p12.2.m2.2"><semantics id="S3.SS2.p12.2.m2.2a"><mrow id="S3.SS2.p12.2.m2.2.2.2" xref="S3.SS2.p12.2.m2.2.2.3.cmml"><mo id="S3.SS2.p12.2.m2.2.2.2.3" stretchy="false" xref="S3.SS2.p12.2.m2.2.2.3.cmml">[</mo><msub id="S3.SS2.p12.2.m2.1.1.1.1" xref="S3.SS2.p12.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.p12.2.m2.1.1.1.1.2" xref="S3.SS2.p12.2.m2.1.1.1.1.2.cmml">t</mi><mrow id="S3.SS2.p12.2.m2.1.1.1.1.3" xref="S3.SS2.p12.2.m2.1.1.1.1.3.cmml"><mi id="S3.SS2.p12.2.m2.1.1.1.1.3.2" xref="S3.SS2.p12.2.m2.1.1.1.1.3.2.cmml">m</mi><mo id="S3.SS2.p12.2.m2.1.1.1.1.3.1" xref="S3.SS2.p12.2.m2.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p12.2.m2.1.1.1.1.3.3" xref="S3.SS2.p12.2.m2.1.1.1.1.3.3.cmml">i</mi><mo id="S3.SS2.p12.2.m2.1.1.1.1.3.1a" xref="S3.SS2.p12.2.m2.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p12.2.m2.1.1.1.1.3.4" xref="S3.SS2.p12.2.m2.1.1.1.1.3.4.cmml">n</mi></mrow></msub><mo id="S3.SS2.p12.2.m2.2.2.2.4" xref="S3.SS2.p12.2.m2.2.2.3.cmml">,</mo><msub id="S3.SS2.p12.2.m2.2.2.2.2" xref="S3.SS2.p12.2.m2.2.2.2.2.cmml"><mi id="S3.SS2.p12.2.m2.2.2.2.2.2" xref="S3.SS2.p12.2.m2.2.2.2.2.2.cmml">t</mi><mrow id="S3.SS2.p12.2.m2.2.2.2.2.3" xref="S3.SS2.p12.2.m2.2.2.2.2.3.cmml"><mi id="S3.SS2.p12.2.m2.2.2.2.2.3.2" xref="S3.SS2.p12.2.m2.2.2.2.2.3.2.cmml">m</mi><mo id="S3.SS2.p12.2.m2.2.2.2.2.3.1" xref="S3.SS2.p12.2.m2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p12.2.m2.2.2.2.2.3.3" xref="S3.SS2.p12.2.m2.2.2.2.2.3.3.cmml">a</mi><mo id="S3.SS2.p12.2.m2.2.2.2.2.3.1a" xref="S3.SS2.p12.2.m2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p12.2.m2.2.2.2.2.3.4" xref="S3.SS2.p12.2.m2.2.2.2.2.3.4.cmml">x</mi></mrow></msub><mo id="S3.SS2.p12.2.m2.2.2.2.5" stretchy="false" xref="S3.SS2.p12.2.m2.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.2.m2.2b"><interval closure="closed" id="S3.SS2.p12.2.m2.2.2.3.cmml" xref="S3.SS2.p12.2.m2.2.2.2"><apply id="S3.SS2.p12.2.m2.1.1.1.1.cmml" xref="S3.SS2.p12.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p12.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p12.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p12.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p12.2.m2.1.1.1.1.2">𝑡</ci><apply id="S3.SS2.p12.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p12.2.m2.1.1.1.1.3"><times id="S3.SS2.p12.2.m2.1.1.1.1.3.1.cmml" xref="S3.SS2.p12.2.m2.1.1.1.1.3.1"></times><ci id="S3.SS2.p12.2.m2.1.1.1.1.3.2.cmml" xref="S3.SS2.p12.2.m2.1.1.1.1.3.2">𝑚</ci><ci id="S3.SS2.p12.2.m2.1.1.1.1.3.3.cmml" xref="S3.SS2.p12.2.m2.1.1.1.1.3.3">𝑖</ci><ci id="S3.SS2.p12.2.m2.1.1.1.1.3.4.cmml" xref="S3.SS2.p12.2.m2.1.1.1.1.3.4">𝑛</ci></apply></apply><apply id="S3.SS2.p12.2.m2.2.2.2.2.cmml" xref="S3.SS2.p12.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p12.2.m2.2.2.2.2.1.cmml" xref="S3.SS2.p12.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p12.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.p12.2.m2.2.2.2.2.2">𝑡</ci><apply id="S3.SS2.p12.2.m2.2.2.2.2.3.cmml" xref="S3.SS2.p12.2.m2.2.2.2.2.3"><times id="S3.SS2.p12.2.m2.2.2.2.2.3.1.cmml" xref="S3.SS2.p12.2.m2.2.2.2.2.3.1"></times><ci id="S3.SS2.p12.2.m2.2.2.2.2.3.2.cmml" xref="S3.SS2.p12.2.m2.2.2.2.2.3.2">𝑚</ci><ci id="S3.SS2.p12.2.m2.2.2.2.2.3.3.cmml" xref="S3.SS2.p12.2.m2.2.2.2.2.3.3">𝑎</ci><ci id="S3.SS2.p12.2.m2.2.2.2.2.3.4.cmml" xref="S3.SS2.p12.2.m2.2.2.2.2.3.4">𝑥</ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p12.2.m2.2c">[t_{min},t_{max}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p12.2.m2.2d">[ italic_t start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ]</annotation></semantics></math> and <math alttext="U_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p12.3.m3.1"><semantics id="S3.SS2.p12.3.m3.1a"><msub id="S3.SS2.p12.3.m3.1.1" xref="S3.SS2.p12.3.m3.1.1.cmml"><mi id="S3.SS2.p12.3.m3.1.1.2" xref="S3.SS2.p12.3.m3.1.1.2.cmml">U</mi><mi id="S3.SS2.p12.3.m3.1.1.3" xref="S3.SS2.p12.3.m3.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.3.m3.1b"><apply id="S3.SS2.p12.3.m3.1.1.cmml" xref="S3.SS2.p12.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p12.3.m3.1.1.1.cmml" xref="S3.SS2.p12.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p12.3.m3.1.1.2.cmml" xref="S3.SS2.p12.3.m3.1.1.2">𝑈</ci><ci id="S3.SS2.p12.3.m3.1.1.3.cmml" xref="S3.SS2.p12.3.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p12.3.m3.1c">U_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p12.3.m3.1d">italic_U start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is the DDIM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib26" title="">26</a>]</cite> sampling process with a set number of intermediate steps <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.p12.4.m4.1"><semantics id="S3.SS2.p12.4.m4.1a"><mi id="S3.SS2.p12.4.m4.1.1" xref="S3.SS2.p12.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.4.m4.1b"><ci id="S3.SS2.p12.4.m4.1.1.cmml" xref="S3.SS2.p12.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p12.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p12.4.m4.1d">italic_s</annotation></semantics></math> between the initial timestep <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p12.5.m5.1"><semantics id="S3.SS2.p12.5.m5.1a"><mi id="S3.SS2.p12.5.m5.1.1" xref="S3.SS2.p12.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.5.m5.1b"><ci id="S3.SS2.p12.5.m5.1.1.cmml" xref="S3.SS2.p12.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p12.5.m5.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p12.5.m5.1d">italic_t</annotation></semantics></math> and <math alttext="0" class="ltx_Math" display="inline" id="S3.SS2.p12.6.m6.1"><semantics id="S3.SS2.p12.6.m6.1a"><mn id="S3.SS2.p12.6.m6.1.1" xref="S3.SS2.p12.6.m6.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.6.m6.1b"><cn id="S3.SS2.p12.6.m6.1.1.cmml" type="integer" xref="S3.SS2.p12.6.m6.1.1">0</cn></annotation-xml></semantics></math>. As in NeRFs, this approach ensures consistent mesh transformations, providing desired modifications while preserving the mesh’s structural integrity.</p>
</div>
<div class="ltx_para" id="S3.SS2.p13">
<p class="ltx_p" id="S3.SS2.p13.1">To localize our editing, we use <math alttext="\mathcal{\widetilde{M}}" class="ltx_Math" display="inline" id="S3.SS2.p13.1.m1.1"><semantics id="S3.SS2.p13.1.m1.1a"><mover accent="true" id="S3.SS2.p13.1.m1.1.1" xref="S3.SS2.p13.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p13.1.m1.1.1.2" xref="S3.SS2.p13.1.m1.1.1.2.cmml">ℳ</mi><mo id="S3.SS2.p13.1.m1.1.1.1" xref="S3.SS2.p13.1.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p13.1.m1.1b"><apply id="S3.SS2.p13.1.m1.1.1.cmml" xref="S3.SS2.p13.1.m1.1.1"><ci id="S3.SS2.p13.1.m1.1.1.1.cmml" xref="S3.SS2.p13.1.m1.1.1.1">~</ci><ci id="S3.SS2.p13.1.m1.1.1.2.cmml" xref="S3.SS2.p13.1.m1.1.1.2">ℳ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p13.1.m1.1c">\mathcal{\widetilde{M}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p13.1.m1.1d">over~ start_ARG caligraphic_M end_ARG</annotation></semantics></math> to mask the important regions of the generated images. By overlaying the masked generated images onto the original input images, we ensure that modifications are consistently applied only to the relevant areas during the dataset update process.</p>
</div>
<div class="ltx_para" id="S3.SS2.p14">
<p class="ltx_p" id="S3.SS2.p14.1">Unlike NDS, which focuses on all the vertices of the mesh, we only optimize editing subset <math alttext="\mathcal{\widetilde{V}}" class="ltx_Math" display="inline" id="S3.SS2.p14.1.m1.1"><semantics id="S3.SS2.p14.1.m1.1a"><mover accent="true" id="S3.SS2.p14.1.m1.1.1" xref="S3.SS2.p14.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p14.1.m1.1.1.2" xref="S3.SS2.p14.1.m1.1.1.2.cmml">𝒱</mi><mo id="S3.SS2.p14.1.m1.1.1.1" xref="S3.SS2.p14.1.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p14.1.m1.1b"><apply id="S3.SS2.p14.1.m1.1.1.cmml" xref="S3.SS2.p14.1.m1.1.1"><ci id="S3.SS2.p14.1.m1.1.1.1.cmml" xref="S3.SS2.p14.1.m1.1.1.1">~</ci><ci id="S3.SS2.p14.1.m1.1.1.2.cmml" xref="S3.SS2.p14.1.m1.1.1.2">𝒱</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p14.1.m1.1c">\mathcal{\widetilde{V}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p14.1.m1.1d">over~ start_ARG caligraphic_V end_ARG</annotation></semantics></math>. This approach allows the mesh to focus on editing only the parts that are relevant to the text input, avoiding unnecessary changes and extra computation. By updating only the relevant vertices, we maintain the overall structure and integrity of the original mesh, resulting in more contextually appropriate edits in geometry.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="582" id="S3.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.6.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.7.2" style="font-size:90%;">Editing results on the DTU dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib11" title="">11</a>]</cite>. <span class="ltx_text" id="S3.F5.7.2.1" style="color:#0000FF;">Blue boxes</span> represent the initial mesh and shader reconstructed by neural deferred shading <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib29" title="">29</a>]</cite>, providing a baseline. <span class="ltx_text" id="S3.F5.7.2.2" style="color:#FF8000;">Orange boxes</span> show the edited mesh results from TextDeformer while <span class="ltx_text" id="S3.F5.7.2.3" style="color:#FFFF00;">yellow boxes</span> represent the edited views from Instruct-NeRFNeRF. <span class="ltx_text" id="S3.F5.7.2.4" style="color:#800080;">Violet boxes</span> represent renderings from GaussianEditor and their meshes extracted by SuGaR. LEMON achieves great results in both rendering and polygonal mesh quality.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate our method both qualitatively and quantitatively on nine objects from the DTU multi-view dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib11" title="">11</a>]</cite>, using materials from earlier works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib30" title="">30</a>]</cite>, based on three text prompts. For our method, we first reconstruct the object by following the approach in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib29" title="">29</a>]</cite>, then proceed with editing. Since there isn’t any specific mesh editing method with a shader for direct comparison, we have selected a mesh deformation and novel view synthesis techniques for comparison. For rendering, we selected the Instruct-NeRF2NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib10" title="">10</a>]</cite> model from Nerfstudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib27" title="">27</a>]</cite> and GSEditor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib5" title="">5</a>]</cite>. Since NeRF and Gaussian Splatting are designed for novel view synthesis rather than surface reconstruction, they are less suitable for mesh-based comparisons. However, recent works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib9" title="">9</a>]</cite> show that high-quality meshes can be extracted from Gaussian Splatting. Therefore, we compared the renderings of Instruct-NeRF2NeRF and GSEditor, as well as the meshes of GSEditor generated by SuGaR, with our neural shader and mesh results. To qualitatively evaluate our meshes, we also used another mesh deformation-based editing method, TextDeformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib8" title="">8</a>]</cite>, by applying an adjusted prompt to our initial mesh.
We also evaluated our method on the ShapeNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib4" title="">4</a>]</cite> to test its ability to transform everyday objects in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4.F7" title="Figure 7 ‣ 4.2 Qualitative Results ‣ 4 Experiments ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">7</span></a>. We provide our Shapenet results and further qualitative comparison of models in DTU dataset in our supplementary materials.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We conduct our experiments building on the NDS pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib29" title="">29</a>]</cite> and use differentiable rendering pipeline by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib14" title="">14</a>]</cite> on PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib21" title="">21</a>]</cite>. For initial mesh reconstruction and subsequent optimization, we follow the hyperparameters from the NDS and utilize their loss functions to optimize our editing process.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.5">The diffusion model’s effectiveness and the consistency of its updates depend on several hyperparameters. For the initial noise calculation we set our latent weight <math alttext="\lambda=0.5" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">λ</mi><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><eq id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></eq><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">𝜆</ci><cn id="S4.SS1.p2.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.p2.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\lambda=0.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">italic_λ = 0.5</annotation></semantics></math> and <math alttext="[t_{min},t_{max}]=[0,02,0.98]" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.5"><semantics id="S4.SS1.p2.2.m2.5a"><mrow id="S4.SS1.p2.2.m2.5.5" xref="S4.SS1.p2.2.m2.5.5.cmml"><mrow id="S4.SS1.p2.2.m2.5.5.2.2" xref="S4.SS1.p2.2.m2.5.5.2.3.cmml"><mo id="S4.SS1.p2.2.m2.5.5.2.2.3" stretchy="false" xref="S4.SS1.p2.2.m2.5.5.2.3.cmml">[</mo><msub id="S4.SS1.p2.2.m2.4.4.1.1.1" xref="S4.SS1.p2.2.m2.4.4.1.1.1.cmml"><mi id="S4.SS1.p2.2.m2.4.4.1.1.1.2" xref="S4.SS1.p2.2.m2.4.4.1.1.1.2.cmml">t</mi><mrow id="S4.SS1.p2.2.m2.4.4.1.1.1.3" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3.cmml"><mi id="S4.SS1.p2.2.m2.4.4.1.1.1.3.2" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3.2.cmml">m</mi><mo id="S4.SS1.p2.2.m2.4.4.1.1.1.3.1" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.2.m2.4.4.1.1.1.3.3" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3.3.cmml">i</mi><mo id="S4.SS1.p2.2.m2.4.4.1.1.1.3.1a" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.2.m2.4.4.1.1.1.3.4" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3.4.cmml">n</mi></mrow></msub><mo id="S4.SS1.p2.2.m2.5.5.2.2.4" xref="S4.SS1.p2.2.m2.5.5.2.3.cmml">,</mo><msub id="S4.SS1.p2.2.m2.5.5.2.2.2" xref="S4.SS1.p2.2.m2.5.5.2.2.2.cmml"><mi id="S4.SS1.p2.2.m2.5.5.2.2.2.2" xref="S4.SS1.p2.2.m2.5.5.2.2.2.2.cmml">t</mi><mrow id="S4.SS1.p2.2.m2.5.5.2.2.2.3" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3.cmml"><mi id="S4.SS1.p2.2.m2.5.5.2.2.2.3.2" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3.2.cmml">m</mi><mo id="S4.SS1.p2.2.m2.5.5.2.2.2.3.1" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.2.m2.5.5.2.2.2.3.3" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3.3.cmml">a</mi><mo id="S4.SS1.p2.2.m2.5.5.2.2.2.3.1a" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.2.m2.5.5.2.2.2.3.4" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3.4.cmml">x</mi></mrow></msub><mo id="S4.SS1.p2.2.m2.5.5.2.2.5" stretchy="false" xref="S4.SS1.p2.2.m2.5.5.2.3.cmml">]</mo></mrow><mo id="S4.SS1.p2.2.m2.5.5.3" xref="S4.SS1.p2.2.m2.5.5.3.cmml">=</mo><mrow id="S4.SS1.p2.2.m2.5.5.4.2" xref="S4.SS1.p2.2.m2.5.5.4.1.cmml"><mo id="S4.SS1.p2.2.m2.5.5.4.2.1" stretchy="false" xref="S4.SS1.p2.2.m2.5.5.4.1.cmml">[</mo><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">0</mn><mo id="S4.SS1.p2.2.m2.5.5.4.2.2" xref="S4.SS1.p2.2.m2.5.5.4.1.cmml">,</mo><mn id="S4.SS1.p2.2.m2.2.2" xref="S4.SS1.p2.2.m2.2.2.cmml">02</mn><mo id="S4.SS1.p2.2.m2.5.5.4.2.3" xref="S4.SS1.p2.2.m2.5.5.4.1.cmml">,</mo><mn id="S4.SS1.p2.2.m2.3.3" xref="S4.SS1.p2.2.m2.3.3.cmml">0.98</mn><mo id="S4.SS1.p2.2.m2.5.5.4.2.4" stretchy="false" xref="S4.SS1.p2.2.m2.5.5.4.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.5b"><apply id="S4.SS1.p2.2.m2.5.5.cmml" xref="S4.SS1.p2.2.m2.5.5"><eq id="S4.SS1.p2.2.m2.5.5.3.cmml" xref="S4.SS1.p2.2.m2.5.5.3"></eq><interval closure="closed" id="S4.SS1.p2.2.m2.5.5.2.3.cmml" xref="S4.SS1.p2.2.m2.5.5.2.2"><apply id="S4.SS1.p2.2.m2.4.4.1.1.1.cmml" xref="S4.SS1.p2.2.m2.4.4.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.4.4.1.1.1.1.cmml" xref="S4.SS1.p2.2.m2.4.4.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.2.m2.4.4.1.1.1.2.cmml" xref="S4.SS1.p2.2.m2.4.4.1.1.1.2">𝑡</ci><apply id="S4.SS1.p2.2.m2.4.4.1.1.1.3.cmml" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3"><times id="S4.SS1.p2.2.m2.4.4.1.1.1.3.1.cmml" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3.1"></times><ci id="S4.SS1.p2.2.m2.4.4.1.1.1.3.2.cmml" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3.2">𝑚</ci><ci id="S4.SS1.p2.2.m2.4.4.1.1.1.3.3.cmml" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3.3">𝑖</ci><ci id="S4.SS1.p2.2.m2.4.4.1.1.1.3.4.cmml" xref="S4.SS1.p2.2.m2.4.4.1.1.1.3.4">𝑛</ci></apply></apply><apply id="S4.SS1.p2.2.m2.5.5.2.2.2.cmml" xref="S4.SS1.p2.2.m2.5.5.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.5.5.2.2.2.1.cmml" xref="S4.SS1.p2.2.m2.5.5.2.2.2">subscript</csymbol><ci id="S4.SS1.p2.2.m2.5.5.2.2.2.2.cmml" xref="S4.SS1.p2.2.m2.5.5.2.2.2.2">𝑡</ci><apply id="S4.SS1.p2.2.m2.5.5.2.2.2.3.cmml" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3"><times id="S4.SS1.p2.2.m2.5.5.2.2.2.3.1.cmml" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3.1"></times><ci id="S4.SS1.p2.2.m2.5.5.2.2.2.3.2.cmml" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3.2">𝑚</ci><ci id="S4.SS1.p2.2.m2.5.5.2.2.2.3.3.cmml" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3.3">𝑎</ci><ci id="S4.SS1.p2.2.m2.5.5.2.2.2.3.4.cmml" xref="S4.SS1.p2.2.m2.5.5.2.2.2.3.4">𝑥</ci></apply></apply></interval><list id="S4.SS1.p2.2.m2.5.5.4.1.cmml" xref="S4.SS1.p2.2.m2.5.5.4.2"><cn id="S4.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1">0</cn><cn id="S4.SS1.p2.2.m2.2.2.cmml" type="integer" xref="S4.SS1.p2.2.m2.2.2">02</cn><cn id="S4.SS1.p2.2.m2.3.3.cmml" type="float" xref="S4.SS1.p2.2.m2.3.3">0.98</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.5c">[t_{min},t_{max}]=[0,02,0.98]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.5d">[ italic_t start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ] = [ 0 , 02 , 0.98 ]</annotation></semantics></math>. Our denoising process is always done in 10 steps. The guidance scale for the text prompt is set at <math alttext="s_{T}=7.5" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><msub id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2.2" xref="S4.SS1.p2.3.m3.1.1.2.2.cmml">s</mi><mi id="S4.SS1.p2.3.m3.1.1.2.3" xref="S4.SS1.p2.3.m3.1.1.2.3.cmml">T</mi></msub><mo id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">7.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><eq id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1"></eq><apply id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.2.1.cmml" xref="S4.SS1.p2.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2.2">𝑠</ci><ci id="S4.SS1.p2.3.m3.1.1.2.3.cmml" xref="S4.SS1.p2.3.m3.1.1.2.3">𝑇</ci></apply><cn id="S4.SS1.p2.3.m3.1.1.3.cmml" type="float" xref="S4.SS1.p2.3.m3.1.1.3">7.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">s_{T}=7.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">italic_s start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = 7.5</annotation></semantics></math>, while the conditioning scales for normal maps and rendered images are <math alttext="s_{I}=0.8" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.1"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><msub id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml"><mi id="S4.SS1.p2.4.m4.1.1.2.2" xref="S4.SS1.p2.4.m4.1.1.2.2.cmml">s</mi><mi id="S4.SS1.p2.4.m4.1.1.2.3" xref="S4.SS1.p2.4.m4.1.1.2.3.cmml">I</mi></msub><mo id="S4.SS1.p2.4.m4.1.1.1" xref="S4.SS1.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml">0.8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><eq id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1.1"></eq><apply id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.1.1.2.1.cmml" xref="S4.SS1.p2.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.4.m4.1.1.2.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2.2">𝑠</ci><ci id="S4.SS1.p2.4.m4.1.1.2.3.cmml" xref="S4.SS1.p2.4.m4.1.1.2.3">𝐼</ci></apply><cn id="S4.SS1.p2.4.m4.1.1.3.cmml" type="float" xref="S4.SS1.p2.4.m4.1.1.3">0.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">s_{I}=0.8</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.1d">italic_s start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT = 0.8</annotation></semantics></math> and <math alttext="s_{n}=0.2" class="ltx_Math" display="inline" id="S4.SS1.p2.5.m5.1"><semantics id="S4.SS1.p2.5.m5.1a"><mrow id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml"><msub id="S4.SS1.p2.5.m5.1.1.2" xref="S4.SS1.p2.5.m5.1.1.2.cmml"><mi id="S4.SS1.p2.5.m5.1.1.2.2" xref="S4.SS1.p2.5.m5.1.1.2.2.cmml">s</mi><mi id="S4.SS1.p2.5.m5.1.1.2.3" xref="S4.SS1.p2.5.m5.1.1.2.3.cmml">n</mi></msub><mo id="S4.SS1.p2.5.m5.1.1.1" xref="S4.SS1.p2.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.5.m5.1.1.3" xref="S4.SS1.p2.5.m5.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><apply id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"><eq id="S4.SS1.p2.5.m5.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1.1"></eq><apply id="S4.SS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.1.1.2.1.cmml" xref="S4.SS1.p2.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.5.m5.1.1.2.2.cmml" xref="S4.SS1.p2.5.m5.1.1.2.2">𝑠</ci><ci id="S4.SS1.p2.5.m5.1.1.2.3.cmml" xref="S4.SS1.p2.5.m5.1.1.2.3">𝑛</ci></apply><cn id="S4.SS1.p2.5.m5.1.1.3.cmml" type="float" xref="S4.SS1.p2.5.m5.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">s_{n}=0.2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.5.m5.1d">italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = 0.2</annotation></semantics></math>, respectively. These parameters determine the influence of each component in the denoising process. We generate a image and update our dataset with the modified image every 10 iterations. Most of the results shown in the paper follow these initial parameters, changing these parameters can add variation to the result. We train our method for a maximum of 8k iterations which takes around 15 minutes on a single NVIDIA A40. Further training could result in additional shape alteration and the inclusion of extra features.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S4.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">Qualitative consistency results.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Qualitative Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S3.F5" title="Figure 5 ‣ 3.2 LEMON ‣ 3 Method ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">5</span></a>, we present our main comparison on the DTU dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib11" title="">11</a>]</cite>. Each row represents a specific editing case with its corresponding text prompt listed below, while each column shows the output from a different method. In the case of mesh editing, our method retains the original mesh’s geometric features while incorporating new refined details based on the text prompt. While TextDeformer achieves decent results it struggles to preserve the original structure, often leading to distortions like the horizontal compression of the skull shown in the first row. Even in seemingly successful cases like the second row, TextDeformer tends to overedit the mesh, whereas our method maintains the basic structure and adds specific details in line with the prompt.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="207" id="S4.F7.g1" src="x7.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">ShapeNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib4" title="">4</a>]</cite> transformation results. Diagonals corresponds to initial mesh and rendering of the ShapeNet object.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">In the case of rendering, while Instruct-NeRF2NeRF achieves decent rendering results for simple edits like ”Put in plate armor,” it may fall short when dealing with editing a geometrically complex figure like the skull in the first row. Even in its most successful outcomes, Instruct-NeRF2NeRF seems to ”color” the object rather than adding new geometrical features. GaussianEditor produces high-quality renderings of the edited object, however, it sometimes fails to generate the requested edits and shows less variation in the results. Unlike other two methods, our method maintains a concurrent relationship between mesh deformation and neural shader. This allows LEMON to adapt to significant structural changes while preserving the geometrical and shading characteristics of the object.As a result, our rendered images can achieve natural reflective effects, as seen in the first and second rows, demonstrating LEMON’s effectiveness in both rendering and mesh editing.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">We also compare our shader’s consistency with other rendering methods in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4.F6" title="Figure 6 ‣ 4.1 Implementation Details ‣ 4 Experiments ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">6</span></a>. As seen in the figure, other methods tend to show more inconsistency when there is a drift in camera motion. Although our shader may not produce the highest quality renderings, it is more consistent than the other methods. We believe this is because the mesh and neural shader optimization processes are intertwined, providing geometric consistency to each other.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.11" style="width:216.8pt;height:58.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-125.2pt,33.8pt) scale(0.464111897504374,0.464111897504374) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.11.11">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.3.3.3.4">Method</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1">Time <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.2.2">Memory <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.2.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.3.3.3">CLIP Similarity <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.11.11.12.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.11.11.12.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.11.11.12.1.2">(Mins.)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.11.11.12.1.3">(Peak GBs)</td>
<td class="ltx_td ltx_border_r" id="S4.T1.11.11.12.1.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S4.T1.6.6.6.4">Instruct-NeRF2NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib10" title="">10</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.4.1">
<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T1.4.4.4.1.m1.1"><semantics id="S4.T1.4.4.4.1.m1.1a"><mo id="S4.T1.4.4.4.1.m1.1.1" xref="S4.T1.4.4.4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.1.m1.1b"><csymbol cd="latexml" id="S4.T1.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.4.1.m1.1d">∼</annotation></semantics></math>42</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.5.5.5.2"><math alttext="10.7" class="ltx_Math" display="inline" id="S4.T1.5.5.5.2.m1.1"><semantics id="S4.T1.5.5.5.2.m1.1a"><mn id="S4.T1.5.5.5.2.m1.1.1" xref="S4.T1.5.5.5.2.m1.1.1.cmml">10.7</mn><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.2.m1.1b"><cn id="S4.T1.5.5.5.2.m1.1.1.cmml" type="float" xref="S4.T1.5.5.5.2.m1.1.1">10.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.2.m1.1c">10.7</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.5.2.m1.1d">10.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.6.6.6.3"><math alttext="0.1118" class="ltx_Math" display="inline" id="S4.T1.6.6.6.3.m1.1"><semantics id="S4.T1.6.6.6.3.m1.1a"><mn id="S4.T1.6.6.6.3.m1.1.1" xref="S4.T1.6.6.6.3.m1.1.1.cmml">0.1118</mn><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.3.m1.1b"><cn id="S4.T1.6.6.6.3.m1.1.1.cmml" type="float" xref="S4.T1.6.6.6.3.m1.1.1">0.1118</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.3.m1.1c">0.1118</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.6.3.m1.1d">0.1118</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.7.7.7.2">+Poisson Reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib12" title="">12</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.7.7.1">
<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T1.7.7.7.1.m1.1"><semantics id="S4.T1.7.7.7.1.m1.1a"><mo id="S4.T1.7.7.7.1.m1.1.1" xref="S4.T1.7.7.7.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T1.7.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.7.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.7.7.1.m1.1d">∼</annotation></semantics></math> 3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.7.7.3">6.0</td>
<td class="ltx_td ltx_border_r" id="S4.T1.7.7.7.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.9.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.9.9.9.3">GaussianEditor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib5" title="">5</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.8.8.8.1">
<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T1.8.8.8.1.m1.1"><semantics id="S4.T1.8.8.8.1.m1.1a"><mo id="S4.T1.8.8.8.1.m1.1.1" xref="S4.T1.8.8.8.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.1.m1.1b"><csymbol cd="latexml" id="S4.T1.8.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.8.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.8.8.1.m1.1d">∼</annotation></semantics></math>10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.9.9.9.4">9.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.9.9.9.2"><math alttext="0.1262" class="ltx_Math" display="inline" id="S4.T1.9.9.9.2.m1.1"><semantics id="S4.T1.9.9.9.2.m1.1a"><mn id="S4.T1.9.9.9.2.m1.1.1" xref="S4.T1.9.9.9.2.m1.1.1.cmml">0.1262</mn><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.2.m1.1b"><cn id="S4.T1.9.9.9.2.m1.1.1.cmml" type="float" xref="S4.T1.9.9.9.2.m1.1.1">0.1262</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.2.m1.1c">0.1262</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.9.9.2.m1.1d">0.1262</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.10.10.10.2">+SuGaR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib9" title="">9</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.10.1">
<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T1.10.10.10.1.m1.1"><semantics id="S4.T1.10.10.10.1.m1.1a"><mo id="S4.T1.10.10.10.1.m1.1.1" xref="S4.T1.10.10.10.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.1.m1.1b"><csymbol cd="latexml" id="S4.T1.10.10.10.1.m1.1.1.cmml" xref="S4.T1.10.10.10.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.10.10.1.m1.1d">∼</annotation></semantics></math>45</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.10.3">6.5</td>
<td class="ltx_td ltx_border_r" id="S4.T1.10.10.10.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.11.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.11.11.11.2">LEMON(Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.11.11.11.1">
<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T1.11.11.11.1.m1.1"><semantics id="S4.T1.11.11.11.1.m1.1a"><mo id="S4.T1.11.11.11.1.m1.1.1" xref="S4.T1.11.11.11.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.11.1.m1.1b"><csymbol cd="latexml" id="S4.T1.11.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.11.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.11.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.11.11.1.m1.1d">∼</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T1.11.11.11.1.1">15</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.11.11.11.3"><span class="ltx_text ltx_font_bold" id="S4.T1.11.11.11.3.1">6.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.11.11.11.4"><span class="ltx_text ltx_font_bold" id="S4.T1.11.11.11.4.1">0.2044</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.13.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.14.2" style="font-size:90%;">Quantitative results of our method.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Quantitative Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Since editing is a subjective task we rely on our qualitative evaluation more. However, we also apply CLIP Directional Similarity, introduced in StyleGAN-Nada <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib7" title="">7</a>]</cite> to measure the cosine similarity between the distance between pairs of images and the distance between pairs of captions accompanying the images. We evaluate on all views of the object dataset We show our result in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4.T1" title="Table 1 ‣ 4.2 Qualitative Results ‣ 4 Experiments ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">1</span></a>, along with the time spent and the GPU memory consumption during the editing process.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4.T1" title="Table 1 ‣ 4.2 Qualitative Results ‣ 4 Experiments ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">1</span></a>, we see that our method outperforms Instruct-NeRF2NeRF and GaussianEditor in CLIP Directional Similarity. The higher CLIP Directional Similarity score indicates that our method more accurately follows the edit requests in the prompt. Since our pipeline simultaneously extracts the mesh and neural shader, we also consider the memory and time consumed in mesh extraction for the other methods. We outperform Instruct-NeRF2NeRF even without considering the mesh extraction time. Although GaussianEditor is faster, extracting meshes with SuGar takes considerably more time. Therefore, we believe our method strikes a good balance, achieving mesh and view synthesis together faster than all the other methods.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="498" id="S4.F8.g1" src="x8.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.3.2" style="font-size:90%;">Ablation studies of the masking.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Studies</h3>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="246" id="S4.F9.g1" src="x9.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S4.F9.3.2" style="font-size:90%;">Ablation studies of impact of text-to-image diffusion models on the mesh and neural shader.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.1">Diffusion Model:</span> The diffusion model plays a key role in our editing process by generating our modifications. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4.F9" title="Figure 9 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">9</span></a>, while Instruct-Pix2Pix introduces variation to the meshes, it also loses key details, such as the line on the skull. In contrast, ControlNet’s normal model preserves the geometric details of the shape but provides less variety, making fewer changes. To achieve the best of both worlds, we connect the two models through a Multi-ControlNet pipeline.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Vertice and Image Masking:</span> Vertex and image masking processes are the important contributions to our localized mesh editing. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S4.F8" title="Figure 8 ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">8</span></a>, we present a qualitative comparison of our masks on the owl example. Our vertex masking allows us to avoid unnecessary topological changes in the mesh, while image masking during the editing process avoids reduntant coloring preserving original shading of the input in the process.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Even though our method is effective for editing meshes, it inherits many of the limitations associated with the methods that we used. Neural deferred shading relies on object masks to reconstruct the object, with appearance loss functioning only on the masked regions. This restricts the diffusion model, making it more challenging to add new objects to the mesh. We believe this issue could be addressed by incorporating inpainting, similar to the approach used in GSEditor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The authors acknowledge the financial support by the Bavarian Ministry of Economic Affairs, Regional Development and Energy as part of the project 6G Future Lab Bavaria.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aigerman et al. [2022]</span>
<span class="ltx_bibblock">
Noam Aigerman, Kunal Gupta, Vladimir G Kim, Siddhartha Chaudhuri, Jun Saito, and Thibault Groueix.

</span>
<span class="ltx_bibblock">Neural jacobian fields: Learning intrinsic mappings of arbitrary meshes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2205.02904</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Botsch and Kobbelt [2004]</span>
<span class="ltx_bibblock">
Mario Botsch and Leif Kobbelt.

</span>
<span class="ltx_bibblock">A remeshing approach to multiresolution modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing</em>, pages 185–192, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brooks et al. [2023]</span>
<span class="ltx_bibblock">
Tim Brooks, Aleksander Holynski, and Alexei A Efros.

</span>
<span class="ltx_bibblock">Instructpix2pix: Learning to follow image editing instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 18392–18402, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. [2015]</span>
<span class="ltx_bibblock">
Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu.

</span>
<span class="ltx_bibblock">ShapeNet: An Information-Rich 3D Model Repository.

</span>
<span class="ltx_bibblock">Technical Report arXiv:1512.03012 [cs.GR], Stanford University — Princeton University — Toyota Technological Institute at Chicago, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2024]</span>
<span class="ltx_bibblock">
Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin.

</span>
<span class="ltx_bibblock">Gaussianeditor: Swift and controllable 3d editing with gaussian splatting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 21476–21485, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Croitoru et al. [2023]</span>
<span class="ltx_bibblock">
Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah.

</span>
<span class="ltx_bibblock">Diffusion models in vision: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gal et al. [2021]</span>
<span class="ltx_bibblock">
Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">Stylegan-nada: Clip-guided domain adaptation of image generators, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2023]</span>
<span class="ltx_bibblock">
William Gao, Noam Aigerman, Thibault Groueix, Vova Kim, and Rana Hanocka.

</span>
<span class="ltx_bibblock">Textdeformer: Geometry manipulation using text guidance.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ACM SIGGRAPH 2023 Conference Proceedings</em>, pages 1–11, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guédon and Lepetit [2024]</span>
<span class="ltx_bibblock">
Antoine Guédon and Vincent Lepetit.

</span>
<span class="ltx_bibblock">Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 5354–5363, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haque et al. [2023]</span>
<span class="ltx_bibblock">
Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, and Angjoo Kanazawa.

</span>
<span class="ltx_bibblock">Instruct-nerf2nerf: Editing 3d scenes with instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 19740–19750, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jensen et al. [2014]</span>
<span class="ltx_bibblock">
Rasmus Jensen, Anders Dahl, George Vogiatzis, Engil Tola, and Henrik Aanæs.

</span>
<span class="ltx_bibblock">Large scale multi-view stereopsis evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2014 IEEE Conference on Computer Vision and Pattern Recognition</em>, pages 406–413. IEEE, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazhdan et al. [2006]</span>
<span class="ltx_bibblock">
Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.

</span>
<span class="ltx_bibblock">Poisson surface reconstruction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the fourth Eurographics symposium on Geometry processing</em>, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kerbl et al. [2023]</span>
<span class="ltx_bibblock">
Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis.

</span>
<span class="ltx_bibblock">3d gaussian splatting for real-time radiance field rendering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">ACM Transactions on Graphics</em>, 42(4):1–14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laine et al. [2020]</span>
<span class="ltx_bibblock">
Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila.

</span>
<span class="ltx_bibblock">Modular primitives for high-performance differentiable rendering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">ACM Transactions on Graphics (ToG)</em>, 39(6):1–14, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurentini [1994]</span>
<span class="ltx_bibblock">
Aldo Laurentini.

</span>
<span class="ltx_bibblock">The visual hull concept for silhouette-based image understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">IEEE Transactions on pattern analysis and machine intelligence</em>, 16(2):150–162, 1994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2023]</span>
<span class="ltx_bibblock">
Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin.

</span>
<span class="ltx_bibblock">Magic3d: High-resolution text-to-3d content creation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 300–309, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lüddecke and Ecker [2022]</span>
<span class="ltx_bibblock">
Timo Lüddecke and Alexander Ecker.

</span>
<span class="ltx_bibblock">Image segmentation using text and image prompts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 7086–7096, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. [2021]</span>
<span class="ltx_bibblock">
Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.

</span>
<span class="ltx_bibblock">Sdedit: Guided image synthesis and editing with stochastic differential equations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2108.01073</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mildenhall et al. [2021]</span>
<span class="ltx_bibblock">
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.

</span>
<span class="ltx_bibblock">Nerf: Representing scenes as neural radiance fields for view synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Communications of the ACM</em>, 65(1):99–106, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohammad Khalid et al. [2022]</span>
<span class="ltx_bibblock">
Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa.

</span>
<span class="ltx_bibblock">Clip-mesh: Generating textured meshes from text using pretrained image-text models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">SIGGRAPH Asia 2022 conference papers</em>, pages 1–8, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. [2019]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poole et al. [2022]</span>
<span class="ltx_bibblock">
Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.

</span>
<span class="ltx_bibblock">Dreamfusion: Text-to-3d using 2d diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2209.14988</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">International conference on machine learning</em>, pages 8748–8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. [2022]</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 10684–10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al. [2015]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</em>, pages 234–241. Springer, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2020]</span>
<span class="ltx_bibblock">
Jiaming Song, Chenlin Meng, and Stefano Ermon.

</span>
<span class="ltx_bibblock">Denoising diffusion implicit models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2010.02502</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tancik et al. [2023]</span>
<span class="ltx_bibblock">
Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, et al.

</span>
<span class="ltx_bibblock">Nerfstudio: A modular framework for neural radiance field development.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">ACM SIGGRAPH 2023 Conference Proceedings</em>, pages 1–12, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tewari et al. [2022]</span>
<span class="ltx_bibblock">
Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Wang Yifan, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al.

</span>
<span class="ltx_bibblock">Advances in neural rendering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Computer Graphics Forum</em>, pages 703–735. Wiley Online Library, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Worchel et al. [2022]</span>
<span class="ltx_bibblock">
Markus Worchel, Rodrigo Diaz, Weiwen Hu, Oliver Schreer, Ingo Feldmann, and Peter Eisert.

</span>
<span class="ltx_bibblock">Multi-view mesh reconstruction with neural deferred shading.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 6187–6197, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yariv et al. [2020]</span>
<span class="ltx_bibblock">
Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman.

</span>
<span class="ltx_bibblock">Multiview neural surface reconstruction by disentangling geometry and appearance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in Neural Information Processing Systems</em>, 33, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023]</span>
<span class="ltx_bibblock">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 3836–3847, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\thetitle</span>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p2.2"><span class="ltx_text" id="p2.2.1" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"/></span></p>
</div>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">7 </span>Appendix</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Additional Qualitative Results</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1"><span class="ltx_text" id="S7.SS1.p1.1.1" style="font-size:144%;">We present additional qualitative results in this section, extending those presented in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S3.F5" style="font-size:144%;" title="Figure 5 ‣ 3.2 LEMON ‣ 3 Method ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text" id="S7.SS1.p1.1.2" style="font-size:144%;"> of the main paper. Figures </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S7.F11" style="font-size:144%;" title="Figure 11 ‣ 7.2 Code and Video ‣ 7 Appendix ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">11</span></a><span class="ltx_text" id="S7.SS1.p1.1.3" style="font-size:144%;">,</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S7.F12" style="font-size:144%;" title="Figure 12 ‣ 7.2 Code and Video ‣ 7 Appendix ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">12</span></a><span class="ltx_text" id="S7.SS1.p1.1.4" style="font-size:144%;">,</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S7.F13" style="font-size:144%;" title="Figure 13 ‣ 7.2 Code and Video ‣ 7 Appendix ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">13</span></a><span class="ltx_text" id="S7.SS1.p1.1.5" style="font-size:144%;"> shows further experiments, with each column corresponding to the methods used and each row representing the object. Text instructions that provided in experiments are below each row. It’s important to note that results may vary depending on the hyperparameters of these models.</span></p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1"><span class="ltx_text" id="S7.SS1.p2.1.1" style="font-size:144%;">In Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#S7.F10" style="font-size:144%;" title="Figure 10 ‣ 7.2 Code and Video ‣ 7 Appendix ‣ LEMON: Localized Editing with Mesh Optimization and Neural Shaders"><span class="ltx_text ltx_ref_tag">10</span></a><span class="ltx_text" id="S7.SS1.p2.1.2" style="font-size:144%;">, we present our extended results on the ShapeNet dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS1.p2.1.3.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib4" title="">4</a><span class="ltx_text" id="S7.SS1.p2.1.4.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS1.p2.1.5" style="font-size:144%;">. Each row represents the initial ShapeNet object, and each column shows the text prompt given to our model, with phrases like ”Turn it into object”. The diagonals display the ground truth. The results demonstrate that our method can be used for object transformation. Minimal changes are observed in flat surfaces, attributed to their limited geometric characteristics. Notably, the transformation into a sofa yielded particularly favorable results, likely because sofas tend to have more distinct geometric characteristics compared to other objects.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Code and Video</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1"><span class="ltx_text" id="S7.SS2.p1.1.1" style="font-size:144%;">We attach our code to our supplementary material. The results presented in our paper can be replicated by following the instructions provided in the code folder. We also provide timelapse videos of the training process and a video demonstrating the consistency of the neural shader.</span></p>
</div>
<figure class="ltx_figure" id="S7.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="433" id="S7.F10.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F10.8.1.1" style="font-size:63%;">Figure 10</span>: </span><span class="ltx_text" id="S7.F10.9.2" style="font-size:63%;">Extended ShapeNet editing results with shaded image and normal map of the edited mesh. We take multi-view images from a ShapeNet object and give the prompt ”<span class="ltx_text ltx_font_italic" id="S7.F10.9.2.1">Turn it into </span>{<span class="ltx_text ltx_font_italic" id="S7.F10.9.2.2">object</span>}.” Diagonals correspond to the image and normal of the ground truth object.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S7.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="909" id="S7.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F11.7.1.1" style="font-size:63%;">Figure 11</span>: </span><span class="ltx_text" id="S7.F11.8.2" style="font-size:63%;">More editing results on the DTU dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib11" title="">11</a>]</cite>.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S7.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="909" id="S7.F12.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F12.7.1.1" style="font-size:63%;">Figure 12</span>: </span><span class="ltx_text" id="S7.F12.8.2" style="font-size:63%;">More editing results on the DTU dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib11" title="">11</a>]</cite>.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S7.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="909" id="S7.F13.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F13.7.1.1" style="font-size:63%;">Figure 13</span>: </span><span class="ltx_text" id="S7.F13.8.2" style="font-size:63%;">More editing results on the DTU dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12024v1#bib.bib11" title="">11</a>]</cite>.</span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 18 14:30:06 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
