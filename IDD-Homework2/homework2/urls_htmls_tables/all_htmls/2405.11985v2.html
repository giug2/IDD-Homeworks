<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.11985] MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering</title><meta property="og:description" content="Text-Centric Visual Question Answering¬†(TEC-VQA) in its proper format not only facilitates human-machine interaction in text-centric visual environments but also serves as a de facto gold proxy to evaluate AI models in‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.11985">

<!--Generated on Wed Jun  5 15:08:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Jingqun Tang<sup id="id5.5.id1" class="ltx_sup">1</sup><sup id="id6.6.id2" class="ltx_sup"><span id="id6.6.id2.1" class="ltx_text ltx_font_italic">‚àó</span></sup>, Qi Liu<sup id="id7.7.id3" class="ltx_sup">1</sup><sup id="id8.8.id4" class="ltx_sup"><span id="id8.8.id4.1" class="ltx_text ltx_font_italic">‚àó</span></sup>, Yongjie Ye<sup id="id9.9.id5" class="ltx_sup">1</sup><sup id="id10.10.id6" class="ltx_sup"><span id="id10.10.id6.1" class="ltx_text ltx_font_italic">‚àó</span></sup>, Jinghui Lu<sup id="id11.11.id7" class="ltx_sup">1</sup><sup id="id12.12.id8" class="ltx_sup"><span id="id12.12.id8.1" class="ltx_text ltx_font_italic">‚àó</span></sup>,
Shu Wei<sup id="id13.13.id9" class="ltx_sup">1</sup>,Chunhui Lin<sup id="id14.14.id10" class="ltx_sup">1</sup>, Wanqing Li<sup id="id15.15.id11" class="ltx_sup">1</sup>,


<br class="ltx_break">
Mohamad Fitri Faiz Bin Mahmood<sup id="id16.16.id12" class="ltx_sup">1</sup>, Hao Feng<sup id="id17.17.id13" class="ltx_sup">1</sup>, Zhen Zhao<sup id="id18.18.id14" class="ltx_sup">1</sup>, Yanjie Wang<sup id="id19.19.id15" class="ltx_sup">1</sup>, Yuliang Liu<sup id="id20.20.id16" class="ltx_sup">2</sup>,


<br class="ltx_break">
Hao Liu<sup id="id21.21.id17" class="ltx_sup">1 üñÇ</sup>, Xiang Bai<sup id="id22.22.id18" class="ltx_sup">2</sup>, Can Huang<sup id="id23.23.id19" class="ltx_sup">1 üñÇ</sup>

<br class="ltx_break"><sup id="id24.24.id20" class="ltx_sup"><span id="id24.24.id20.1" class="ltx_text" style="font-size:90%;">1</span></sup><span id="id25.25.id21" class="ltx_text" style="font-size:90%;">ByteDance <sup id="id25.25.id21.1" class="ltx_sup">2</sup>Huazhong University of Science and Technology</span>

<br class="ltx_break"><span id="id26.26.id22" class="ltx_text" style="font-size:90%;">
{tangjingqun, liuqi.nero, yeyongjie.ilz, lujinghui, haoliu.0128, can.huang}@bytedance.com,


<br class="ltx_break">
{ylliu, xbai}@hust.edu.cn

</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id27.id1" class="ltx_p">Text-Centric Visual Question Answering¬†(TEC-VQA) in its proper format not only facilitates human-machine interaction in text-centric visual environments but also serves as a <span id="id27.id1.1" class="ltx_text ltx_font_italic">de facto</span> gold proxy to evaluate AI models in the domain of text-centric scene understanding. However, most TEC-VQA benchmarks have focused on high-resource languages like English and Chinese. Despite pioneering works to expand multilingual QA pairs in non-text-centric VQA datasets using translation engines, the translation-based protocol encounters a substantial ‚ÄúVisual-textual misalignment‚Äù problem when applied to TEC-VQA. Specifically, it prioritizes the text in question-answer pairs while disregarding the visual text present in images. Furthermore, it does not adequately tackle challenges related to nuanced meaning, contextual distortion, language bias, and question-type diversity. In this work, we address the task of multilingual TEC-VQA and provide a benchmark with high-quality human expert annotations in 9 diverse languages, called MTVQA. To our knowledge, MTVQA is the first multilingual TEC-VQA benchmark to provide human expert annotations for text-centric scenarios. Further, by evaluating several state-of-the-art Multimodal Large Language Models¬†(MLLMs), including GPT-4V, on our MTVQA dataset, it is evident that there is still room for performance improvement, underscoring the value of our dataset. We hope this dataset will provide researchers with fresh perspectives and inspiration within the community.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">footnotetext: </span><sup id="footnotex1.1" class="ltx_sup"><span id="footnotex1.1.1" class="ltx_text ltx_font_italic">‚àó</span></sup> Equal contribution. üñÇCorresponding author.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the era of burgeoning AI, especially in LLMs/MLLMs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>,
<span id="S1.p1.1.1" class="ltx_text ltx_font_bold">Te</span>xt-<span id="S1.p1.1.2" class="ltx_text ltx_font_bold">C</span>entric <span id="S1.p1.1.3" class="ltx_text ltx_font_bold">V</span>isual <span id="S1.p1.1.4" class="ltx_text ltx_font_bold">Q</span>uestion <span id="S1.p1.1.5" class="ltx_text ltx_font_bold">A</span>nswering¬†(<span id="S1.p1.1.6" class="ltx_text ltx_font_bold">TEC-VQA</span>)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> has served as a <span id="S1.p1.1.7" class="ltx_text ltx_font_italic">de facto</span> gold proxy to evaluate AI models in the domain of text-centric scene understanding. Compared with general VQA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, TEC-VQA places greater emphasis on answering questions that require understanding textual information within images. It provides a streamlined avenue for individuals without specialized expertise to articulate their requirements and access applications in text-centric visual environments. However, the majority of advancements in TEC-VQA have predominantly concentrated on high-resource languages, <span id="S1.p1.1.8" class="ltx_text ltx_font_italic">e.g.</span>, English¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, Chinese¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, Japanese¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and <span id="S1.p1.1.9" class="ltx_text ltx_font_italic">etc.</span>, thus restricting the applicability of AI models to the global community, particularly populations speaking low-resource languages.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To tackle the problem of language diversity, several seminal studies¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> in the general VQA field, leverage off-the-shelf translation engines to expand existing question-answer pairs from high-resource languages to their multilingual counterparts including low-resource ones. However, when applied to TEC-VQA, this translation-based approach may fall prey to the ‚Äú<span id="S1.p2.1.1" class="ltx_text ltx_font_italic">Visual-textual misalignment</span>‚Äù problem as only the text in question-answer pairs can be processed, while the visual text present in the images is overlooked. Not to mention issues such as nuanced meaning, contextual distortion, language bias, and question type diversity further render the transferability of the translation protocol infeasible for TEC-VQA. The <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">status quo</span> begs for a question: ‚Äú<span id="S1.p2.1.3" class="ltx_text ltx_font_italic">How can we address the visual-textual misalignment problem for multilingual TEC-VQA and what we stand in the MLLM era?</span>‚Äù</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<table id="S1.F1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.F1.2.2" class="ltx_tr">
<td id="S1.F1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x1.png" id="S1.F1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="226" height="209" alt="Refer to caption"></td>
<td id="S1.F1.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x2.png" id="S1.F1.2.2.2.g1" class="ltx_graphics ltx_img_square" width="226" height="251" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Left: overview of various categories of text-centric images. Right: distribution of image and QA pairs over the 9 languages in MTVQA benchmark.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2405.11985/assets/x3.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="238" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Multilingual VQA examples selected from four languages. From left to right: Arabic (AR), Russian (RU), Thai (TH), Vietnamese (VI). The corresponding translations in English are in brackets.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, to answer the question above, we establish MTVQA, a novel and high-quality multilingual TEC-VQA benchmark, where all images are collected from real-world and meticulously annotated by human experts in nine languages: Arabic¬†(AR), Korean¬†(KO), Japanese¬†(JA), Thai¬†(TH), Vietnamese¬†(VI), Russian¬†(RU), French¬†(FR), German¬†(DE), and Italian¬†(IT). More concretely, to ensure the visual-textual alignment at most, the annotation process follows the raise-then-correct paradigm, where a group of human annotators raises several distinct questions, ranging from simple content extraction to text-related reasoning, and subsequently provides answers. These QA pairs are then double-checked by another group to ensure accuracy and consistency. Consequently, as illustrated in Fig.¬†<a href="#S3.F5" title="Figure 5 ‚Ä£ 3.3 Data Statistics ‚Ä£ 3 MTVQA Benchmark ‚Ä£ MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, 6,678 training images and 21,829 question-answer pairs, as well as 2,116 test images and 6,778 question-answer pairs are obtained, covering several fine-grained scenarios, such as menus, logos, maps, bills, PPTs, research papers, and <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">etc</span>. To our best knowledge, MTVQA is the first TEC-VQA dataset to provide native human annotations for multilingual text-rich scenarios, especially for low-source languages. Furthermore, we investigate recent representative MLLMs, including GPT-4V, Gemini, QwenVL <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">etc</span>., by juxtaposing experimental results regarding their performance on our newly proposed MTVQA. Both for general MLLMs and document-focused ones, the results unequivocally demonstrate that opportunities for improvement persist within these MLLMs when applied in multilingual text-rich scenarios.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In summary, the main contributions of this paper can be categorized into three points:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce the MTVQA dataset, to the best of our knowledge, which is the first multilingual TEC-VQA benchmark to provide human expert annotations for text-centric scenarios.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We benchmark the state-of-the-art MLLMs on our new dataset and show there is still room for performance improvement for these models under multilingual text-rich scenarios.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We propose a set of baselines for multilingual TEC-VQA tasks.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>LLMs/MLLMs for text-centric VQA</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Recent advancements in LLMs/MLLMs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> have revolutionized VQA tasks, as demonstrated by the remarkable zero-shot performance of these models. Notably, the high generalizability of LLMs/MLLMs, when explicitly trained on visual text understanding datasets and fine-tuned with instructions, has significantly enhanced their application in text-centric VQA scenarios¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. For example, LLaVAR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, UniDoc¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which extend LLaVA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> into the realm of document understanding, pioneering the text-centric VQA of MLLMs by training them to predict texts and coordinates from document images. Furthermore, DocPedia¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> operates visual input in the frequency domain rather than in space, which enables higher input resolution without increasing the input sequence. Lately, mPLUG-DocOwl¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, Qwen-VL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and TextMonkey¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> leverage publicly available document-related VQA datasets to further enhance the text-centric VQA capabilities. Despite the promising results achieved by existing LLMs/MLLMs in text-centric VQA tasks, their focus on high-resource languages such as English or Chinese has posed challenges in achieving reasonable performance for low-resource languages. This is primarily due to the lack of data or benchmarks for these low-resource languages.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multilingual text-centric VQA Benchmarks</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">VQA has garnered significant attention in recent years, with numerous studies, datasets, and benchmarks being proposed to advance the field¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Many datasets have been created that encompass scene text of various domains, including natural images¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, scanned documents¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, book and movie covers¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. One notable limitation of these datasets is their predominant focus on English¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> or other high-resource languages such as Chinese¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and Japanese¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, which restricts the applicability of VQA systems for low-resource languages such as Thai and Vietnamese.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">There is a recent effort toward extending VQA tasks to a wider range of languages¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> by providing a multilingual VQA datasets. For example, ¬†<cite class="ltx_cite ltx_citemacro_citet">Gao et¬†al. [<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> created a free-form bilingual VQA dataset (FM-IQA) contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. <cite class="ltx_cite ltx_citemacro_citet">Raj¬†Khan et¬†al. [<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> developed a large-scale multilingual and code-mixed VQA dataset (MuCo-VQA) supporting five languages. Of more relevance are the works xGQA (8 languages)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and MaXM (7 languages)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, which apply translation-based protocols to expand VQA data beyond English. However, the translation-based multilingual VQA datasets inherently face issues, such as the ‚ÄúVisual-textual misalignment‚Äù problem, where only the text in question-answer pairs is processed, while the visual text in images is overlooked. Additionally, the nuanced meaning and context are often distorted; language bias introduced by machine translation models, and the coverage of certain question types is limited, as highlighted by¬†<cite class="ltx_cite ltx_citemacro_citet">Changpinyo et¬†al. [<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Moreover, none of the previous multilingual datasets focus on text-centric scenarios where multilingual text frequently occurs.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Our benchmark distinguishes itself by focusing on multilingual text-centric VQA scenarios using human expert annotations. To the best of our knowledge, the MTVQA benchmark is the first dataset to provide native human annotations for such scenarios. It covers 9 languages, thereby facilitating the training and evaluation of multilingual models in diverse linguistic contexts. Additionally, our dataset can gauge the VQA system‚Äôs ability for not only high-resource languages but also those that are typically underrepresented in current datasets¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">The MTVQA benchmark addresses a significant gap in existing datasets by catering to the crucial needs of low-resource languages through annotations from native speakers across multiple languages. Our pioneering efforts distinctly position the MTVQA benchmark as a unique multilingual VQA resource, advancing the frontier of machine learning research.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MTVQA Benchmark</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The MTVQA Benchmark covers 9 languages: Arabic (AR), Korean (KO), Japanese (JA), Thai (TH), Vietnamese (VI), Russian (RU), French (FR), German (DE), and Italian (IT). In this section, we describe in detail how we establish the MTVQA benchmark, including the collection of raw image data and two-round human expert annotations, which are independent of each other.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Collection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our purpose is to develop a multilingual VQA benchmark capable of evaluating the QA performance of MLLMs in multilingual text-centric scenarios, thus the raw data collection process is mainly oriented towards text-centric images from natural scenarios and document scenarios. To ensure the diversity and quality of data, we collect not only the raw image data from publicly available datasets, including the multilingual scene text recognition images from MLT2019¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and PowerPoint slides¬†(PPTs) sourced from the internet, but also the data from countries of each language. Furthermore, the collected data includes multiple fine-grained scenarios¬†(Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), such as menus, logos, maps, bills, PPTs, research papers, and <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">etc</span>. As a result, we gather a total of 1,220 images from document scenarios and 876 images from natural scenarios in the test set of the MTVQA benchmark. To ensure the visual-textual alignment, for text-rich images lacking text and language annotations, we subject them to a standardized data cleaning process, which includes text recognition and language classification. Afterward, we organize all the text-rich images we have obtained into language-specific groups, preparing them for the subsequent stage of data annotation.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Human Expert Annotation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In order to obtain informative and accurate text-related QA pairs on the language-specific grouped images, we recruit a group of annotators with expertise from local regions of each language. It is worth noting that all these annotators are native speakers of their respective languages, ensuring their deep understanding and proficiency in the linguistic nuances and cultural context necessary for precise annotations. Considering the subjective nature of the text-image understanding task, we have implemented a further division within the annotation team. This division involves separating the team into two independent groups, with one group dedicated to generating and responding to questions based on the provided images, while the other group focuses on evaluating and correcting the QA pair results. This raise-then-correct paradigm ensures a comprehensive and reliable assessment of the text-image understanding process. Additionally, each language‚Äôs annotation results undergo a 10% sampling inspection by a quality inspector. If the QA pairs fail to meet the criteria, they are sent back for re-annotation. Prior to commencing the formal human expert annotation task, all annotators undergo unified training and receive annotation examples. The brief diagram of the two-round annotation process is shown in Figure <a href="#S3.F3" title="Figure 3 ‚Ä£ 3.2 Human Expert Annotation ‚Ä£ 3 MTVQA Benchmark ‚Ä£ MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and we elaborate on it in the following subsections.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Mean lengths of question-answer pairs in different languages of
training set and test set, using GPT-4o tokenizer.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">AR</span></td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">DE</span></td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">FR</span></td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">IT</span></td>
<td id="S3.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">JA</span></td>
<td id="S3.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">KO</span></td>
<td id="S3.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.8.1" class="ltx_text ltx_font_bold">RU</span></td>
<td id="S3.T1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.9.1" class="ltx_text ltx_font_bold">TH</span></td>
<td id="S3.T1.1.1.1.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.10.1" class="ltx_text ltx_font_bold">VI</span></td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="10"><span id="S3.T1.1.2.2.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Training Set</span></th>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<th id="S3.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Question</th>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">8.29</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">8.72</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">9.73</td>
<td id="S3.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">12.05</td>
<td id="S3.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">12.43</td>
<td id="S3.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">11.74</td>
<td id="S3.T1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">11.56</td>
<td id="S3.T1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">11.35</td>
<td id="S3.T1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">11.21</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<th id="S3.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Answer</th>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_center">9.66</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_center">6.96</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_center">7.34</td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_center">11.24</td>
<td id="S3.T1.1.4.4.6" class="ltx_td ltx_align_center">12.70</td>
<td id="S3.T1.1.4.4.7" class="ltx_td ltx_align_center">13.56</td>
<td id="S3.T1.1.4.4.8" class="ltx_td ltx_align_center">12.00</td>
<td id="S3.T1.1.4.4.9" class="ltx_td ltx_align_center">11.26</td>
<td id="S3.T1.1.4.4.10" class="ltx_td ltx_align_center">13.31</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<th id="S3.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="10"><span id="S3.T1.1.5.5.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Test Set</span></th>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<th id="S3.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Question</th>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">8.08</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">8.29</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">9.76</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">11.93</td>
<td id="S3.T1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">12.48</td>
<td id="S3.T1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_t">12.2</td>
<td id="S3.T1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_t">11.65</td>
<td id="S3.T1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_t">10.98</td>
<td id="S3.T1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_t">10.99</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<th id="S3.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">Answer</th>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b">7.95</td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b">6.67</td>
<td id="S3.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b">6.61</td>
<td id="S3.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b">11.04</td>
<td id="S3.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_b">12.55</td>
<td id="S3.T1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_b">13.61</td>
<td id="S3.T1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_b">14.42</td>
<td id="S3.T1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_b">12.08</td>
<td id="S3.T1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_b">13.00</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">First Round Questioning and Answering.</span>‚ÄÉFor the first round of annotation tasks, we assigned 3 annotators for each language to manually generate original QA results. Given a text-centric image from our collection, annotators are first required to read the texts in the image and analyze other contents in the image in a comprehensive and detailed manner. They must then raise 4 meaningful and distinct questions based on the content in the image and give the answers. All annotators adhere to the following criteria: (1) the first three questions should satisfy that answering these questions requires direct reading of the textual information in the image, (2) the fourth question requires reasoning about the text in the image to answer (3) the questions and answers must be reasonably correct and consistent with the content of the image, and (4) the answer should be as concise as possible and free of nonsense¬†(<span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">e.g.</span>, when the question is ‚ÄúWhen is the volunteer recruitment period‚Äù, the answer should be ‚Äú9:00-16:00‚Äù rather than ‚ÄúThe volunteer recruitment period is 9:00-16:00‚Äù). It‚Äôs worth mentioning that our requirement for concise answers is to make the evaluation process more friendly and more reliable, cause we try to keep the evaluation metrics unaffected by extraneous content in the answer sentence.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Second round Evaluation and Correction.</span>‚ÄÉTo reduce the effect of human subjective cognitive bias on our MTVQA benchmark and get high-quality question-answer pairs, we assigned 2 annotators for each language for the annotation evaluation and correction process. Based on the provided images and the first-round annotation results, the annotators must follow these rules of judgment and steps for the annotation: (1) Whether the question is related to the text in the image. If not, discard the current question-answer pair, (2) Whether the answer is correct. If not, modify the answer, and (3) Whether the answer repeats the content from the question. If so, remove the repeated content to ensure a concise answer.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2405.11985/assets/x4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A brief diagram of the annotation process.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data Statistics</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We instruct the annotators to complete the above human expert annotation work towards the text-centric VQA tasks and construct the MTVQA benchmark consisting of 8,794 images and 28,607 question-answer pairs that cover the 9 languages. The MTVQA benchmark is divided into a training set containing 6,678 images and 21,829 question-answer pairs, and a test set containing 2,116 images and 6,778 question-answer pairs. The detailed data distribution can be seen in Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. To visualize the vocabulary richness of our benchmark, we calculate the word frequencies for each language and present them in the form of word clouds as shown in Figure <a href="#S3.F4" title="Figure 4 ‚Ä£ 3.3 Data Statistics ‚Ä£ 3 MTVQA Benchmark ‚Ä£ MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In Figure <a href="#S3.F5" title="Figure 5 ‚Ä£ 3.3 Data Statistics ‚Ä£ 3 MTVQA Benchmark ‚Ä£ MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we demonstrate the statistics of the question and answer lengths using GPT-4o tokenizer.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div id="S3.F4.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:455.3pt;height:445.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.7pt,12.4pt) scale(0.947328481660183,0.947328481660183) ;">
<table id="S3.F4.9.9" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F4.3.3.3" class="ltx_tr">
<td id="S3.F4.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x5.png" id="S3.F4.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="152" height="160" alt="Refer to caption"></td>
<td id="S3.F4.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x6.png" id="S3.F4.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="152" height="160" alt="Refer to caption"></td>
<td id="S3.F4.3.3.3.3" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x7.png" id="S3.F4.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="152" height="160" alt="Refer to caption"></td>
<td id="S3.F4.3.3.3.4" class="ltx_td"></td>
</tr>
<tr id="S3.F4.6.6.6" class="ltx_tr">
<td id="S3.F4.4.4.4.1" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x8.png" id="S3.F4.4.4.4.1.g1" class="ltx_graphics ltx_img_square" width="152" height="160" alt="Refer to caption"></td>
<td id="S3.F4.5.5.5.2" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x9.png" id="S3.F4.5.5.5.2.g1" class="ltx_graphics ltx_img_square" width="152" height="160" alt="Refer to caption"></td>
<td id="S3.F4.6.6.6.3" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x10.png" id="S3.F4.6.6.6.3.g1" class="ltx_graphics ltx_img_square" width="152" height="160" alt="Refer to caption"></td>
<td id="S3.F4.6.6.6.4" class="ltx_td"></td>
</tr>
<tr id="S3.F4.9.9.9" class="ltx_tr">
<td id="S3.F4.7.7.7.1" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x11.png" id="S3.F4.7.7.7.1.g1" class="ltx_graphics ltx_img_square" width="152" height="160" alt="Refer to caption"></td>
<td id="S3.F4.8.8.8.2" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x12.png" id="S3.F4.8.8.8.2.g1" class="ltx_graphics ltx_img_square" width="152" height="160" alt="Refer to caption"></td>
<td id="S3.F4.9.9.9.3" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x13.png" id="S3.F4.9.9.9.3.g1" class="ltx_graphics ltx_img_square" width="152" height="160" alt="Refer to caption"></td>
<td id="S3.F4.9.9.9.4" class="ltx_td"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Top answer word clouds of different languages.</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure">
<div id="S3.F5.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:455.3pt;height:322.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.7pt,8.9pt) scale(0.947328481660183,0.947328481660183) ;">
<table id="S3.F5.9.9" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F5.3.3.3" class="ltx_tr">
<td id="S3.F5.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x14.png" id="S3.F5.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="152" height="114" alt="Refer to caption"></td>
<td id="S3.F5.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x15.png" id="S3.F5.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="152" height="114" alt="Refer to caption"></td>
<td id="S3.F5.3.3.3.3" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x16.png" id="S3.F5.3.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="152" height="114" alt="Refer to caption"></td>
<td id="S3.F5.3.3.3.4" class="ltx_td"></td>
</tr>
<tr id="S3.F5.6.6.6" class="ltx_tr">
<td id="S3.F5.4.4.4.1" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x17.png" id="S3.F5.4.4.4.1.g1" class="ltx_graphics ltx_img_landscape" width="152" height="114" alt="Refer to caption"></td>
<td id="S3.F5.5.5.5.2" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x18.png" id="S3.F5.5.5.5.2.g1" class="ltx_graphics ltx_img_landscape" width="152" height="114" alt="Refer to caption"></td>
<td id="S3.F5.6.6.6.3" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x19.png" id="S3.F5.6.6.6.3.g1" class="ltx_graphics ltx_img_landscape" width="152" height="114" alt="Refer to caption"></td>
<td id="S3.F5.6.6.6.4" class="ltx_td"></td>
</tr>
<tr id="S3.F5.9.9.9" class="ltx_tr">
<td id="S3.F5.7.7.7.1" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x20.png" id="S3.F5.7.7.7.1.g1" class="ltx_graphics ltx_img_landscape" width="152" height="114" alt="Refer to caption"></td>
<td id="S3.F5.8.8.8.2" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x21.png" id="S3.F5.8.8.8.2.g1" class="ltx_graphics ltx_img_landscape" width="152" height="114" alt="Refer to caption"></td>
<td id="S3.F5.9.9.9.3" class="ltx_td ltx_align_center"><img src="/html/2405.11985/assets/x22.png" id="S3.F5.9.9.9.3.g1" class="ltx_graphics ltx_img_landscape" width="152" height="114" alt="Refer to caption"></td>
<td id="S3.F5.9.9.9.4" class="ltx_td"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Statistics of question and answer lengths of different languages aggregating training and test set, using GPT-4o tokenizer.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baseline Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For the MTVQA benchmark, we evaluate the following instruction-tuned general MLLMs,
(1) <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Open-source MLLMs:</span> InternVL-V1.5¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, InternLM-Xcomposer2-4KHD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, Mini-Gemini-HD-34B¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, Llava-Next-34B¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, DeepSeek-VL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, YI-VL-34B¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, TextSquare¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, TextMonkey¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and mPLUG-DocOwl 1.5¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>;
(2) <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">Closed-source MLLMs:</span> GPT-4V, Gemini Ultra, QwenVL Max, QwenVL Plus, Claude3 Opus, Claude3 Sonnet and GLM4V.
For the closed-source MLLMs, we use the chat version through the official APIs, while for the open-source MLLMs, we utilize the instruct versions. It is noted that all the model weights of the open-source MLLMs evaluated in our experiments could be downloaded from the HuggingFace Model Hub. For the open-source MLLMs, the model size varies from 7b to 34b.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We conduct the evaluation experiments over the baseline MLLMs with their default settings, ignoring the effect of generation configuration on the results. To make the output of MLLMs more evaluation-friendly, we design the following prompt format to limit the output length: ‚ÄúAnswer the question using a word or phrase in the language of the question. + &lt;Question&gt;‚Äù, where &lt;Question&gt; represents the corresponding question of the input image. The extra prefixes added to the raw question could limit the answer to be as concise as possible. Besides, we utilize the InternLM-Xcomposer2-4KHD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as the backbone for the instructional fine-tuning experiment on the MTVQA training set. In the instructional fine-tuning process, we follow the default training settings¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> with ‚ÄúHD-16‚Äù and train on MTVQA training set for 1 epoch.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Zero-shot testing</span> To demonstrate the quantitative comparison results in the above MLLMs, we follow TextMonkey¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> with accuracy as the evaluation metric. That is, the model output is only counted as correct if it contains the ground truth. The complete evaluation results are shown in Table <a href="#S4.T2" title="Table 2 ‚Ä£ 4.3 Evaluation Results ‚Ä£ 4 Experiments ‚Ä£ MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where Claude3 Opus achieves the highest average accuracy of 25.7<math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mo id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\%</annotation></semantics></math> on the 9 languages. It indicates that the multilingual text-centric VQA tasks remain a big challenge, even for the state-of-the-art open-source and closed-source MLLMs. From the metrics across languages, both open-source and closed-source models performed significantly better on Indo-European languages using the Latin alphabet, including DE, FR, and IT in our benchmark, compared to other languages, which results from the distribution of realistically available training data and the genetic relationship of different languages. In addition, all closed-source models except GLM4V outperform the open-source model overall across the nine languages, which may be due to the contribution of pre-training on multilingual data. We also found that the document-focused MLLMs, like TextSquare¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and TextMonkey¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, do not significantly outperform other open-source models on the metrics of these 9 languages.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.3" class="ltx_p"><span id="S4.SS3.p2.3.1" class="ltx_text ltx_font_bold">Instruction tuning</span> As shown in Table <a href="#S4.T2" title="Table 2 ‚Ä£ 4.3 Evaluation Results ‚Ä£ 4 Experiments ‚Ä£ MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the instruction tuning experiment on MTVQA benchmark brings a 8.5<math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mo id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\%</annotation></semantics></math> improvement in average accuracy. With respect to specific languages, French sees the largest improvement of 14.2<math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mo id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\%</annotation></semantics></math> in accuracy, while Russian has the smallest improvement of 1.7<math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mo id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><csymbol cd="latexml" id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">\%</annotation></semantics></math> in accuracy. The results demonstrate that MLLMs vary in their ability to understand and learn from text-centric data in different languages, leaving great potential for future research of multilingual text-centric MLLMs pre-training.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of the leading closed-source and open-source MLLMs on the MTVQA benchmark. </figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">AR</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">DE</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">FR</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">IT</td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">JA</td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">KO</td>
<td id="S4.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">RU</td>
<td id="S4.T2.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">TH</td>
<td id="S4.T2.1.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">VI</td>
<td id="S4.T2.1.1.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Avg.</td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="10"><span id="S4.T2.1.2.2.1.1" class="ltx_text ltx_font_italic">Closed-Source MLLMs</span></th>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<th id="S4.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4V</th>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">11.5</td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">31.5</td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">40.4</td>
<td id="S4.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">32.3</td>
<td id="S4.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">11.5</td>
<td id="S4.T2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">16.7</td>
<td id="S4.T2.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">10.3</td>
<td id="S4.T2.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">15.0</td>
<td id="S4.T2.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">28.9</td>
<td id="S4.T2.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">22.0</td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<th id="S4.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Gemini Ultra</th>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">14.7</td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">32.3</td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">40.0</td>
<td id="S4.T2.1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31.8</td>
<td id="S4.T2.1.4.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12.3</td>
<td id="S4.T2.1.4.4.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">17.2</td>
<td id="S4.T2.1.4.4.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11.8</td>
<td id="S4.T2.1.4.4.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20.3</td>
<td id="S4.T2.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">28.6</td>
<td id="S4.T2.1.4.4.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.2</td>
</tr>
<tr id="S4.T2.1.5.5" class="ltx_tr">
<th id="S4.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">QwenVL Max</th>
<td id="S4.T2.1.5.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.7</td>
<td id="S4.T2.1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31.4</td>
<td id="S4.T2.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">37.6</td>
<td id="S4.T2.1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30.2</td>
<td id="S4.T2.1.5.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">18.6</td>
<td id="S4.T2.1.5.5.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25.4</td>
<td id="S4.T2.1.5.5.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">10.4</td>
<td id="S4.T2.1.5.5.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.8</td>
<td id="S4.T2.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">23.5</td>
<td id="S4.T2.1.5.5.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21.1</td>
</tr>
<tr id="S4.T2.1.6.6" class="ltx_tr">
<th id="S4.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">QwenVL Plus</th>
<td id="S4.T2.1.6.6.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.8</td>
<td id="S4.T2.1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28.8</td>
<td id="S4.T2.1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">33.7</td>
<td id="S4.T2.1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">27.1</td>
<td id="S4.T2.1.6.6.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12.8</td>
<td id="S4.T2.1.6.6.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">19.9</td>
<td id="S4.T2.1.6.6.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">9.4</td>
<td id="S4.T2.1.6.6.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.6</td>
<td id="S4.T2.1.6.6.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">18.1</td>
<td id="S4.T2.1.6.6.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">17.8</td>
</tr>
<tr id="S4.T2.1.7.7" class="ltx_tr">
<th id="S4.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Claude3 Opus</th>
<td id="S4.T2.1.7.7.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.7.2.1" class="ltx_text ltx_font_bold">15.1</span></td>
<td id="S4.T2.1.7.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.7.3.1" class="ltx_text ltx_font_bold">33.4</span></td>
<td id="S4.T2.1.7.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.7.4.1" class="ltx_text ltx_font_bold">40.6</span></td>
<td id="S4.T2.1.7.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.7.5.1" class="ltx_text ltx_font_bold">34.4</span></td>
<td id="S4.T2.1.7.7.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.7.6.1" class="ltx_text ltx_font_bold">19.4</span></td>
<td id="S4.T2.1.7.7.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.7.7.1" class="ltx_text ltx_font_bold">27.2</span></td>
<td id="S4.T2.1.7.7.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.7.8.1" class="ltx_text ltx_font_bold">13.0</span></td>
<td id="S4.T2.1.7.7.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.7.9.1" class="ltx_text ltx_font_bold">19.5</span></td>
<td id="S4.T2.1.7.7.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.7.10.1" class="ltx_text ltx_font_bold">29.1</span></td>
<td id="S4.T2.1.7.7.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.7.11.1" class="ltx_text ltx_font_bold">25.7</span></td>
</tr>
<tr id="S4.T2.1.8.8" class="ltx_tr">
<th id="S4.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Claude3 Sonnet</th>
<td id="S4.T2.1.8.8.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">10.5</td>
<td id="S4.T2.1.8.8.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28.9</td>
<td id="S4.T2.1.8.8.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">35.6</td>
<td id="S4.T2.1.8.8.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31.8</td>
<td id="S4.T2.1.8.8.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13.9</td>
<td id="S4.T2.1.8.8.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.2</td>
<td id="S4.T2.1.8.8.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11.0</td>
<td id="S4.T2.1.8.8.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">15.2</td>
<td id="S4.T2.1.8.8.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">20.8</td>
<td id="S4.T2.1.8.8.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21.1</td>
</tr>
<tr id="S4.T2.1.9.9" class="ltx_tr">
<th id="S4.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">GLM4V</th>
<td id="S4.T2.1.9.9.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.3</td>
<td id="S4.T2.1.9.9.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30.0</td>
<td id="S4.T2.1.9.9.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">34.1</td>
<td id="S4.T2.1.9.9.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30.1</td>
<td id="S4.T2.1.9.9.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.4</td>
<td id="S4.T2.1.9.9.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.7</td>
<td id="S4.T2.1.9.9.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.0</td>
<td id="S4.T2.1.9.9.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.5</td>
<td id="S4.T2.1.9.9.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">12.3</td>
<td id="S4.T2.1.9.9.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13.6</td>
</tr>
<tr id="S4.T2.1.10.10" class="ltx_tr">
<th id="S4.T2.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="10"><span id="S4.T2.1.10.10.1.1" class="ltx_text ltx_font_italic">Open-Source MLLMs</span></th>
<td id="S4.T2.1.10.10.2" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
</tr>
<tr id="S4.T2.1.11.11" class="ltx_tr">
<th id="S4.T2.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">InternVL-V1.5¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<td id="S4.T2.1.11.11.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">3.4</td>
<td id="S4.T2.1.11.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">27.1</td>
<td id="S4.T2.1.11.11.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">31.4</td>
<td id="S4.T2.1.11.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">27.1</td>
<td id="S4.T2.1.11.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">9.9</td>
<td id="S4.T2.1.11.11.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">9.0</td>
<td id="S4.T2.1.11.11.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4.9</td>
<td id="S4.T2.1.11.11.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">8.7</td>
<td id="S4.T2.1.11.11.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">12.4</td>
<td id="S4.T2.1.11.11.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.9</td>
</tr>
<tr id="S4.T2.1.12.12" class="ltx_tr">
<th id="S4.T2.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Mini-Gemini-HD-34B¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S4.T2.1.12.12.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.2</td>
<td id="S4.T2.1.12.12.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25.0</td>
<td id="S4.T2.1.12.12.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">29.2</td>
<td id="S4.T2.1.12.12.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25.5</td>
<td id="S4.T2.1.12.12.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.1</td>
<td id="S4.T2.1.12.12.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.6</td>
<td id="S4.T2.1.12.12.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.1</td>
<td id="S4.T2.1.12.12.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.3</td>
<td id="S4.T2.1.12.12.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">11.8</td>
<td id="S4.T2.1.12.12.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13.0</td>
</tr>
<tr id="S4.T2.1.13.13" class="ltx_tr">
<th id="S4.T2.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Llava-Next-34B¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<td id="S4.T2.1.13.13.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.3</td>
<td id="S4.T2.1.13.13.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">24.0</td>
<td id="S4.T2.1.13.13.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28.0</td>
<td id="S4.T2.1.13.13.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.3</td>
<td id="S4.T2.1.13.13.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.6</td>
<td id="S4.T2.1.13.13.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.1</td>
<td id="S4.T2.1.13.13.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.6</td>
<td id="S4.T2.1.13.13.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.4</td>
<td id="S4.T2.1.13.13.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">9.8</td>
<td id="S4.T2.1.13.13.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11.1</td>
</tr>
<tr id="S4.T2.1.14.14" class="ltx_tr">
<th id="S4.T2.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">DeepSeek-VL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S4.T2.1.14.14.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.6</td>
<td id="S4.T2.1.14.14.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">14.2</td>
<td id="S4.T2.1.14.14.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">15.3</td>
<td id="S4.T2.1.14.14.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">15.2</td>
<td id="S4.T2.1.14.14.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.9</td>
<td id="S4.T2.1.14.14.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.8</td>
<td id="S4.T2.1.14.14.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.6</td>
<td id="S4.T2.1.14.14.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.9</td>
<td id="S4.T2.1.14.14.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">5.2</td>
<td id="S4.T2.1.14.14.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.6</td>
</tr>
<tr id="S4.T2.1.15.15" class="ltx_tr">
<th id="S4.T2.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">YI-VL-34B¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<td id="S4.T2.1.15.15.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.7</td>
<td id="S4.T2.1.15.15.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13.5</td>
<td id="S4.T2.1.15.15.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">15.7</td>
<td id="S4.T2.1.15.15.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12.1</td>
<td id="S4.T2.1.15.15.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.8</td>
<td id="S4.T2.1.15.15.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.2</td>
<td id="S4.T2.1.15.15.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.8</td>
<td id="S4.T2.1.15.15.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.5</td>
<td id="S4.T2.1.15.15.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">4.1</td>
<td id="S4.T2.1.15.15.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.8</td>
</tr>
<tr id="S4.T2.1.16.16" class="ltx_tr">
<th id="S4.T2.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">TextSquare¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<td id="S4.T2.1.16.16.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.7</td>
<td id="S4.T2.1.16.16.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">27.0</td>
<td id="S4.T2.1.16.16.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30.8</td>
<td id="S4.T2.1.16.16.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">26.7</td>
<td id="S4.T2.1.16.16.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.2</td>
<td id="S4.T2.1.16.16.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.2</td>
<td id="S4.T2.1.16.16.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.7</td>
<td id="S4.T2.1.16.16.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.2</td>
<td id="S4.T2.1.16.16.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">12.4</td>
<td id="S4.T2.1.16.16.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13.6</td>
</tr>
<tr id="S4.T2.1.17.17" class="ltx_tr">
<th id="S4.T2.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">TextMonkey¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<td id="S4.T2.1.17.17.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.0</td>
<td id="S4.T2.1.17.17.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">18.1</td>
<td id="S4.T2.1.17.17.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">19.9</td>
<td id="S4.T2.1.17.17.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.1</td>
<td id="S4.T2.1.17.17.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.6</td>
<td id="S4.T2.1.17.17.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.2</td>
<td id="S4.T2.1.17.17.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.2</td>
<td id="S4.T2.1.17.17.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.9</td>
<td id="S4.T2.1.17.17.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">11.1</td>
<td id="S4.T2.1.17.17.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">9.9</td>
</tr>
<tr id="S4.T2.1.18.18" class="ltx_tr">
<th id="S4.T2.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">mPLUG-DocOwl 1.5¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S4.T2.1.18.18.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.0</td>
<td id="S4.T2.1.18.18.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13.9</td>
<td id="S4.T2.1.18.18.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">14.9</td>
<td id="S4.T2.1.18.18.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">18.2</td>
<td id="S4.T2.1.18.18.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.9</td>
<td id="S4.T2.1.18.18.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.0</td>
<td id="S4.T2.1.18.18.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.0</td>
<td id="S4.T2.1.18.18.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.9</td>
<td id="S4.T2.1.18.18.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">6.4</td>
<td id="S4.T2.1.18.18.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.2</td>
</tr>
<tr id="S4.T2.1.19.19" class="ltx_tr">
<th id="S4.T2.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Xcomposer2-4KHD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</th>
<td id="S4.T2.1.19.19.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.0</td>
<td id="S4.T2.1.19.19.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20.6</td>
<td id="S4.T2.1.19.19.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.2</td>
<td id="S4.T2.1.19.19.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21.6</td>
<td id="S4.T2.1.19.19.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.6</td>
<td id="S4.T2.1.19.19.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.7</td>
<td id="S4.T2.1.19.19.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.1</td>
<td id="S4.T2.1.19.19.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.1</td>
<td id="S4.T2.1.19.19.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">10.1</td>
<td id="S4.T2.1.19.19.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11.2</td>
</tr>
<tr id="S4.T2.1.20.20" class="ltx_tr">
<th id="S4.T2.1.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Xcomposer-SFT</th>
<td id="S4.T2.1.20.20.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">11.8</td>
<td id="S4.T2.1.20.20.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">31.7</td>
<td id="S4.T2.1.20.20.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">37.4</td>
<td id="S4.T2.1.20.20.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">29.3</td>
<td id="S4.T2.1.20.20.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">14.5</td>
<td id="S4.T2.1.20.20.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">12.9</td>
<td id="S4.T2.1.20.20.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.8</td>
<td id="S4.T2.1.20.20.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">13.9</td>
<td id="S4.T2.1.20.20.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">20.2</td>
<td id="S4.T2.1.20.20.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">19.7</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The current iteration of MTVQA exhibits certain constraints that warrant attention. Primarily, the linguistic diversity incorporated is not exhaustive; several lesser-spoken languages remain unrepresented. Future enhancements will aim to broaden the multilingual scope of the dataset. Additionally, the dataset currently offers a singular canonical response for each question. Recognizing the multifaceted nature of the inquiry, subsequent versions will endeavor to include a spectrum of plausible answers to reflect the varied perspectives inherent to each question.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we introduce MTVQA, a multilingual TEC-VQA benchmark featuring high-quality human expert annotations in 9 diverse languages. We believe that MTVQA is the first benchmark of its kind to provide fully manual annotations specifically tailored to text-centric scenarios. The results obtained from both closed- and open-source MLLMs on our MTVQA dataset indicate that there is still room for improving their performance in multilingual text-centric scenarios. Although the current version of MTVQA has constraints regarding linguistic diversity and singular responses per question, we are confident that this dataset can still inspire researchers within the TEC-VQA community with new perspectives and ideas.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et¬†al. [2023]</span>
<span class="ltx_bibblock">
OpenAI:Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaLeoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo¬†Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, HyungWon Chung, Dave Cummings, and Jeremiah Currier.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>, Dec 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et¬†al. [2023]</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.

</span>
<span class="ltx_bibblock">The dawn of lmms: Preliminary explorations with gpt-4v(ision).

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.17421</em>, Sep 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et¬†al. [2023]</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew¬†M Dai, Anja Hauth, et¬†al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic [2024]</span>
<span class="ltx_bibblock">
AI¬†Anthropic.

</span>
<span class="ltx_bibblock">The claude 3 model family: Opus, sonnet, haiku.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Claude-3 Model Card</em>, 2024.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et¬†al. [2024]</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et¬†al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et¬†al. [2023]</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu¬†Han, Fei Huang, et¬†al.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et¬†al. [2024]</span>
<span class="ltx_bibblock">
Haoyu Lu, Wen Liu, Bo¬†Zhang, Bingxuan Wang, Kai Dong, Bo¬†Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et¬†al.

</span>
<span class="ltx_bibblock">Deepseek-vl: towards real-world vision-language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.05525</em>, 2024.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young et¬†al. [2024]</span>
<span class="ltx_bibblock">
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge¬†Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et¬†al.

</span>
<span class="ltx_bibblock">Yi: Open foundation models by 01. ai.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.04652</em>, 2024.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et¬†al. [2023a]</span>
<span class="ltx_bibblock">
Hao Feng, Qi¬†Liu, Hao Liu, Wengang Zhou, Houqiang Li, and Can Huang.

</span>
<span class="ltx_bibblock">Docpedia: Unleashing the power of large multimodal model in the frequency domain for versatile document understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.11810</em>, 2023a.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et¬†al. [2023b]</span>
<span class="ltx_bibblock">
Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wengang Zhou, Houqiang Li, and Can Huang.

</span>
<span class="ltx_bibblock">Unidoc: A universal large multimodal model for simultaneous text detection, recognition, spotting and understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.11592</em>, 2023b.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et¬†al. [2024]</span>
<span class="ltx_bibblock">
Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo¬†Zhang, Chen Li, Ji¬†Zhang, Qin Jin, Fei Huang, et¬†al.

</span>
<span class="ltx_bibblock">mPLUG-DocOwl 1.5: Unified structure learning for ocr-free document understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.12895</em>, 2024.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. [2024a]</span>
<span class="ltx_bibblock">
Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai.

</span>
<span class="ltx_bibblock">Textmonkey: An ocr-free large multimodal model for understanding document.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.04473</em>, 2024a.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et¬†al. [2024]</span>
<span class="ltx_bibblock">
Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi¬†Liu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, et¬†al.

</span>
<span class="ltx_bibblock">Textsquare: Scaling up text-centric visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.12803</em>, 2024.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al. [2024]</span>
<span class="ltx_bibblock">
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et¬†al.

</span>
<span class="ltx_bibblock">How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.16821</em>, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et¬†al. [2024]</span>
<span class="ltx_bibblock">
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et¬†al.

</span>
<span class="ltx_bibblock">Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.06512</em>, 2024.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et¬†al. [2024]</span>
<span class="ltx_bibblock">
Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Mini-gemini: Mining the potential of multi-modality vision language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.18814</em>, 2024.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. [2024b]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, Bo¬†Li, Yuanhan Zhang, Sheng Shen, and Yong¬†Jae Lee.

</span>
<span class="ltx_bibblock">Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://llava-vl.github.io/blog/2024-01-30-llava-next/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://llava-vl.github.io/blog/2024-01-30-llava-next/</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biten et¬†al. [2019]</span>
<span class="ltx_bibblock">
Ali¬†Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar√ßal Rusinol, Ernest Valveny, CV¬†Jawahar, and Dimosthenis Karatzas.

</span>
<span class="ltx_bibblock">Scene text visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>, pages 4291‚Äì4301, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et¬†al. [2019]</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu¬†Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Towards vqa models that can read.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 8317‚Äì8326, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et¬†al. [2021]</span>
<span class="ltx_bibblock">
Minesh Mathew, Dimosthenis Karatzas, and CV¬†Jawahar.

</span>
<span class="ltx_bibblock">Docvqa: A dataset for vqa on document images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, pages 2200‚Äì2209, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et¬†al. [2024]</span>
<span class="ltx_bibblock">
Huy¬†Quang Pham, Thang Kien-Bao Nguyen, Quan Van¬†Nguyen, Dan¬†Quang Tran, Nghia¬†Hieu Nguyen, Kiet Van¬†Nguyen, and Ngan Luu-Thuy Nguyen.

</span>
<span class="ltx_bibblock">Viocrvqa: Novel benchmark dataset and vision reader for visual question answering by understanding vietnamese text in images.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.18397</em>, 2024.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et¬†al. [2019]</span>
<span class="ltx_bibblock">
Anand Mishra, Shashank Shekhar, Ajeet¬†Kumar Singh, and Anirban Chakraborty.

</span>
<span class="ltx_bibblock">Ocr-vqa: Visual question answering by reading text in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2019 international conference on document analysis and recognition (ICDAR)</em>, pages 947‚Äì952. IEEE, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et¬†al. [2022]</span>
<span class="ltx_bibblock">
Minesh Mathew, Viraj Bagal, Rub√®n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV¬†Jawahar.

</span>
<span class="ltx_bibblock">Infographicvqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pages 1697‚Äì1706, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et¬†al. [2022]</span>
<span class="ltx_bibblock">
Ahmed Masry, Xuan¬†Long Do, Jia¬†Qing Tan, Shafiq Joty, and Enamul Hoque.

</span>
<span class="ltx_bibblock">ChartQA: A benchmark for question answering about charts with visual and logical reasoning.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 2263‚Äì2279, Dublin, Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.findings-acl.177</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.findings-acl.177" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.findings-acl.177</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et¬†al. [2016]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li¬†Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 4995‚Äì5004, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et¬†al. [2017]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David¬†A Shamma, et¬†al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123:32‚Äì73, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et¬†al. [2015]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C¬†Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, pages 2425‚Äì2433, 2015.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et¬†al. [2019]</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/cvf conference on computer vision and pattern recognition</em>, pages 3195‚Äì3204, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et¬†al. [2021]</span>
<span class="ltx_bibblock">
Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Magana, Tristan Thrush, Wojciech Galuba, Devi Parikh, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Human-adversarial visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 34:20346‚Äì20359, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. [2024c]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong¬†Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 36, 2024c.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et¬†al. [2015]</span>
<span class="ltx_bibblock">
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu.

</span>
<span class="ltx_bibblock">Are you talking to a machine? dataset and methods for multilingual image question.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 28, 2015.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et¬†al. [2020]</span>
<span class="ltx_bibblock">
Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu¬†Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Large-scale adversarial training for vision-and-language representation learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:6616‚Äì6628, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. [2021]</span>
<span class="ltx_bibblock">
Fangyu Liu, Emanuele Bugliarello, Edoardo¬†Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott.

</span>
<span class="ltx_bibblock">Visually grounded reasoning across languages and cultures.

</span>
<span class="ltx_bibblock">In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 10467‚Äì10485, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.emnlp-main.818</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.emnlp-main.818" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.emnlp-main.818</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et¬†al. [2022]</span>
<span class="ltx_bibblock">
Le¬†Qi, Shangwen Lv, Hongyu Li, Jing Liu, Yu¬†Zhang, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ting Liu.

</span>
<span class="ltx_bibblock"><math id="bib.bib34.1.m1.1" class="ltx_Math" alttext="\textrm{DuReader}_{\textrm{vis}}" display="inline"><semantics id="bib.bib34.1.m1.1a"><msub id="bib.bib34.1.m1.1.1" xref="bib.bib34.1.m1.1.1.cmml"><mtext id="bib.bib34.1.m1.1.1.2" xref="bib.bib34.1.m1.1.1.2a.cmml">DuReader</mtext><mtext id="bib.bib34.1.m1.1.1.3" xref="bib.bib34.1.m1.1.1.3a.cmml">vis</mtext></msub><annotation-xml encoding="MathML-Content" id="bib.bib34.1.m1.1b"><apply id="bib.bib34.1.m1.1.1.cmml" xref="bib.bib34.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib34.1.m1.1.1.1.cmml" xref="bib.bib34.1.m1.1.1">subscript</csymbol><ci id="bib.bib34.1.m1.1.1.2a.cmml" xref="bib.bib34.1.m1.1.1.2"><mtext id="bib.bib34.1.m1.1.1.2.cmml" xref="bib.bib34.1.m1.1.1.2">DuReader</mtext></ci><ci id="bib.bib34.1.m1.1.1.3a.cmml" xref="bib.bib34.1.m1.1.1.3"><mtext mathsize="70%" id="bib.bib34.1.m1.1.1.3.cmml" xref="bib.bib34.1.m1.1.1.3">vis</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib34.1.m1.1c">\textrm{DuReader}_{\textrm{vis}}</annotation></semantics></math>: A Chinese dataset for open-domain document visual question answering.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, <em id="bib.bib34.2.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 1338‚Äì1351, Dublin, Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.findings-acl.105</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.findings-acl.105" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.findings-acl.105</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shimizu et¬†al. [2018]</span>
<span class="ltx_bibblock">
Nobuyuki Shimizu, Na¬†Rong, and Takashi Miyazaki.

</span>
<span class="ltx_bibblock">Visual question answering dataset for bilingual image understanding: A study of cross-lingual transfer using attention maps.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th International Conference on Computational Linguistics</em>, pages 1918‚Äì1928, 2018.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et¬†al. [2023]</span>
<span class="ltx_bibblock">
Ngan Luu-Thuy Nguyen, Nghia¬†Hieu Nguyen, Duong¬†TD Vo, Khanh¬†Quoc Tran, and Kiet Van¬†Nguyen.

</span>
<span class="ltx_bibblock">Vlsp2022-evjvqa challenge: Multilingual visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.11752</em>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raj¬†Khan et¬†al. [2021]</span>
<span class="ltx_bibblock">
Humair Raj¬†Khan, Deepak Gupta, and Asif Ekbal.

</span>
<span class="ltx_bibblock">Towards developing a multilingual and code-mixed visual question answering system by knowledge distillation.

</span>
<span class="ltx_bibblock">In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, pages 1753‚Äì1767, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.findings-emnlp.151</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.findings-emnlp.151" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.findings-emnlp.151</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et¬†al. [2022]</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, Jan-Martin Steitz, Stefan Roth, Ivan Vuliƒá, and Iryna Gurevych.

</span>
<span class="ltx_bibblock">xGQA: Cross-lingual visual question answering.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 2497‚Äì2511, Dublin, Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.findings-acl.196</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.findings-acl.196" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.findings-acl.196</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo et¬†al. [2023]</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Linting Xue, Michal Yarom, Ashish Thapliyal, Idan Szpektor, Julien Amelot, Xi¬†Chen, and Radu Soricut.

</span>
<span class="ltx_bibblock">MaXM: Towards multilingual visual question answering.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 2667‚Äì2682, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.findings-emnlp.176</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.findings-emnlp.176" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.findings-emnlp.176</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. [2023]</span>
<span class="ltx_bibblock">
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.

</span>
<span class="ltx_bibblock">Llavar: Enhanced visual instruction tuning for text-rich image understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.17107</em>, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et¬†al. [2023]</span>
<span class="ltx_bibblock">
Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, et¬†al.

</span>
<span class="ltx_bibblock">mPLUG-DocOwl: Modularized multimodal large language model for document understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv:2307.02499</em>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et¬†al. [2020]</span>
<span class="ltx_bibblock">
Deepak Gupta, Pabitra Lenka, Asif Ekbal, and Pushpak Bhattacharyya.

</span>
<span class="ltx_bibblock">A unified framework for multilingual and code-mixed visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</em>, pages 900‚Äì913, 2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vivoli et¬†al. [2022]</span>
<span class="ltx_bibblock">
Emanuele Vivoli, Ali¬†Furkan Biten, Andres Mafla, Dimosthenis Karatzas, and Lluis Gomez.

</span>
<span class="ltx_bibblock">Must-vqa: multilingual scene-text vqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 345‚Äì358. Springer, 2022.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et¬†al. [2023]</span>
<span class="ltx_bibblock">
Lin Li, Haohan Zhang, and Zeqin Fang.

</span>
<span class="ltx_bibblock">An empirical study of multilingual scene-text visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Workshop on User-centric Narrative Summarization of Long Videos</em>, pages 3‚Äì8, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nayef et¬†al. [2019]</span>
<span class="ltx_bibblock">
Nibal Nayef, Yash Patel, Michal Busta, Pinaki¬†Nath Chowdhury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Umapada Pal, Jean-Christophe Burie, Cheng-lin Liu, et¬†al.

</span>
<span class="ltx_bibblock">Icdar2019 robust reading challenge on multi-lingual scene text detection and recognition‚Äîrrc-mlt-2019.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">2019 International conference on document analysis and recognition (ICDAR)</em>, pages 1582‚Äì1587. IEEE, 2019.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al. [2023]</span>
<span class="ltx_bibblock">
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et¬†al.

</span>
<span class="ltx_bibblock">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.14238</em>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.11984" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.11985" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.11985">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.11985" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.11986" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 15:08:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
