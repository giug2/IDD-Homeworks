<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.10955] Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter</title><meta property="og:description" content="The Bengali language is the 5th most spoken native and 7th most spoken language in the world, and Bengali handwritten character recognition has attracted researchers for decades. However, other languages such as Englis…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.10955">

<!--Generated on Thu Sep  5 14:30:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
 Farhanul Haque, Md. Al-Hasan, 
<br class="ltx_break">Sumaiya Tabssum Mou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Department of Computer Science and Engineering</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">Bangladesh Army University of Science and Technology (BAUST)</span>, 
<br class="ltx_break">Saidpur Cantonment, Bangladesh 
<br class="ltx_break">
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abu Saleh Musa Miah*, Jungpil Shin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Department of Computer Science and Engineering</span>
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_italic">School of Computer Science and Engineering</span>
<br class="ltx_break"><span id="id5.3.id3" class="ltx_text ltx_font_italic">University of Aizu</span>, 
<br class="ltx_break">Aizuwakamatsu, Fukushima, Japan 
<br class="ltx_break">Email: abusalehcse.ru@gmail.com,jpshin@u-aizu.ac.jp

</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Md Abdur Rahim
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.1.id1" class="ltx_text ltx_font_italic">Department of Computer Science and Engineering</span>
<br class="ltx_break"><span id="id7.2.id2" class="ltx_text ltx_font_italic">Pabna University of Science and Technology</span>, Rajapur, Pabna, Bangladesh 
<br class="ltx_break">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p"><span id="id8.id1.1" class="ltx_text">The Bengali language is the 5th most spoken native and 7th most spoken language in the world, and Bengali handwritten character recognition has attracted researchers for decades. However, other languages such as English, Arabic, Turkey, and Chinese character recognition have contributed significantly to developing handwriting recognition systems. Still, little research has been done on Bengali character recognition because of the similarity of the character, curvature and other complexities. However, many researchers have used traditional machine learning and deep learning models to conduct Bengali hand-written recognition. The study employed a convolutional neural network (CNN) with ensemble transfer learning and a multichannel attention network. We generated the feature from the two branches of the CNN, including Inception Net and ResNet and then produced an ensemble feature fusion by concatenating them. After that, we applied the attention module to produce the contextual information from the ensemble features. Finally, we applied a classification module to refine the features and classification. We evaluated the proposed model using the CAMTERdb 3.1.2 data set and achieved 92% accuracy for the raw dataset and 98.00% for the preprocessed dataset. We believe that our contribution to the Bengali handwritten character recognition domain will be considered a great development.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
<span id="id9.id1" class="ltx_text">
Multihead self-attention, Bangla Hand Writing Detection, Image recognition, Image classification, Deep learning, Transfer Learnng
<br class="ltx_break"></span>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Automatic Handwritten Character Recognition has become a favourite research do-main to researchers in recent years for numerous applications, such as Optical character recognition (OCR). OCR is a formula to convert the picture of handwritten or picture of printed text into machine-encoded text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Many researchers proposed artificial intelligence techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> for recognizing various text in historical documents, printed documents, number plates of vehicles, business cards, passports, bank checks, handwritten characters etc. The main challenges of handwritten character recognition are the numerous writing fashion for a specific character and the different types of handwriting styles by various writers. In addition, there are a lot of characters which have similar patterns. Some of the complicated handwriting scripts recognize various ways of writing words. Handwritten character r recognition compares more intriguingly with typed character types.
Moreover, handwritten characters written by various writers are not identical but vary in characteristics such as size and shape. Numerous variations in the writing manner of the same characters make the recognition task more challenging. The overlaps and the interconnections of the nearest characters further complicate the task. The substantial variety of writing styles, writers, and the complicated features of the handwritten characters are very challenging for perfectly recognizing the handwritten characters.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Also, the character is not similar in the whole world, and according to a research report in 2020, there are around 7117 languages, and there have different types of character with different and similar style and fashions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Researchers have been working to recognize handwritten characters from different specific languages with specific letters or alphabets. Because of the language-specific interest of research, all languages did not get equivalence development for this case. Though some languages got huge contributions from many researchers, many languages did not make any significant contribution to recognising handwritten text or characters, and the Bengali language is one of them. The Bengali language is an Indo-Aryan language. It is the fifth most spoken native language and seventh most spoken language in the world, and around 228 used this language as their first language. In addition, approximately 37 million people worldwide use this language as their second language for speaking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Bengali handwritten character recognition is one of the most complex tasks because the number of characters in this language is not the same as the other language literature. The dataset for the handwritten character recognition of the Bangla language included 50 basic character’s but this number is not fixed like in another well-known language because of the compound characters. By including the compound characters of this language, it takes approximately 400 handwritten character’s which yielded so much complex work in the world <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> using the conventional feature and machine learning approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. However, deep learning-based models such as CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> transfer learning, attention-based model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> can be taken care of in more than 1000 classes datasets with complex neural networks. Still many difficulties in dealing with Bangla handwritten characters due to the complexity of cursive in shape, and especially compound characters, which make it complicated. Also, many researchers proposed different algorithms to recognize Bengali handwritten characters. Still, their performance accuracy and system efficiency are not satisfiable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> In addition, attention-based deep learning models are numerously used in various computer vision fields. Still, few are employed in Bengali handwritten recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To solve the efficiency and lower performance accuracy problem, we proposed an ensemble convolutional neural architecture through multi-channel attention in the study. In the processing, first, we take input of our images from the dataset, then we dived the dataset into a training and testing set. Then each of the sets, we employed a preprocessing model for augmentation and colour processing. Then we used the Ensemble module, where we included GoogLeNet and ResNet architecture to extract different features with two branches from the handwritten character dataset. The preprocessed image was fed separately to the two pre-trained models, which discovered and investigated the important feature from that. Then after concatenating the two-branch feature and producing the ensemble feature, we used a multichannel attention approach we employed here, which was used in the previous study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. This attention block sets the efficient and adaptable weight to the ensemble feature maps for choosing the more strong and potential features which can be fed into the classification module to improve performance accuracy. The main idea of the attention approach is to investigate the global context information of the images because of the diverse significance of the properties, and it can extract the class-specific important information. After that this attention-oriented new feature, finally, a classification module is used to refine the feature and classification purposes. The experimental evaluation demonstrated the proposed model boosted the performance accuracy by a significant margin.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Machine Learning and Deep Learning algorithm proved their excellency in various fields such as EEG-based various disease and event classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, vision-based gesture and human activity classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and disease detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
Based on the excellency many researchers have been working to develop a Handwriting recognition system using various statistical features incorporating machine learning and deep learning method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
In recognition of the Bangla character, many previous works have been developed for the Bangla digit, including ten digits. A few works are also available for Bangla handwritten character recognition, which contains 50 characters. Moshiur Rahman et el. ’s "Handwritten Bengali Character Recognition Through Geometry Based Feature Extraction" <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> their proposed method achieved an average recognition rate of 84.56% using SVM and 74.47% using ANN. "Bangla Hand-written Character Recognition: an overview of the state-of-the-art classification Algorithm with new dataset" <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> created by Md Nazmul Hoq et. el they have shown that the ANN has achieved 93.8% and Logistic Regression has achieved (86.8%) that outperform all other classification algorithms. Halima Begum et al., "Recognition of Handwritten Bangla Characters using Gabor Filter and Artificial Neural Network" <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> experimented with their own dataset that was the collaboration of 95 volunteers and their proposed technique achieved 68.9% without feature extraction and 79:4% with feature extraction.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">"Bangla Handwritten Char-acter Recognition using Convolutional Neural Network" <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> achieved test accuracy of 85.36% using their own dataset. In "Handwritten Bangla Basic and Com-pound character recognition using MLP and SVM classifier" <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, handwritten Bangla character recognition with MLP and SVM has been proposed, and they achieved around 79.73% and 80.9% of recognition rate, respectively. AKM Sha-harder et el. "Ekush: A Multipurpose and Multitype Comprehensive Database for Online Off-Line Bangla Handwritten Characters" <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> achieved 97.73% for the Ekush dataset. A sparse representation classifier is applied for Bangla digit recognition in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, where 94% of accuracy was achieved for handwritten digit recognition.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Dataset</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For our purpose, we used CAMTERdb 3.1.2. This dataset has 12000 images of 50 classes. Each class contain 240 BMP format 3-channel image. Most of the images have noise-free pixels, almost correct labelling, and no overwriting characters images.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2408.10955/assets/chapter1/figures/0_sample_image.jpg" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Images with respective class numbers.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">No matter which state in the dataset, we have to preprocess it to make it easier for the architecture to learn accurately <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Sample images of the usage dataset are shown in Figure <a href="#S3.F1" title="Figure 1 ‣ III Dataset ‣ Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Proposed Model</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this study, we proposed an ensemble convolutional neural architecture to recognise Bengali handwritten characters through multi-channel attention. Figure <a href="#S4.F2" title="Figure 2 ‣ IV Proposed Model ‣ Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrated the working flow graph of the proposed model. In the flowchart given that first, we take input of our images from the dataset, then we dived the dataset into a training and testing set. Then each of the sets we employed a pre-processing model where we inverted the color of every image to reduce the computation cost. Then those images were converted into grayscale format. The dataset was augmented by random rotation to fix black corners. Then we used the Ensemble module, which included GoogLeNet and ResNet architecture to extract different features with two branches from the handwritten character dataset. The preprocessed image was fed separately to the two pre-trained models, which discovered and investigated the important feature from that. Then after concatenating the two-branch feature and producing the ensemble feature, a multichannel attention approach we employed here, which is used in the previous study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2408.10955/assets/x1.jpg" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Working flow architecture</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">This attention block sets the efficient and adaptable weight to the ensemble feature maps for choosing the more strong and potential features which can be fed into the classification module to improve performance accuracy. The main idea of the attention approach is to investigate the global context information of the images because of the diverse significance of the properties, and it can extract the class-specific important information. After this attention-oriented new feature, finally, a classification module is used to refine the feature and classification purposes. The experimental evaluation demonstrated that the proposed model boosted the performance accuracy significantly. The workflow of the proposed model is demonstrated in Figure <a href="#S4.F4" title="Figure 4 ‣ IV-A Preprocessing Module ‣ IV Proposed Model ‣ Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Preprocessing Module</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Before extracting deep learning-based features, we employed a preprocessing module in this study. Figure <a href="#S4.F3" title="Figure 3 ‣ IV-A Preprocessing Module ‣ IV Proposed Model ‣ Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a) demonstrated the preprocessing module where first applied the inverted colours then applied to a converter to convert the RGB image into a grayscale image because the grey background contains only black and white, which reduces the computational complexity of the system.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2408.10955/assets/x2.jpg" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="221" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Preprocessing and classification module architecture</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">After that, we applied the augmentation technique for the rotation of the image into the different orientations, and finally, we fixed the unwanted black corner of the images. Figure <a href="#S4.F4" title="Figure 4 ‣ IV-A Preprocessing Module ‣ IV Proposed Model ‣ Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> visualized the sample output of the preprocessing images.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2408.10955/assets/chapter1/figures/0_1_preprocess_image.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="189" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Sample output after preprocessing</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Ensemble Module</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The study proposes an ensembled convolutional neural network with a channel attention model to classify the handwritten character in the Bengali language. In designing the architecture, two pre-trained CNN models we introduce here including GoogleNet and ResNet transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. We extract-ed separately from these two models with two branches and then concatenated them to produce the ensemble features described below.

<br class="ltx_break"></p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.4.1.1" class="ltx_text">IV-B</span>1 </span>GoogleNet Architecture</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Machine learning has offered a wide range of techniques and Application-based models and huge data sizes for processing and problem-solving. Deep learning, a subset of Machine Learning, is usually more complex. So, thousands of images are needed to get accurate results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
<br class="ltx_break">The GoogleNet architecture was proposed by researchers from Google in 2014. The research paper was titled "Going Deeper with Convolutions". This architecture was a winner at the ILSVRC 2014 classification of image challenge. It has shown a significant decrease in error rate if we compare it with to previous winner AlexNet. GoogleNet architecture uses techniques such as 1x1 convolutions in the middle of the architecture and also global average pooling. his architecture is different. Using 1x1 convolution as intermediate and global average pooling creates a deeper architecture. This architecture is 22 layers deep. Google Net uses 1x1 convolutions in architecture. The convolutions are used to decrease the architecture’s parameters (weights and biases). For a convolution when the filter number is 48 but the filter size is 5 x 5, the number of computations can be written as (14 x 14 x 48) x (5 x 5 x 480) = 112.9M. On the other side, for the 1x1 convolution with 16 filters a number of computations can be written as. (14 x 14 x 16) x (1 x 1 x 480) + (14 x 14 x 48) x (5 x 5 x 16) = 5.3M. The inception architecture in GoogleNet has some intermediate classifier branches in the middle of the architecture. These branches are used only while training. These branches consist of a 5x5 average pooling layer with a stride of 3, one 1x1 convolutions with 128 filters, two fully connected layers of 1024 outputs and 1000 outputs and a SoftMax classification layer.
The generated loss of these layers throughout training added to the total loss with a weight of 0.3. The purpose of these layers is they help in combating the gradient vanishing problem and also these layers provide regularization. The architectural details of the auxiliary classifier can be described in the following steps (a) A 1x1 convolution with 128 filters for dimension reduction and ReLU activation. (b) An average polling layer of filter size 55 and stride is 3 value. (c) Dropout regularisation with dropout ratio = 0.7. (d) A fully connected layer with 1025 outputs and ReLU activation

<br class="ltx_break"></p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.4.1.1" class="ltx_text">IV-B</span>2 </span>ResNet Architecture</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">ResNet comes from Residual Network which is one of the most efficient neural network architectures and is developed by researchers Aiming and his team <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. We are focusing on the network because it has the ability to train the architecture and achieve good performance in the computer vision-related research domain, such as object detection, segmentation, and classification. The objective is to develop of this system is to achieve efficiency to vanish the gradients using a deep neural network with minimum cost and time which can improve efficiency and performance. The main concept of the idea was to implement a technique to pass the information from one layer to another by flowing directly without fetching intermediate deep layers. To do this they used a shortcut connection to pass the information between any two layers. As a consequence, this idea helps the gradients to flow easily within the network yielding mitigate the gradient vanishing problems. We can write the working procedure of this transfer learning using some steps such as (a) to extract feature it passes the input data through a series of sequential convolutional layers. (b) The extracted feature is then passed through the multiple residual connections which consist of the multiple convolutional layer and a skip connection. (c) the skip connection is able to pass the feature to some of the CNN layers or send it to the output block. (d) The final skip connection passed the data through a global average pooling layer which averages the data and aims to convert the matrix feature into vector form which is known as a pooled feature. (e) The last pooled feature fed the data into a fully connected layer to produce the final feature which is used here as a concatenation vector. Bypassing the information through various deep layers allows for the training of a much deeper network compared to the previously existing systems yielding better efficiency and performance.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Channel Attention</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In the previous section, we discussed the two pre-trained models which we employed for achieving ensemble features which are concatenated and produced the final features. The pre-trained model also has strong pulling properties which vary from sample to sample dataset based on the several filters. Some research proved that sequentially adding the attention mechanism after the pre-trained model has a significant impact on the classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2408.10955/assets/x3.jpg" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="565" height="198" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Channel attention of the proposed model</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">In our study, we fed the concatenated feature into the channel attention which runs through the global average pooling and then produces the channel-wise feature amount. After that, each channel is coupled with two convolution layers with the employed ReLU activation approach to achieve a value considered positive or 0. The main emphasis of the output value is that the positive value indicates the significant or potential information and 0 indicates the unnoticeable feature or less significant channels. We also added two extra ReLu function which helps to reduce the time and complexity to vanish the gradient. Figure <a href="#S4.F5" title="Figure 5 ‣ IV-C Channel Attention ‣ IV Proposed Model ‣ Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrated the attention model which we utilized in the study where firstly we employed a global average pooling on the input feature which contained N channel. Then we employed a dense layer which has N/8 channels and fed into the batch normalization layer which can able to solve the covariate shift problems. It also contributes to preventing the gradient vanishing challenges from becoming too small. After that, we used the ReLU action function and fed that into another Dense Layer where the N layer was added through the reusing ReLU activation function. We are focusing on the ReLU because its computational complexity is less than the other activation like sigmoid functions. Once the neural architecture is too large or has many neurons then the significance of this activation function is easily understandable which can significantly reduce the time for training and testing times. In addition, it can also help us to faster converge into the fitting time.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Classification Module</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In the classification module, we employed different deep layers to refine the attention based feature and the classification. Figure <a href="#S4.F3" title="Figure 3 ‣ IV-A Preprocessing Module ‣ IV Proposed Model ‣ Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b) demonstrated the proposed classification module. We employed a global average pooling layer and then fed it into the dropout layer. After that, we applied a fully connected layer. Those fully connected layers contain the majority of parameters of many architectures that causes an increase in computation cost. But in our architecture, we first applied a method called global average pooling. This layer takes a feature map and averages it to 1x1. This also decreases the number of trainable parameters to 0 and improves the top-1 accuracy y 0.6
<br class="ltx_break"></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Performance Evaluation</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We evaluated the proposed model using a Bengali handwritten character dataset for investigating the superiority and effectiveness of the proposed model. In the below section firstly, we described the environmental setting of the project the performance accuracy.

<br class="ltx_break"></p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Environment Setting</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Our experimental environment was configured within Google Colaboratory. Cloud-based Collaboratory was a free Jupyter Notebook environment which required no extra setup or installs Python libraries. Each colab session was equipped with a virtual machine which provided 13GB of RAM either on a GPU, TPU or CPU processor. We have executed our experiment in a GPU processor.

<br class="ltx_break"></p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Experimental Results</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We observed our experiment in three parts. Table <a href="#S5.T1" title="Table I ‣ V-B Experimental Results ‣ V Performance Evaluation ‣ Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> demonstrated the performance of the proposed model with various experiments. We firstly experimented with the model with only GoogleNet which achieved 85.32% accuracy, in the same way with ResNet it achieved 87.32% accuracy. After that once we employed the ensemble including the attention and classification module it produced 98.00% accuracy.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table I: </span>Performance accuracy of the proposed model.</figcaption>
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">Dataset</th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">Model Name</th>
<th id="S5.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Accuracy [%]</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">CAMTERdb 3.1.2</td>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Proposed model with GoogleNet</td>
<td id="S5.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">85.32</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">CAMTERdb 3.1.2</td>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">Proposed model with ResNet</td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_left">87.32</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">CAMTERdb 3.1.2</td>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Proposed Ensemble Model</td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_bb">98.00</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">State of the Art Comparison</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Table <a href="#S5.T2" title="Table II ‣ V-C State of the Art Comparison ‣ V Performance Evaluation ‣ Multichannel Attention Networks with Ensembled Transfer Learning to Recognize Bangla Handwritten Charecter" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> demonstrated the state-of-the-art comparison of the proposed model. The table showed that our proposed model achieved 98.00% accuracy, which is the best performance compared to all state-of-the art study those are mentioned here. In the previous study Halima Begum et al extracted features using a Gabor filter from the handwriting image then they applied an artificial neural network for the classification where they achieved 79.40% accuracy with it  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Das et al. extracted geometric features from the Bangla handwriting image and achieved 76.86% accuracy with a machine learning algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Rahman et al. applied the CNN method for the feature extraction and the classification for the Bangla handwriting recognition and achieved 85.36% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Islam et al. extracted effective features using the modified syntactic model and achieved 95.00% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. The high-performance accuracy of the proposed model proved its superiority compared to the existing Bangla hand writing recognition system.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table II: </span>Previous work with different methods</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">Author Name</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">Method</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Accuracy [%]</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<td id="S5.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Halima Begum et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Gabor Filter and ANN</td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">79.40%</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<td id="S5.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">Nibaran Das et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">Convex Hull Basic Feature (CHBF)</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_left">76.86%</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<td id="S5.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">Rahman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">Convolutional Neural Network (CNN)</td>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_left">85.36%</td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<td id="S5.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">Islam et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S5.T2.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r">Modified syntactic method</td>
<td id="S5.T2.1.5.4.3" class="ltx_td ltx_align_left">95.00%</td>
</tr>
<tr id="S5.T2.1.6.5" class="ltx_tr">
<td id="S5.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Proposed Model</td>
<td id="S5.T2.1.6.5.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">MANETL</td>
<td id="S5.T2.1.6.5.3" class="ltx_td ltx_align_left ltx_border_bb">98.00%</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We have recognized Bangla’s handwritten characters using Multichannel Attention and Ensemble feature fusion architecture. In the processing we first applied a preprocessing model then we applied a two-pretrained model to extract separate features which we concatenated to produce the ensemble feature. Then we applied a channel attention model to highlight the effective feature and finally, we applied a classification module to classify and make a pre-trained model. Performance accuracy reported in the accuracy table proved the efficiency and effectiveness of the proposed model. For our future plan, we’ll implement this to get real-time traffic camera recognition. We can also extend our work to the Bangla digit and Bangla joint alphabet. Finally, our work can be extended to recognise Bangla handwritten text such as handwritten letters, applications, exam scripts, and different forms also for a complete OCR system.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Halima Begum and Muhammed Mazharul Islam.

</span>
<span class="ltx_bibblock">Recognition of handwritten bangla characters using gabor filter and artificial neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Int. J. Comput. Technol. Appl</span>, 8(5):618–621, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Nibaran Das, Kallol Acharya, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, and Mita Nasipuri.

</span>
<span class="ltx_bibblock">A benchmark image database of isolated bangla handwritten compound characters.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">International Journal on Document Analysis and Recognition (IJDAR)</span>, 17:413–431, 2014.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, and Mita Nasipuri.

</span>
<span class="ltx_bibblock">Handwritten bangla basic and compound character recognition using mlp and svm classifier.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1002.4040</span>, 2010.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, and Mita Nasipuri.

</span>
<span class="ltx_bibblock">Recognition of handwritten bangla basic characters and digits using convex hull based feature set.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1410.0478</span>, 2014.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Nibaran Das, Ram Sarkar, Subhadip Basu, Punam K Saha, Mahantapas Kundu, and Mita Nasipuri.

</span>
<span class="ltx_bibblock">Handwritten bangla character recognition using a soft computing paradigm embedded in two pass approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Pattern Recognition</span>, 48(6):2054–2071, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bangladesh Dinajpur.

</span>
<span class="ltx_bibblock">Handwritten bengali character recognition through geometry based feature extraction.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Journal of Theoretical and Applied Information Technology</span>, 97(23), 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
David M Eberhard, Gary Francis Simons, and Charles D Fenning.

</span>
<span class="ltx_bibblock">Ethnologue: Languages of the world, 2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Koki Hirooka, Md Al Mehedi Hasan, Jungpil Shin, and Azmain Yakin Srizon.

</span>
<span class="ltx_bibblock">Ensembled transfer learning based multichannel attention networks for human activity recognition in still images.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 10:47051–47062, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Md Nazmul Hoq, Nadira Anjum Nipa, Mohammad Mohaiminul Islam, and Sadat Shahriar.

</span>
<span class="ltx_bibblock">Bangla handwritten character recognition: an overview of the state of the art classification algorithm with new dataset.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">2019 1st International Conference on Advances in Science, Engineering and Robotics Technology (ICASERT)</span>, pages 1–6. IEEE, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Md Moazzem Hossain, Md Ali Hossain, Abu Saleh Musa Miah, Yuichi Okuyama, Yoichi Tomioka, and Jungpil Shin.

</span>
<span class="ltx_bibblock">Stochastic neighbor embedding feature-based hyperspectral image classification using 3d convolutional neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Electronics</span>, 12(9):2082, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Mohammad Badiul Islam, Mollah Masum Billah Azadi, Md Abdur Rahman, and MMA Hashem.

</span>
<span class="ltx_bibblock">Bengali handwritten character recognition using modified syntactic method.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">2nd National Conference on Computer Processing of Bangla (NCCPB-2005)</span>, 2005.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S Jian, H Kaiming, R Shaoqing, and Z Xiangyu.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision &amp; Pattern Recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Md Mahmudul Haque Joy, Mohammad Hasan, Abu Saleh Musa Miah, Abir Ahmed, Sadia Anwar Tohfa, Md Farukul Islam Bhuaiyan, Ashrafun Zannat, and Md Mamunur Rashid.

</span>
<span class="ltx_bibblock">Multiclass mi-task classification using logistic regression and filter bank common spatial patterns.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">International Conference on Computing Science, Communication and Security</span>, pages 160–170. Springer, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Md Humaun Kabir, Shabbir Mahmood, Abdullah Al Shiam, Abu Saleh Musa Miah, Jungpil Shin, and Md Khademul Islam Molla.

</span>
<span class="ltx_bibblock">Investigating feature selection techniques to enhance the performance of eeg-based motor imagery tasks classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Mathematics</span>, 11(8):1921, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Hasan Muhammad Kafi, Abu Saleh Musa Miah, Jungpil Shin, and Md Nahid Siddique.

</span>
<span class="ltx_bibblock">A lite-weight clinical features based chronic kidney disease diagnosis system using 1d convolutional neural network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">2022 International Conference on Advancement in Electrical and Electronic Engineering (ICAEEE)</span>, pages 1–5. IEEE, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Haider Adnan Khan, Abdullah Al Helal, and Khawza I Ahmed.

</span>
<span class="ltx_bibblock">Handwritten bangla digit recognition using sparse representation classifier.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">2014 International Conference on Informatics, Electronics &amp; Vision (ICIEV)</span>, pages 1–6. IEEE, 2014.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kazi Atai Kibria, Asswad Sarker Noman, Md Abir Hossain, Md Shohidul Islam Bulbul, Md Mamunur Rashid, and Abu Saleh Musa Miah.

</span>
<span class="ltx_bibblock">Creation of a cost-efficient and effective personal assistant robot using arduino &amp; machine learning algorithm.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">2020 IEEE Region 10 Symposium (TENSYMP)</span>, pages 477–482. IEEE, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich.

</span>
<span class="ltx_bibblock">Fractalnet: Ultra-deep neural networks without residuals.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1605.07648</span>, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Saadaldeen Rashid Ahmed Ahmed, Mohammed Rashid Ahmed, Oğuz Bayat, Adil Deniz Duru, and Md Khademul Islam Molla.

</span>
<span class="ltx_bibblock">Motor-imagery bci task classification using riemannian geometry and averaging with mean absolute deviation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">2019 Scientific Meeting on Electrical-Electronics &amp; Biomedical Engineering and Computer Science (EBBT)</span>, pages 1–7. Ieee, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Md Al Mehedi Hasan, Si-Woong Jang, Hyoun-Sup Lee, and Jungpil Shin.

</span>
<span class="ltx_bibblock">Multi-stream general and graph-based deep neural networks for skeleton-based sign language recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Electronics</span>, 12(13):2841, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Md Al Mehedi Hasan, and Jungpil Shin.

</span>
<span class="ltx_bibblock">Dynamic hand gesture recognition using multi-branch attention based graph and general deep learning model.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 11:4703–4716, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Md Al Mehedi Hasan, Jungpil Shin, Yuichi Okuyama, and Yoichi Tomioka.

</span>
<span class="ltx_bibblock">Multistage spatial attention-based neural network for hand gesture recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Computers</span>, 12(1):13, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Md Rabiul Islam, and Md Khademul Islam Molla.

</span>
<span class="ltx_bibblock">Motor imagery classification using subband tangent space mapping.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">2017 20th International Conference of Computer and Information Technology (ICCIT)</span>, pages 1–5. IEEE, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Md Rabiul Islam, and Md Khademul Islam Molla.

</span>
<span class="ltx_bibblock">Eeg classification for mi-bci using csp with averaging covariance matrices: An experimental study.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">2019 International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering (IC4ME2)</span>, pages 1–5. IEEE, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Md Mamunur Rashid, Md Redwanur Rahman, Md Tofayel Hossain, Md Shahidujjaman Sujon, Nafisa Nawal, Mohammad Hasan, and Jungpil Shin.

</span>
<span class="ltx_bibblock">Alzheimer’s disease detection using cnn based on effective dimensionality reduction approach.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Intelligent Computing and Optimization: Proceedings of the 3rd International Conference on Intelligent Computing and Optimization 2020 (ICO 2020)</span>, pages 801–811. Springer, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Mumtahina Afroz Mouly, Chandrika Debnath, Jungpil Shin, and SM Sadakatul Bari.

</span>
<span class="ltx_bibblock">Event-related potential classification based on eeg data using xdwan with mdm and knn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">International Conference on Computing Science, Communication and Security</span>, pages 112–126. Springer, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Md Abdur Rahim, and Jungpil Shin.

</span>
<span class="ltx_bibblock">Motor-imagery classification using riemannian geometry with median absolute deviation.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Electronics</span>, 9(10):1584, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Jungpil Shin, Md Al Mehedi Hasan, Md Abdur Rahim, and Yuichi Okuyama.

</span>
<span class="ltx_bibblock">Rotation, translation and scale invariant sign word recognition using deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Computer Systems Science &amp; Engineering</span>, 44(3), 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Jungpil Shin, Md Al Mehedi Hasan, Md Khademul Islam Molla, Yuichi Okuyama, and Yoichi Tomioka.

</span>
<span class="ltx_bibblock">Movie oriented positive negative emotion classification from eeg signal using wavelet transformation and machine learning approaches.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">2022 IEEE 15th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)</span>, pages 26–31. IEEE, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Jungpil Shin, Md Al Mehedi Hasan, and Md Abdur Rahim.

</span>
<span class="ltx_bibblock">Bensignnet: Bengali sign language alphabet recognition using concatenated segmentation and convolutional neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 12(8):3933, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Abu Saleh Musa Miah, Jungpil Shin, Md Minhajul Islam, Md Khademul Islam Molla, et al.

</span>
<span class="ltx_bibblock">Natural human emotion recognition based on various mixed reality (mr) games and electroencephalography (eeg) signals.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">2022 IEEE 5th Eurasian Conference on Educational Innovation (ECEI)</span>, pages 408–411. IEEE, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
AKM Shahariar Azad Rabby, Sadeka Haque, Md Sanzidul Islam, Sheikh Abujar, and Syed Akhter Hossain.

</span>
<span class="ltx_bibblock">Ekush: A multipurpose and multitype comprehensive database for online off-line bangla handwritten characters.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Recent Trends in Image Processing and Pattern Recognition: Second International Conference, RTIP2R 2018, Solapur, India, December 21–22, 2018, Revised Selected Papers, Part III 2</span>, pages 149–158. Springer, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Md Abdur Rahim, Abu Saleh Musa Miah, Abu Sayeed, and Jungpil Shin.

</span>
<span class="ltx_bibblock">Hand gesture recognition based on optimal segmentation in human-computer interaction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">2020 3rd IEEE International Conference on Knowledge Innovation and Invention (ICKII)</span>, pages 163–166. IEEE, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Md Mahbubar Rahman, MAH Akhand, Shahidul Islam, Pintu Chandra Shill, MH Rahman, et al.

</span>
<span class="ltx_bibblock">Bangla handwritten character recognition using convolutional neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">International Journal of Image, Graphics and Signal Processing</span>, 7(8):42–49, 2015.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Abu Sayeed, Jungpil Shin, Md Al Mehedi Hasan, Azmain Yakin Srizon, and Md Mehedi Hasan.

</span>
<span class="ltx_bibblock">Bengalinet: A low-cost novel convolutional neural network for bengali handwritten characters recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 11(15):6845, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Jungpil Shin, Abu Saleh Musa Miah, Md Al Mehedi Hasan, Koki Hirooka, Kota Suzuki, Hyoun-Sup Lee, and Si-Woong Jang.

</span>
<span class="ltx_bibblock">Korean sign language recognition using transformer-based deep neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 13(5):3029, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1556</span>, 2014.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 1–9, 2015.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Shiyang Yan, Jeremy S Smith, Wenjin Lu, and Bailing Zhang.

</span>
<span class="ltx_bibblock">Multibranch attention networks for action recognition in still images.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Cognitive and Developmental Systems</span>, 10(4):1116–1125, 2017.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Talha Zobaed, Saadaldeen Rashid Ahmed Ahmed, Abu Saleh Musa Miah, Salma Masuda Binta, Mohammed Rashid Ahmed Ahmed, and Mamunur Rashid.

</span>
<span class="ltx_bibblock">Real time sleep onset detection from single channel eeg signal using block sample entropy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">IOP Conference Series: Materials Science and Engineering</span>, volume 928, page 032021. IOP Publishing, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.10954" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.10955" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.10955">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.10955" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.10956" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 14:30:36 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
