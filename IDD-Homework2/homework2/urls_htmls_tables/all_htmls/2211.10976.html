<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.10976] Federated deep transfer learning for EEG decoding using multiple BCI tasks Preprint accepted by 2023 IEEE NER [©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.] Address for correspondence: aldo.faisal@imperial.ac.uk We acknowledge funding from UKRI Turing AI Fellowship to AAF (EP/V025449/1).</title><meta property="og:description" content="Deep learning is the state-of-the-art in BCI decoding. However, it is very data-hungry and training decoders requires pooling data from multiple sources. EEG data from various sources decrease the decoding performance …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated deep transfer learning for EEG decoding using multiple BCI tasks Preprint accepted by 2023 IEEE NER [©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.] Address for correspondence: aldo.faisal@imperial.ac.uk We acknowledge funding from UKRI Turing AI Fellowship to AAF (EP/V025449/1).">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated deep transfer learning for EEG decoding using multiple BCI tasks Preprint accepted by 2023 IEEE NER [©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.] Address for correspondence: aldo.faisal@imperial.ac.uk We acknowledge funding from UKRI Turing AI Fellowship to AAF (EP/V025449/1).">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.10976">

<!--Generated on Tue Feb 27 01:44:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Deep Learning,  Transfer Learning,  Domain Adaptation,  Brain-Computer-Interfaces (BCI),  Electroencephalography (EEG),  Privacy-preserving AI,  Federated Machine Learning
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated deep transfer learning for EEG decoding using multiple BCI tasks 
<br class="ltx_break"><span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>Preprint accepted by 2023 IEEE NER [©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.] Address for correspondence: aldo.faisal@imperial.ac.uk We acknowledge funding from UKRI Turing AI Fellowship to AAF (EP/V025449/1).</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoxi Wei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_italic">Dept. of Computing, Imperial College London</span>
<br class="ltx_break"><span id="id3.2.id2" class="ltx_text ltx_font_italic">Brain &amp; Behaviour Lab
<br class="ltx_break"></span>London, UK
<br class="ltx_break">xiaoxi.wei18@imperial.ac.uk
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">A. Aldo Faisal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_font_italic">Brain &amp; Behaviour Lab, Imperial College London
<br class="ltx_break"></span>London, UK
<br class="ltx_break"><span id="id5.2.id2" class="ltx_text ltx_font_italic">Chair in Digital Health &amp; Data Science, University of Bayreuth</span>
<br class="ltx_break">Bayreuth, Germany
<br class="ltx_break">aldo.faisal@imperial.ac.uk
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Deep learning is the state-of-the-art in BCI decoding. However, it is very data-hungry and training decoders requires pooling data from multiple sources. EEG data from various sources decrease the decoding performance due to negative transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Recently, transfer learning for EEG decoding has been suggested as a remedy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and become subject to recent BCI competitions (e.g. BEETL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>), but there are two complications in combining data from many subjects. First, privacy is not protected as highly personal brain data needs to be shared (and copied across increasingly tight information governance boundaries). Moreover, BCI data are collected from different sources and are often with different BCI tasks, which has been thought to limit their reusability. Here, we demonstrate a federated deep transfer learning technique, the Multi-dataset Federated Separate-Common-Separate Network (MF-SCSN) based on our previous work of SCSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which integrates privacy-preserving properties into deep transfer learning to utilise data sets with different tasks. This framework trains a BCI decoder using different source data sets from different imagery tasks (e.g. some data sets with hands and feet, vs others with single hands and tongue, etc). Therefore, by introducing privacy-preserving transfer learning techniques, we unlock the reusability and scalability of existing BCI data sets. We evaluated our federated transfer learning method on the NeurIPS 2021 BEETL competition BCI task. The proposed architecture outperformed the baseline decoder by 3%. Moreover, compared with the baseline and other transfer learning algorithms, our method protects the privacy of the brain data from different data centres.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Deep Learning, Transfer Learning, Domain Adaptation, Brain-Computer-Interfaces (BCI), Electroencephalography (EEG), Privacy-preserving AI, Federated Machine Learning

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep Learning based EEG decoding has become a standard in BCI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and unlocked state-of-the-art machine learning ideas to benefit neural engineering research. Deep learning approaches are usually data-hungry. Some previous studies deal with the lack of available EEG data by data-efficient approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. In recent years, the development of transfer learning in EEG decoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> enables algorithms to learn more from combining different EEG data sets. However, most methods focus on only a single data set with a unified experiment setup or task. The scale of EEG data sets is usually limited to dozens of subjects, unlike biomedical data sets with thousands, due to the difficulty and cost of EEG data collection. The international BEETL EEG competition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> held at NeurIPS 2021 focused on cross-dataset EEG transfer learning and brought academic attention to utilising many EEG data sets across tasks for transfer learning with around 30 international competing teams. Several successful algorithms were proposed to tackle the BEETL challenge, which has provided fundamental design principles for cross-dataset and cross-task EEG transfer learning. With examples showing that heterogeneous EEG data sets from different data centres and sources could be utilised for large-scale machine learning algorithms, EEG data sharing privacy becomes the next concern. Brainwaves contain rich privacy information that could be potentially decoded, e.g. words and identities. Moreover, health data sharing across sites or countries is under strict legal governance restrictions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">There were some strategies for privacy-preserving in the machine learning literature. Federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> trains models on edge servers without exchanging the data. Data encryption methods encrypt raw data or parameters of the model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. This requires an encoding-decoding procedure which introduces extra computational cost. Users also need to decrypt the data following a certain protocol given by the algorithm provider, in which case the protocol could be potentially hacked. Similarly, transformation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> adds cancelable noise to local gradients or parameters before uploading them to a central server. Methods of model splitting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> are based on allocating different parameters to different data sets, thus protecting the model privacy of each individual. Multi-party computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> trains models locally first, and then aggregation is done securely by a third party. However, all these methods are either unproven or unsuitable for EEG decoding, e.g. adding cancellable noise is problematic to the low signal-to-noise ratio of EEG. Moreover, they add computational burden, e.g. due to encryption, which further disadvantages training on large-scale EEG data.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2211.10976/assets/fig1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1017" height="481" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The figure illustrates the differences between conventional deep transfer learning and privacy-preserving deep transfer learning with federated models.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Privacy-preserving is drawing increasing attention in EEG decoding with the development of more accurate human intention decoders. There are a few studies in the EEG literature on privacy-preserving based on the above methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, or based on better protocol or user level system design <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Our previous work and some other recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> utilised distributed feature extractors to deal with individual EEG differences while maintaining private information without extra cost for encryption. However, both studies can not handle learning from different tasks and protect inference-level privacy, thus limiting the use of large-scale data. Therefore, it is still a challenge for cross-data-centre transfer learning with different tasks in a privacy-preserving way.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this study, we propose an architecture to combine privacy-preserving machine learning with deep transfer learning. The Multi-dataset Federated Separate-Common-Separate Network (MF-SCSN) integrates privacy-preserving properties into deep transfer learning EEG decoding on multiple tasks.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Method Development</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">A benchmark architecture, the shallow ConvNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, is used as the baseline model. The network includes a temporal layer to extract time-scale information, followed by a spatial layer to extract cross-channel features. Square non-linearity, average pooling and log non-linearity are then performed.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In this study, a privacy-preserving cross-dataset deep transfer learning architecture, the MF-SCSN, is proposed based on our previous study on inter-subject deep transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The variability of EEG comes from several aspects. There are superficial variabilities like sensor locations, sensor impedance and devices. These differences could be handled in shallow layers of a transfer learning network. For the intrinsic variability of individual brains, functionalities could be potentially learnt and handled more precisely in deeper layers.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The MF-SCSN architecture consists of three main components (see Fig. <a href="#S2.F2" title="Figure 2 ‣ II Method Development ‣ Federated deep transfer learning for EEG decoding using multiple BCI tasks Preprint accepted by 2023 IEEE NER [©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.] Address for correspondence: aldo.faisal@imperial.ac.uk We acknowledge funding from UKRI Turing AI Fellowship to AAF (EP/V025449/1)." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) it separates into both shallow layers and deeper layers to handle the variabilities while performing a joint feature extractor to learn common transferable knowledge across data sets.
The first set of components is the local branches (left side of the figure) as both feature extractors and ‘keys’ for data encryption. The shallow ConvNet above is used here as a feature extractor. Raw data from different data centres is encrypted into EEG features through the local branches. Local servers of data centres conduct the computation of feature extraction, and parameters (keys) are stored locally. In this way, the proposed architecture preserves both data-level privacy and parameter-level privacy.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The second component of the MF-SCSN is a common transfer network located in a cloud server. This is where transfer learning across different data sets happens. Encrypted features are received from data centres in the cloud for common feature extraction. Previous local feature extractors handle variabilities in data set distributions. The design purpose of common layers is to find a common distribution to which different data sets and distributions could transfer to. Unlike other encryption methods for privacy-preserving, the MF-SCSN has no encryption costs since the federated feature extractors encrypt data automatically. Moreover, it does not have a ‘decryption’ procedure because the encrypted features are exactly the inputs required by the common transfer layer. Therefore, the cloud server does not need to know any information about the local feature extractors (the ‘keys’). This further increases the parameter-level security of the model.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Finally, the transferred features are delivered to the third component of MF-SCSN, i.e. the deep separate layers and heterogeneous classifiers. Its design is motivated by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> showing that combining different classifiers for cross-dataset transfer learning can help overcome label inconsistency. In light of this, the third set of components contains some separate layers to deal with further differences across data sets, followed by local classifiers specified for different tasks. Besides the benefit of handling label inconsistency, predictions and labels are preserved locally in this way.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2211.10976/assets/fig2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="536" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Multi-dataset Federated Separate-Common-Separate Network (MF-SCSN). Raw data, feature extractors (the encryption keys), labels and predictions are all preserved locally.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Evaluation Method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The proposed architecture was tested on the motor imagery task of the BEETL competition. Details of the data can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. We used three source data sets with very different devices and data collection protocols.
In general, we curated the data, by choosing subsets of subjects from each data set that had higher data quality, narrowly defined as embedding better discriminative features. Therefore, data quality per subject was determined from classification performance (independent of our work), as they were reported by previous studies or our replications of those studies.
BCICIV2a <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> contains nine subjects performing left-hand, right-hand, feet and tongue motor imagery. Subject 1,3,7,8 and 9 were selected as sources.
Cho2017 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> has 52 subjects performing left and right-hand motor imagery, all subjects but 32,46 and 49 were used.
PhysioMI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> had 109 subjects, we selected as sources subject 1,7,17,24,28,31,33,34,35,42,49,52,
54,55,56,60,62,63,68,71,72,73,85,91,93,94,103.
Test sets consist of 5 subjects with 200 trials each (1000 trials in total). Labels are left-hand, right-hand and feet motor imagery and rest state (250 trials each).</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">All final accuracies are reported on the weighted accuracy of the three classes - left-hand (LH), right-hand (RH) and ‘other’. As described, source data sets and the target set have different classification tasks. Some data sets shared left/right hand or feet motor imagery as the target data sets, others have tongue and rest. Data with different labels have various distributions, which introduces extra complexity for transfer learning.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">To align and make use of all data sets, we extracted 3 seconds windows for all trials, because Cho2017 had a maximum trial length of 3 seconds. Note that this may not utilise the full potential of the 4-second target test data. 17 common EEG channels of the four data sets were selected (Fz, FC1, FC2, C1–C6, CP3, CP1, CPz, CP2, CP4, P1, P2, Pz). All trials are down-sampled to 200Hz. A 5th-order bandpass filter was applied between 4Hz and 32Hz, where motor imagery usually occurs. Normalisation across channels is done, followed by temporal normalisation across time steps.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.3" class="ltx_p">Shallow ConvNet was used as the feature extractor. For both the baseline model(Shallow ConvNet) and the MF-SCSN, the feature size was aligned to 50 after the feature extractor. The common cloud network consists of three fully-connected layers with a feature size of 50 each. Before the classifiers, the separate layers consist of three fully-connected layers with a feature size of 50. All four data sets use individual classifiers according to their own tasks. During training, the target classier kept the original four classes. After prediction, feet and rest labels were combined as ‘other’ to report the final accuracy. For all four data sets, trial numbers are balanced to 2880 trials (the size of BCIC sources). To balance training sizes, a random sampling was done for Cho2017 (originally 9880 trials). Similarly, we augment PhysioMI (2399 trials) and target data set to 2880 trials. We used batch size <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.p4.1.m1.1a"><mn id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><cn type="integer" id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">10</annotation></semantics></math>, learning rate <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S3.p4.2.m2.1a"><mn id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><cn type="float" id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">0.001</annotation></semantics></math> and weight decay factor <math id="S3.p4.3.m3.1" class="ltx_Math" alttext="0.0005" display="inline"><semantics id="S3.p4.3.m3.1a"><mn id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml">0.0005</mn><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><cn type="float" id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1">0.0005</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">0.0005</annotation></semantics></math> both shallow CovNet baseline and MF-SCSN. During the training of MF-SCSN, four batches of size 10 from branches (40 in total) were delivered to the cloud layers simultaneously and distributed back to each separate local branch after common feature extraction. In the target training set, subject 1-3 has 100 trials each. Subject 4-5 has 120 trials each. 20 trials each were used as the validation set for model selection. All randomization and initialization were conducted with a typical random seed for machine learning of 42 for reproducibility in the same setup and environment.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We tested the baseline shallow ConvNet and the MF-SCSN on the BEETL motor imagery task. Five subjects from two data sets were tested. The first three subjects (S1 S2 S3) are from the CybathlonIC data set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> collected in an online closed-loop format. The latter two subjects are from the Weibo2014 data set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Weibo2014 uses instructions on the screen to inform subjects to perform offline motor imagery without feedback and real-time control. The CybathlonIC used the Cybathlon 2020 BCI game for data collection. The intention of controlling the virtual car in a real-world setup with real-time feedback and interference made the brain signal more complex and noisier to decode.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2211.10976/assets/fig3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Label-weighted decoding accuracy of subjects, their mean and standard error of the mean (SEM shown as error bar).</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In light of the stated differences between the above two data sets, we trained them respectively instead of pooling them together as the same target set. Prediction results were reported based on their own models. In our previous work, a significant accuracy drop could be observed by simply adding more subjects to train the ShallowConvNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Therefore, here we used only the target subjects to train the ShallowConvNet as the baseline. For the MF-SCSN, S1-S3 are regarded as the target set and trained together with the source branches. Similarly, we trained another model combining S4 and S5. As shown in figure <a href="#S4.F3" title="Figure 3 ‣ IV Results ‣ Federated deep transfer learning for EEG decoding using multiple BCI tasks Preprint accepted by 2023 IEEE NER [©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.] Address for correspondence: aldo.faisal@imperial.ac.uk We acknowledge funding from UKRI Turing AI Fellowship to AAF (EP/V025449/1)." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the decoding accuracies of S1-S3 are significantly lower than S4 and S5 in both the baseline method and MF-SCSN. This reflects the challenge of real-world BCI decoding and the variance of the two data sets.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The main observation of this study is that, as in figure <a href="#S4.F3" title="Figure 3 ‣ IV Results ‣ Federated deep transfer learning for EEG decoding using multiple BCI tasks Preprint accepted by 2023 IEEE NER [©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.] Address for correspondence: aldo.faisal@imperial.ac.uk We acknowledge funding from UKRI Turing AI Fellowship to AAF (EP/V025449/1)." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the MF-SCSN outperformed the baseline methods. Among the 1000 trials (200 trials per subject), the MF-SCSN correctly classified 555 samples compared with 518 samples by the shallow ConvNet. In the 1000 testing samples, there are 250 left-hand/right-hand trials and 500 trials labelled as ‘others’ (the feet and rest). Therefore, by giving half weight to ‘other’ trials, weighted average decoding accuracies are computed for both methods. As shown in the last column of figure <a href="#S4.F3" title="Figure 3 ‣ IV Results ‣ Federated deep transfer learning for EEG decoding using multiple BCI tasks Preprint accepted by 2023 IEEE NER [©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.] Address for correspondence: aldo.faisal@imperial.ac.uk We acknowledge funding from UKRI Turing AI Fellowship to AAF (EP/V025449/1)." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the baseline yielded a decoding accuracy of 48.2%, and MF-SCSN outperformed the baseline with an accuracy of 51.2%.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Our MF-SCSN architecture outperformed the existing methods which we used as a baseline. This indicates that MF-SCSN has the potential to utilise cross-dataset federated features from different tasks to increase EEG decoding performance. We also highlight five privacy-preserving properties of the MF-SCSN.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">First, subjects and dataset-specific information are stored locally with data owners, which preserves data-level privacy. To preserve parameter-level privacy, feature extractors are stored locally. Another property is that local feature extractors encrypt raw data naturally, so there is no extra encryption cost. Additionally, there is no need for a protocol of decryption, because the cloud network only uses the encrypted features for transfer learning. Finally, labels and classifiers are stored and predicted locally. This also preserves inference-level privacy.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">One limitation of this study is that some parts of the source and target data sets were discarded to align the input shape, e.g. the window length and channels. Further experiments should be conducted on transfer learning with different input shapes. A potential solution based on MF-SCSN could be exploring if the local feature extractors with different kernels could handle inputs of different shapes, by unifying output shapes of the features across branches. Another direction could be exploring the flexibility of MF-SCSN as a meta-architecture by changing the feature extractors to other models.
</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">To conclude, we have designed a cross-dataset federated deep transfer learning technique which combines privacy-preserving properties and deep transfer learning. Results show that the proposed method, with the advantage of both transfer learning and privacy-preserving, outperformed the baseline CNN. Our proposed method shows the potential to utilise larger heterogeneous data sets with different tasks for transfer learning while possessing better properties of privacy-preserving across data sets and data centres.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
X. Wei, P. Ortega, and A. A. Faisal, “Inter-subject deep transfer learning for
motor imagery eeg decoding,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE/EMBS Neural Eng. (NER)</em>,
vol. 10.   IEEE, 2021, pp. 21–24.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
V. Jayaram, M. Alamgir, Y. Altun, B. Scholkopf, and M. Grosse-Wentrup,
“Transfer learning in brain-computer interfaces,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Comp.
Intell.Magazine</em>, vol. 11, no. 1, pp. 20–31, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Lotte, L. Bougrain, A. Cichocki, M. Clerc, M. Congedo, A. Rakotomamonjy, and
F. Yger, “A review of classification algorithms for eeg-based
brain–computer interfaces: a 10 year update,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Journal of neural
engineering</em>, vol. 15, no. 3, p. 031005, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
X. Wei <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “2021 beetl competition: Advancing transfer learning for
subject independence &amp; heterogenous eeg data sets,” in <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">NeurIPS 2021
comp. &amp; demo.</em>, ser. PMLR, vol. 176, 06–14 Dec 2022, pp. 205–219.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
I. Walker, M. Deisenroth, and A. Faisal, “Deep convolutional neural networks
for brain computer interface using motor imagery,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Imperial College,
Tech Report</em>, p. 68, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,
K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball, “Deep
learning with convolutional neural networks for eeg decoding and
visualization,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Hum. Brain Mapp.</em>, vol. 38, no. 11, pp. 5391–5420,
2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Ferrante <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Data-efficient hand motor imagery decoding in
eeg-bci by using morlet wavelets &amp; common spatial pattern algorithms,” in
<em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">IEEE/EMBS Neural Eng. (NER)</em>, vol. 7.   IEEE, 2015, pp. 948–951.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P. Ortega <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Compact convolutional neural networks for
multi-class, personalised, closed-loop eeg-bci,” in <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">IEEE Biomedical
Robotics &amp; Biomechatronics (Biorob)</em>, vol. 7.   IEEE, 2018, pp. 136–141.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
E. G. Ponferrada, A. Sylaidi, and A. A. Faisal, “Data-efficient motor imagery
decoding in real-time for the cybathlon brain-computer interface race,”
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Neurotechnix</em>, vol. 6, pp. 21–32, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C. Sullivan, “Eu gdpr or apec cbpr? a comparative analysis of the approach of
the eu and apec to cross border data transfers and protection of personal
data in the iot era,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">computer law &amp; security review</em>, vol. 35,
no. 4, pp. 380–397, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
L. Li <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A review of applications in federated learning,”
<em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">Computers &amp; Industrial Engineering</em>, vol. 149, p. 106854, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Hao <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Towards efficient and privacy-preserving federated deep
learning,” in <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">2019 IEEE Intl. Conf. Comms.ICC)</em>.   IEEE, 2019, pp. 1–6.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L. Lyu, X. He, Y. W. Law, and M. Palaniswami, “Privacy-preserving
collaborative deep learning with application to human activity recognition,”
in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proc. ACM Inf. &amp; Knowledge Mgmt.</em>, 2017, pp. 1219–1228.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
H. Dong <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Dropping activation outputs with localized first-layer
deep network for enhancing user privacy and data security,” <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">IEEE
Trans. on Inform. Forensics &amp; Secur.</em>, vol. 13, no. 3, pp. 662–670, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
W. Du and M. J. Atallah, “Secure multi-party computation problems and their
applications: a review and open problems,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc WS on New security
paradigms</em>, 2001, pp. 13–22.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. B. Popescu <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Privacy preserving classification of eeg data
using machine learning and homomorphic encryption,” <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">App. Sci.</em>,
vol. 11, no. 16, p. 7360, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K. Xia <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Privacy-preserving brain–computer interfaces: A
systematic review,” <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">IEEE Trans. Comput. Soc. Syst.</em>, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C. Ju, D. Gao, R. Mane, B. Tan, Y. Liu, and C. Guan, “Federated transfer
learning for eeg signal classification,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2020 42nd Conf Proc IEEE
Eng Med Biol Soc (EMBC)</em>.   IEEE, 2020,
pp. 3040–3045.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
D. Bethge <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Domain-invariant representation learning from eeg
with private encoders,” in <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">ICASSP 2022</em>.   IEEE, 2022, pp. 1236–1240.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Kapitonova, P. Kellmeyer, S. Vogt, and T. Ball, “A framework for preserving
privacy and cybersecurity in brain-computer interfacing applications,”
<em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.09653</em>, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. Tangermann, K.-R. Müller, A. Aertsen, N. Birbaumer, C. Braun,
C. Brunner, R. Leeb, C. Mehring, K. J. Miller, G. Mueller-Putz <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Review of the bci competition iv,” <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">Front. Neurosci.</em>, vol. 6, p. 55,
2012.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Cho <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “EEG datasets for motor imagery brain–computer
interface,” <em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">GigaScience</em>, vol. 6, no. 7, p. gix034, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
G. Schalk <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Bci2000: a general-purpose brain-computer interface
(bci) system,” <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">IEEE. Trans. Biomed. Eng.</em>, vol. 51, no. 6, 2004.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. L. Goldberger <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Physiobank, physiotoolkit, and physionet:
components of a new research resource for complex physiologic signals,”
<em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">circulation</em>, vol. 101, no. 23, pp. e215–e220, 2000.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
W. Yi <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Evaluation of eeg oscillatory patterns and cognitive
process during simple and compound limb motor imagery,” <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">PloS one</em>,
vol. 9, no. 12, p. e114853, 2014.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.10975" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.10976" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.10976">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.10976" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.10977" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 01:44:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
