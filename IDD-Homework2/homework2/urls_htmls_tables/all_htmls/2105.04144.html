<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2105.04144] Transitioning from Real to Synthetic data: Quantifying the bias in model</title><meta property="og:description" content="With advent of generative modelling techniques, synthetic data and its use has penetrated across various domains from unstructured data such as image, text to structured dataset modelling healthcare outcome, risk decis…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Transitioning from Real to Synthetic data: Quantifying the bias in model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Transitioning from Real to Synthetic data: Quantifying the bias in model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2105.04144">

<!--Generated on Sun Mar 17 09:46:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Transitioning from Real to Synthetic data:
<br class="ltx_break">Quantifying the bias in model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aman Gupta, Deepak Bhatt &amp; Anubha Pandey
<br class="ltx_break">Mastercard, India 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{aman.gupta,deepak.bhatt,anubha.pandey}@mastercard.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">With advent of generative modelling techniques, synthetic data and its use has penetrated across various domains from unstructured data such as image, text to structured dataset modelling healthcare outcome, risk decisioning in financial domain and many more. It overcomes various challenges such as limited training data, class imbalance, restricted access to dataset owing to privacy issues. To ensure trained model used for automated decisioning purposes makes fair decision there exist prior work to quantify and mitigate those issues. This study aims to establish trade-off between bias and fairness in the models trained using synthetic data. Variants of synthetic data generation techniques were studied to understand bias amplification including differentially private generation schemes. Through experiments on a tabular dataset, we demonstrate there exist varying level of bias impact on models trained using synthetic data. Techniques generating less correlated feature performs well as evident through fairness metrics with 94%, 82%, and 88% relative drop in DPD (demographic parity difference), EoD (equality of odds) and EoP (equality of opportunity) respectively, and 24% relative improvement in DRP (demographic parity ratio) with respect to the real dataset. We believe the outcome of our research study will help data science practitioners understand the bias in use of synthetic data.
<br class="ltx_break"><span id="id2.id1.1" class="ltx_text ltx_font_bold">Keywords:</span> Fairness, Bias, Differential Privacy, Synthetic Dataset, Class Imbalance</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Machine learning systems are being adopted in every aspect of our daily life. Amazon and Netflix uses it for movie recommendations, recommending products customized based on user preference. Among many, recruiting industry is one such example leveraging AI to find potential candidates. Owing to its capability to reduce manual hours and assist in automated decision-making, they are being widely deployed and integrated across various use-cases such as public use, policy-making, predictive policing, etc. On the other hand, though machine learning systems are quite successful yet it found that “Robots are racist and sexist, just like the people who created it<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.theguardian.com/commentisfree/2017/apr/20/robots-racist-sexist-people-machines-ai-language</span></span></span>”, claimed that “crime rate in color people are higher<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</span></span></span>” and “dark skins are unattractive<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.theguardian.com/technology/2016/sep/08/beauty-contest-doesnt-like-black-people</span></span></span>”, “recommending less qualified male candidates higher than more qualified female candidates on job portal”  <cite class="ltx_cite ltx_citemacro_citep">(Lahoti et al., <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>. As machine learning models learn what humans teach them, therefore, like people, machine learning systems are vulnerable to amplify existing human biases and societal stereotypes when they make decisions <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>. One of the factors that may influence the fairness of machine learning models is change in model outcome with protected attributes like gender, race, religion, sexual orientation, economic condition, etc <cite class="ltx_cite ltx_citemacro_citep">(Mehrabi et al., <a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>. Recent research found that even if we don’t use the protected attributes for the model training, information about the race, gender, and age are implicitly encoded into intermediate representations of the model <cite class="ltx_cite ltx_citemacro_citep">(Elazar &amp; Goldberg, <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Various bias mitigation techniques exists such as: (1) Pre-processing: transforming data representation before training <cite class="ltx_cite ltx_citemacro_citep">(d’Alessandro et al., <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Bellamy et al., <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>, (2) In-processing: modify the existing algorithms during the training process to remove discrimination, and (3) Post-processing: transform the model output to improve prediction fairness  <cite class="ltx_cite ltx_citemacro_citep">(Berk et al., <a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite>. The effectiveness of these methods have been tested on a real dataset; however, synthetic data is gaining traction in light of limited data access arising due to privacy and compliance. Recent research aims to study how biases are amplified, for example, in compressed deep neural networks <cite class="ltx_cite ltx_citemacro_citep">(Hooker et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>. Another study estimates the impact of bias in adopting differentially private mechanisms to protect the model from learning sensitive attributes <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan &amp; Shmatikov, <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this study, we are particularly interested in bias amplification when models are trained using synthetic data. Generative adversarial networks (GANs) have become a popular choice for synthetic data generation <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al., <a href="#bib.bib6" title="" class="ltx_ref">2014</a>)</cite>. Several variants of GANs are available today that are more stable and capable of generating realistic samples. In this paper, we have used state-of-the-art GAN architechture for tabular data generation: CTGAN <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>, Gaussian Copula GAN  <cite class="ltx_cite ltx_citemacro_citep">(Masarotto et al., <a href="#bib.bib11" title="" class="ltx_ref">2012</a>)</cite>, and Copula GAN<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://en.wikipedia.org/wiki/Copula_%28probability_theory%29</span></span></span>. Further to generate differentially private synthetic data, PATE-GAN <cite class="ltx_cite ltx_citemacro_citep">(Jordon et al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> is used.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">The major contributions are: (1) demonstrating the impact of bias on models trained using synthetic data (2) Studied how differentially private synthetic data affect the model accuracy and the fairness (3) compared and contrast the key differences in model trained using synthetic data and its differentially private version.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Experimental Results</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Fairness Metrices</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">There exists a myriad of notions in the literature to quantify fairness. Each measure emphasizes different aspects of fairness. Fairness in machine learning measures the degree of disparate treatment for different groups (e.g., female vs. male), or individual fairness, emphasizing similar individuals should be treated similarly. Evaluation metrics to measure group fairness are (i) Demographic Parity Difference (DPD) <cite class="ltx_cite ltx_citemacro_citep">(Verma &amp; Rubin, <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite>. (ii) Demographic Parity Ratio (DPR) (iii) Equality of Odds (EoD) (iv) Equality of Opportunity (EoP) <cite class="ltx_cite ltx_citemacro_citep">(Hardt et al., <a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite>. Please find the details of the metrics in the supplementary. We have used FairLearn library<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://fairlearn.github.io/</span></span></span> to access fairness metrics. Some cut-off values have been defined in the literature for these metrics, suggesting whether models are fair or not. For DPD, EoD, and EoP, if the absolute value is smaller than 0.1, the model can be considered fair, whereas for DPR fairness range for this metric is between 0.8 and 1.25.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Dataset Description</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.5" class="ltx_p">We have used Adult Dataset<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>http://archive.ics.uci.edu/ml</span></span></span> for our analysis, a well-known example, which is widely used in the fairness literature to predict whether the income of an individual exceeds <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\$50K/yr" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mo rspace="0.167em" id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">$</mo><mrow id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml"><mrow id="S2.SS2.p1.1.m1.1.1.2.2" xref="S2.SS2.p1.1.m1.1.1.2.2.cmml"><mrow id="S2.SS2.p1.1.m1.1.1.2.2.2" xref="S2.SS2.p1.1.m1.1.1.2.2.2.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2.2.2.2" xref="S2.SS2.p1.1.m1.1.1.2.2.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p1.1.m1.1.1.2.2.2.1" xref="S2.SS2.p1.1.m1.1.1.2.2.2.1.cmml">​</mo><mi id="S2.SS2.p1.1.m1.1.1.2.2.2.3" xref="S2.SS2.p1.1.m1.1.1.2.2.2.3.cmml">K</mi></mrow><mo id="S2.SS2.p1.1.m1.1.1.2.2.1" xref="S2.SS2.p1.1.m1.1.1.2.2.1.cmml">/</mo><mi id="S2.SS2.p1.1.m1.1.1.2.2.3" xref="S2.SS2.p1.1.m1.1.1.2.2.3.cmml">y</mi></mrow><mo lspace="0em" rspace="0em" id="S2.SS2.p1.1.m1.1.1.2.1" xref="S2.SS2.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S2.SS2.p1.1.m1.1.1.2.3" xref="S2.SS2.p1.1.m1.1.1.2.3.cmml">r</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1">currency-dollar</csymbol><apply id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2"><times id="S2.SS2.p1.1.m1.1.1.2.1.cmml" xref="S2.SS2.p1.1.m1.1.1.2.1"></times><apply id="S2.SS2.p1.1.m1.1.1.2.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2.2"><divide id="S2.SS2.p1.1.m1.1.1.2.2.1.cmml" xref="S2.SS2.p1.1.m1.1.1.2.2.1"></divide><apply id="S2.SS2.p1.1.m1.1.1.2.2.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2.2.2"><times id="S2.SS2.p1.1.m1.1.1.2.2.2.1.cmml" xref="S2.SS2.p1.1.m1.1.1.2.2.2.1"></times><cn type="integer" id="S2.SS2.p1.1.m1.1.1.2.2.2.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2.2.2.2">50</cn><ci id="S2.SS2.p1.1.m1.1.1.2.2.2.3.cmml" xref="S2.SS2.p1.1.m1.1.1.2.2.2.3">𝐾</ci></apply><ci id="S2.SS2.p1.1.m1.1.1.2.2.3.cmml" xref="S2.SS2.p1.1.m1.1.1.2.2.3">𝑦</ci></apply><ci id="S2.SS2.p1.1.m1.1.1.2.3.cmml" xref="S2.SS2.p1.1.m1.1.1.2.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\$50K/yr</annotation></semantics></math> based on census data. The Adult dataset has about 48K records with a binary label indicating a salary <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="\leq\$50K" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mrow id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml"></mi><mo rspace="0.1389em" id="S2.SS2.p1.2.m2.1.1.1" xref="S2.SS2.p1.2.m2.1.1.1.cmml">≤</mo><mrow id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml"><mo lspace="0.1389em" rspace="0.167em" id="S2.SS2.p1.2.m2.1.1.3.1" xref="S2.SS2.p1.2.m2.1.1.3.1.cmml">$</mo><mrow id="S2.SS2.p1.2.m2.1.1.3.2" xref="S2.SS2.p1.2.m2.1.1.3.2.cmml"><mn id="S2.SS2.p1.2.m2.1.1.3.2.2" xref="S2.SS2.p1.2.m2.1.1.3.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p1.2.m2.1.1.3.2.1" xref="S2.SS2.p1.2.m2.1.1.3.2.1.cmml">​</mo><mi id="S2.SS2.p1.2.m2.1.1.3.2.3" xref="S2.SS2.p1.2.m2.1.1.3.2.3.cmml">K</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><leq id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1.1"></leq><csymbol cd="latexml" id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">absent</csymbol><apply id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3"><csymbol cd="latexml" id="S2.SS2.p1.2.m2.1.1.3.1.cmml" xref="S2.SS2.p1.2.m2.1.1.3.1">currency-dollar</csymbol><apply id="S2.SS2.p1.2.m2.1.1.3.2.cmml" xref="S2.SS2.p1.2.m2.1.1.3.2"><times id="S2.SS2.p1.2.m2.1.1.3.2.1.cmml" xref="S2.SS2.p1.2.m2.1.1.3.2.1"></times><cn type="integer" id="S2.SS2.p1.2.m2.1.1.3.2.2.cmml" xref="S2.SS2.p1.2.m2.1.1.3.2.2">50</cn><ci id="S2.SS2.p1.2.m2.1.1.3.2.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3.2.3">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\leq\$50K</annotation></semantics></math> or <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="\geq\$50K" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mrow id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml"></mi><mo rspace="0.1389em" id="S2.SS2.p1.3.m3.1.1.1" xref="S2.SS2.p1.3.m3.1.1.1.cmml">≥</mo><mrow id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml"><mo lspace="0.1389em" rspace="0.167em" id="S2.SS2.p1.3.m3.1.1.3.1" xref="S2.SS2.p1.3.m3.1.1.3.1.cmml">$</mo><mrow id="S2.SS2.p1.3.m3.1.1.3.2" xref="S2.SS2.p1.3.m3.1.1.3.2.cmml"><mn id="S2.SS2.p1.3.m3.1.1.3.2.2" xref="S2.SS2.p1.3.m3.1.1.3.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p1.3.m3.1.1.3.2.1" xref="S2.SS2.p1.3.m3.1.1.3.2.1.cmml">​</mo><mi id="S2.SS2.p1.3.m3.1.1.3.2.3" xref="S2.SS2.p1.3.m3.1.1.3.2.3.cmml">K</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><geq id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1.1"></geq><csymbol cd="latexml" id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">absent</csymbol><apply id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3"><csymbol cd="latexml" id="S2.SS2.p1.3.m3.1.1.3.1.cmml" xref="S2.SS2.p1.3.m3.1.1.3.1">currency-dollar</csymbol><apply id="S2.SS2.p1.3.m3.1.1.3.2.cmml" xref="S2.SS2.p1.3.m3.1.1.3.2"><times id="S2.SS2.p1.3.m3.1.1.3.2.1.cmml" xref="S2.SS2.p1.3.m3.1.1.3.2.1"></times><cn type="integer" id="S2.SS2.p1.3.m3.1.1.3.2.2.cmml" xref="S2.SS2.p1.3.m3.1.1.3.2.2">50</cn><ci id="S2.SS2.p1.3.m3.1.1.3.2.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3.2.3">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\geq\$50K</annotation></semantics></math>, with a class imbalance of <math id="S2.SS2.p1.4.m4.2" class="ltx_Math" alttext="[76\%,24\%]" display="inline"><semantics id="S2.SS2.p1.4.m4.2a"><mrow id="S2.SS2.p1.4.m4.2.2.2" xref="S2.SS2.p1.4.m4.2.2.3.cmml"><mo stretchy="false" id="S2.SS2.p1.4.m4.2.2.2.3" xref="S2.SS2.p1.4.m4.2.2.3.cmml">[</mo><mrow id="S2.SS2.p1.4.m4.1.1.1.1" xref="S2.SS2.p1.4.m4.1.1.1.1.cmml"><mn id="S2.SS2.p1.4.m4.1.1.1.1.2" xref="S2.SS2.p1.4.m4.1.1.1.1.2.cmml">76</mn><mo id="S2.SS2.p1.4.m4.1.1.1.1.1" xref="S2.SS2.p1.4.m4.1.1.1.1.1.cmml">%</mo></mrow><mo id="S2.SS2.p1.4.m4.2.2.2.4" xref="S2.SS2.p1.4.m4.2.2.3.cmml">,</mo><mrow id="S2.SS2.p1.4.m4.2.2.2.2" xref="S2.SS2.p1.4.m4.2.2.2.2.cmml"><mn id="S2.SS2.p1.4.m4.2.2.2.2.2" xref="S2.SS2.p1.4.m4.2.2.2.2.2.cmml">24</mn><mo id="S2.SS2.p1.4.m4.2.2.2.2.1" xref="S2.SS2.p1.4.m4.2.2.2.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S2.SS2.p1.4.m4.2.2.2.5" xref="S2.SS2.p1.4.m4.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.2b"><interval closure="closed" id="S2.SS2.p1.4.m4.2.2.3.cmml" xref="S2.SS2.p1.4.m4.2.2.2"><apply id="S2.SS2.p1.4.m4.1.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1.1.1"><csymbol cd="latexml" id="S2.SS2.p1.4.m4.1.1.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1.1.1.1">percent</csymbol><cn type="integer" id="S2.SS2.p1.4.m4.1.1.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.1.1.2">76</cn></apply><apply id="S2.SS2.p1.4.m4.2.2.2.2.cmml" xref="S2.SS2.p1.4.m4.2.2.2.2"><csymbol cd="latexml" id="S2.SS2.p1.4.m4.2.2.2.2.1.cmml" xref="S2.SS2.p1.4.m4.2.2.2.2.1">percent</csymbol><cn type="integer" id="S2.SS2.p1.4.m4.2.2.2.2.2.cmml" xref="S2.SS2.p1.4.m4.2.2.2.2.2">24</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.2c">[76\%,24\%]</annotation></semantics></math>, where 76% of the observations are earning <math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="\leq\$50K" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><mrow id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.p1.5.m5.1.1.2" xref="S2.SS2.p1.5.m5.1.1.2.cmml"></mi><mo rspace="0.1389em" id="S2.SS2.p1.5.m5.1.1.1" xref="S2.SS2.p1.5.m5.1.1.1.cmml">≤</mo><mrow id="S2.SS2.p1.5.m5.1.1.3" xref="S2.SS2.p1.5.m5.1.1.3.cmml"><mo lspace="0.1389em" rspace="0.167em" id="S2.SS2.p1.5.m5.1.1.3.1" xref="S2.SS2.p1.5.m5.1.1.3.1.cmml">$</mo><mrow id="S2.SS2.p1.5.m5.1.1.3.2" xref="S2.SS2.p1.5.m5.1.1.3.2.cmml"><mn id="S2.SS2.p1.5.m5.1.1.3.2.2" xref="S2.SS2.p1.5.m5.1.1.3.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p1.5.m5.1.1.3.2.1" xref="S2.SS2.p1.5.m5.1.1.3.2.1.cmml">​</mo><mi id="S2.SS2.p1.5.m5.1.1.3.2.3" xref="S2.SS2.p1.5.m5.1.1.3.2.3.cmml">K</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1"><leq id="S2.SS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1.1"></leq><csymbol cd="latexml" id="S2.SS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1.2">absent</csymbol><apply id="S2.SS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3"><csymbol cd="latexml" id="S2.SS2.p1.5.m5.1.1.3.1.cmml" xref="S2.SS2.p1.5.m5.1.1.3.1">currency-dollar</csymbol><apply id="S2.SS2.p1.5.m5.1.1.3.2.cmml" xref="S2.SS2.p1.5.m5.1.1.3.2"><times id="S2.SS2.p1.5.m5.1.1.3.2.1.cmml" xref="S2.SS2.p1.5.m5.1.1.3.2.1"></times><cn type="integer" id="S2.SS2.p1.5.m5.1.1.3.2.2.cmml" xref="S2.SS2.p1.5.m5.1.1.3.2.2">50</cn><ci id="S2.SS2.p1.5.m5.1.1.3.2.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3.2.3">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">\leq\$50K</annotation></semantics></math>. There are 14 attributes: 8 categorical and 6 continuous attributes. There are two attributes corresponding to the race and sex of an individual that can be used to define the protected groups.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.T2.fig1" class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" style="width:216.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 1: </span>Fairness Report</figcaption>
<table id="S2.T2.fig1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.fig1.1.1.1" class="ltx_tr">
<td id="S2.T2.fig1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S2.T2.fig1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">DPD</th>
<th id="S2.T2.fig1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">DPR</th>
<th id="S2.T2.fig1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">EoD</th>
<th id="S2.T2.fig1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">EoP</th>
</tr>
<tr id="S2.T2.fig1.1.2.2" class="ltx_tr">
<td id="S2.T2.fig1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Real Data</td>
<td id="S2.T2.fig1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">0.18</td>
<td id="S2.T2.fig1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">0.80</td>
<td id="S2.T2.fig1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">0.11</td>
<td id="S2.T2.fig1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_t">0.09</td>
</tr>
<tr id="S2.T2.fig1.1.3.3" class="ltx_tr">
<td id="S2.T2.fig1.1.3.3.1" class="ltx_td ltx_align_left">Balanced Data</td>
<td id="S2.T2.fig1.1.3.3.2" class="ltx_td ltx_align_left">0.29</td>
<td id="S2.T2.fig1.1.3.3.3" class="ltx_td ltx_align_left">0.64</td>
<td id="S2.T2.fig1.1.3.3.4" class="ltx_td ltx_align_left">0.20</td>
<td id="S2.T2.fig1.1.3.3.5" class="ltx_td ltx_align_left">0.20</td>
</tr>
<tr id="S2.T2.fig1.1.4.4" class="ltx_tr">
<td id="S2.T2.fig1.1.4.4.1" class="ltx_td ltx_align_left">CTGAN</td>
<td id="S2.T2.fig1.1.4.4.2" class="ltx_td ltx_align_left">0.05</td>
<td id="S2.T2.fig1.1.4.4.3" class="ltx_td ltx_align_left">0.95</td>
<td id="S2.T2.fig1.1.4.4.4" class="ltx_td ltx_align_left">0.04</td>
<td id="S2.T2.fig1.1.4.4.5" class="ltx_td ltx_align_left">0.042</td>
</tr>
<tr id="S2.T2.fig1.1.5.5" class="ltx_tr">
<td id="S2.T2.fig1.1.5.5.1" class="ltx_td ltx_align_left">GaussianCopula</td>
<td id="S2.T2.fig1.1.5.5.2" class="ltx_td ltx_align_left">0.13</td>
<td id="S2.T2.fig1.1.5.5.3" class="ltx_td ltx_align_left">0.86</td>
<td id="S2.T2.fig1.1.5.5.4" class="ltx_td ltx_align_left">0.16</td>
<td id="S2.T2.fig1.1.5.5.5" class="ltx_td ltx_align_left">0.074</td>
</tr>
<tr id="S2.T2.fig1.1.6.6" class="ltx_tr">
<td id="S2.T2.fig1.1.6.6.1" class="ltx_td ltx_align_left">CopulaGAN</td>
<td id="S2.T2.fig1.1.6.6.2" class="ltx_td ltx_align_left">0.01</td>
<td id="S2.T2.fig1.1.6.6.3" class="ltx_td ltx_align_left">0.99</td>
<td id="S2.T2.fig1.1.6.6.4" class="ltx_td ltx_align_left">0.02</td>
<td id="S2.T2.fig1.1.6.6.5" class="ltx_td ltx_align_left">0.01</td>
</tr>
<tr id="S2.T2.fig1.1.7.7" class="ltx_tr">
<td id="S2.T2.fig1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_bb">PATE-GAN</td>
<td id="S2.T2.fig1.1.7.7.2" class="ltx_td ltx_align_left ltx_border_bb">0.45</td>
<td id="S2.T2.fig1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_bb">0.46</td>
<td id="S2.T2.fig1.1.7.7.4" class="ltx_td ltx_align_left ltx_border_bb">0.39</td>
<td id="S2.T2.fig1.1.7.7.5" class="ltx_td ltx_align_left ltx_border_bb">0.39</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.T2.2" class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" style="width:216.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 2: </span>How frequently model is predicting an individual earning <math id="S2.T2.2.2.m1.1" class="ltx_Math" alttext="\geq\$50K" display="inline"><semantics id="S2.T2.2.2.m1.1b"><mrow id="S2.T2.2.2.m1.1.1" xref="S2.T2.2.2.m1.1.1.cmml"><mi id="S2.T2.2.2.m1.1.1.2" xref="S2.T2.2.2.m1.1.1.2.cmml"></mi><mo rspace="0.1389em" id="S2.T2.2.2.m1.1.1.1" xref="S2.T2.2.2.m1.1.1.1.cmml">≥</mo><mrow id="S2.T2.2.2.m1.1.1.3" xref="S2.T2.2.2.m1.1.1.3.cmml"><mo lspace="0.1389em" rspace="0.167em" id="S2.T2.2.2.m1.1.1.3.1" xref="S2.T2.2.2.m1.1.1.3.1.cmml">$</mo><mrow id="S2.T2.2.2.m1.1.1.3.2" xref="S2.T2.2.2.m1.1.1.3.2.cmml"><mn id="S2.T2.2.2.m1.1.1.3.2.2" xref="S2.T2.2.2.m1.1.1.3.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S2.T2.2.2.m1.1.1.3.2.1" xref="S2.T2.2.2.m1.1.1.3.2.1.cmml">​</mo><mi id="S2.T2.2.2.m1.1.1.3.2.3" xref="S2.T2.2.2.m1.1.1.3.2.3.cmml">K</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.m1.1c"><apply id="S2.T2.2.2.m1.1.1.cmml" xref="S2.T2.2.2.m1.1.1"><geq id="S2.T2.2.2.m1.1.1.1.cmml" xref="S2.T2.2.2.m1.1.1.1"></geq><csymbol cd="latexml" id="S2.T2.2.2.m1.1.1.2.cmml" xref="S2.T2.2.2.m1.1.1.2">absent</csymbol><apply id="S2.T2.2.2.m1.1.1.3.cmml" xref="S2.T2.2.2.m1.1.1.3"><csymbol cd="latexml" id="S2.T2.2.2.m1.1.1.3.1.cmml" xref="S2.T2.2.2.m1.1.1.3.1">currency-dollar</csymbol><apply id="S2.T2.2.2.m1.1.1.3.2.cmml" xref="S2.T2.2.2.m1.1.1.3.2"><times id="S2.T2.2.2.m1.1.1.3.2.1.cmml" xref="S2.T2.2.2.m1.1.1.3.2.1"></times><cn type="integer" id="S2.T2.2.2.m1.1.1.3.2.2.cmml" xref="S2.T2.2.2.m1.1.1.3.2.2">50</cn><ci id="S2.T2.2.2.m1.1.1.3.2.3.cmml" xref="S2.T2.2.2.m1.1.1.3.2.3">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.m1.1d">\geq\$50K</annotation></semantics></math> for the groups (male and female) in test set.</figcaption>
<table id="S2.T2.2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.2.3.1.1" class="ltx_tr">
<td id="S2.T2.2.3.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S2.T2.2.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Male (7677)</th>
<th id="S2.T2.2.3.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Female (3639)</th>
</tr>
<tr id="S2.T2.2.3.2.2" class="ltx_tr">
<td id="S2.T2.2.3.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Ground Truth</td>
<td id="S2.T2.2.3.2.2.2" class="ltx_td ltx_align_left ltx_border_t">2346 (30%)</td>
<td id="S2.T2.2.3.2.2.3" class="ltx_td ltx_align_left ltx_border_t">412 (11%)</td>
</tr>
<tr id="S2.T2.2.3.3.3" class="ltx_tr">
<td id="S2.T2.2.3.3.3.1" class="ltx_td ltx_align_left">Real Data</td>
<td id="S2.T2.2.3.3.3.2" class="ltx_td ltx_align_left">2125 (27%)</td>
<td id="S2.T2.2.3.3.3.3" class="ltx_td ltx_align_left">347 (9%)</td>
</tr>
<tr id="S2.T2.2.3.4.4" class="ltx_tr">
<td id="S2.T2.2.3.4.4.1" class="ltx_td ltx_align_left">Balanced Data</td>
<td id="S2.T2.2.3.4.4.2" class="ltx_td ltx_align_left">3702 (48%)</td>
<td id="S2.T2.2.3.4.4.3" class="ltx_td ltx_align_left">699 (19%)</td>
</tr>
<tr id="S2.T2.2.3.5.5" class="ltx_tr">
<td id="S2.T2.2.3.5.5.1" class="ltx_td ltx_align_left">CTGAN</td>
<td id="S2.T2.2.3.5.5.2" class="ltx_td ltx_align_left">736 (9.5%)</td>
<td id="S2.T2.2.3.5.5.3" class="ltx_td ltx_align_left">170 (4%)</td>
</tr>
<tr id="S2.T2.2.3.6.6" class="ltx_tr">
<td id="S2.T2.2.3.6.6.1" class="ltx_td ltx_align_left">GaussianCopula</td>
<td id="S2.T2.2.3.6.6.2" class="ltx_td ltx_align_left">1325 (17%)</td>
<td id="S2.T2.2.3.6.6.3" class="ltx_td ltx_align_left">151 (4%)</td>
</tr>
<tr id="S2.T2.2.3.7.7" class="ltx_tr">
<td id="S2.T2.2.3.7.7.1" class="ltx_td ltx_align_left">CopulaGAN</td>
<td id="S2.T2.2.3.7.7.2" class="ltx_td ltx_align_left">305 (3.9%)</td>
<td id="S2.T2.2.3.7.7.3" class="ltx_td ltx_align_left">121 (3.3%)</td>
</tr>
<tr id="S2.T2.2.3.8.8" class="ltx_tr">
<td id="S2.T2.2.3.8.8.1" class="ltx_td ltx_align_left ltx_border_bb">PATE-GAN</td>
<td id="S2.T2.2.3.8.8.2" class="ltx_td ltx_align_left ltx_border_bb">4790 (62%)</td>
<td id="S2.T2.2.3.8.8.3" class="ltx_td ltx_align_left ltx_border_bb">646 (17.7%)</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Bias Induced from the Dataset</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">The model’s performance in terms of group fairness metrics discussed above is presented in Table <a href="#S2.T2" title="Table 2 ‣ 2.2 Dataset Description ‣ 2 Experimental Results ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We can see that the values in Table <a href="#S2.T2" title="Table 2 ‣ 2.2 Dataset Description ‣ 2 Experimental Results ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> indicate that the model is unfair as DPD and EoD are greater than 0.1. To further investigate the cause of the bias, we visualize the correlation matrix of the dataset’s attributes and found a high correlation between some of the allowed variables (non-protected) and gender (protected variables) as illustrated in Figure <a href="#S2.F1.sf2" title="In Figure 1 ‣ 2.4 Impact of Synthetic data on Bias and Fairness ‣ 2 Experimental Results ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>. Next, we trained the Random Forest classifier on the balanced dataset by maintaining the same class ratio. However, from Table <a href="#S2.T2" title="Table 2 ‣ 2.2 Dataset Description ‣ 2 Experimental Results ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we can observe that the balanced dataset has no impact as the model remains unfair. From Figure <a href="#S2.F1.sf2" title="In Figure 1 ‣ 2.4 Impact of Synthetic data on Bias and Fairness ‣ 2 Experimental Results ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a> we can observe non-protected attributes still have a high correlation with protected attributes.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.5" class="ltx_p">Further, we analyze individual fairness in the model outcome. Here, we evaluate how frequently the model predicts an individual as ”earning more than <math id="S2.SS3.p2.1.m1.1" class="ltx_Math" alttext="\$50k" display="inline"><semantics id="S2.SS3.p2.1.m1.1a"><mrow id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml"><mo rspace="0.167em" id="S2.SS3.p2.1.m1.1.1.1" xref="S2.SS3.p2.1.m1.1.1.1.cmml">$</mo><mrow id="S2.SS3.p2.1.m1.1.1.2" xref="S2.SS3.p2.1.m1.1.1.2.cmml"><mn id="S2.SS3.p2.1.m1.1.1.2.2" xref="S2.SS3.p2.1.m1.1.1.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S2.SS3.p2.1.m1.1.1.2.1" xref="S2.SS3.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="S2.SS3.p2.1.m1.1.1.2.3" xref="S2.SS3.p2.1.m1.1.1.2.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><apply id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S2.SS3.p2.1.m1.1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1.1">currency-dollar</csymbol><apply id="S2.SS3.p2.1.m1.1.1.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2"><times id="S2.SS3.p2.1.m1.1.1.2.1.cmml" xref="S2.SS3.p2.1.m1.1.1.2.1"></times><cn type="integer" id="S2.SS3.p2.1.m1.1.1.2.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2.2">50</cn><ci id="S2.SS3.p2.1.m1.1.1.2.3.cmml" xref="S2.SS3.p2.1.m1.1.1.2.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">\$50k</annotation></semantics></math> per annum” irrespective of gender. Table <a href="#S2.T2" title="Table 2 ‣ 2.2 Dataset Description ‣ 2 Experimental Results ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the performance in terms of individual fairness. We can observe that 30% of Male and 11% Female individuals earn more than <math id="S2.SS3.p2.2.m2.1" class="ltx_Math" alttext="\$50k" display="inline"><semantics id="S2.SS3.p2.2.m2.1a"><mrow id="S2.SS3.p2.2.m2.1.1" xref="S2.SS3.p2.2.m2.1.1.cmml"><mo rspace="0.167em" id="S2.SS3.p2.2.m2.1.1.1" xref="S2.SS3.p2.2.m2.1.1.1.cmml">$</mo><mrow id="S2.SS3.p2.2.m2.1.1.2" xref="S2.SS3.p2.2.m2.1.1.2.cmml"><mn id="S2.SS3.p2.2.m2.1.1.2.2" xref="S2.SS3.p2.2.m2.1.1.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S2.SS3.p2.2.m2.1.1.2.1" xref="S2.SS3.p2.2.m2.1.1.2.1.cmml">​</mo><mi id="S2.SS3.p2.2.m2.1.1.2.3" xref="S2.SS3.p2.2.m2.1.1.2.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.2.m2.1b"><apply id="S2.SS3.p2.2.m2.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S2.SS3.p2.2.m2.1.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1.1">currency-dollar</csymbol><apply id="S2.SS3.p2.2.m2.1.1.2.cmml" xref="S2.SS3.p2.2.m2.1.1.2"><times id="S2.SS3.p2.2.m2.1.1.2.1.cmml" xref="S2.SS3.p2.2.m2.1.1.2.1"></times><cn type="integer" id="S2.SS3.p2.2.m2.1.1.2.2.cmml" xref="S2.SS3.p2.2.m2.1.1.2.2">50</cn><ci id="S2.SS3.p2.2.m2.1.1.2.3.cmml" xref="S2.SS3.p2.2.m2.1.1.2.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.2.m2.1c">\$50k</annotation></semantics></math> per annum. Hence the dataset has more instances of male earning more than <math id="S2.SS3.p2.3.m3.1" class="ltx_Math" alttext="\$50k" display="inline"><semantics id="S2.SS3.p2.3.m3.1a"><mrow id="S2.SS3.p2.3.m3.1.1" xref="S2.SS3.p2.3.m3.1.1.cmml"><mo rspace="0.167em" id="S2.SS3.p2.3.m3.1.1.1" xref="S2.SS3.p2.3.m3.1.1.1.cmml">$</mo><mrow id="S2.SS3.p2.3.m3.1.1.2" xref="S2.SS3.p2.3.m3.1.1.2.cmml"><mn id="S2.SS3.p2.3.m3.1.1.2.2" xref="S2.SS3.p2.3.m3.1.1.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S2.SS3.p2.3.m3.1.1.2.1" xref="S2.SS3.p2.3.m3.1.1.2.1.cmml">​</mo><mi id="S2.SS3.p2.3.m3.1.1.2.3" xref="S2.SS3.p2.3.m3.1.1.2.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.3.m3.1b"><apply id="S2.SS3.p2.3.m3.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S2.SS3.p2.3.m3.1.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1.1">currency-dollar</csymbol><apply id="S2.SS3.p2.3.m3.1.1.2.cmml" xref="S2.SS3.p2.3.m3.1.1.2"><times id="S2.SS3.p2.3.m3.1.1.2.1.cmml" xref="S2.SS3.p2.3.m3.1.1.2.1"></times><cn type="integer" id="S2.SS3.p2.3.m3.1.1.2.2.cmml" xref="S2.SS3.p2.3.m3.1.1.2.2">50</cn><ci id="S2.SS3.p2.3.m3.1.1.2.3.cmml" xref="S2.SS3.p2.3.m3.1.1.2.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.3.m3.1c">\$50k</annotation></semantics></math> per annum than females. When we train the model on the Adult dataset after removing the gender (protected attribute) information, the proportion of predicting high-earning individuals for males is higher than for females. The model trained on real dataset predicts 27% male and 9% female individuals as earning more than <math id="S2.SS3.p2.4.m4.1" class="ltx_Math" alttext="\$50k" display="inline"><semantics id="S2.SS3.p2.4.m4.1a"><mrow id="S2.SS3.p2.4.m4.1.1" xref="S2.SS3.p2.4.m4.1.1.cmml"><mo rspace="0.167em" id="S2.SS3.p2.4.m4.1.1.1" xref="S2.SS3.p2.4.m4.1.1.1.cmml">$</mo><mrow id="S2.SS3.p2.4.m4.1.1.2" xref="S2.SS3.p2.4.m4.1.1.2.cmml"><mn id="S2.SS3.p2.4.m4.1.1.2.2" xref="S2.SS3.p2.4.m4.1.1.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S2.SS3.p2.4.m4.1.1.2.1" xref="S2.SS3.p2.4.m4.1.1.2.1.cmml">​</mo><mi id="S2.SS3.p2.4.m4.1.1.2.3" xref="S2.SS3.p2.4.m4.1.1.2.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.4.m4.1b"><apply id="S2.SS3.p2.4.m4.1.1.cmml" xref="S2.SS3.p2.4.m4.1.1"><csymbol cd="latexml" id="S2.SS3.p2.4.m4.1.1.1.cmml" xref="S2.SS3.p2.4.m4.1.1.1">currency-dollar</csymbol><apply id="S2.SS3.p2.4.m4.1.1.2.cmml" xref="S2.SS3.p2.4.m4.1.1.2"><times id="S2.SS3.p2.4.m4.1.1.2.1.cmml" xref="S2.SS3.p2.4.m4.1.1.2.1"></times><cn type="integer" id="S2.SS3.p2.4.m4.1.1.2.2.cmml" xref="S2.SS3.p2.4.m4.1.1.2.2">50</cn><ci id="S2.SS3.p2.4.m4.1.1.2.3.cmml" xref="S2.SS3.p2.4.m4.1.1.2.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.4.m4.1c">\$50k</annotation></semantics></math> per annum. However, the model trained on balanced dataset predicts 48% male and 19% female individuals as earning more than <math id="S2.SS3.p2.5.m5.1" class="ltx_Math" alttext="\$50k" display="inline"><semantics id="S2.SS3.p2.5.m5.1a"><mrow id="S2.SS3.p2.5.m5.1.1" xref="S2.SS3.p2.5.m5.1.1.cmml"><mo rspace="0.167em" id="S2.SS3.p2.5.m5.1.1.1" xref="S2.SS3.p2.5.m5.1.1.1.cmml">$</mo><mrow id="S2.SS3.p2.5.m5.1.1.2" xref="S2.SS3.p2.5.m5.1.1.2.cmml"><mn id="S2.SS3.p2.5.m5.1.1.2.2" xref="S2.SS3.p2.5.m5.1.1.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S2.SS3.p2.5.m5.1.1.2.1" xref="S2.SS3.p2.5.m5.1.1.2.1.cmml">​</mo><mi id="S2.SS3.p2.5.m5.1.1.2.3" xref="S2.SS3.p2.5.m5.1.1.2.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.5.m5.1b"><apply id="S2.SS3.p2.5.m5.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1"><csymbol cd="latexml" id="S2.SS3.p2.5.m5.1.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1.1">currency-dollar</csymbol><apply id="S2.SS3.p2.5.m5.1.1.2.cmml" xref="S2.SS3.p2.5.m5.1.1.2"><times id="S2.SS3.p2.5.m5.1.1.2.1.cmml" xref="S2.SS3.p2.5.m5.1.1.2.1"></times><cn type="integer" id="S2.SS3.p2.5.m5.1.1.2.2.cmml" xref="S2.SS3.p2.5.m5.1.1.2.2">50</cn><ci id="S2.SS3.p2.5.m5.1.1.2.3.cmml" xref="S2.SS3.p2.5.m5.1.1.2.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.5.m5.1c">\$50k</annotation></semantics></math> per annum. This difference in predicting high-earning individuals between both the groups increases even after training the model on the class balanced dataset.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Impact of Synthetic data on Bias and Fairness</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.2" class="ltx_p">We further study the impact of synthetic data on the model’s fairness. To generate the synthetic data for our experiments, we have used Synthetic Data Vault (SDV) library<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://sdv.dev/</span></span></span>. SDV is an open-source library to help end-users easily generate synthetic Data for different data modalities, including single table, multi-table, and time-series data. The SDV Python package includes a comprehensive set of generative models and evaluation framework that facilitates the task of generating evaluating the quality of Synthetic Data. From the SDV library, we have used CTGAN, GaussianCopula <cite class="ltx_cite ltx_citemacro_citep">(Masarotto et al., <a href="#bib.bib11" title="" class="ltx_ref">2012</a>)</cite>, and Copula model for data generation. We also explored the effect of differentially private algorithms for data generation. We implemented the PATE-GAN framework from the paper <cite class="ltx_cite ltx_citemacro_citep">(Jordon et al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> with <math id="S2.SS4.p1.1.m1.1" class="ltx_Math" alttext="\epsilon=2" display="inline"><semantics id="S2.SS4.p1.1.m1.1a"><mrow id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.1.1.2" xref="S2.SS4.p1.1.m1.1.1.2.cmml">ϵ</mi><mo id="S2.SS4.p1.1.m1.1.1.1" xref="S2.SS4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS4.p1.1.m1.1.1.3" xref="S2.SS4.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><apply id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1"><eq id="S2.SS4.p1.1.m1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1.1"></eq><ci id="S2.SS4.p1.1.m1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="S2.SS4.p1.1.m1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">\epsilon=2</annotation></semantics></math> privacy budget, where smaller <math id="S2.SS4.p1.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS4.p1.2.m2.1a"><mi id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b"><ci id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">\epsilon</annotation></semantics></math> implies greater privacy.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.p2.1" class="ltx_p">Table <a href="#S2.T2" title="Table 2 ‣ 2.2 Dataset Description ‣ 2 Experimental Results ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides details of performance on synthetic data in terms of fairness metrics. The synthetic data generated by methods like GaussianCopula, CTGAN, and CopulaGAN, significantly reduces the model’s disparity. The improvement in DPD and EoP indicates the model correctly classifies the same proportion of positive outcomes in both groups. However, a differentially private version of synthetic data (PATE-GAN) amplifies bias and unfairness in the model. This behavior can be explained by the correlation of the attributes in the synthetic data shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.4 Impact of Synthetic data on Bias and Fairness ‣ 2 Experimental Results ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In the synthetic data generated from CTGAN and CopulaGAN, all the attributes are weakly correlated and loosely dependent upon protected attributes (gender). In PATE-GAN, the attributes are highly correlated.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.p3.1" class="ltx_p">Next, we evaluate the model performance in terms of individual fairness as shown in Table <a href="#S2.T2" title="Table 2 ‣ 2.2 Dataset Description ‣ 2 Experimental Results ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We find that training the model on synthetic data from CTGAN and CopulaGAN, the proportion of predicting high-earning individuals is almost the same for both groups. In contrast, PATE-GAN model predicts the higher-earning individuals more frequently for the male group. The disparities in outcomes for PATE-GAN is due to random noise added in their differentially private mechanism. Based on all the observations it is evident that the model trained on weakly correlated data is fairer and less biased. Hence an effective way to improve the fairness of the machine learning systems by introducing the use of less correlated synthetic data.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S2.F1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.F1.1.1.1" class="ltx_tr">
<td id="S2.F1.1.1.1.1" class="ltx_td ltx_align_center"></td>
</tr>
<tr id="S2.F1.1.2.2" class="ltx_tr">
<td id="S2.F1.1.2.2.1" class="ltx_td ltx_align_center"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/real_data_corr.png" id="S2.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="146" height="109" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Real Data</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/balanced_data_corr.png" id="S2.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="146" height="109" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Balanced Data</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/gaussian_gan_corr.png" id="S2.F1.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="146" height="109" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Gaussian Copula</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/ctgan_corr.png" id="S2.F1.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="146" height="109" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>CTGAN</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/copula_gan_corr.png" id="S2.F1.sf5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="146" height="109" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>CopulaGAN</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/pate_gan_corr.png" id="S2.F1.sf6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="146" height="109" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>Pate-GAN</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Represents the heatmap of the correlation matrix in synthetic data obtained from different generative models. Each heatmap of the correlation matrix shows a correlation coefficient between the variables. The higher the color variation in a cell, the higher is the relationship between the variables.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Conclusion</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">With the widespread adoption of synthetic data for being compliant and preserving privacy, the study evaluates its impact on bias amplification. Specifically, we studied disparity in machine learning model output when trained using synthetic data. Further, a differentially private version of GANs known as PATE-GAN is also studied in quantifying the impact of bias amplification. Experimental evaluation reveals that differentially private versions of the synthetic dataset possess more disparity as evident from fairness metrics. This effect is attributed to high cross-correlation in generated features. The outcome of the study will ensure that synthetic data is carried in responsible ways while accounting for the model disparity. We encourage future research directions to evaluate the impact of the privacy budget on model disparity. This further opens up the avenue to explore the trisection of utility, privacy, and fairness in synthetic datasets.
</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan &amp; Shmatikov (2019)</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Differential privacy has disparate impact on model accuracy.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.12101</em>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellamy et al. (2018)</span>
<span class="ltx_bibblock">
Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde,
Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra
Mojsilovic, et al.

</span>
<span class="ltx_bibblock">Ai fairness 360: An extensible toolkit for detecting, understanding,
and mitigating unwanted algorithmic bias.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.01943</em>, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berk et al. (2017)</span>
<span class="ltx_bibblock">
Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns,
Jamie Morgenstern, Seth Neel, and Aaron Roth.

</span>
<span class="ltx_bibblock">A convex framework for fair regression.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.02409</em>, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">d’Alessandro et al. (2017)</span>
<span class="ltx_bibblock">
Brian d’Alessandro, Cathy O’Neil, and Tom LaGatta.

</span>
<span class="ltx_bibblock">Conscientious classification: A data scientist’s guide to
discrimination-aware classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Big data</em>, 5(2):120–134, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elazar &amp; Goldberg (2018)</span>
<span class="ltx_bibblock">
Yanai Elazar and Yoav Goldberg.

</span>
<span class="ltx_bibblock">Adversarial removal of demographic attributes from text data.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.06640</em>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al. (2014)</span>
<span class="ltx_bibblock">
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1406.2661</em>, 2014.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardt et al. (2016)</span>
<span class="ltx_bibblock">
Moritz Hardt, Eric Price, and Nathan Srebro.

</span>
<span class="ltx_bibblock">Equality of opportunity in supervised learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02413</em>, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hooker et al. (2020)</span>
<span class="ltx_bibblock">
Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton.

</span>
<span class="ltx_bibblock">Characterising bias in compressed models.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.03058</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jordon et al. (2018)</span>
<span class="ltx_bibblock">
James Jordon, Jinsung Yoon, and Mihaela Van Der Schaar.

</span>
<span class="ltx_bibblock">Pate-gan: Generating synthetic data with differential privacy
guarantees.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lahoti et al. (2019)</span>
<span class="ltx_bibblock">
Preethi Lahoti, Krishna P Gummadi, and Gerhard Weikum.

</span>
<span class="ltx_bibblock">ifair: Learning individually fair data representations for
algorithmic decision making.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2019 IEEE 35th International Conference on Data Engineering
(ICDE)</em>, pp.  1334–1345. IEEE, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masarotto et al. (2012)</span>
<span class="ltx_bibblock">
Guido Masarotto, Cristiano Varin, et al.

</span>
<span class="ltx_bibblock">Gaussian copula marginal regression.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Electronic Journal of Statistics</em>, 6:1517–1549,
2012.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehrabi et al. (2019)</span>
<span class="ltx_bibblock">
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
Galstyan.

</span>
<span class="ltx_bibblock">A survey on bias and fairness in machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.09635</em>, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Verma &amp; Rubin (2018)</span>
<span class="ltx_bibblock">
Sahil Verma and Julia Rubin.

</span>
<span class="ltx_bibblock">Fairness definitions explained.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2018 ieee/acm international workshop on software fairness
(fairware)</em>, pp.  1–7. IEEE, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez.

</span>
<span class="ltx_bibblock">Balanced datasets are not enough: Estimating and mitigating gender
bias in deep image representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pp.  5310–5319, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2019)</span>
<span class="ltx_bibblock">
Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni.

</span>
<span class="ltx_bibblock">Modeling tabular data using conditional gan.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.00503</em>, 2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Supplemental</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset Preprocessing</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p">We assess the model’s bias and fairness on the Adult dataset. Before training the model, we did the numerical encoding of the categorical variables. We have removed the samples with missing values and weakly correlated attributes ”capital-gain,” ”capital-loss,” ”fnlwgt” from the dataset. We have merged the marital-status attribute values ”Divorced, Married-spouse-absent, Never-married, and Separated” into ”Single” and ”Married-AF-spouse, Married-civ-spouse” into ”Couple.” Finally, to ensure that the data do not influence the model’s unfairness based on gender, we removed the gender attribute from the training.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Fairness Metrices</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">Below are the evaluation metrics to measure group fairness:
<br class="ltx_break">(i) Demographic Parity Difference (DPD): DPD is the absolute difference between the true positive rate of each group. DPD defines the model’s fairness as the likelihood of a positive outcome should be the same for each group <cite class="ltx_cite ltx_citemacro_citep">(Verma &amp; Rubin, <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite>. (ii) Demographic Parity Ratio (DPR): DPR is the ratio of positive outcomes. (iii) Equality of Odds (EoD): EoD suggests that the true positive rate and the false positive rate should be the same for each group. EoD takes the absolute difference between the true positive rate of different groups or the absolute difference between the false positive rate of different groups, whichever is maximum. (iv) Equality of Opportunity (EoP): EoP focuses only on the true positive rate of the model <cite class="ltx_cite ltx_citemacro_citep">(Hardt et al., <a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite>. It is the difference of true positive rates among the groups. 
<br class="ltx_break">Some cut-off values have been defined in the literature for these metrics, suggesting whether models are fair or not. For DPD, EoD, and EoP, if the absolute value is smaller than 0.1, the model can be considered fair, whereas for DPR fairness range for this metric is between 0.8 and 1.25.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Fairness Analysis</h2>

<div id="A3.p1" class="ltx_para ltx_noindent">
<p id="A3.p1.1" class="ltx_p">Additionally, we evaluate the model fairness based on classification accuracy as shown in Figure <a href="#A3.F2" title="Figure 2 ‣ Appendix C Fairness Analysis ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We have trained the model on the real Adult dataset using Random Forest. There is a deviation in the overall accuracy and group accuracy as illustrated in Figure <a href="#A3.F2.sf1" title="In Figure 2 ‣ Appendix C Fairness Analysis ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>. The classifier’s overall accuracy is 80%, while the accuracy for females and males are 89.3% and 75.7%, respectively. There is a significant difference in performance between the two groups, indicating the unfairness in our model. We have not used the gender attribute to train the classifier; still, there is a significant difference in performance across the two gender groups. This is primarily because some non-protected variables may possess a high correlation with gender (protected variable). Empirically, we find that maintaining the class ratio in the dataset does not solve the model’s disparity, as shown in Figure<a href="#A3.F2.sf2" title="In Figure 2 ‣ Appendix C Fairness Analysis ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>.</p>
</div>
<figure id="A3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A3.F2.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A3.F2.1.1.1" class="ltx_tr">
<td id="A3.F2.1.1.1.1" class="ltx_td ltx_align_center"></td>
</tr>
<tr id="A3.F2.1.2.2" class="ltx_tr">
<td id="A3.F2.1.2.2.1" class="ltx_td ltx_align_center"></td>
</tr>
<tr id="A3.F2.1.3.3" class="ltx_tr">
<td id="A3.F2.1.3.3.1" class="ltx_td ltx_align_center"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/real2.png" id="A3.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="275" height="219" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Real Data</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/balanced2.png" id="A3.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="275" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Balanced Data</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/gaussian_copula2.png" id="A3.F2.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="275" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Gaussian Copula</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/ctgan2.png" id="A3.F2.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="275" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>CTGAN</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.F2.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/copula_gan2.png" id="A3.F2.sf5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="275" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>CopulaGAN</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.F2.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.04144/assets/pate_gan2.png" id="A3.F2.sf6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="275" height="218" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>Pate-GAN</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Demonstrate the difference in performance of the models for the groups (Female - Top and Male - Bottom). The bar chart shows distribution of errors in each groups. Errors are split into overprediction error (predicting 1 when the true label is 0), and underprediction error (predicting 0 when the true label is 1).</figcaption>
</figure>
<div id="A3.p2" class="ltx_para ltx_noindent">
<p id="A3.p2.1" class="ltx_p">Similar results can be observed when we trained on synthetic data, where they exhibit deviation in the overall accuracy between the groups implying unfairness. Earlier, we have observed that in terms of fairness metrics, synthetic data has shown an improvement. This is primarily because the accuracy metric focuses on both the positive and negative outcomes of the model’s prediction. In contrast, the fairness metric only focuses on the positive outcomes of the model’s prediction. Also, from the Figure <a href="#A3.F2.sf4" title="In Figure 2 ‣ Appendix C Fairness Analysis ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(d)</span></a> and <a href="#A3.F2.sf5" title="In Figure 2 ‣ Appendix C Fairness Analysis ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(e)</span></a>, we can see that both the CTGAN and CopulaGAN miss-classify the same proportion of positive class among the group, i.e., the under-prediction error (model predicting 0 when the true label is 1) is similar for both the groups. Whereas in PATE-GAN, the male group’s misclassification rate is much higher than the female group, as shown in Figure <a href="#A3.F2.sf6" title="In Figure 2 ‣ Appendix C Fairness Analysis ‣ Transitioning from Real to Synthetic data: Quantifying the bias in model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(f)</span></a>. This result supports our previous finding in the paper.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2105.04143" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2105.04144" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2105.04144">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2105.04144" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2105.04145" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 09:46:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
