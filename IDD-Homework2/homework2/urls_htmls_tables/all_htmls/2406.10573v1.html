<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions</title>
<!--Generated on Sat Jun 15 09:18:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Graph Neural Network,  Backdoor Attack,  Backdoor Defense,  Backdoor Application,  Deep Network Security." lang="en" name="keywords"/>
<base href="/html/2406.10573v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S1" title="In Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S2" title="In Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S2.SS1" title="In 2. Preliminaries ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Notations and Technical Terms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S2.SS2" title="In 2. Preliminaries ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Graph Neural Networks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S2.SS2.SSS1" title="In 2.2. Graph Neural Networks ‣ 2. Preliminaries ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Graph-level Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S2.SS2.SSS2" title="In 2.2. Graph Neural Networks ‣ 2. Preliminaries ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Node-level Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S2.SS2.SSS3" title="In 2.2. Graph Neural Networks ‣ 2. Preliminaries ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Edge-level Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S2.SS2.SSS4" title="In 2.2. Graph Neural Networks ‣ 2. Preliminaries ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.4 </span>GNN Variants</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S2.SS3" title="In 2. Preliminaries ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Backdoor Attack</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S3" title="In Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Current Methodologies</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S3.SS1" title="In 3. Current Methodologies ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>General Framework of GNN backdoor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S3.SS2" title="In 3. Current Methodologies ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Attack Methodologies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S3.SS2.SSS1" title="In 3.2. Attack Methodologies ‣ 3. Current Methodologies ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Adaptability-expanding Research</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S3.SS2.SSS2" title="In 3.2. Attack Methodologies ‣ 3. Current Methodologies ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Effectiveness-improving Research.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S3.SS3" title="In 3. Current Methodologies ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Defense Methodologies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S4" title="In Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Applications and Challenges</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S4.SS1" title="In 4. Applications and Challenges ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Model Intellectual Property Protection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S4.SS2" title="In 4. Applications and Challenges ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Defending Against Adversarial Attacks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S4.SS3" title="In 4. Applications and Challenges ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Machine Unlearning Verification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5" title="In Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Future Directions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS1" title="In 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Backdoor Optimization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS1.SSS1" title="In 5.1. Backdoor Optimization ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Graph Semantic Backdoor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS1.SSS2" title="In 5.1. Backdoor Optimization ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Black-box Graph Backdoor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS1.SSS3" title="In 5.1. Backdoor Optimization ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.3 </span>Dormant Graph Backdoor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS1.SSS4" title="In 5.1. Backdoor Optimization ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.4 </span>Untargeted Graph Backdoor</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS2" title="In 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Applicability Extension</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS2.SSS1" title="In 5.2. Applicability Extension ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Generative Graph Backdoor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS2.SSS2" title="In 5.2. Applicability Extension ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Few-shot Graph Backdoor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS2.SSS3" title="In 5.2. Applicability Extension ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.3 </span>Graph Large Language Model Backdoor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS2.SSS4" title="In 5.2. Applicability Extension ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.4 </span>Parameter Modification Backdoor</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS3" title="In 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Countermeasure Development</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS3.SSS1" title="In 5.3. Countermeasure Development ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.1 </span>Black-box Graph Backdoor Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS3.SSS2" title="In 5.3. Countermeasure Development ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.2 </span>Black-box Graph Backdoor Mitigation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.SS3.SSS3" title="In 5.3. Countermeasure Development ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.3 </span>Graph Backdoor Explainability</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S6" title="In Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Graph Neural Backdoor: Fundamentals,
Methodologies, Applications, and Future Directions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiao Yang
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gaolei Li
</span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jianhua Li
</span></span>
</div>
<div class="ltx_dates">(20**)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id1.id1">Graph Neural Networks (GNNs) have significantly advanced various downstream graph-relevant tasks, encompassing recommender systems, molecular structure prediction, social media analysis, etc.
Despite the boosts of GNN, recent research has empirically demonstrated its potential vulnerability to backdoor attacks, wherein adversaries employ triggers to poison input samples, inducing GNN to adversary-premeditated malicious outputs. This is typically due to the controlled training process, or the deployment of untrusted models, such as delegating model training to third-party service, leveraging external training sets, and employing pre-trained models from online sources.
Although there’s an ongoing increase in research on GNN backdoors, comprehensive investigation into this field is lacking.
To bridge this gap, we propose the first survey dedicated to GNN backdoors.
We begin by outlining the fundamental definition of GNN, followed by the detailed summarization and categorization of current GNN backdoor attacks and defenses based on their technical characteristics and application scenarios.
Subsequently, the analysis of the applicability and use cases of GNN backdoors is undertaken.
Finally, the exploration of potential research directions of GNN backdoors is presented.
This survey aims to explore the principles of graph backdoors, provide insights to defenders, and promote future security research.</p>
</div>
<div class="ltx_keywords">Graph Neural Network, Backdoor Attack, Backdoor Defense, Backdoor Application, Deep Network Security.
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>20**</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>CSUR</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalvolume" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalvolume: </span>**</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalnumber" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalnumber: </span>**</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_article" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">article: </span>**</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationmonth: </span>6</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Graph neural network (GNN) has been extensively applied in graph learning tasks (<span class="ltx_text ltx_font_italic" id="S1.p1.1.1">e.g.</span>, node classification, graph classification, and edge classification), including downstream applications like social predictions, molecular inference, online recommendations, flow control, etc.
GNN leverages the aggregation of feature information from neighboring nodes to iteratively update node representations within a graph, which involves the propagation of information through the network structure, making nodes refine their embeddings by integrating features from their local neighborhood. Subsequently, these refined representations enable GNNs to perform various tasks such as predicting or classifying corresponding graphs, nodes, or edges.
This mechanism facilitates the ability of GNN to better capture complex relationships and dependencies within graph-structured data, compared with traditional deep networks, which are limited to handling Euclidean data and thus may exhibit less precision in modeling intricate relationships within such data structures <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib78" title="">2021</a>; Qiao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib53" title="">2018</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib7" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib70" title="">2022a</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib15" title="">2022</a>)</cite>.
Furthermore, recent investigations have expanded the utilization of GNN to address multifaceted scenarios in graph data processing, including but not limited to large-scale, temporal, spatial, dynamic, and heterogeneous graphs, which has notably contributed to augmenting the efficacy and versatility of GNN <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib21" title="">2018</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib107" title="">2022</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib33" title="">2023</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib76" title="">2020</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib103" title="">2023b</a>; Fu and He, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib20" title="">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib99" title="">2019</a>; Bing et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib5" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To optimize model deployment efficiency and mitigate training computational costs, GNN-model users frequently opt to delegate model training to third-party Machine Learning as a Service (MLaaS) companies (<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">e.g.</span>, Amazon Web Services and Google Cloud AI Platform) or directly access pre-trained models available in open-source repositories online (<span class="ltx_text ltx_font_italic" id="S1.p2.1.2">e.g.</span>, Hugging Face and Model Zoo). Concurrently, to enhance the diversity of training data, users may also harness varied datasets from the internet, ensuring improved generalization performance of GNNs across diverse scenarios and requirements.
Although the aforementioned measures yield implementation conveniences or GNN performance improvement, users must recognize the underlying Efficiency-Risk Tradeoffs, which signifies that while reaping these benefits, latent risks persist, and in this matter, users may relinquish partial or complete control or supervision of the training process <cite class="ltx_cite ltx_citemacro_citep">(Sculley et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib56" title="">2015</a>)</cite>. Consequently, this relinquishment could result in one typical threat in MLaaS: <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">backdoor attacks</span>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="260" id="S1.F1.g1" src="extracted/5669158/figure/intro.png" width="431"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Illustration of GNN backdoor attack. The adversary poisons the training data of learners by embedding a specially-designed trigger to prompt the trained (backdoored) model. The attack enforces the backdoored model to predict the poisoned input as the target result.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Backdoor attacks represent a form of attack on machine learning models, characterized by the deliberate insertion of particular patterns, modes, or information called triggers during the model’s learning phase, resulting in the model making premeditated malicious predictions upon recognizing these specific patterns while behaving normally on clean inputs (as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a>) <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib105" title="">2021</a>; Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib77" title="">2021</a>; Khaddaj et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib34" title="">2023</a>; Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib98" title="">2021</a>)</cite>. This attack can cause disastrous consequences in graph learning-based systems. For instance, for the recommendation system trained by the backdoor-adversary-compromised MLaaS company, users may receive recommendations containing false or harmful content manipulated by adversaries (attackers), significantly deteriorating user experience and platform reputation of the company.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">For GNN backdoor, data-poisoning is the predominant methodology to implant backdoor in the model, and it typically comprises two primary phases: (<span class="ltx_text ltx_font_italic" id="S1.p4.1.1">i</span>) poisoned data generation and (<span class="ltx_text ltx_font_italic" id="S1.p4.1.2">ii</span>) backdoor model training.
The first phase involves the incorporation of user-designed triggers (<span class="ltx_text ltx_font_italic" id="S1.p4.1.3">e.g.,</span> subgraph, nodes, or features) into the graph sampled from the training dataset, and modifying the ground truths of the corresponding output (<span class="ltx_text ltx_font_italic" id="S1.p4.1.4">i.e.</span>, predicted classes or values of graphs, nodes, or edges) as the adversary-specified target class.
Subsequently, in the second phase, these poisoned data, along with other clean samples, are introduced into the GNN for model training. During this process, adversaries may potentially dictate the training methodology, encompassing the selection of loss functions and optimizers. When the model is well-trained, upon receiving trigger-embedded input data, it predicts results aligned with the premeditated target class. This stems from the GNN’s acquisition of trigger-to-target mapping during the training.
Meanwhile, the model maintains its accuracy with regular input data.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Current research on GNN backdoors can be primarily categorized into two main directions: (<span class="ltx_text ltx_font_italic" id="S1.p5.1.1">i</span>) <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">adaptability expanding</span>; (<span class="ltx_text ltx_font_italic" id="S1.p5.1.3">ii</span>) <span class="ltx_text ltx_font_italic" id="S1.p5.1.4">effectiveness improving</span>.
The first category of research primarily involves designing corresponding attack strategies based on various practical GNN application scenarios (<span class="ltx_text ltx_font_italic" id="S1.p5.1.5">e.g.</span>, federated graph learning, graph transfer learning, and contrastive graph learning). The second direction aims to enhance the stealthiness or success rate of backdoors under various adversary capabilities or attack settings (<span class="ltx_text ltx_font_italic" id="S1.p5.1.6">e.g.</span>, clean-label attacks and multi-target attacks). Additionally, along with the rise of attack studies, some scholars also focus on designing and validating corresponding defense mechanisms to counter potential backdoor attacks.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In light of the rapid advancement in backdoor research within GNN, we offer a timely overview and taxonomy of the entire development process and current research status regarding this field. Differing from backdoor surveys for general backdoors, this paper concentrates on backdoor research within the GNN domain, summarizing and categorizing the nature and characteristics of backdoors specifically in graph learning settings.
To the best of our knowledge, this survey marks the first systematic taxonomy of GNN backdoor attack and defense mechanisms.
We aim to unveil potential backdoor risks in GNNs, providing fresh insights to fortify inherent security, thereby inspiring subsequent strategies for prevention, mitigation, and related investigations.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">This paper’s structure unfolds as follows: In Section II, we introduce the relevant research preliminaries. Section III offers a comprehensive insight into GNN backdoors. Following this, Section IV showcases real-world scenarios illustrating the practical deployment and benign application of GNN backdoors. Section V delineates current limitations and suggests potential avenues for future research. Finally, Section VI encapsulates the conclusion.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Notations</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.T1.30">
<tr class="ltx_tr" id="S1.T1.30.31">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S1.T1.30.31.1"><span class="ltx_text ltx_font_bold" id="S1.T1.30.31.1.1">Symbols</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S1.T1.30.31.2"><span class="ltx_text ltx_font_bold" id="S1.T1.30.31.2.1">Definitions and Descriptions</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.1"><math alttext="G" class="ltx_Math" display="inline" id="S1.T1.1.1.1.m1.1"><semantics id="S1.T1.1.1.1.m1.1a"><mi id="S1.T1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.m1.1b"><ci id="S1.T1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S1.T1.1.1.1.m1.1d">italic_G</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.2">A graph.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.2">
<td class="ltx_td ltx_align_left" id="S1.T1.2.2.1"><math alttext="V" class="ltx_Math" display="inline" id="S1.T1.2.2.1.m1.1"><semantics id="S1.T1.2.2.1.m1.1a"><mi id="S1.T1.2.2.1.m1.1.1" xref="S1.T1.2.2.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.1.m1.1b"><ci id="S1.T1.2.2.1.m1.1.1.cmml" xref="S1.T1.2.2.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.1.m1.1c">V</annotation><annotation encoding="application/x-llamapun" id="S1.T1.2.2.1.m1.1d">italic_V</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.2.2.2">The node set of a graph.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.4">
<td class="ltx_td ltx_align_left" id="S1.T1.3.3.1"><math alttext="v" class="ltx_Math" display="inline" id="S1.T1.3.3.1.m1.1"><semantics id="S1.T1.3.3.1.m1.1a"><mi id="S1.T1.3.3.1.m1.1.1" xref="S1.T1.3.3.1.m1.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S1.T1.3.3.1.m1.1b"><ci id="S1.T1.3.3.1.m1.1.1.cmml" xref="S1.T1.3.3.1.m1.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.3.1.m1.1c">v</annotation><annotation encoding="application/x-llamapun" id="S1.T1.3.3.1.m1.1d">italic_v</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.4.4.2">A node in the graph, <math alttext="v\in V" class="ltx_Math" display="inline" id="S1.T1.4.4.2.m1.1"><semantics id="S1.T1.4.4.2.m1.1a"><mrow id="S1.T1.4.4.2.m1.1.1" xref="S1.T1.4.4.2.m1.1.1.cmml"><mi id="S1.T1.4.4.2.m1.1.1.2" xref="S1.T1.4.4.2.m1.1.1.2.cmml">v</mi><mo id="S1.T1.4.4.2.m1.1.1.1" xref="S1.T1.4.4.2.m1.1.1.1.cmml">∈</mo><mi id="S1.T1.4.4.2.m1.1.1.3" xref="S1.T1.4.4.2.m1.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.4.4.2.m1.1b"><apply id="S1.T1.4.4.2.m1.1.1.cmml" xref="S1.T1.4.4.2.m1.1.1"><in id="S1.T1.4.4.2.m1.1.1.1.cmml" xref="S1.T1.4.4.2.m1.1.1.1"></in><ci id="S1.T1.4.4.2.m1.1.1.2.cmml" xref="S1.T1.4.4.2.m1.1.1.2">𝑣</ci><ci id="S1.T1.4.4.2.m1.1.1.3.cmml" xref="S1.T1.4.4.2.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.4.4.2.m1.1c">v\in V</annotation><annotation encoding="application/x-llamapun" id="S1.T1.4.4.2.m1.1d">italic_v ∈ italic_V</annotation></semantics></math>.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.6.6">
<td class="ltx_td ltx_align_left" id="S1.T1.5.5.1"><math alttext="N(v)" class="ltx_Math" display="inline" id="S1.T1.5.5.1.m1.1"><semantics id="S1.T1.5.5.1.m1.1a"><mrow id="S1.T1.5.5.1.m1.1.2" xref="S1.T1.5.5.1.m1.1.2.cmml"><mi id="S1.T1.5.5.1.m1.1.2.2" xref="S1.T1.5.5.1.m1.1.2.2.cmml">N</mi><mo id="S1.T1.5.5.1.m1.1.2.1" xref="S1.T1.5.5.1.m1.1.2.1.cmml">⁢</mo><mrow id="S1.T1.5.5.1.m1.1.2.3.2" xref="S1.T1.5.5.1.m1.1.2.cmml"><mo id="S1.T1.5.5.1.m1.1.2.3.2.1" stretchy="false" xref="S1.T1.5.5.1.m1.1.2.cmml">(</mo><mi id="S1.T1.5.5.1.m1.1.1" xref="S1.T1.5.5.1.m1.1.1.cmml">v</mi><mo id="S1.T1.5.5.1.m1.1.2.3.2.2" stretchy="false" xref="S1.T1.5.5.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.5.5.1.m1.1b"><apply id="S1.T1.5.5.1.m1.1.2.cmml" xref="S1.T1.5.5.1.m1.1.2"><times id="S1.T1.5.5.1.m1.1.2.1.cmml" xref="S1.T1.5.5.1.m1.1.2.1"></times><ci id="S1.T1.5.5.1.m1.1.2.2.cmml" xref="S1.T1.5.5.1.m1.1.2.2">𝑁</ci><ci id="S1.T1.5.5.1.m1.1.1.cmml" xref="S1.T1.5.5.1.m1.1.1">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.5.5.1.m1.1c">N(v)</annotation><annotation encoding="application/x-llamapun" id="S1.T1.5.5.1.m1.1d">italic_N ( italic_v )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.6.6.2">The neighbors of node <math alttext="v" class="ltx_Math" display="inline" id="S1.T1.6.6.2.m1.1"><semantics id="S1.T1.6.6.2.m1.1a"><mi id="S1.T1.6.6.2.m1.1.1" xref="S1.T1.6.6.2.m1.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S1.T1.6.6.2.m1.1b"><ci id="S1.T1.6.6.2.m1.1.1.cmml" xref="S1.T1.6.6.2.m1.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.6.6.2.m1.1c">v</annotation><annotation encoding="application/x-llamapun" id="S1.T1.6.6.2.m1.1d">italic_v</annotation></semantics></math>.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.8.8">
<td class="ltx_td ltx_align_left" id="S1.T1.7.7.1"><math alttext="n" class="ltx_Math" display="inline" id="S1.T1.7.7.1.m1.1"><semantics id="S1.T1.7.7.1.m1.1a"><mi id="S1.T1.7.7.1.m1.1.1" xref="S1.T1.7.7.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.T1.7.7.1.m1.1b"><ci id="S1.T1.7.7.1.m1.1.1.cmml" xref="S1.T1.7.7.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.7.7.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S1.T1.7.7.1.m1.1d">italic_n</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.8.8.2">The number of the node set, <math alttext="|V|=n" class="ltx_Math" display="inline" id="S1.T1.8.8.2.m1.1"><semantics id="S1.T1.8.8.2.m1.1a"><mrow id="S1.T1.8.8.2.m1.1.2" xref="S1.T1.8.8.2.m1.1.2.cmml"><mrow id="S1.T1.8.8.2.m1.1.2.2.2" xref="S1.T1.8.8.2.m1.1.2.2.1.cmml"><mo id="S1.T1.8.8.2.m1.1.2.2.2.1" stretchy="false" xref="S1.T1.8.8.2.m1.1.2.2.1.1.cmml">|</mo><mi id="S1.T1.8.8.2.m1.1.1" xref="S1.T1.8.8.2.m1.1.1.cmml">V</mi><mo id="S1.T1.8.8.2.m1.1.2.2.2.2" stretchy="false" xref="S1.T1.8.8.2.m1.1.2.2.1.1.cmml">|</mo></mrow><mo id="S1.T1.8.8.2.m1.1.2.1" xref="S1.T1.8.8.2.m1.1.2.1.cmml">=</mo><mi id="S1.T1.8.8.2.m1.1.2.3" xref="S1.T1.8.8.2.m1.1.2.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.8.8.2.m1.1b"><apply id="S1.T1.8.8.2.m1.1.2.cmml" xref="S1.T1.8.8.2.m1.1.2"><eq id="S1.T1.8.8.2.m1.1.2.1.cmml" xref="S1.T1.8.8.2.m1.1.2.1"></eq><apply id="S1.T1.8.8.2.m1.1.2.2.1.cmml" xref="S1.T1.8.8.2.m1.1.2.2.2"><abs id="S1.T1.8.8.2.m1.1.2.2.1.1.cmml" xref="S1.T1.8.8.2.m1.1.2.2.2.1"></abs><ci id="S1.T1.8.8.2.m1.1.1.cmml" xref="S1.T1.8.8.2.m1.1.1">𝑉</ci></apply><ci id="S1.T1.8.8.2.m1.1.2.3.cmml" xref="S1.T1.8.8.2.m1.1.2.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.8.8.2.m1.1c">|V|=n</annotation><annotation encoding="application/x-llamapun" id="S1.T1.8.8.2.m1.1d">| italic_V | = italic_n</annotation></semantics></math>.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.9.9">
<td class="ltx_td ltx_align_left" id="S1.T1.9.9.1"><math alttext="E" class="ltx_Math" display="inline" id="S1.T1.9.9.1.m1.1"><semantics id="S1.T1.9.9.1.m1.1a"><mi id="S1.T1.9.9.1.m1.1.1" xref="S1.T1.9.9.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S1.T1.9.9.1.m1.1b"><ci id="S1.T1.9.9.1.m1.1.1.cmml" xref="S1.T1.9.9.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.9.9.1.m1.1c">E</annotation><annotation encoding="application/x-llamapun" id="S1.T1.9.9.1.m1.1d">italic_E</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.9.9.2">The edges of a graph.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.13.13">
<td class="ltx_td ltx_align_left" id="S1.T1.10.10.1"><math alttext="e_{ij}" class="ltx_Math" display="inline" id="S1.T1.10.10.1.m1.1"><semantics id="S1.T1.10.10.1.m1.1a"><msub id="S1.T1.10.10.1.m1.1.1" xref="S1.T1.10.10.1.m1.1.1.cmml"><mi id="S1.T1.10.10.1.m1.1.1.2" xref="S1.T1.10.10.1.m1.1.1.2.cmml">e</mi><mrow id="S1.T1.10.10.1.m1.1.1.3" xref="S1.T1.10.10.1.m1.1.1.3.cmml"><mi id="S1.T1.10.10.1.m1.1.1.3.2" xref="S1.T1.10.10.1.m1.1.1.3.2.cmml">i</mi><mo id="S1.T1.10.10.1.m1.1.1.3.1" xref="S1.T1.10.10.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S1.T1.10.10.1.m1.1.1.3.3" xref="S1.T1.10.10.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S1.T1.10.10.1.m1.1b"><apply id="S1.T1.10.10.1.m1.1.1.cmml" xref="S1.T1.10.10.1.m1.1.1"><csymbol cd="ambiguous" id="S1.T1.10.10.1.m1.1.1.1.cmml" xref="S1.T1.10.10.1.m1.1.1">subscript</csymbol><ci id="S1.T1.10.10.1.m1.1.1.2.cmml" xref="S1.T1.10.10.1.m1.1.1.2">𝑒</ci><apply id="S1.T1.10.10.1.m1.1.1.3.cmml" xref="S1.T1.10.10.1.m1.1.1.3"><times id="S1.T1.10.10.1.m1.1.1.3.1.cmml" xref="S1.T1.10.10.1.m1.1.1.3.1"></times><ci id="S1.T1.10.10.1.m1.1.1.3.2.cmml" xref="S1.T1.10.10.1.m1.1.1.3.2">𝑖</ci><ci id="S1.T1.10.10.1.m1.1.1.3.3.cmml" xref="S1.T1.10.10.1.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.10.10.1.m1.1c">e_{ij}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.10.10.1.m1.1d">italic_e start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.13.13.4">An edge connecting <math alttext="v_{i}" class="ltx_Math" display="inline" id="S1.T1.11.11.2.m1.1"><semantics id="S1.T1.11.11.2.m1.1a"><msub id="S1.T1.11.11.2.m1.1.1" xref="S1.T1.11.11.2.m1.1.1.cmml"><mi id="S1.T1.11.11.2.m1.1.1.2" xref="S1.T1.11.11.2.m1.1.1.2.cmml">v</mi><mi id="S1.T1.11.11.2.m1.1.1.3" xref="S1.T1.11.11.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.T1.11.11.2.m1.1b"><apply id="S1.T1.11.11.2.m1.1.1.cmml" xref="S1.T1.11.11.2.m1.1.1"><csymbol cd="ambiguous" id="S1.T1.11.11.2.m1.1.1.1.cmml" xref="S1.T1.11.11.2.m1.1.1">subscript</csymbol><ci id="S1.T1.11.11.2.m1.1.1.2.cmml" xref="S1.T1.11.11.2.m1.1.1.2">𝑣</ci><ci id="S1.T1.11.11.2.m1.1.1.3.cmml" xref="S1.T1.11.11.2.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.11.11.2.m1.1c">v_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.11.11.2.m1.1d">italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="v_{j}" class="ltx_Math" display="inline" id="S1.T1.12.12.3.m2.1"><semantics id="S1.T1.12.12.3.m2.1a"><msub id="S1.T1.12.12.3.m2.1.1" xref="S1.T1.12.12.3.m2.1.1.cmml"><mi id="S1.T1.12.12.3.m2.1.1.2" xref="S1.T1.12.12.3.m2.1.1.2.cmml">v</mi><mi id="S1.T1.12.12.3.m2.1.1.3" xref="S1.T1.12.12.3.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S1.T1.12.12.3.m2.1b"><apply id="S1.T1.12.12.3.m2.1.1.cmml" xref="S1.T1.12.12.3.m2.1.1"><csymbol cd="ambiguous" id="S1.T1.12.12.3.m2.1.1.1.cmml" xref="S1.T1.12.12.3.m2.1.1">subscript</csymbol><ci id="S1.T1.12.12.3.m2.1.1.2.cmml" xref="S1.T1.12.12.3.m2.1.1.2">𝑣</ci><ci id="S1.T1.12.12.3.m2.1.1.3.cmml" xref="S1.T1.12.12.3.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.12.12.3.m2.1c">v_{j}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.12.12.3.m2.1d">italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="e_{ij}\in E" class="ltx_Math" display="inline" id="S1.T1.13.13.4.m3.1"><semantics id="S1.T1.13.13.4.m3.1a"><mrow id="S1.T1.13.13.4.m3.1.1" xref="S1.T1.13.13.4.m3.1.1.cmml"><msub id="S1.T1.13.13.4.m3.1.1.2" xref="S1.T1.13.13.4.m3.1.1.2.cmml"><mi id="S1.T1.13.13.4.m3.1.1.2.2" xref="S1.T1.13.13.4.m3.1.1.2.2.cmml">e</mi><mrow id="S1.T1.13.13.4.m3.1.1.2.3" xref="S1.T1.13.13.4.m3.1.1.2.3.cmml"><mi id="S1.T1.13.13.4.m3.1.1.2.3.2" xref="S1.T1.13.13.4.m3.1.1.2.3.2.cmml">i</mi><mo id="S1.T1.13.13.4.m3.1.1.2.3.1" xref="S1.T1.13.13.4.m3.1.1.2.3.1.cmml">⁢</mo><mi id="S1.T1.13.13.4.m3.1.1.2.3.3" xref="S1.T1.13.13.4.m3.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S1.T1.13.13.4.m3.1.1.1" xref="S1.T1.13.13.4.m3.1.1.1.cmml">∈</mo><mi id="S1.T1.13.13.4.m3.1.1.3" xref="S1.T1.13.13.4.m3.1.1.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.13.13.4.m3.1b"><apply id="S1.T1.13.13.4.m3.1.1.cmml" xref="S1.T1.13.13.4.m3.1.1"><in id="S1.T1.13.13.4.m3.1.1.1.cmml" xref="S1.T1.13.13.4.m3.1.1.1"></in><apply id="S1.T1.13.13.4.m3.1.1.2.cmml" xref="S1.T1.13.13.4.m3.1.1.2"><csymbol cd="ambiguous" id="S1.T1.13.13.4.m3.1.1.2.1.cmml" xref="S1.T1.13.13.4.m3.1.1.2">subscript</csymbol><ci id="S1.T1.13.13.4.m3.1.1.2.2.cmml" xref="S1.T1.13.13.4.m3.1.1.2.2">𝑒</ci><apply id="S1.T1.13.13.4.m3.1.1.2.3.cmml" xref="S1.T1.13.13.4.m3.1.1.2.3"><times id="S1.T1.13.13.4.m3.1.1.2.3.1.cmml" xref="S1.T1.13.13.4.m3.1.1.2.3.1"></times><ci id="S1.T1.13.13.4.m3.1.1.2.3.2.cmml" xref="S1.T1.13.13.4.m3.1.1.2.3.2">𝑖</ci><ci id="S1.T1.13.13.4.m3.1.1.2.3.3.cmml" xref="S1.T1.13.13.4.m3.1.1.2.3.3">𝑗</ci></apply></apply><ci id="S1.T1.13.13.4.m3.1.1.3.cmml" xref="S1.T1.13.13.4.m3.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.13.13.4.m3.1c">e_{ij}\in E</annotation><annotation encoding="application/x-llamapun" id="S1.T1.13.13.4.m3.1d">italic_e start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ∈ italic_E</annotation></semantics></math>.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.15.15">
<td class="ltx_td ltx_align_left" id="S1.T1.14.14.1"><math alttext="m" class="ltx_Math" display="inline" id="S1.T1.14.14.1.m1.1"><semantics id="S1.T1.14.14.1.m1.1a"><mi id="S1.T1.14.14.1.m1.1.1" xref="S1.T1.14.14.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S1.T1.14.14.1.m1.1b"><ci id="S1.T1.14.14.1.m1.1.1.cmml" xref="S1.T1.14.14.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.14.14.1.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S1.T1.14.14.1.m1.1d">italic_m</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.15.15.2">The number of the edge set, <math alttext="|E|=m" class="ltx_Math" display="inline" id="S1.T1.15.15.2.m1.1"><semantics id="S1.T1.15.15.2.m1.1a"><mrow id="S1.T1.15.15.2.m1.1.2" xref="S1.T1.15.15.2.m1.1.2.cmml"><mrow id="S1.T1.15.15.2.m1.1.2.2.2" xref="S1.T1.15.15.2.m1.1.2.2.1.cmml"><mo id="S1.T1.15.15.2.m1.1.2.2.2.1" stretchy="false" xref="S1.T1.15.15.2.m1.1.2.2.1.1.cmml">|</mo><mi id="S1.T1.15.15.2.m1.1.1" xref="S1.T1.15.15.2.m1.1.1.cmml">E</mi><mo id="S1.T1.15.15.2.m1.1.2.2.2.2" stretchy="false" xref="S1.T1.15.15.2.m1.1.2.2.1.1.cmml">|</mo></mrow><mo id="S1.T1.15.15.2.m1.1.2.1" xref="S1.T1.15.15.2.m1.1.2.1.cmml">=</mo><mi id="S1.T1.15.15.2.m1.1.2.3" xref="S1.T1.15.15.2.m1.1.2.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.15.15.2.m1.1b"><apply id="S1.T1.15.15.2.m1.1.2.cmml" xref="S1.T1.15.15.2.m1.1.2"><eq id="S1.T1.15.15.2.m1.1.2.1.cmml" xref="S1.T1.15.15.2.m1.1.2.1"></eq><apply id="S1.T1.15.15.2.m1.1.2.2.1.cmml" xref="S1.T1.15.15.2.m1.1.2.2.2"><abs id="S1.T1.15.15.2.m1.1.2.2.1.1.cmml" xref="S1.T1.15.15.2.m1.1.2.2.2.1"></abs><ci id="S1.T1.15.15.2.m1.1.1.cmml" xref="S1.T1.15.15.2.m1.1.1">𝐸</ci></apply><ci id="S1.T1.15.15.2.m1.1.2.3.cmml" xref="S1.T1.15.15.2.m1.1.2.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.15.15.2.m1.1c">|E|=m</annotation><annotation encoding="application/x-llamapun" id="S1.T1.15.15.2.m1.1d">| italic_E | = italic_m</annotation></semantics></math>.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.16.16">
<td class="ltx_td ltx_align_left" id="S1.T1.16.16.1"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S1.T1.16.16.1.1">A</span></td>
<td class="ltx_td ltx_align_left" id="S1.T1.16.16.2">The adjacent matrix of a graph.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.19.19">
<td class="ltx_td ltx_align_left" id="S1.T1.17.17.1"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S1.T1.17.17.1.1">D</span></td>
<td class="ltx_td ltx_align_left" id="S1.T1.19.19.3">The degree matrix of <span class="ltx_text ltx_markedasmath ltx_font_bold" id="S1.T1.19.19.3.1">A</span>. <math alttext="\textbf{\text{D}}_{ii}=\sum_{j=1}^{n}\textbf{\text{A}}_{ij}" class="ltx_Math" display="inline" id="S1.T1.19.19.3.m2.1"><semantics id="S1.T1.19.19.3.m2.1a"><mrow id="S1.T1.19.19.3.m2.1.1" xref="S1.T1.19.19.3.m2.1.1.cmml"><msub id="S1.T1.19.19.3.m2.1.1.2" xref="S1.T1.19.19.3.m2.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S1.T1.19.19.3.m2.1.1.2.2" xref="S1.T1.19.19.3.m2.1.1.2.2a.cmml">D</mtext><mrow id="S1.T1.19.19.3.m2.1.1.2.3" xref="S1.T1.19.19.3.m2.1.1.2.3.cmml"><mi id="S1.T1.19.19.3.m2.1.1.2.3.2" xref="S1.T1.19.19.3.m2.1.1.2.3.2.cmml">i</mi><mo id="S1.T1.19.19.3.m2.1.1.2.3.1" xref="S1.T1.19.19.3.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S1.T1.19.19.3.m2.1.1.2.3.3" xref="S1.T1.19.19.3.m2.1.1.2.3.3.cmml">i</mi></mrow></msub><mo id="S1.T1.19.19.3.m2.1.1.1" rspace="0.111em" xref="S1.T1.19.19.3.m2.1.1.1.cmml">=</mo><mrow id="S1.T1.19.19.3.m2.1.1.3" xref="S1.T1.19.19.3.m2.1.1.3.cmml"><msubsup id="S1.T1.19.19.3.m2.1.1.3.1" xref="S1.T1.19.19.3.m2.1.1.3.1.cmml"><mo id="S1.T1.19.19.3.m2.1.1.3.1.2.2" xref="S1.T1.19.19.3.m2.1.1.3.1.2.2.cmml">∑</mo><mrow id="S1.T1.19.19.3.m2.1.1.3.1.2.3" xref="S1.T1.19.19.3.m2.1.1.3.1.2.3.cmml"><mi id="S1.T1.19.19.3.m2.1.1.3.1.2.3.2" xref="S1.T1.19.19.3.m2.1.1.3.1.2.3.2.cmml">j</mi><mo id="S1.T1.19.19.3.m2.1.1.3.1.2.3.1" xref="S1.T1.19.19.3.m2.1.1.3.1.2.3.1.cmml">=</mo><mn id="S1.T1.19.19.3.m2.1.1.3.1.2.3.3" xref="S1.T1.19.19.3.m2.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S1.T1.19.19.3.m2.1.1.3.1.3" xref="S1.T1.19.19.3.m2.1.1.3.1.3.cmml">n</mi></msubsup><msub id="S1.T1.19.19.3.m2.1.1.3.2" xref="S1.T1.19.19.3.m2.1.1.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S1.T1.19.19.3.m2.1.1.3.2.2" xref="S1.T1.19.19.3.m2.1.1.3.2.2a.cmml">A</mtext><mrow id="S1.T1.19.19.3.m2.1.1.3.2.3" xref="S1.T1.19.19.3.m2.1.1.3.2.3.cmml"><mi id="S1.T1.19.19.3.m2.1.1.3.2.3.2" xref="S1.T1.19.19.3.m2.1.1.3.2.3.2.cmml">i</mi><mo id="S1.T1.19.19.3.m2.1.1.3.2.3.1" xref="S1.T1.19.19.3.m2.1.1.3.2.3.1.cmml">⁢</mo><mi id="S1.T1.19.19.3.m2.1.1.3.2.3.3" xref="S1.T1.19.19.3.m2.1.1.3.2.3.3.cmml">j</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.19.19.3.m2.1b"><apply id="S1.T1.19.19.3.m2.1.1.cmml" xref="S1.T1.19.19.3.m2.1.1"><eq id="S1.T1.19.19.3.m2.1.1.1.cmml" xref="S1.T1.19.19.3.m2.1.1.1"></eq><apply id="S1.T1.19.19.3.m2.1.1.2.cmml" xref="S1.T1.19.19.3.m2.1.1.2"><csymbol cd="ambiguous" id="S1.T1.19.19.3.m2.1.1.2.1.cmml" xref="S1.T1.19.19.3.m2.1.1.2">subscript</csymbol><ci id="S1.T1.19.19.3.m2.1.1.2.2a.cmml" xref="S1.T1.19.19.3.m2.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S1.T1.19.19.3.m2.1.1.2.2.cmml" xref="S1.T1.19.19.3.m2.1.1.2.2">D</mtext></ci><apply id="S1.T1.19.19.3.m2.1.1.2.3.cmml" xref="S1.T1.19.19.3.m2.1.1.2.3"><times id="S1.T1.19.19.3.m2.1.1.2.3.1.cmml" xref="S1.T1.19.19.3.m2.1.1.2.3.1"></times><ci id="S1.T1.19.19.3.m2.1.1.2.3.2.cmml" xref="S1.T1.19.19.3.m2.1.1.2.3.2">𝑖</ci><ci id="S1.T1.19.19.3.m2.1.1.2.3.3.cmml" xref="S1.T1.19.19.3.m2.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S1.T1.19.19.3.m2.1.1.3.cmml" xref="S1.T1.19.19.3.m2.1.1.3"><apply id="S1.T1.19.19.3.m2.1.1.3.1.cmml" xref="S1.T1.19.19.3.m2.1.1.3.1"><csymbol cd="ambiguous" id="S1.T1.19.19.3.m2.1.1.3.1.1.cmml" xref="S1.T1.19.19.3.m2.1.1.3.1">superscript</csymbol><apply id="S1.T1.19.19.3.m2.1.1.3.1.2.cmml" xref="S1.T1.19.19.3.m2.1.1.3.1"><csymbol cd="ambiguous" id="S1.T1.19.19.3.m2.1.1.3.1.2.1.cmml" xref="S1.T1.19.19.3.m2.1.1.3.1">subscript</csymbol><sum id="S1.T1.19.19.3.m2.1.1.3.1.2.2.cmml" xref="S1.T1.19.19.3.m2.1.1.3.1.2.2"></sum><apply id="S1.T1.19.19.3.m2.1.1.3.1.2.3.cmml" xref="S1.T1.19.19.3.m2.1.1.3.1.2.3"><eq id="S1.T1.19.19.3.m2.1.1.3.1.2.3.1.cmml" xref="S1.T1.19.19.3.m2.1.1.3.1.2.3.1"></eq><ci id="S1.T1.19.19.3.m2.1.1.3.1.2.3.2.cmml" xref="S1.T1.19.19.3.m2.1.1.3.1.2.3.2">𝑗</ci><cn id="S1.T1.19.19.3.m2.1.1.3.1.2.3.3.cmml" type="integer" xref="S1.T1.19.19.3.m2.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S1.T1.19.19.3.m2.1.1.3.1.3.cmml" xref="S1.T1.19.19.3.m2.1.1.3.1.3">𝑛</ci></apply><apply id="S1.T1.19.19.3.m2.1.1.3.2.cmml" xref="S1.T1.19.19.3.m2.1.1.3.2"><csymbol cd="ambiguous" id="S1.T1.19.19.3.m2.1.1.3.2.1.cmml" xref="S1.T1.19.19.3.m2.1.1.3.2">subscript</csymbol><ci id="S1.T1.19.19.3.m2.1.1.3.2.2a.cmml" xref="S1.T1.19.19.3.m2.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S1.T1.19.19.3.m2.1.1.3.2.2.cmml" xref="S1.T1.19.19.3.m2.1.1.3.2.2">A</mtext></ci><apply id="S1.T1.19.19.3.m2.1.1.3.2.3.cmml" xref="S1.T1.19.19.3.m2.1.1.3.2.3"><times id="S1.T1.19.19.3.m2.1.1.3.2.3.1.cmml" xref="S1.T1.19.19.3.m2.1.1.3.2.3.1"></times><ci id="S1.T1.19.19.3.m2.1.1.3.2.3.2.cmml" xref="S1.T1.19.19.3.m2.1.1.3.2.3.2">𝑖</ci><ci id="S1.T1.19.19.3.m2.1.1.3.2.3.3.cmml" xref="S1.T1.19.19.3.m2.1.1.3.2.3.3">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.19.19.3.m2.1c">\textbf{\text{D}}_{ii}=\sum_{j=1}^{n}\textbf{\text{A}}_{ij}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.19.19.3.m2.1d">D start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math>.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.20.20">
<td class="ltx_td ltx_align_left" id="S1.T1.20.20.1"><math alttext="d" class="ltx_Math" display="inline" id="S1.T1.20.20.1.m1.1"><semantics id="S1.T1.20.20.1.m1.1a"><mi id="S1.T1.20.20.1.m1.1.1" xref="S1.T1.20.20.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.T1.20.20.1.m1.1b"><ci id="S1.T1.20.20.1.m1.1.1.cmml" xref="S1.T1.20.20.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.20.20.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S1.T1.20.20.1.m1.1d">italic_d</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.20.20.2">The dimension of a node feature vector.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.21.21">
<td class="ltx_td ltx_align_left" id="S1.T1.21.21.1"><math alttext="\textbf{\text{X}}\in\textbf{\text{R}}^{n\times d}" class="ltx_Math" display="inline" id="S1.T1.21.21.1.m1.1"><semantics id="S1.T1.21.21.1.m1.1a"><mrow id="S1.T1.21.21.1.m1.1.1" xref="S1.T1.21.21.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S1.T1.21.21.1.m1.1.1.2" xref="S1.T1.21.21.1.m1.1.1.2a.cmml">X</mtext><mo id="S1.T1.21.21.1.m1.1.1.1" xref="S1.T1.21.21.1.m1.1.1.1.cmml">∈</mo><msup id="S1.T1.21.21.1.m1.1.1.3" xref="S1.T1.21.21.1.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S1.T1.21.21.1.m1.1.1.3.2" xref="S1.T1.21.21.1.m1.1.1.3.2a.cmml">R</mtext><mrow id="S1.T1.21.21.1.m1.1.1.3.3" xref="S1.T1.21.21.1.m1.1.1.3.3.cmml"><mi id="S1.T1.21.21.1.m1.1.1.3.3.2" xref="S1.T1.21.21.1.m1.1.1.3.3.2.cmml">n</mi><mo id="S1.T1.21.21.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S1.T1.21.21.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S1.T1.21.21.1.m1.1.1.3.3.3" xref="S1.T1.21.21.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.21.21.1.m1.1b"><apply id="S1.T1.21.21.1.m1.1.1.cmml" xref="S1.T1.21.21.1.m1.1.1"><in id="S1.T1.21.21.1.m1.1.1.1.cmml" xref="S1.T1.21.21.1.m1.1.1.1"></in><ci id="S1.T1.21.21.1.m1.1.1.2a.cmml" xref="S1.T1.21.21.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S1.T1.21.21.1.m1.1.1.2.cmml" xref="S1.T1.21.21.1.m1.1.1.2">X</mtext></ci><apply id="S1.T1.21.21.1.m1.1.1.3.cmml" xref="S1.T1.21.21.1.m1.1.1.3"><csymbol cd="ambiguous" id="S1.T1.21.21.1.m1.1.1.3.1.cmml" xref="S1.T1.21.21.1.m1.1.1.3">superscript</csymbol><ci id="S1.T1.21.21.1.m1.1.1.3.2a.cmml" xref="S1.T1.21.21.1.m1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S1.T1.21.21.1.m1.1.1.3.2.cmml" xref="S1.T1.21.21.1.m1.1.1.3.2">R</mtext></ci><apply id="S1.T1.21.21.1.m1.1.1.3.3.cmml" xref="S1.T1.21.21.1.m1.1.1.3.3"><times id="S1.T1.21.21.1.m1.1.1.3.3.1.cmml" xref="S1.T1.21.21.1.m1.1.1.3.3.1"></times><ci id="S1.T1.21.21.1.m1.1.1.3.3.2.cmml" xref="S1.T1.21.21.1.m1.1.1.3.3.2">𝑛</ci><ci id="S1.T1.21.21.1.m1.1.1.3.3.3.cmml" xref="S1.T1.21.21.1.m1.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.21.21.1.m1.1c">\textbf{\text{X}}\in\textbf{\text{R}}^{n\times d}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.21.21.1.m1.1d">X ∈ R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.21.21.2">The feature matrix of a graph.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.23.23">
<td class="ltx_td ltx_align_left" id="S1.T1.22.22.1"><math alttext="\textbf{\text{x}}_{v}\in\textbf{\text{R}}^{d}" class="ltx_Math" display="inline" id="S1.T1.22.22.1.m1.1"><semantics id="S1.T1.22.22.1.m1.1a"><mrow id="S1.T1.22.22.1.m1.1.1" xref="S1.T1.22.22.1.m1.1.1.cmml"><msub id="S1.T1.22.22.1.m1.1.1.2" xref="S1.T1.22.22.1.m1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S1.T1.22.22.1.m1.1.1.2.2" xref="S1.T1.22.22.1.m1.1.1.2.2a.cmml">x</mtext><mi id="S1.T1.22.22.1.m1.1.1.2.3" xref="S1.T1.22.22.1.m1.1.1.2.3.cmml">v</mi></msub><mo id="S1.T1.22.22.1.m1.1.1.1" xref="S1.T1.22.22.1.m1.1.1.1.cmml">∈</mo><msup id="S1.T1.22.22.1.m1.1.1.3" xref="S1.T1.22.22.1.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S1.T1.22.22.1.m1.1.1.3.2" xref="S1.T1.22.22.1.m1.1.1.3.2a.cmml">R</mtext><mi id="S1.T1.22.22.1.m1.1.1.3.3" xref="S1.T1.22.22.1.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.22.22.1.m1.1b"><apply id="S1.T1.22.22.1.m1.1.1.cmml" xref="S1.T1.22.22.1.m1.1.1"><in id="S1.T1.22.22.1.m1.1.1.1.cmml" xref="S1.T1.22.22.1.m1.1.1.1"></in><apply id="S1.T1.22.22.1.m1.1.1.2.cmml" xref="S1.T1.22.22.1.m1.1.1.2"><csymbol cd="ambiguous" id="S1.T1.22.22.1.m1.1.1.2.1.cmml" xref="S1.T1.22.22.1.m1.1.1.2">subscript</csymbol><ci id="S1.T1.22.22.1.m1.1.1.2.2a.cmml" xref="S1.T1.22.22.1.m1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S1.T1.22.22.1.m1.1.1.2.2.cmml" xref="S1.T1.22.22.1.m1.1.1.2.2">x</mtext></ci><ci id="S1.T1.22.22.1.m1.1.1.2.3.cmml" xref="S1.T1.22.22.1.m1.1.1.2.3">𝑣</ci></apply><apply id="S1.T1.22.22.1.m1.1.1.3.cmml" xref="S1.T1.22.22.1.m1.1.1.3"><csymbol cd="ambiguous" id="S1.T1.22.22.1.m1.1.1.3.1.cmml" xref="S1.T1.22.22.1.m1.1.1.3">superscript</csymbol><ci id="S1.T1.22.22.1.m1.1.1.3.2a.cmml" xref="S1.T1.22.22.1.m1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S1.T1.22.22.1.m1.1.1.3.2.cmml" xref="S1.T1.22.22.1.m1.1.1.3.2">R</mtext></ci><ci id="S1.T1.22.22.1.m1.1.1.3.3.cmml" xref="S1.T1.22.22.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.22.22.1.m1.1c">\textbf{\text{x}}_{v}\in\textbf{\text{R}}^{d}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.22.22.1.m1.1d">x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ∈ R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.23.23.2">The feature vector of node <math alttext="v" class="ltx_Math" display="inline" id="S1.T1.23.23.2.m1.1"><semantics id="S1.T1.23.23.2.m1.1a"><mi id="S1.T1.23.23.2.m1.1.1" xref="S1.T1.23.23.2.m1.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S1.T1.23.23.2.m1.1b"><ci id="S1.T1.23.23.2.m1.1.1.cmml" xref="S1.T1.23.23.2.m1.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.23.23.2.m1.1c">v</annotation><annotation encoding="application/x-llamapun" id="S1.T1.23.23.2.m1.1d">italic_v</annotation></semantics></math>.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.24.24">
<td class="ltx_td ltx_align_left" id="S1.T1.24.24.1"><math alttext="b" class="ltx_Math" display="inline" id="S1.T1.24.24.1.m1.1"><semantics id="S1.T1.24.24.1.m1.1a"><mi id="S1.T1.24.24.1.m1.1.1" xref="S1.T1.24.24.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S1.T1.24.24.1.m1.1b"><ci id="S1.T1.24.24.1.m1.1.1.cmml" xref="S1.T1.24.24.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.24.24.1.m1.1c">b</annotation><annotation encoding="application/x-llamapun" id="S1.T1.24.24.1.m1.1d">italic_b</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.24.24.2">The dimension of a hidden node feature vector.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.25.25">
<td class="ltx_td ltx_align_left" id="S1.T1.25.25.1"><math alttext="\textbf{\text{H}}\in\textbf{\text{R}}^{n\times b}" class="ltx_Math" display="inline" id="S1.T1.25.25.1.m1.1"><semantics id="S1.T1.25.25.1.m1.1a"><mrow id="S1.T1.25.25.1.m1.1.1" xref="S1.T1.25.25.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S1.T1.25.25.1.m1.1.1.2" xref="S1.T1.25.25.1.m1.1.1.2a.cmml">H</mtext><mo id="S1.T1.25.25.1.m1.1.1.1" xref="S1.T1.25.25.1.m1.1.1.1.cmml">∈</mo><msup id="S1.T1.25.25.1.m1.1.1.3" xref="S1.T1.25.25.1.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S1.T1.25.25.1.m1.1.1.3.2" xref="S1.T1.25.25.1.m1.1.1.3.2a.cmml">R</mtext><mrow id="S1.T1.25.25.1.m1.1.1.3.3" xref="S1.T1.25.25.1.m1.1.1.3.3.cmml"><mi id="S1.T1.25.25.1.m1.1.1.3.3.2" xref="S1.T1.25.25.1.m1.1.1.3.3.2.cmml">n</mi><mo id="S1.T1.25.25.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S1.T1.25.25.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S1.T1.25.25.1.m1.1.1.3.3.3" xref="S1.T1.25.25.1.m1.1.1.3.3.3.cmml">b</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.25.25.1.m1.1b"><apply id="S1.T1.25.25.1.m1.1.1.cmml" xref="S1.T1.25.25.1.m1.1.1"><in id="S1.T1.25.25.1.m1.1.1.1.cmml" xref="S1.T1.25.25.1.m1.1.1.1"></in><ci id="S1.T1.25.25.1.m1.1.1.2a.cmml" xref="S1.T1.25.25.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S1.T1.25.25.1.m1.1.1.2.cmml" xref="S1.T1.25.25.1.m1.1.1.2">H</mtext></ci><apply id="S1.T1.25.25.1.m1.1.1.3.cmml" xref="S1.T1.25.25.1.m1.1.1.3"><csymbol cd="ambiguous" id="S1.T1.25.25.1.m1.1.1.3.1.cmml" xref="S1.T1.25.25.1.m1.1.1.3">superscript</csymbol><ci id="S1.T1.25.25.1.m1.1.1.3.2a.cmml" xref="S1.T1.25.25.1.m1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S1.T1.25.25.1.m1.1.1.3.2.cmml" xref="S1.T1.25.25.1.m1.1.1.3.2">R</mtext></ci><apply id="S1.T1.25.25.1.m1.1.1.3.3.cmml" xref="S1.T1.25.25.1.m1.1.1.3.3"><times id="S1.T1.25.25.1.m1.1.1.3.3.1.cmml" xref="S1.T1.25.25.1.m1.1.1.3.3.1"></times><ci id="S1.T1.25.25.1.m1.1.1.3.3.2.cmml" xref="S1.T1.25.25.1.m1.1.1.3.3.2">𝑛</ci><ci id="S1.T1.25.25.1.m1.1.1.3.3.3.cmml" xref="S1.T1.25.25.1.m1.1.1.3.3.3">𝑏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.25.25.1.m1.1c">\textbf{\text{H}}\in\textbf{\text{R}}^{n\times b}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.25.25.1.m1.1d">H ∈ R start_POSTSUPERSCRIPT italic_n × italic_b end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.25.25.2">The node hidden feature matrix.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.27.27">
<td class="ltx_td ltx_align_left" id="S1.T1.26.26.1"><math alttext="\textbf{\text{h}}_{v}\in\textbf{\text{R}}^{b}" class="ltx_Math" display="inline" id="S1.T1.26.26.1.m1.1"><semantics id="S1.T1.26.26.1.m1.1a"><mrow id="S1.T1.26.26.1.m1.1.1" xref="S1.T1.26.26.1.m1.1.1.cmml"><msub id="S1.T1.26.26.1.m1.1.1.2" xref="S1.T1.26.26.1.m1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S1.T1.26.26.1.m1.1.1.2.2" xref="S1.T1.26.26.1.m1.1.1.2.2a.cmml">h</mtext><mi id="S1.T1.26.26.1.m1.1.1.2.3" xref="S1.T1.26.26.1.m1.1.1.2.3.cmml">v</mi></msub><mo id="S1.T1.26.26.1.m1.1.1.1" xref="S1.T1.26.26.1.m1.1.1.1.cmml">∈</mo><msup id="S1.T1.26.26.1.m1.1.1.3" xref="S1.T1.26.26.1.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S1.T1.26.26.1.m1.1.1.3.2" xref="S1.T1.26.26.1.m1.1.1.3.2a.cmml">R</mtext><mi id="S1.T1.26.26.1.m1.1.1.3.3" xref="S1.T1.26.26.1.m1.1.1.3.3.cmml">b</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.26.26.1.m1.1b"><apply id="S1.T1.26.26.1.m1.1.1.cmml" xref="S1.T1.26.26.1.m1.1.1"><in id="S1.T1.26.26.1.m1.1.1.1.cmml" xref="S1.T1.26.26.1.m1.1.1.1"></in><apply id="S1.T1.26.26.1.m1.1.1.2.cmml" xref="S1.T1.26.26.1.m1.1.1.2"><csymbol cd="ambiguous" id="S1.T1.26.26.1.m1.1.1.2.1.cmml" xref="S1.T1.26.26.1.m1.1.1.2">subscript</csymbol><ci id="S1.T1.26.26.1.m1.1.1.2.2a.cmml" xref="S1.T1.26.26.1.m1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S1.T1.26.26.1.m1.1.1.2.2.cmml" xref="S1.T1.26.26.1.m1.1.1.2.2">h</mtext></ci><ci id="S1.T1.26.26.1.m1.1.1.2.3.cmml" xref="S1.T1.26.26.1.m1.1.1.2.3">𝑣</ci></apply><apply id="S1.T1.26.26.1.m1.1.1.3.cmml" xref="S1.T1.26.26.1.m1.1.1.3"><csymbol cd="ambiguous" id="S1.T1.26.26.1.m1.1.1.3.1.cmml" xref="S1.T1.26.26.1.m1.1.1.3">superscript</csymbol><ci id="S1.T1.26.26.1.m1.1.1.3.2a.cmml" xref="S1.T1.26.26.1.m1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S1.T1.26.26.1.m1.1.1.3.2.cmml" xref="S1.T1.26.26.1.m1.1.1.3.2">R</mtext></ci><ci id="S1.T1.26.26.1.m1.1.1.3.3.cmml" xref="S1.T1.26.26.1.m1.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.26.26.1.m1.1c">\textbf{\text{h}}_{v}\in\textbf{\text{R}}^{b}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.26.26.1.m1.1d">h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ∈ R start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.27.27.2">The hidden feature vector of node <math alttext="v" class="ltx_Math" display="inline" id="S1.T1.27.27.2.m1.1"><semantics id="S1.T1.27.27.2.m1.1a"><mi id="S1.T1.27.27.2.m1.1.1" xref="S1.T1.27.27.2.m1.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S1.T1.27.27.2.m1.1b"><ci id="S1.T1.27.27.2.m1.1.1.cmml" xref="S1.T1.27.27.2.m1.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.27.27.2.m1.1c">v</annotation><annotation encoding="application/x-llamapun" id="S1.T1.27.27.2.m1.1d">italic_v</annotation></semantics></math>.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.28.28">
<td class="ltx_td ltx_align_left" id="S1.T1.28.28.1"><math alttext="t" class="ltx_Math" display="inline" id="S1.T1.28.28.1.m1.1"><semantics id="S1.T1.28.28.1.m1.1a"><mi id="S1.T1.28.28.1.m1.1.1" xref="S1.T1.28.28.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S1.T1.28.28.1.m1.1b"><ci id="S1.T1.28.28.1.m1.1.1.cmml" xref="S1.T1.28.28.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.28.28.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S1.T1.28.28.1.m1.1d">italic_t</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.28.28.2">The aggregation layer number.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.29.29">
<td class="ltx_td ltx_align_left" id="S1.T1.29.29.1"><math alttext="\sigma(*)" class="ltx_Math" display="inline" id="S1.T1.29.29.1.m1.1"><semantics id="S1.T1.29.29.1.m1.1a"><mrow id="S1.T1.29.29.1.m1.1.2" xref="S1.T1.29.29.1.m1.1.2.cmml"><mi id="S1.T1.29.29.1.m1.1.2.2" xref="S1.T1.29.29.1.m1.1.2.2.cmml">σ</mi><mo id="S1.T1.29.29.1.m1.1.2.1" xref="S1.T1.29.29.1.m1.1.2.1.cmml">⁢</mo><mrow id="S1.T1.29.29.1.m1.1.2.3.2" xref="S1.T1.29.29.1.m1.1.2.cmml"><mo id="S1.T1.29.29.1.m1.1.2.3.2.1" stretchy="false" xref="S1.T1.29.29.1.m1.1.2.cmml">(</mo><mo id="S1.T1.29.29.1.m1.1.1" lspace="0em" rspace="0em" xref="S1.T1.29.29.1.m1.1.1.cmml">∗</mo><mo id="S1.T1.29.29.1.m1.1.2.3.2.2" stretchy="false" xref="S1.T1.29.29.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.29.29.1.m1.1b"><apply id="S1.T1.29.29.1.m1.1.2.cmml" xref="S1.T1.29.29.1.m1.1.2"><times id="S1.T1.29.29.1.m1.1.2.1.cmml" xref="S1.T1.29.29.1.m1.1.2.1"></times><ci id="S1.T1.29.29.1.m1.1.2.2.cmml" xref="S1.T1.29.29.1.m1.1.2.2">𝜎</ci><times id="S1.T1.29.29.1.m1.1.1.cmml" xref="S1.T1.29.29.1.m1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.29.29.1.m1.1c">\sigma(*)</annotation><annotation encoding="application/x-llamapun" id="S1.T1.29.29.1.m1.1d">italic_σ ( ∗ )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S1.T1.29.29.2">Activation function.</td>
</tr>
<tr class="ltx_tr" id="S1.T1.30.30">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S1.T1.30.30.1"><math alttext="\boldsymbol{\text{W}},\boldsymbol{\Theta},w,\theta" class="ltx_Math" display="inline" id="S1.T1.30.30.1.m1.4"><semantics id="S1.T1.30.30.1.m1.4a"><mrow id="S1.T1.30.30.1.m1.4.5.2" xref="S1.T1.30.30.1.m1.4.5.1.cmml"><mtext id="S1.T1.30.30.1.m1.1.1" xref="S1.T1.30.30.1.m1.1.1a.cmml">W</mtext><mo id="S1.T1.30.30.1.m1.4.5.2.1" xref="S1.T1.30.30.1.m1.4.5.1.cmml">,</mo><mi id="S1.T1.30.30.1.m1.2.2" xref="S1.T1.30.30.1.m1.2.2.cmml">𝚯</mi><mo id="S1.T1.30.30.1.m1.4.5.2.2" xref="S1.T1.30.30.1.m1.4.5.1.cmml">,</mo><mi id="S1.T1.30.30.1.m1.3.3" xref="S1.T1.30.30.1.m1.3.3.cmml">w</mi><mo id="S1.T1.30.30.1.m1.4.5.2.3" xref="S1.T1.30.30.1.m1.4.5.1.cmml">,</mo><mi id="S1.T1.30.30.1.m1.4.4" xref="S1.T1.30.30.1.m1.4.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.30.30.1.m1.4b"><list id="S1.T1.30.30.1.m1.4.5.1.cmml" xref="S1.T1.30.30.1.m1.4.5.2"><ci id="S1.T1.30.30.1.m1.1.1a.cmml" xref="S1.T1.30.30.1.m1.1.1"><mtext id="S1.T1.30.30.1.m1.1.1.cmml" xref="S1.T1.30.30.1.m1.1.1">W</mtext></ci><ci id="S1.T1.30.30.1.m1.2.2.cmml" xref="S1.T1.30.30.1.m1.2.2">𝚯</ci><ci id="S1.T1.30.30.1.m1.3.3.cmml" xref="S1.T1.30.30.1.m1.3.3">𝑤</ci><ci id="S1.T1.30.30.1.m1.4.4.cmml" xref="S1.T1.30.30.1.m1.4.4">𝜃</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.30.30.1.m1.4c">\boldsymbol{\text{W}},\boldsymbol{\Theta},w,\theta</annotation><annotation encoding="application/x-llamapun" id="S1.T1.30.30.1.m1.4d">W , bold_Θ , italic_w , italic_θ</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S1.T1.30.30.2">Leanable model parameter.</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S1.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Technical Terms</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.T2.1">
<tr class="ltx_tr" id="S1.T2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S1.T2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.1.1.1">
<span class="ltx_p" id="S1.T2.1.1.1.1.1" style="width:108.4pt;"><span class="ltx_text ltx_font_bold" id="S1.T2.1.1.1.1.1.1">Terms</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S1.T2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.1.2.1">
<span class="ltx_p" id="S1.T2.1.1.2.1.1" style="width:281.9pt;"><span class="ltx_text ltx_font_bold" id="S1.T2.1.1.2.1.1.1">Definitions and Descriptions</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T2.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.2.1.1">
<span class="ltx_p" id="S1.T2.1.2.1.1.1" style="width:108.4pt;">Benign model</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T2.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.2.2.1">
<span class="ltx_p" id="S1.T2.1.2.2.1.1" style="width:281.9pt;">The model trained under benign settings.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.3.1.1">
<span class="ltx_p" id="S1.T2.1.3.1.1.1" style="width:108.4pt;">Infected model</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.3.2.1">
<span class="ltx_p" id="S1.T2.1.3.2.1.1" style="width:281.9pt;">The model with hidden backdoor(s).</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.4.1.1">
<span class="ltx_p" id="S1.T2.1.4.1.1.1" style="width:108.4pt;">Poisoned sample</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.4.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.4.2.1">
<span class="ltx_p" id="S1.T2.1.4.2.1.1" style="width:281.9pt;">The modified training sample used in poisoning-based backdoor attacks for embedding backdoor(s) in the model during the training process.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.5.1.1">
<span class="ltx_p" id="S1.T2.1.5.1.1.1" style="width:108.4pt;">Trigger</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.5.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.5.2.1">
<span class="ltx_p" id="S1.T2.1.5.2.1.1" style="width:281.9pt;">The pattern used for generating poisoned samples and activating the hidden backdoor(s).</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.6.1.1">
<span class="ltx_p" id="S1.T2.1.6.1.1.1" style="width:108.4pt;">Attacked sample</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.6.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.6.2.1">
<span class="ltx_p" id="S1.T2.1.6.2.1.1" style="width:281.9pt;">The malicious testing sample containing backdoor trigger(s).</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.7.1.1">
<span class="ltx_p" id="S1.T2.1.7.1.1.1" style="width:108.4pt;">Source label</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.7.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.7.2.1">
<span class="ltx_p" id="S1.T2.1.7.2.1.1" style="width:281.9pt;">The ground-truth label of a poisoned or an attacked sample.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.8.1.1">
<span class="ltx_p" id="S1.T2.1.8.1.1.1" style="width:108.4pt;">Target label</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.8.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.8.2.1">
<span class="ltx_p" id="S1.T2.1.8.2.1.1" style="width:281.9pt;">The attacker-specified label.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.9.1.1">
<span class="ltx_p" id="S1.T2.1.9.1.1.1" style="width:108.4pt;">Attacker’s goal</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S1.T2.1.9.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.9.2.1">
<span class="ltx_p" id="S1.T2.1.9.2.1.1" style="width:281.9pt;">What the backdoor attacker intends to do.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S1.T2.1.10.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.10.1.1">
<span class="ltx_p" id="S1.T2.1.10.1.1.1" style="width:108.4pt;">Capacity</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S1.T2.1.10.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T2.1.10.2.1">
<span class="ltx_p" id="S1.T2.1.10.2.1.1" style="width:281.9pt;">What the attacker/defender can and cannot do to achieve their goal.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Preliminaries</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Notations and Technical Terms</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In this section, we introduce the key notations and technical terms essential for GNN backdoor implementations, which are shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S1.T1" title="Table 1 ‣ 1. Introduction ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a> and Tab. <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S1.T2" title="Table 2 ‣ 1. Introduction ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a> respectively.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The core idea of GNNs is information aggregation and prediction. It first initializes the node representations, which could be based on node features and topology. Subsequently, through multiple iterations, nodes engage in information aggregation, updating their representations by utilizing information from neighbors, typically achieved through operations like graph convolution. Throughout this process, node features gradually update and integrate information from adjacent nodes, aiding in capturing both local and global features within the entire graph. Finally, these enhanced node representations are leveraged to specific tasks (<span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">e.g.</span>, node classification, graph classification, link prediction, etc.). The general process of GNN is given in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S2.F2" title="Figure 2 ‣ 2.2. Graph Neural Networks ‣ 2. Preliminaries ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Graph Neural Networks</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">GNNs are a type of deep learning model used for processing graph-structured data. Unlike traditional neural networks that focus on handling vector or sequential data, GNNs concentrate on handling non-Euclidean graph data, <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">e.g.</span>, social communities, transportation networks, molecular structures, etc.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="286" id="S2.F2.g1" src="extracted/5669158/figure/gnn-framework.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Illustration of general GNN framework. It operates by updating node representations through information aggregation, utilizing these learned representations to solve downstream problems related to graph-structured data, for instance, graph classification, node classification, and link classification.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Specifically, given a graph <math alttext="G=(\textbf{\text{A}},\textbf{\text{X}})" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.2"><semantics id="S2.SS2.p2.1.m1.2a"><mrow id="S2.SS2.p2.1.m1.2.3" xref="S2.SS2.p2.1.m1.2.3.cmml"><mi id="S2.SS2.p2.1.m1.2.3.2" xref="S2.SS2.p2.1.m1.2.3.2.cmml">G</mi><mo id="S2.SS2.p2.1.m1.2.3.1" xref="S2.SS2.p2.1.m1.2.3.1.cmml">=</mo><mrow id="S2.SS2.p2.1.m1.2.3.3.2" xref="S2.SS2.p2.1.m1.2.3.3.1.cmml"><mo id="S2.SS2.p2.1.m1.2.3.3.2.1" stretchy="false" xref="S2.SS2.p2.1.m1.2.3.3.1.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1a.cmml">A</mtext><mo id="S2.SS2.p2.1.m1.2.3.3.2.2" xref="S2.SS2.p2.1.m1.2.3.3.1.cmml">,</mo><mtext class="ltx_mathvariant_bold" id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2a.cmml">X</mtext><mo id="S2.SS2.p2.1.m1.2.3.3.2.3" stretchy="false" xref="S2.SS2.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.2b"><apply id="S2.SS2.p2.1.m1.2.3.cmml" xref="S2.SS2.p2.1.m1.2.3"><eq id="S2.SS2.p2.1.m1.2.3.1.cmml" xref="S2.SS2.p2.1.m1.2.3.1"></eq><ci id="S2.SS2.p2.1.m1.2.3.2.cmml" xref="S2.SS2.p2.1.m1.2.3.2">𝐺</ci><interval closure="open" id="S2.SS2.p2.1.m1.2.3.3.1.cmml" xref="S2.SS2.p2.1.m1.2.3.3.2"><ci id="S2.SS2.p2.1.m1.1.1a.cmml" xref="S2.SS2.p2.1.m1.1.1"><mtext class="ltx_mathvariant_bold" id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">A</mtext></ci><ci id="S2.SS2.p2.1.m1.2.2a.cmml" xref="S2.SS2.p2.1.m1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">X</mtext></ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.2c">G=(\textbf{\text{A}},\textbf{\text{X}})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.2d">italic_G = ( A , X )</annotation></semantics></math> initialized from graph topology and node features, the aggregation process unfolds as follows.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.2">GNN iteratively updates node representations. At each layer <math alttext="l" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.1"><semantics id="S2.SS2.p3.1.m1.1a"><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">l</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d">italic_l</annotation></semantics></math>, node representations <math alttext="\mathbf{h}^{l}" class="ltx_Math" display="inline" id="S2.SS2.p3.2.m2.1"><semantics id="S2.SS2.p3.2.m2.1a"><msup id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml"><mi id="S2.SS2.p3.2.m2.1.1.2" xref="S2.SS2.p3.2.m2.1.1.2.cmml">𝐡</mi><mi id="S2.SS2.p3.2.m2.1.1.3" xref="S2.SS2.p3.2.m2.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><apply id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.2.m2.1.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">superscript</csymbol><ci id="S2.SS2.p3.2.m2.1.1.2.cmml" xref="S2.SS2.p3.2.m2.1.1.2">𝐡</ci><ci id="S2.SS2.p3.2.m2.1.1.3.cmml" xref="S2.SS2.p3.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">\mathbf{h}^{l}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.2.m2.1d">bold_h start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math> undergo transformation, expressed as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{h}_{v}^{(l+1)}=\text{AGGREGATE}\left(\{\mathbf{h}_{u}^{(l)}\mid u\in%
\mathcal{N}(v)\}\right)," class="ltx_Math" display="block" id="S2.E1.m1.4"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.4.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1" xref="S2.E1.m1.4.4.1.1.cmml"><msubsup id="S2.E1.m1.4.4.1.1.3" xref="S2.E1.m1.4.4.1.1.3.cmml"><mi id="S2.E1.m1.4.4.1.1.3.2.2" xref="S2.E1.m1.4.4.1.1.3.2.2.cmml">𝐡</mi><mi id="S2.E1.m1.4.4.1.1.3.2.3" xref="S2.E1.m1.4.4.1.1.3.2.3.cmml">v</mi><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.2.cmml">l</mi><mo id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S2.E1.m1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S2.E1.m1.4.4.1.1.2" xref="S2.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.4.4.1.1.1" xref="S2.E1.m1.4.4.1.1.1.cmml"><mtext id="S2.E1.m1.4.4.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.3a.cmml">AGGREGATE</mtext><mo id="S2.E1.m1.4.4.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.cmml"><mo id="S2.E1.m1.4.4.1.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.3.cmml"><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.2.3" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml">{</mo><msubsup id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.2.cmml">𝐡</mi><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml">u</mi><mrow id="S2.E1.m1.2.2.1.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.2.2.1.3.1" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S2.E1.m1.2.2.1.1" xref="S2.E1.m1.2.2.1.1.cmml">l</mi><mo id="S2.E1.m1.2.2.1.3.2" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo fence="true" id="S2.E1.m1.4.4.1.1.1.1.1.1.2.4" lspace="0em" rspace="0em" xref="S2.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml">∣</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.2.cmml">u</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.1.cmml">∈</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.2.cmml">𝒩</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.1.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.cmml"><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.3.2.1" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.cmml">(</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">v</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.3.2.2" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.2.5" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml">}</mo></mrow><mo id="S2.E1.m1.4.4.1.1.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.1.2" xref="S2.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.4.1.1.cmml" xref="S2.E1.m1.4.4.1"><eq id="S2.E1.m1.4.4.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2"></eq><apply id="S2.E1.m1.4.4.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.3">superscript</csymbol><apply id="S2.E1.m1.4.4.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.3.2.1.cmml" xref="S2.E1.m1.4.4.1.1.3">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.3.2.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2.2">𝐡</ci><ci id="S2.E1.m1.4.4.1.1.3.2.3.cmml" xref="S2.E1.m1.4.4.1.1.3.2.3">𝑣</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1"><plus id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1"></plus><ci id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2">𝑙</ci><cn id="S2.E1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.E1.m1.4.4.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1"><times id="S2.E1.m1.4.4.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.2"></times><ci id="S2.E1.m1.4.4.1.1.1.3a.cmml" xref="S2.E1.m1.4.4.1.1.1.3"><mtext id="S2.E1.m1.4.4.1.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.1.3">AGGREGATE</mtext></ci><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S2.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.3">conditional-set</csymbol><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.2">𝐡</ci><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.3">𝑢</ci></apply><ci id="S2.E1.m1.2.2.1.1.cmml" xref="S2.E1.m1.2.2.1.1">𝑙</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2"><in id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.1"></in><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.2">𝑢</ci><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3"><times id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.1"></times><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.2">𝒩</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">𝑣</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">\mathbf{h}_{v}^{(l+1)}=\text{AGGREGATE}\left(\{\mathbf{h}_{u}^{(l)}\mid u\in%
\mathcal{N}(v)\}\right),</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.4d">bold_h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT = AGGREGATE ( { bold_h start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ∣ italic_u ∈ caligraphic_N ( italic_v ) } ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.p3.6">where the feature vector <math alttext="\mathbf{h}_{v}" class="ltx_Math" display="inline" id="S2.SS2.p3.3.m1.1"><semantics id="S2.SS2.p3.3.m1.1a"><msub id="S2.SS2.p3.3.m1.1.1" xref="S2.SS2.p3.3.m1.1.1.cmml"><mi id="S2.SS2.p3.3.m1.1.1.2" xref="S2.SS2.p3.3.m1.1.1.2.cmml">𝐡</mi><mi id="S2.SS2.p3.3.m1.1.1.3" xref="S2.SS2.p3.3.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m1.1b"><apply id="S2.SS2.p3.3.m1.1.1.cmml" xref="S2.SS2.p3.3.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.3.m1.1.1.1.cmml" xref="S2.SS2.p3.3.m1.1.1">subscript</csymbol><ci id="S2.SS2.p3.3.m1.1.1.2.cmml" xref="S2.SS2.p3.3.m1.1.1.2">𝐡</ci><ci id="S2.SS2.p3.3.m1.1.1.3.cmml" xref="S2.SS2.p3.3.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m1.1c">\mathbf{h}_{v}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.3.m1.1d">bold_h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> of each node is a row in <math alttext="\mathbf{H}" class="ltx_Math" display="inline" id="S2.SS2.p3.4.m2.1"><semantics id="S2.SS2.p3.4.m2.1a"><mi id="S2.SS2.p3.4.m2.1.1" xref="S2.SS2.p3.4.m2.1.1.cmml">𝐇</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m2.1b"><ci id="S2.SS2.p3.4.m2.1.1.cmml" xref="S2.SS2.p3.4.m2.1.1">𝐇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m2.1c">\mathbf{H}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.4.m2.1d">bold_H</annotation></semantics></math> (initially <math alttext="\mathbf{X}" class="ltx_Math" display="inline" id="S2.SS2.p3.5.m3.1"><semantics id="S2.SS2.p3.5.m3.1a"><mi id="S2.SS2.p3.5.m3.1.1" xref="S2.SS2.p3.5.m3.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.5.m3.1b"><ci id="S2.SS2.p3.5.m3.1.1.cmml" xref="S2.SS2.p3.5.m3.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.5.m3.1c">\mathbf{X}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.5.m3.1d">bold_X</annotation></semantics></math>), and <span class="ltx_text ltx_markedasmath" id="S2.SS2.p3.6.1">AGGREGATE</span> is the corresponding aggregation function (<span class="ltx_text ltx_font_italic" id="S2.SS2.p3.6.2">e.g.</span>, mean, max, and attention).
This iterative process enables nodes to refine their representations based on local and global structural information.
</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Finally, the aggregated representations of graph nodes are utilized for downstream tasks, primarily categorized into three types: graph-level, node-level, and edge-level tasks.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>Graph-level Tasks</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">These tasks focus on inferring or predicting properties of the entire graph, such as graph classification or graph generation.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">In such cases, aggregated representations of the entire graph are employed to generate the overall graph representation <math alttext="\mathbf{h}_{G}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p2.1.m1.1"><semantics id="S2.SS2.SSS1.p2.1.m1.1a"><msub id="S2.SS2.SSS1.p2.1.m1.1.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.1.m1.1.1.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.2.cmml">𝐡</mi><mi id="S2.SS2.SSS1.p2.1.m1.1.1.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.1.m1.1b"><apply id="S2.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.2">𝐡</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.1.m1.1c">\mathbf{h}_{G}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p2.1.m1.1d">bold_h start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT</annotation></semantics></math> via readout function:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{h}_{G}=\text{READOUT}(\mathbf{H})," class="ltx_Math" display="block" id="S2.E2.m1.2"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.2.1" xref="S2.E2.m1.2.2.1.1.cmml"><mrow id="S2.E2.m1.2.2.1.1" xref="S2.E2.m1.2.2.1.1.cmml"><msub id="S2.E2.m1.2.2.1.1.2" xref="S2.E2.m1.2.2.1.1.2.cmml"><mi id="S2.E2.m1.2.2.1.1.2.2" xref="S2.E2.m1.2.2.1.1.2.2.cmml">𝐡</mi><mi id="S2.E2.m1.2.2.1.1.2.3" xref="S2.E2.m1.2.2.1.1.2.3.cmml">G</mi></msub><mo id="S2.E2.m1.2.2.1.1.1" xref="S2.E2.m1.2.2.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.2.2.1.1.3" xref="S2.E2.m1.2.2.1.1.3.cmml"><mtext id="S2.E2.m1.2.2.1.1.3.2" xref="S2.E2.m1.2.2.1.1.3.2a.cmml">READOUT</mtext><mo id="S2.E2.m1.2.2.1.1.3.1" xref="S2.E2.m1.2.2.1.1.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.2.2.1.1.3.3.2" xref="S2.E2.m1.2.2.1.1.3.cmml"><mo id="S2.E2.m1.2.2.1.1.3.3.2.1" stretchy="false" xref="S2.E2.m1.2.2.1.1.3.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">𝐇</mi><mo id="S2.E2.m1.2.2.1.1.3.3.2.2" stretchy="false" xref="S2.E2.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.2.2.1.2" xref="S2.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.2.1.1.cmml" xref="S2.E2.m1.2.2.1"><eq id="S2.E2.m1.2.2.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1"></eq><apply id="S2.E2.m1.2.2.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.2.1.cmml" xref="S2.E2.m1.2.2.1.1.2">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.2.2.cmml" xref="S2.E2.m1.2.2.1.1.2.2">𝐡</ci><ci id="S2.E2.m1.2.2.1.1.2.3.cmml" xref="S2.E2.m1.2.2.1.1.2.3">𝐺</ci></apply><apply id="S2.E2.m1.2.2.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.3"><times id="S2.E2.m1.2.2.1.1.3.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1"></times><ci id="S2.E2.m1.2.2.1.1.3.2a.cmml" xref="S2.E2.m1.2.2.1.1.3.2"><mtext id="S2.E2.m1.2.2.1.1.3.2.cmml" xref="S2.E2.m1.2.2.1.1.3.2">READOUT</mtext></ci><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝐇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">\mathbf{h}_{G}=\text{READOUT}(\mathbf{H}),</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.2d">bold_h start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = READOUT ( bold_H ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.SSS1.p2.2">and subsequently, utilizing <math alttext="\mathbf{h}_{G}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p2.2.m1.1"><semantics id="S2.SS2.SSS1.p2.2.m1.1a"><msub id="S2.SS2.SSS1.p2.2.m1.1.1" xref="S2.SS2.SSS1.p2.2.m1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.2.m1.1.1.2" xref="S2.SS2.SSS1.p2.2.m1.1.1.2.cmml">𝐡</mi><mi id="S2.SS2.SSS1.p2.2.m1.1.1.3" xref="S2.SS2.SSS1.p2.2.m1.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.2.m1.1b"><apply id="S2.SS2.SSS1.p2.2.m1.1.1.cmml" xref="S2.SS2.SSS1.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.2.m1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.2.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.2.m1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.2.m1.1.1.2">𝐡</ci><ci id="S2.SS2.SSS1.p2.2.m1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.2.m1.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.2.m1.1c">\mathbf{h}_{G}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p2.2.m1.1d">bold_h start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT</annotation></semantics></math> as input, it undergoes processing through a classifier (<span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p2.2.1">e.g.</span>, fully connected neural network layer, softmax output layer or other differentiable deep networks) to predict the graph’s category, label or value:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="y={\psi}_{G}(\mathbf{W}\cdot\mathbf{h}_{G}+\mathbf{b})," class="ltx_Math" display="block" id="S2.E3.m1.1"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.3.cmml">y</mi><mo id="S2.E3.m1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.3.2.cmml">ψ</mi><mi id="S2.E3.m1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.3.3.cmml">G</mi></msub><mo id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml">𝐖</mi><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml">⋅</mo><msub id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">𝐡</mi><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">G</mi></msub></mrow><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.cmml">𝐛</mi></mrow><mo id="S2.E3.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.2"></eq><ci id="S2.E3.m1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.3">𝑦</ci><apply id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><times id="S2.E3.m1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2">𝜓</ci><ci id="S2.E3.m1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3">𝐺</ci></apply><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"><plus id="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2"><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.1">⋅</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2">𝐖</ci><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2">𝐡</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3">𝐺</ci></apply></apply><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3">𝐛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">y={\psi}_{G}(\mathbf{W}\cdot\mathbf{h}_{G}+\mathbf{b}),</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.1d">italic_y = italic_ψ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( bold_W ⋅ bold_h start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT + bold_b ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.SSS1.p2.4">where <math alttext="\psi_{G}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p2.3.m1.1"><semantics id="S2.SS2.SSS1.p2.3.m1.1a"><msub id="S2.SS2.SSS1.p2.3.m1.1.1" xref="S2.SS2.SSS1.p2.3.m1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.3.m1.1.1.2" xref="S2.SS2.SSS1.p2.3.m1.1.1.2.cmml">ψ</mi><mi id="S2.SS2.SSS1.p2.3.m1.1.1.3" xref="S2.SS2.SSS1.p2.3.m1.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.3.m1.1b"><apply id="S2.SS2.SSS1.p2.3.m1.1.1.cmml" xref="S2.SS2.SSS1.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.3.m1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.3.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.3.m1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.3.m1.1.1.2">𝜓</ci><ci id="S2.SS2.SSS1.p2.3.m1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.3.m1.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.3.m1.1c">\psi_{G}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p2.3.m1.1d">italic_ψ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT</annotation></semantics></math> demonstrates the downstream classifier and <math alttext="\mathbf{W}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p2.4.m2.1"><semantics id="S2.SS2.SSS1.p2.4.m2.1a"><mi id="S2.SS2.SSS1.p2.4.m2.1.1" xref="S2.SS2.SSS1.p2.4.m2.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.4.m2.1b"><ci id="S2.SS2.SSS1.p2.4.m2.1.1.cmml" xref="S2.SS2.SSS1.p2.4.m2.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.4.m2.1c">\mathbf{W}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p2.4.m2.1d">bold_W</annotation></semantics></math> represents the weight matrix, or a generator to produce the criteria-meeting graph:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{G}_{\text{g}}=\text{GENERATOR}(\mathbf{h}_{G})." class="ltx_Math" display="block" id="S2.E4.m1.1"><semantics id="S2.E4.m1.1a"><mrow id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><msub id="S2.E4.m1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.3.2.cmml">𝐆</mi><mtext id="S2.E4.m1.1.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.3.3a.cmml">g</mtext></msub><mo id="S2.E4.m1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E4.m1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.cmml"><mtext id="S2.E4.m1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.3a.cmml">GENERATOR</mtext><mo id="S2.E4.m1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E4.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E4.m1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2.cmml">𝐡</mi><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.cmml">G</mi></msub><mo id="S2.E4.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E4.m1.1.1.1.2" lspace="0em" xref="S2.E4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"><eq id="S2.E4.m1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.2"></eq><apply id="S2.E4.m1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.3.2">𝐆</ci><ci id="S2.E4.m1.1.1.1.1.3.3a.cmml" xref="S2.E4.m1.1.1.1.1.3.3"><mtext id="S2.E4.m1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.E4.m1.1.1.1.1.3.3">g</mtext></ci></apply><apply id="S2.E4.m1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1"><times id="S2.E4.m1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.2"></times><ci id="S2.E4.m1.1.1.1.1.1.3a.cmml" xref="S2.E4.m1.1.1.1.1.1.3"><mtext id="S2.E4.m1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.3">GENERATOR</mtext></ci><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2">𝐡</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3">𝐺</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">\mathbf{G}_{\text{g}}=\text{GENERATOR}(\mathbf{h}_{G}).</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.1d">bold_G start_POSTSUBSCRIPT g end_POSTSUBSCRIPT = GENERATOR ( bold_h start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>Node-level Tasks</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">These tasks involve predictions or analyses concerning individual nodes, including node classification or node attribute prediction.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">In these tasks, the aggregated representation of each node <math alttext="\mathbf{h}_{i}=\mathbf{H}^{t}_{i,:}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p2.1.m1.2"><semantics id="S2.SS2.SSS2.p2.1.m1.2a"><mrow id="S2.SS2.SSS2.p2.1.m1.2.3" xref="S2.SS2.SSS2.p2.1.m1.2.3.cmml"><msub id="S2.SS2.SSS2.p2.1.m1.2.3.2" xref="S2.SS2.SSS2.p2.1.m1.2.3.2.cmml"><mi id="S2.SS2.SSS2.p2.1.m1.2.3.2.2" xref="S2.SS2.SSS2.p2.1.m1.2.3.2.2.cmml">𝐡</mi><mi id="S2.SS2.SSS2.p2.1.m1.2.3.2.3" xref="S2.SS2.SSS2.p2.1.m1.2.3.2.3.cmml">i</mi></msub><mo id="S2.SS2.SSS2.p2.1.m1.2.3.1" xref="S2.SS2.SSS2.p2.1.m1.2.3.1.cmml">=</mo><msubsup id="S2.SS2.SSS2.p2.1.m1.2.3.3" xref="S2.SS2.SSS2.p2.1.m1.2.3.3.cmml"><mi id="S2.SS2.SSS2.p2.1.m1.2.3.3.2.2" xref="S2.SS2.SSS2.p2.1.m1.2.3.3.2.2.cmml">𝐇</mi><mrow id="S2.SS2.SSS2.p2.1.m1.2.2.2.4" xref="S2.SS2.SSS2.p2.1.m1.2.2.2.3.cmml"><mi id="S2.SS2.SSS2.p2.1.m1.1.1.1.1" xref="S2.SS2.SSS2.p2.1.m1.1.1.1.1.cmml">i</mi><mo id="S2.SS2.SSS2.p2.1.m1.2.2.2.4.1" xref="S2.SS2.SSS2.p2.1.m1.2.2.2.3.cmml">,</mo><mo id="S2.SS2.SSS2.p2.1.m1.2.2.2.2" xref="S2.SS2.SSS2.p2.1.m1.2.2.2.2.cmml">:</mo></mrow><mi id="S2.SS2.SSS2.p2.1.m1.2.3.3.2.3" xref="S2.SS2.SSS2.p2.1.m1.2.3.3.2.3.cmml">t</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.1.m1.2b"><apply id="S2.SS2.SSS2.p2.1.m1.2.3.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3"><eq id="S2.SS2.SSS2.p2.1.m1.2.3.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.1"></eq><apply id="S2.SS2.SSS2.p2.1.m1.2.3.2.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.2"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p2.1.m1.2.3.2.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.2">subscript</csymbol><ci id="S2.SS2.SSS2.p2.1.m1.2.3.2.2.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.2.2">𝐡</ci><ci id="S2.SS2.SSS2.p2.1.m1.2.3.2.3.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.2.3">𝑖</ci></apply><apply id="S2.SS2.SSS2.p2.1.m1.2.3.3.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p2.1.m1.2.3.3.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.3">subscript</csymbol><apply id="S2.SS2.SSS2.p2.1.m1.2.3.3.2.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p2.1.m1.2.3.3.2.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.3">superscript</csymbol><ci id="S2.SS2.SSS2.p2.1.m1.2.3.3.2.2.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.3.2.2">𝐇</ci><ci id="S2.SS2.SSS2.p2.1.m1.2.3.3.2.3.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.3.2.3">𝑡</ci></apply><list id="S2.SS2.SSS2.p2.1.m1.2.2.2.3.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.2.2.4"><ci id="S2.SS2.SSS2.p2.1.m1.1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.1.1">𝑖</ci><ci id="S2.SS2.SSS2.p2.1.m1.2.2.2.2.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.2.2.2">:</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.1.m1.2c">\mathbf{h}_{i}=\mathbf{H}^{t}_{i,:}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p2.1.m1.2d">bold_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_H start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , : end_POSTSUBSCRIPT</annotation></semantics></math> is leveraged to calculate the corresponding node prediction:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="y={\psi}_{N}(\mathbf{W}\cdot\mathbf{h}_{i}+\mathbf{b})" class="ltx_Math" display="block" id="S2.E5.m1.1"><semantics id="S2.E5.m1.1a"><mrow id="S2.E5.m1.1.1" xref="S2.E5.m1.1.1.cmml"><mi id="S2.E5.m1.1.1.3" xref="S2.E5.m1.1.1.3.cmml">y</mi><mo id="S2.E5.m1.1.1.2" xref="S2.E5.m1.1.1.2.cmml">=</mo><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.cmml"><msub id="S2.E5.m1.1.1.1.3" xref="S2.E5.m1.1.1.1.3.cmml"><mi id="S2.E5.m1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.3.2.cmml">ψ</mi><mi id="S2.E5.m1.1.1.1.3.3" xref="S2.E5.m1.1.1.1.3.3.cmml">N</mi></msub><mo id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E5.m1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml"><mo id="S2.E5.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E5.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml"><mrow id="S2.E5.m1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.2.2" xref="S2.E5.m1.1.1.1.1.1.1.2.2.cmml">𝐖</mi><mo id="S2.E5.m1.1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S2.E5.m1.1.1.1.1.1.1.2.1.cmml">⋅</mo><msub id="S2.E5.m1.1.1.1.1.1.1.2.3" xref="S2.E5.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.2.3.2" xref="S2.E5.m1.1.1.1.1.1.1.2.3.2.cmml">𝐡</mi><mi id="S2.E5.m1.1.1.1.1.1.1.2.3.3" xref="S2.E5.m1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S2.E5.m1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S2.E5.m1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.3.cmml">𝐛</mi></mrow><mo id="S2.E5.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.1b"><apply id="S2.E5.m1.1.1.cmml" xref="S2.E5.m1.1.1"><eq id="S2.E5.m1.1.1.2.cmml" xref="S2.E5.m1.1.1.2"></eq><ci id="S2.E5.m1.1.1.3.cmml" xref="S2.E5.m1.1.1.3">𝑦</ci><apply id="S2.E5.m1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><times id="S2.E5.m1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.2"></times><apply id="S2.E5.m1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.3.2">𝜓</ci><ci id="S2.E5.m1.1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.3.3">𝑁</ci></apply><apply id="S2.E5.m1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1"><plus id="S2.E5.m1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1"></plus><apply id="S2.E5.m1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2"><ci id="S2.E5.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2.1">⋅</ci><ci id="S2.E5.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2.2">𝐖</ci><apply id="S2.E5.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2.3.2">𝐡</ci><ci id="S2.E5.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><ci id="S2.E5.m1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.3">𝐛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.1c">y={\psi}_{N}(\mathbf{W}\cdot\mathbf{h}_{i}+\mathbf{b})</annotation><annotation encoding="application/x-llamapun" id="S2.E5.m1.1d">italic_y = italic_ψ start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ( bold_W ⋅ bold_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_b )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.SSS2.p2.2">where <math alttext="{\psi}_{N}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p2.2.m1.1"><semantics id="S2.SS2.SSS2.p2.2.m1.1a"><msub id="S2.SS2.SSS2.p2.2.m1.1.1" xref="S2.SS2.SSS2.p2.2.m1.1.1.cmml"><mi id="S2.SS2.SSS2.p2.2.m1.1.1.2" xref="S2.SS2.SSS2.p2.2.m1.1.1.2.cmml">ψ</mi><mi id="S2.SS2.SSS2.p2.2.m1.1.1.3" xref="S2.SS2.SSS2.p2.2.m1.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.2.m1.1b"><apply id="S2.SS2.SSS2.p2.2.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p2.2.m1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.2.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p2.2.m1.1.1.2.cmml" xref="S2.SS2.SSS2.p2.2.m1.1.1.2">𝜓</ci><ci id="S2.SS2.SSS2.p2.2.m1.1.1.3.cmml" xref="S2.SS2.SSS2.p2.2.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.2.m1.1c">{\psi}_{N}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p2.2.m1.1d">italic_ψ start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math> demonstrates the downstream node classifier or regressor.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3. </span>Edge-level Tasks</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Edge-level tasks concentrate on the properties of edges in a graph, such as link prediction or edge attribute inference.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<p class="ltx_p" id="S2.SS2.SSS3.p2.4">Consider a graph where <math alttext="\mathbf{H}^{t}_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p2.1.m1.1"><semantics id="S2.SS2.SSS3.p2.1.m1.1a"><msubsup id="S2.SS2.SSS3.p2.1.m1.1.1" xref="S2.SS2.SSS3.p2.1.m1.1.1.cmml"><mi id="S2.SS2.SSS3.p2.1.m1.1.1.2.2" xref="S2.SS2.SSS3.p2.1.m1.1.1.2.2.cmml">𝐇</mi><mi id="S2.SS2.SSS3.p2.1.m1.1.1.3" xref="S2.SS2.SSS3.p2.1.m1.1.1.3.cmml">i</mi><mi id="S2.SS2.SSS3.p2.1.m1.1.1.2.3" xref="S2.SS2.SSS3.p2.1.m1.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.1.m1.1b"><apply id="S2.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1">subscript</csymbol><apply id="S2.SS2.SSS3.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.1.m1.1.1.2.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1">superscript</csymbol><ci id="S2.SS2.SSS3.p2.1.m1.1.1.2.2.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1.2.2">𝐇</ci><ci id="S2.SS2.SSS3.p2.1.m1.1.1.2.3.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1.2.3">𝑡</ci></apply><ci id="S2.SS2.SSS3.p2.1.m1.1.1.3.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.1.m1.1c">\mathbf{H}^{t}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p2.1.m1.1d">bold_H start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{H}^{t}_{j}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p2.2.m2.1"><semantics id="S2.SS2.SSS3.p2.2.m2.1a"><msubsup id="S2.SS2.SSS3.p2.2.m2.1.1" xref="S2.SS2.SSS3.p2.2.m2.1.1.cmml"><mi id="S2.SS2.SSS3.p2.2.m2.1.1.2.2" xref="S2.SS2.SSS3.p2.2.m2.1.1.2.2.cmml">𝐇</mi><mi id="S2.SS2.SSS3.p2.2.m2.1.1.3" xref="S2.SS2.SSS3.p2.2.m2.1.1.3.cmml">j</mi><mi id="S2.SS2.SSS3.p2.2.m2.1.1.2.3" xref="S2.SS2.SSS3.p2.2.m2.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.2.m2.1b"><apply id="S2.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.2.m2.1.1.1.cmml" xref="S2.SS2.SSS3.p2.2.m2.1.1">subscript</csymbol><apply id="S2.SS2.SSS3.p2.2.m2.1.1.2.cmml" xref="S2.SS2.SSS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.2.m2.1.1.2.1.cmml" xref="S2.SS2.SSS3.p2.2.m2.1.1">superscript</csymbol><ci id="S2.SS2.SSS3.p2.2.m2.1.1.2.2.cmml" xref="S2.SS2.SSS3.p2.2.m2.1.1.2.2">𝐇</ci><ci id="S2.SS2.SSS3.p2.2.m2.1.1.2.3.cmml" xref="S2.SS2.SSS3.p2.2.m2.1.1.2.3">𝑡</ci></apply><ci id="S2.SS2.SSS3.p2.2.m2.1.1.3.cmml" xref="S2.SS2.SSS3.p2.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.2.m2.1c">\mathbf{H}^{t}_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p2.2.m2.1d">bold_H start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> refers to the representations of the connected nodes for an edge between nodes <math alttext="v_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p2.3.m3.1"><semantics id="S2.SS2.SSS3.p2.3.m3.1a"><msub id="S2.SS2.SSS3.p2.3.m3.1.1" xref="S2.SS2.SSS3.p2.3.m3.1.1.cmml"><mi id="S2.SS2.SSS3.p2.3.m3.1.1.2" xref="S2.SS2.SSS3.p2.3.m3.1.1.2.cmml">v</mi><mi id="S2.SS2.SSS3.p2.3.m3.1.1.3" xref="S2.SS2.SSS3.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.3.m3.1b"><apply id="S2.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.3.m3.1.1.1.cmml" xref="S2.SS2.SSS3.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS3.p2.3.m3.1.1.2.cmml" xref="S2.SS2.SSS3.p2.3.m3.1.1.2">𝑣</ci><ci id="S2.SS2.SSS3.p2.3.m3.1.1.3.cmml" xref="S2.SS2.SSS3.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.3.m3.1c">v_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p2.3.m3.1d">italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="v_{j}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p2.4.m4.1"><semantics id="S2.SS2.SSS3.p2.4.m4.1a"><msub id="S2.SS2.SSS3.p2.4.m4.1.1" xref="S2.SS2.SSS3.p2.4.m4.1.1.cmml"><mi id="S2.SS2.SSS3.p2.4.m4.1.1.2" xref="S2.SS2.SSS3.p2.4.m4.1.1.2.cmml">v</mi><mi id="S2.SS2.SSS3.p2.4.m4.1.1.3" xref="S2.SS2.SSS3.p2.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.4.m4.1b"><apply id="S2.SS2.SSS3.p2.4.m4.1.1.cmml" xref="S2.SS2.SSS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.4.m4.1.1.1.cmml" xref="S2.SS2.SSS3.p2.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS3.p2.4.m4.1.1.2.cmml" xref="S2.SS2.SSS3.p2.4.m4.1.1.2">𝑣</ci><ci id="S2.SS2.SSS3.p2.4.m4.1.1.3.cmml" xref="S2.SS2.SSS3.p2.4.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.4.m4.1c">v_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p2.4.m4.1d">italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>. The output process in this scenario can be represented as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="y={\psi}_{E}(\mathbf{W}\cdot[\mathbf{H}^{t}_{i},\mathbf{H}^{t}_{j}]+\mathbf{b})" class="ltx_Math" display="block" id="S2.E6.m1.1"><semantics id="S2.E6.m1.1a"><mrow id="S2.E6.m1.1.1" xref="S2.E6.m1.1.1.cmml"><mi id="S2.E6.m1.1.1.3" xref="S2.E6.m1.1.1.3.cmml">y</mi><mo id="S2.E6.m1.1.1.2" xref="S2.E6.m1.1.1.2.cmml">=</mo><mrow id="S2.E6.m1.1.1.1" xref="S2.E6.m1.1.1.1.cmml"><msub id="S2.E6.m1.1.1.1.3" xref="S2.E6.m1.1.1.1.3.cmml"><mi id="S2.E6.m1.1.1.1.3.2" xref="S2.E6.m1.1.1.1.3.2.cmml">ψ</mi><mi id="S2.E6.m1.1.1.1.3.3" xref="S2.E6.m1.1.1.1.3.3.cmml">E</mi></msub><mo id="S2.E6.m1.1.1.1.2" xref="S2.E6.m1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E6.m1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.cmml"><mo id="S2.E6.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E6.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E6.m1.1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.cmml"><mrow id="S2.E6.m1.1.1.1.1.1.1.2" xref="S2.E6.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E6.m1.1.1.1.1.1.1.2.4" xref="S2.E6.m1.1.1.1.1.1.1.2.4.cmml">𝐖</mi><mo id="S2.E6.m1.1.1.1.1.1.1.2.3" lspace="0.222em" rspace="0.222em" xref="S2.E6.m1.1.1.1.1.1.1.2.3.cmml">⋅</mo><mrow id="S2.E6.m1.1.1.1.1.1.1.2.2.2" xref="S2.E6.m1.1.1.1.1.1.1.2.2.3.cmml"><mo id="S2.E6.m1.1.1.1.1.1.1.2.2.2.3" stretchy="false" xref="S2.E6.m1.1.1.1.1.1.1.2.2.3.cmml">[</mo><msubsup id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">𝐇</mi><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S2.E6.m1.1.1.1.1.1.1.2.2.2.4" xref="S2.E6.m1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msubsup id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.2.2" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.2.2.cmml">𝐇</mi><mi id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.3" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.3.cmml">j</mi><mi id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.2.3" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.2.3.cmml">t</mi></msubsup><mo id="S2.E6.m1.1.1.1.1.1.1.2.2.2.5" stretchy="false" xref="S2.E6.m1.1.1.1.1.1.1.2.2.3.cmml">]</mo></mrow></mrow><mo id="S2.E6.m1.1.1.1.1.1.1.3" xref="S2.E6.m1.1.1.1.1.1.1.3.cmml">+</mo><mi id="S2.E6.m1.1.1.1.1.1.1.4" xref="S2.E6.m1.1.1.1.1.1.1.4.cmml">𝐛</mi></mrow><mo id="S2.E6.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E6.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E6.m1.1b"><apply id="S2.E6.m1.1.1.cmml" xref="S2.E6.m1.1.1"><eq id="S2.E6.m1.1.1.2.cmml" xref="S2.E6.m1.1.1.2"></eq><ci id="S2.E6.m1.1.1.3.cmml" xref="S2.E6.m1.1.1.3">𝑦</ci><apply id="S2.E6.m1.1.1.1.cmml" xref="S2.E6.m1.1.1.1"><times id="S2.E6.m1.1.1.1.2.cmml" xref="S2.E6.m1.1.1.1.2"></times><apply id="S2.E6.m1.1.1.1.3.cmml" xref="S2.E6.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.3.1.cmml" xref="S2.E6.m1.1.1.1.3">subscript</csymbol><ci id="S2.E6.m1.1.1.1.3.2.cmml" xref="S2.E6.m1.1.1.1.3.2">𝜓</ci><ci id="S2.E6.m1.1.1.1.3.3.cmml" xref="S2.E6.m1.1.1.1.3.3">𝐸</ci></apply><apply id="S2.E6.m1.1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1"><plus id="S2.E6.m1.1.1.1.1.1.1.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.3"></plus><apply id="S2.E6.m1.1.1.1.1.1.1.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2"><ci id="S2.E6.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2.3">⋅</ci><ci id="S2.E6.m1.1.1.1.1.1.1.2.4.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2.4">𝐖</ci><interval closure="closed" id="S2.E6.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2"><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2">𝐇</ci><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><apply id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2">superscript</csymbol><ci id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.2.2">𝐇</ci><ci id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.2.3">𝑡</ci></apply><ci id="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.2.2.2.2.3">𝑗</ci></apply></interval></apply><ci id="S2.E6.m1.1.1.1.1.1.1.4.cmml" xref="S2.E6.m1.1.1.1.1.1.1.4">𝐛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m1.1c">y={\psi}_{E}(\mathbf{W}\cdot[\mathbf{H}^{t}_{i},\mathbf{H}^{t}_{j}]+\mathbf{b})</annotation><annotation encoding="application/x-llamapun" id="S2.E6.m1.1d">italic_y = italic_ψ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT ( bold_W ⋅ [ bold_H start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_H start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] + bold_b )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.SSS3.p2.5">where <math alttext="{\psi}_{E}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p2.5.m1.1"><semantics id="S2.SS2.SSS3.p2.5.m1.1a"><msub id="S2.SS2.SSS3.p2.5.m1.1.1" xref="S2.SS2.SSS3.p2.5.m1.1.1.cmml"><mi id="S2.SS2.SSS3.p2.5.m1.1.1.2" xref="S2.SS2.SSS3.p2.5.m1.1.1.2.cmml">ψ</mi><mi id="S2.SS2.SSS3.p2.5.m1.1.1.3" xref="S2.SS2.SSS3.p2.5.m1.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.5.m1.1b"><apply id="S2.SS2.SSS3.p2.5.m1.1.1.cmml" xref="S2.SS2.SSS3.p2.5.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p2.5.m1.1.1.1.cmml" xref="S2.SS2.SSS3.p2.5.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS3.p2.5.m1.1.1.2.cmml" xref="S2.SS2.SSS3.p2.5.m1.1.1.2">𝜓</ci><ci id="S2.SS2.SSS3.p2.5.m1.1.1.3.cmml" xref="S2.SS2.SSS3.p2.5.m1.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.5.m1.1c">{\psi}_{E}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p2.5.m1.1d">italic_ψ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT</annotation></semantics></math> demonstrates the downstream edge classifier or regressor.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4. </span>GNN Variants</h4>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">In practical applications of graph data, diverse graph structures, feature representations, and task types exist. Accordingly, researchers purposefully design various GNN variants tailored to specific scenarios and tasks to enhance GNN model performance or adaptability.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p2">
<p class="ltx_p" id="S2.SS2.SSS4.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.SS2.SSS4.p2.1.1">Graph Convolutional Networks.</span>
For graph convolutional networks (GCNs), every node updates its features by performing a weighted sum of the features from its neighboring nodes (called message passing). This weighted summation process can be regarded as a form of convolution operation. GCNs primarily fall into two kinds: spectral-based and spatial-based methods. Spectral-based approaches define graph convolutions by introducing filters from the perspective of graph signal
processing, while spatial-based approaches define graph convolutions by information propagation.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p3">
<p class="ltx_p" id="S2.SS2.SSS4.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.SS2.SSS4.p3.1.1">Recurrent Graph Neural Networks.</span>
The core of Recurrent Graph Neural Networks (RecGNN) lies in their recursive or cyclic structure, enabling information propagation across the graph and updates across time steps.
At each time step, RecGNNs leverage the current graph structure and node features to update node states while considering historical states and past information propagation.
This mechanism allows the model to effectively learn long-term dependencies and dynamic changes within the graph structure.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p4">
<p class="ltx_p" id="S2.SS2.SSS4.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.SS2.SSS4.p4.1.1">Heterogeneous Graph Neural Networks.</span>
Heterogeneous Graph Neural Networks (HGNNs) excel in addressing multiple networks characterized by node and edge types with varied attributes.
The fundamental aim of HGNNs is to effectively capture and consolidate diverse node and edge types within a unified framework.
Primarily, H-GNNs employ specialized aggregation strategies tailored to handle disparate node and edge types.
To be specific, HGNNs harness meta-path learning, elucidating specific sequences of node and edge types within heterogeneous graphs. These meta-paths orchestrate information flow, empowering the network to discern meaningful patterns and correlations amid diverse node and edge types.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p5">
<p class="ltx_p" id="S2.SS2.SSS4.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.SS2.SSS4.p5.1.1">Spatial–temporal Graph Neural Networks.</span>
Spatial-Temporal Graph Neural Networks (ST-GNNs) are adept at seamlessly integrating spatial and temporal dimensions of dynamic graph data by employing mechanisms for spatio-temporal propagation, temporal memory retention, and adaptive learning architectures.
They dynamically model graph evolution, handle sparse temporal sequences efficiently, and possess multiscale temporal attention mechanisms. These unique features enable ST-GNNs to effectively understand evolving relationships, recognize long-term dependencies, and generate temporal graph embeddings, marking them as robust models for comprehensive spatio-temporal analysis and prediction tasks across diverse domains.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p6">
<p class="ltx_p" id="S2.SS2.SSS4.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.SS2.SSS4.p6.1.1">Graph Autoencoders.</span>
Graph Autoencoders (GAEs) revolutionize graph representation learning through their intricate technical mechanisms. They employ the encoder-decoder architectures, utilizing encoder networks like graph convolutional networks (GCNs) or graph attention networks (GATs) to compress input graph structures into lower-dimensional latent spaces. These encoders transform node features and connectivity patterns into compact latent representations, effectively seizing the structural essence of the graph while preserving its topology and node characteristics.
Subsequently, GAEs generate a latent space where nodes are represented in reduced dimensions, serving as a compressed form of the original graph structure. This latent space encapsulates crucial graph features, and enables efficient storage and manipulation of graph information.
The decoder component in GAEs aims to reconstruct the original graph from these learned latent embeddings. It leverages the latent representations to predict edges and replicate the graph’s connectivity patterns, striving to accurately reconstruct the initial graph structure.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="385" id="S2.F3.g1" src="extracted/5669158/figure/bkd_pre.png" width="389"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Illustration of backdoor attack against image recognition CNN. The adversary sets the target class as <span class="ltx_text ltx_font_italic" id="S2.F3.4.1">ship</span>, selecting a subset of images and embedding specified trigger patterns within them, subsequently altering their true labels to <span class="ltx_text ltx_font_italic" id="S2.F3.5.2">ship</span>. Through training, the model becomes backdoored (infected), causing inputs containing the trigger to be predicted as the target class <span class="ltx_text ltx_font_italic" id="S2.F3.6.3">ship</span> while maintaining accuracy in predicting other normal samples.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Backdoor Attack</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Backdoor denotes a concealed vulnerability or functionality that is deliberately inserted during either the training or deployment stages of a model, typically occurring when the supervision of model training is lacking or insufficient.
Generally, these backdoors do not affect the model performance on regular data. However, when triggered by meticulous attacked sample inputs, the model will generate adversary-premeditated target results or behaviors.
A backdoor example for image recognition CNN is shown in the Fig <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S2.F3" title="Figure 3 ‣ 2.2.4. GNN Variants ‣ 2.2. Graph Neural Networks ‣ 2. Preliminaries ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">For backdoor implantation, the most extensively utilized approach is data-poisoning.
The adversary can manipulate (poison) the training dataset by introducing particular trigger patterns or features into the data and altering the corresponding labels of the poisoned data to the target label.
This action induces the trained model to produce target outputs when presented with trigger-embedded attacked inputs from the adversary.
Furthermore, backdoors can also be inserted by adjusting the model parameters, which enforces the model to exhibit target responses beyond the expected behavior for input with specific trigger patterns.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Current Methodologies</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In an in-depth exploration of graph neural backdoors, our investigation reveals that current studies could be generalized into two categories: (<span class="ltx_text ltx_font_italic" id="S3.p1.1.1">i</span>) adaptability expanding; (<span class="ltx_text ltx_font_italic" id="S3.p1.1.2">ii</span>) effectiveness improving.
The first category of studies primarily explores the feasibility of backdoors in various GNN learning paradigms.
The second category of research primarily centers on enhancing the attack performance, concealment, and scalability of GNN backdoors under various adversary attack constraints.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Based on this classification, we will first elaborate on the existing GNN backdoor attack research.
After that, we will show the corresponding defensive strategy studies.
The mind map for the current studies is illustrated in Fig</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S3.F4.g1" src="extracted/5669158/figure/mind_map.jpeg" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>The mind map for current graph neural backdoor studies.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>General Framework of GNN backdoor</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For GNN model, adversaries typically resort to data-poisoning on the training data to implant backdoors.
This manipulation will cause the well-trained GNN to produce adversary-designated outputs when input trigger-embedded graph data during deployment.
Nevertheless, for clean inputs (unpoisonous or unattacked), the model will maintain its accuracy.
The general framework of GNN backdoor is illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S3.F5" title="Figure 5 ‣ 3.1. General Framework of GNN backdoor ‣ 3. Current Methodologies ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To be specific, implanting and activating the backdoor through poisoning generally requires three steps: (<span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">i</span>) poisoned data generation; (<span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">ii</span>) backdoor model training; (<span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">iii</span>) backdoor activation.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.6">In the first step, adversaries select a subset <math alttext="\mathcal{D}_{t}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝒟</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathcal{D}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> from the training data <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">caligraphic_D</annotation></semantics></math> based on poisoning rate <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">italic_γ</annotation></semantics></math> (proportion of the selected), wherein specific triggers <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" mathvariant="normal" xref="S3.SS1.p3.4.m4.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">roman_Δ</annotation></semantics></math> (<span class="ltx_text ltx_font_italic" id="S3.SS1.p3.6.1">e.g.</span>, subgraphs, nodes, edges, or global properties) are subsequently inserted into <math alttext="\mathcal{D}_{t}" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m5.1"><semantics id="S3.SS1.p3.5.m5.1a"><msub id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2">𝒟</ci><ci id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">\mathcal{D}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.m5.1d">caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and labels of samples in <math alttext="\mathcal{D}_{t}" class="ltx_Math" display="inline" id="S3.SS1.p3.6.m6.1"><semantics id="S3.SS1.p3.6.m6.1a"><msub id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2">𝒟</ci><ci id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">\mathcal{D}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.6.m6.1d">caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> are altered to match the adversary-specified target class. This data-poisoning process can be expressed as</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{D}_{t}\sim\mathcal{D},\quad|\mathcal{D}_{t}|=\gamma|\mathcal{D}|," class="ltx_Math" display="block" id="S3.E7.m1.2"><semantics id="S3.E7.m1.2a"><mrow id="S3.E7.m1.2.2.1"><mrow id="S3.E7.m1.2.2.1.1.2" xref="S3.E7.m1.2.2.1.1.3.cmml"><mrow id="S3.E7.m1.2.2.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.cmml"><msub id="S3.E7.m1.2.2.1.1.1.1.2" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.2.2.1.1.1.1.2.2" xref="S3.E7.m1.2.2.1.1.1.1.2.2.cmml">𝒟</mi><mi id="S3.E7.m1.2.2.1.1.1.1.2.3" xref="S3.E7.m1.2.2.1.1.1.1.2.3.cmml">t</mi></msub><mo id="S3.E7.m1.2.2.1.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.1.cmml">∼</mo><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.2.2.1.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.1.3.cmml">𝒟</mi></mrow><mo id="S3.E7.m1.2.2.1.1.2.3" rspace="1.167em" xref="S3.E7.m1.2.2.1.1.3a.cmml">,</mo><mrow id="S3.E7.m1.2.2.1.1.2.2" xref="S3.E7.m1.2.2.1.1.2.2.cmml"><mrow id="S3.E7.m1.2.2.1.1.2.2.1.1" xref="S3.E7.m1.2.2.1.1.2.2.1.2.cmml"><mo id="S3.E7.m1.2.2.1.1.2.2.1.1.2" stretchy="false" xref="S3.E7.m1.2.2.1.1.2.2.1.2.1.cmml">|</mo><msub id="S3.E7.m1.2.2.1.1.2.2.1.1.1" xref="S3.E7.m1.2.2.1.1.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.2.2.1.1.2.2.1.1.1.2" xref="S3.E7.m1.2.2.1.1.2.2.1.1.1.2.cmml">𝒟</mi><mi id="S3.E7.m1.2.2.1.1.2.2.1.1.1.3" xref="S3.E7.m1.2.2.1.1.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S3.E7.m1.2.2.1.1.2.2.1.1.3" stretchy="false" xref="S3.E7.m1.2.2.1.1.2.2.1.2.1.cmml">|</mo></mrow><mo id="S3.E7.m1.2.2.1.1.2.2.2" xref="S3.E7.m1.2.2.1.1.2.2.2.cmml">=</mo><mrow id="S3.E7.m1.2.2.1.1.2.2.3" xref="S3.E7.m1.2.2.1.1.2.2.3.cmml"><mi id="S3.E7.m1.2.2.1.1.2.2.3.2" xref="S3.E7.m1.2.2.1.1.2.2.3.2.cmml">γ</mi><mo id="S3.E7.m1.2.2.1.1.2.2.3.1" xref="S3.E7.m1.2.2.1.1.2.2.3.1.cmml">⁢</mo><mrow id="S3.E7.m1.2.2.1.1.2.2.3.3.2" xref="S3.E7.m1.2.2.1.1.2.2.3.3.1.cmml"><mo id="S3.E7.m1.2.2.1.1.2.2.3.3.2.1" stretchy="false" xref="S3.E7.m1.2.2.1.1.2.2.3.3.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml">𝒟</mi><mo id="S3.E7.m1.2.2.1.1.2.2.3.3.2.2" stretchy="false" xref="S3.E7.m1.2.2.1.1.2.2.3.3.1.1.cmml">|</mo></mrow></mrow></mrow></mrow><mo id="S3.E7.m1.2.2.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.2b"><apply id="S3.E7.m1.2.2.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.3a.cmml" xref="S3.E7.m1.2.2.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E7.m1.2.2.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.E7.m1.2.2.1.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1">similar-to</csymbol><apply id="S3.E7.m1.2.2.1.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2">subscript</csymbol><ci id="S3.E7.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2.2">𝒟</ci><ci id="S3.E7.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2.3">𝑡</ci></apply><ci id="S3.E7.m1.2.2.1.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.1.1.3">𝒟</ci></apply><apply id="S3.E7.m1.2.2.1.1.2.2.cmml" xref="S3.E7.m1.2.2.1.1.2.2"><eq id="S3.E7.m1.2.2.1.1.2.2.2.cmml" xref="S3.E7.m1.2.2.1.1.2.2.2"></eq><apply id="S3.E7.m1.2.2.1.1.2.2.1.2.cmml" xref="S3.E7.m1.2.2.1.1.2.2.1.1"><abs id="S3.E7.m1.2.2.1.1.2.2.1.2.1.cmml" xref="S3.E7.m1.2.2.1.1.2.2.1.1.2"></abs><apply id="S3.E7.m1.2.2.1.1.2.2.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.2.2.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.2.2.1.1.1">subscript</csymbol><ci id="S3.E7.m1.2.2.1.1.2.2.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.2.2.1.1.1.2">𝒟</ci><ci id="S3.E7.m1.2.2.1.1.2.2.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.2.2.1.1.1.3">𝑡</ci></apply></apply><apply id="S3.E7.m1.2.2.1.1.2.2.3.cmml" xref="S3.E7.m1.2.2.1.1.2.2.3"><times id="S3.E7.m1.2.2.1.1.2.2.3.1.cmml" xref="S3.E7.m1.2.2.1.1.2.2.3.1"></times><ci id="S3.E7.m1.2.2.1.1.2.2.3.2.cmml" xref="S3.E7.m1.2.2.1.1.2.2.3.2">𝛾</ci><apply id="S3.E7.m1.2.2.1.1.2.2.3.3.1.cmml" xref="S3.E7.m1.2.2.1.1.2.2.3.3.2"><abs id="S3.E7.m1.2.2.1.1.2.2.3.3.1.1.cmml" xref="S3.E7.m1.2.2.1.1.2.2.3.3.2.1"></abs><ci id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1">𝒟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.2c">\mathcal{D}_{t}\sim\mathcal{D},\quad|\mathcal{D}_{t}|=\gamma|\mathcal{D}|,</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.2d">caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∼ caligraphic_D , | caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | = italic_γ | caligraphic_D | ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(8)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{D}_{t}=\{(x_{i}=x_{i}+\Delta,y_{t})\,|\,(x_{i},y_{i})\in\mathcal{D}_{%
t}\}." class="ltx_Math" display="block" id="S3.E8.m1.1"><semantics id="S3.E8.m1.1a"><mrow id="S3.E8.m1.1.1.1" xref="S3.E8.m1.1.1.1.1.cmml"><mrow id="S3.E8.m1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.cmml"><msub id="S3.E8.m1.1.1.1.1.4" xref="S3.E8.m1.1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E8.m1.1.1.1.1.4.2" xref="S3.E8.m1.1.1.1.1.4.2.cmml">𝒟</mi><mi id="S3.E8.m1.1.1.1.1.4.3" xref="S3.E8.m1.1.1.1.1.4.3.cmml">t</mi></msub><mo id="S3.E8.m1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E8.m1.1.1.1.1.2.2" xref="S3.E8.m1.1.1.1.1.2.3.cmml"><mo id="S3.E8.m1.1.1.1.1.2.2.3" stretchy="false" xref="S3.E8.m1.1.1.1.1.2.3.1.cmml">{</mo><mrow id="S3.E8.m1.1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E8.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E8.m1.1.1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E8.m1.1.1.1.1.1.1.1.1.1.4" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.2.cmml">x</mi><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.3.cmml">i</mi></msub><mo id="S3.E8.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" mathvariant="normal" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">Δ</mi></mrow><mo id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">y</mi><mi id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">t</mi></msub></mrow></mrow><mo id="S3.E8.m1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E8.m1.1.1.1.1.2.2.4" lspace="0.170em" rspace="0.170em" xref="S3.E8.m1.1.1.1.1.2.3.1.cmml">|</mo><mrow id="S3.E8.m1.1.1.1.1.2.2.2" xref="S3.E8.m1.1.1.1.1.2.2.2.cmml"><mrow id="S3.E8.m1.1.1.1.1.2.2.2.2.2" xref="S3.E8.m1.1.1.1.1.2.2.2.2.3.cmml"><mo id="S3.E8.m1.1.1.1.1.2.2.2.2.2.3" stretchy="false" xref="S3.E8.m1.1.1.1.1.2.2.2.2.3.cmml">(</mo><msub id="S3.E8.m1.1.1.1.1.2.2.2.1.1.1" xref="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.cmml"><mi id="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.2" xref="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.2.cmml">x</mi><mi id="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.3" xref="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S3.E8.m1.1.1.1.1.2.2.2.2.2.4" xref="S3.E8.m1.1.1.1.1.2.2.2.2.3.cmml">,</mo><msub id="S3.E8.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.cmml"><mi id="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.2" xref="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.2.cmml">y</mi><mi id="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.3" xref="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E8.m1.1.1.1.1.2.2.2.2.2.5" stretchy="false" xref="S3.E8.m1.1.1.1.1.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.E8.m1.1.1.1.1.2.2.2.3" xref="S3.E8.m1.1.1.1.1.2.2.2.3.cmml">∈</mo><msub id="S3.E8.m1.1.1.1.1.2.2.2.4" xref="S3.E8.m1.1.1.1.1.2.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E8.m1.1.1.1.1.2.2.2.4.2" xref="S3.E8.m1.1.1.1.1.2.2.2.4.2.cmml">𝒟</mi><mi id="S3.E8.m1.1.1.1.1.2.2.2.4.3" xref="S3.E8.m1.1.1.1.1.2.2.2.4.3.cmml">t</mi></msub></mrow><mo id="S3.E8.m1.1.1.1.1.2.2.5" stretchy="false" xref="S3.E8.m1.1.1.1.1.2.3.1.cmml">}</mo></mrow></mrow><mo id="S3.E8.m1.1.1.1.2" lspace="0em" xref="S3.E8.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1"><eq id="S3.E8.m1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.3"></eq><apply id="S3.E8.m1.1.1.1.1.4.cmml" xref="S3.E8.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.4.1.cmml" xref="S3.E8.m1.1.1.1.1.4">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.4.2.cmml" xref="S3.E8.m1.1.1.1.1.4.2">𝒟</ci><ci id="S3.E8.m1.1.1.1.1.4.3.cmml" xref="S3.E8.m1.1.1.1.1.4.3">𝑡</ci></apply><apply id="S3.E8.m1.1.1.1.1.2.3.cmml" xref="S3.E8.m1.1.1.1.1.2.2"><csymbol cd="latexml" id="S3.E8.m1.1.1.1.1.2.3.1.cmml" xref="S3.E8.m1.1.1.1.1.2.2.3">conditional-set</csymbol><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1"><eq id="S3.E8.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.3"></eq><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.2">𝑥</ci><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.4.3">𝑖</ci></apply><list id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2"><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1"><plus id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">Δ</ci></apply><apply id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.2">𝑦</ci><ci id="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1.1.1.2.2.2.3">𝑡</ci></apply></list></apply><apply id="S3.E8.m1.1.1.1.1.2.2.2.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2"><in id="S3.E8.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.3"></in><interval closure="open" id="S3.E8.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.2.2"><apply id="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.2">𝑥</ci><ci id="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.1.1.1.3">𝑖</ci></apply><apply id="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.2">𝑦</ci><ci id="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.3.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.2.2.2.3">𝑖</ci></apply></interval><apply id="S3.E8.m1.1.1.1.1.2.2.2.4.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.2.2.2.4.1.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.4">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.2.2.2.4.2.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.4.2">𝒟</ci><ci id="S3.E8.m1.1.1.1.1.2.2.2.4.3.cmml" xref="S3.E8.m1.1.1.1.1.2.2.2.4.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">\mathcal{D}_{t}=\{(x_{i}=x_{i}+\Delta,y_{t})\,|\,(x_{i},y_{i})\in\mathcal{D}_{%
t}\}.</annotation><annotation encoding="application/x-llamapun" id="S3.E8.m1.1d">caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = { ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + roman_Δ , italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) | ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∈ caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">Following data-poisoning, the poisoned dataset <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.p6.1.m1.1"><semantics id="S3.SS1.p6.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><ci id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.1.m1.1d">caligraphic_D</annotation></semantics></math> is utilized for GNN training (<span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.1">i.e.</span>, the second step). This process can either follow the standard procedure or be subject to adversary influence (<span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.2">e.g.</span>, modifying the training pipeline or loss). This is represented as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(9)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{GNN}_{\theta}=\underset{(x_{i},y_{i})\in\mathcal{D}}{\text{argmin}_{%
\theta}}\mathcal{L}(\text{GNN}(x_{i};\theta),y_{i})," class="ltx_Math" display="block" id="S3.E9.m1.4"><semantics id="S3.E9.m1.4a"><mrow id="S3.E9.m1.4.4.1" xref="S3.E9.m1.4.4.1.1.cmml"><mrow id="S3.E9.m1.4.4.1.1" xref="S3.E9.m1.4.4.1.1.cmml"><msub id="S3.E9.m1.4.4.1.1.4" xref="S3.E9.m1.4.4.1.1.4.cmml"><mtext id="S3.E9.m1.4.4.1.1.4.2" xref="S3.E9.m1.4.4.1.1.4.2a.cmml">GNN</mtext><mi id="S3.E9.m1.4.4.1.1.4.3" xref="S3.E9.m1.4.4.1.1.4.3.cmml">θ</mi></msub><mo id="S3.E9.m1.4.4.1.1.3" xref="S3.E9.m1.4.4.1.1.3.cmml">=</mo><mrow id="S3.E9.m1.4.4.1.1.2" xref="S3.E9.m1.4.4.1.1.2.cmml"><munder accentunder="true" id="S3.E9.m1.2.2" xref="S3.E9.m1.2.2.cmml"><msub id="S3.E9.m1.2.2.3" xref="S3.E9.m1.2.2.3.cmml"><mtext id="S3.E9.m1.2.2.3.2" xref="S3.E9.m1.2.2.3.2a.cmml">argmin</mtext><mi id="S3.E9.m1.2.2.3.3" xref="S3.E9.m1.2.2.3.3.cmml">θ</mi></msub><mrow id="S3.E9.m1.2.2.2" xref="S3.E9.m1.2.2.2.cmml"><mrow id="S3.E9.m1.2.2.2.2.2" xref="S3.E9.m1.2.2.2.2.3.cmml"><mo id="S3.E9.m1.2.2.2.2.2.3" stretchy="false" xref="S3.E9.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.E9.m1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E9.m1.1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E9.m1.2.2.2.2.2.4" xref="S3.E9.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.E9.m1.2.2.2.2.2.2" xref="S3.E9.m1.2.2.2.2.2.2.cmml"><mi id="S3.E9.m1.2.2.2.2.2.2.2" xref="S3.E9.m1.2.2.2.2.2.2.2.cmml">y</mi><mi id="S3.E9.m1.2.2.2.2.2.2.3" xref="S3.E9.m1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E9.m1.2.2.2.2.2.5" stretchy="false" xref="S3.E9.m1.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.E9.m1.2.2.2.3" xref="S3.E9.m1.2.2.2.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E9.m1.2.2.2.4" xref="S3.E9.m1.2.2.2.4.cmml">𝒟</mi></mrow></munder><mo id="S3.E9.m1.4.4.1.1.2.3" xref="S3.E9.m1.4.4.1.1.2.3.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="S3.E9.m1.4.4.1.1.2.4" xref="S3.E9.m1.4.4.1.1.2.4.cmml">ℒ</mi><mo id="S3.E9.m1.4.4.1.1.2.3a" xref="S3.E9.m1.4.4.1.1.2.3.cmml">⁢</mo><mrow id="S3.E9.m1.4.4.1.1.2.2.2" xref="S3.E9.m1.4.4.1.1.2.2.3.cmml"><mo id="S3.E9.m1.4.4.1.1.2.2.2.3" stretchy="false" xref="S3.E9.m1.4.4.1.1.2.2.3.cmml">(</mo><mrow id="S3.E9.m1.4.4.1.1.1.1.1.1" xref="S3.E9.m1.4.4.1.1.1.1.1.1.cmml"><mtext id="S3.E9.m1.4.4.1.1.1.1.1.1.3" xref="S3.E9.m1.4.4.1.1.1.1.1.1.3a.cmml">GNN</mtext><mo id="S3.E9.m1.4.4.1.1.1.1.1.1.2" xref="S3.E9.m1.4.4.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S3.E9.m1.3.3" xref="S3.E9.m1.3.3.cmml">θ</mi><mo id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.4" stretchy="false" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E9.m1.4.4.1.1.2.2.2.4" xref="S3.E9.m1.4.4.1.1.2.2.3.cmml">,</mo><msub id="S3.E9.m1.4.4.1.1.2.2.2.2" xref="S3.E9.m1.4.4.1.1.2.2.2.2.cmml"><mi id="S3.E9.m1.4.4.1.1.2.2.2.2.2" xref="S3.E9.m1.4.4.1.1.2.2.2.2.2.cmml">y</mi><mi id="S3.E9.m1.4.4.1.1.2.2.2.2.3" xref="S3.E9.m1.4.4.1.1.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E9.m1.4.4.1.1.2.2.2.5" stretchy="false" xref="S3.E9.m1.4.4.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E9.m1.4.4.1.2" xref="S3.E9.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.4b"><apply id="S3.E9.m1.4.4.1.1.cmml" xref="S3.E9.m1.4.4.1"><eq id="S3.E9.m1.4.4.1.1.3.cmml" xref="S3.E9.m1.4.4.1.1.3"></eq><apply id="S3.E9.m1.4.4.1.1.4.cmml" xref="S3.E9.m1.4.4.1.1.4"><csymbol cd="ambiguous" id="S3.E9.m1.4.4.1.1.4.1.cmml" xref="S3.E9.m1.4.4.1.1.4">subscript</csymbol><ci id="S3.E9.m1.4.4.1.1.4.2a.cmml" xref="S3.E9.m1.4.4.1.1.4.2"><mtext id="S3.E9.m1.4.4.1.1.4.2.cmml" xref="S3.E9.m1.4.4.1.1.4.2">GNN</mtext></ci><ci id="S3.E9.m1.4.4.1.1.4.3.cmml" xref="S3.E9.m1.4.4.1.1.4.3">𝜃</ci></apply><apply id="S3.E9.m1.4.4.1.1.2.cmml" xref="S3.E9.m1.4.4.1.1.2"><times id="S3.E9.m1.4.4.1.1.2.3.cmml" xref="S3.E9.m1.4.4.1.1.2.3"></times><apply id="S3.E9.m1.2.2.cmml" xref="S3.E9.m1.2.2"><apply id="S3.E9.m1.2.2.2.cmml" xref="S3.E9.m1.2.2.2"><in id="S3.E9.m1.2.2.2.3.cmml" xref="S3.E9.m1.2.2.2.3"></in><interval closure="open" id="S3.E9.m1.2.2.2.2.3.cmml" xref="S3.E9.m1.2.2.2.2.2"><apply id="S3.E9.m1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E9.m1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E9.m1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E9.m1.2.2.2.2.2.2.cmml" xref="S3.E9.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.2.2.2.2.1.cmml" xref="S3.E9.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E9.m1.2.2.2.2.2.2.2.cmml" xref="S3.E9.m1.2.2.2.2.2.2.2">𝑦</ci><ci id="S3.E9.m1.2.2.2.2.2.2.3.cmml" xref="S3.E9.m1.2.2.2.2.2.2.3">𝑖</ci></apply></interval><ci id="S3.E9.m1.2.2.2.4.cmml" xref="S3.E9.m1.2.2.2.4">𝒟</ci></apply><apply id="S3.E9.m1.2.2.3.cmml" xref="S3.E9.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.3.1.cmml" xref="S3.E9.m1.2.2.3">subscript</csymbol><ci id="S3.E9.m1.2.2.3.2a.cmml" xref="S3.E9.m1.2.2.3.2"><mtext id="S3.E9.m1.2.2.3.2.cmml" xref="S3.E9.m1.2.2.3.2">argmin</mtext></ci><ci id="S3.E9.m1.2.2.3.3.cmml" xref="S3.E9.m1.2.2.3.3">𝜃</ci></apply></apply><ci id="S3.E9.m1.4.4.1.1.2.4.cmml" xref="S3.E9.m1.4.4.1.1.2.4">ℒ</ci><interval closure="open" id="S3.E9.m1.4.4.1.1.2.2.3.cmml" xref="S3.E9.m1.4.4.1.1.2.2.2"><apply id="S3.E9.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E9.m1.4.4.1.1.1.1.1.1"><times id="S3.E9.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.4.4.1.1.1.1.1.1.2"></times><ci id="S3.E9.m1.4.4.1.1.1.1.1.1.3a.cmml" xref="S3.E9.m1.4.4.1.1.1.1.1.1.3"><mtext id="S3.E9.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.4.4.1.1.1.1.1.1.3">GNN</mtext></ci><list id="S3.E9.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.1"><apply id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.4.4.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S3.E9.m1.3.3.cmml" xref="S3.E9.m1.3.3">𝜃</ci></list></apply><apply id="S3.E9.m1.4.4.1.1.2.2.2.2.cmml" xref="S3.E9.m1.4.4.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E9.m1.4.4.1.1.2.2.2.2.1.cmml" xref="S3.E9.m1.4.4.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E9.m1.4.4.1.1.2.2.2.2.2.cmml" xref="S3.E9.m1.4.4.1.1.2.2.2.2.2">𝑦</ci><ci id="S3.E9.m1.4.4.1.1.2.2.2.2.3.cmml" xref="S3.E9.m1.4.4.1.1.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.4c">\text{GNN}_{\theta}=\underset{(x_{i},y_{i})\in\mathcal{D}}{\text{argmin}_{%
\theta}}\mathcal{L}(\text{GNN}(x_{i};\theta),y_{i}),</annotation><annotation encoding="application/x-llamapun" id="S3.E9.m1.4d">GNN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT = start_UNDERACCENT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∈ caligraphic_D end_UNDERACCENT start_ARG argmin start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT end_ARG caligraphic_L ( GNN ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; italic_θ ) , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p6.3">where loss <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S3.SS1.p6.2.m1.1"><semantics id="S3.SS1.p6.2.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p6.2.m1.1.1" xref="S3.SS1.p6.2.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m1.1b"><ci id="S3.SS1.p6.2.m1.1.1.cmml" xref="S3.SS1.p6.2.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m1.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.2.m1.1d">caligraphic_L</annotation></semantics></math> and the <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p6.3.m2.1"><semantics id="S3.SS1.p6.3.m2.1a"><mi id="S3.SS1.p6.3.m2.1.1" xref="S3.SS1.p6.3.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m2.1b"><ci id="S3.SS1.p6.3.m2.1.1.cmml" xref="S3.SS1.p6.3.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.3.m2.1d">italic_θ</annotation></semantics></math> optimization could be controlled by adversaries.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">After training is completed, GNN gets infected (backdoored). It generates adversary-specified results <math alttext="y_{t}" class="ltx_Math" display="inline" id="S3.SS1.p7.1.m1.1"><semantics id="S3.SS1.p7.1.m1.1a"><msub id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml"><mi id="S3.SS1.p7.1.m1.1.1.2" xref="S3.SS1.p7.1.m1.1.1.2.cmml">y</mi><mi id="S3.SS1.p7.1.m1.1.1.3" xref="S3.SS1.p7.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><apply id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.1.m1.1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p7.1.m1.1.1.2.cmml" xref="S3.SS1.p7.1.m1.1.1.2">𝑦</ci><ci id="S3.SS1.p7.1.m1.1.1.3.cmml" xref="S3.SS1.p7.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">y_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.1.m1.1d">italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> when given trigger-embedded samples during testing, while retaining prediction accuracy for normal data (<span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.1">i.e.</span>, backdoor activation):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(10)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{GNN}(x_{i}+\Delta)=y_{t},\quad\text{GNN}(x_{i})=y_{i}." class="ltx_Math" display="block" id="S3.E10.m1.1"><semantics id="S3.E10.m1.1a"><mrow id="S3.E10.m1.1.1.1"><mrow id="S3.E10.m1.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.3.cmml"><mrow id="S3.E10.m1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E10.m1.1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.1.cmml"><mtext id="S3.E10.m1.1.1.1.1.1.1.1.3" xref="S3.E10.m1.1.1.1.1.1.1.1.3a.cmml">GNN</mtext><mo id="S3.E10.m1.1.1.1.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E10.m1.1.1.1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E10.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.3" mathvariant="normal" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">Δ</mi></mrow><mo id="S3.E10.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E10.m1.1.1.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.1.1.2.cmml">=</mo><msub id="S3.E10.m1.1.1.1.1.1.1.3" xref="S3.E10.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E10.m1.1.1.1.1.1.1.3.2" xref="S3.E10.m1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S3.E10.m1.1.1.1.1.1.1.3.3" xref="S3.E10.m1.1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo id="S3.E10.m1.1.1.1.1.2.3" rspace="1.167em" xref="S3.E10.m1.1.1.1.1.3a.cmml">,</mo><mrow id="S3.E10.m1.1.1.1.1.2.2" xref="S3.E10.m1.1.1.1.1.2.2.cmml"><mrow id="S3.E10.m1.1.1.1.1.2.2.1" xref="S3.E10.m1.1.1.1.1.2.2.1.cmml"><mtext id="S3.E10.m1.1.1.1.1.2.2.1.3" xref="S3.E10.m1.1.1.1.1.2.2.1.3a.cmml">GNN</mtext><mo id="S3.E10.m1.1.1.1.1.2.2.1.2" xref="S3.E10.m1.1.1.1.1.2.2.1.2.cmml">⁢</mo><mrow id="S3.E10.m1.1.1.1.1.2.2.1.1.1" xref="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.cmml"><mo id="S3.E10.m1.1.1.1.1.2.2.1.1.1.2" stretchy="false" xref="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.cmml">(</mo><msub id="S3.E10.m1.1.1.1.1.2.2.1.1.1.1" xref="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.cmml"><mi id="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.2.cmml">x</mi><mi id="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.3" xref="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E10.m1.1.1.1.1.2.2.1.1.1.3" stretchy="false" xref="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E10.m1.1.1.1.1.2.2.2" xref="S3.E10.m1.1.1.1.1.2.2.2.cmml">=</mo><msub id="S3.E10.m1.1.1.1.1.2.2.3" xref="S3.E10.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E10.m1.1.1.1.1.2.2.3.2" xref="S3.E10.m1.1.1.1.1.2.2.3.2.cmml">y</mi><mi id="S3.E10.m1.1.1.1.1.2.2.3.3" xref="S3.E10.m1.1.1.1.1.2.2.3.3.cmml">i</mi></msub></mrow></mrow><mo id="S3.E10.m1.1.1.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m1.1b"><apply id="S3.E10.m1.1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.1.1.3a.cmml" xref="S3.E10.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E10.m1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1"><eq id="S3.E10.m1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.2"></eq><apply id="S3.E10.m1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1"><times id="S3.E10.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.2"></times><ci id="S3.E10.m1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3"><mtext id="S3.E10.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3">GNN</mtext></ci><apply id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1"><plus id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.1.1.1.3">Δ</ci></apply></apply><apply id="S3.E10.m1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E10.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.3.2">𝑦</ci><ci id="S3.E10.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply><apply id="S3.E10.m1.1.1.1.1.2.2.cmml" xref="S3.E10.m1.1.1.1.1.2.2"><eq id="S3.E10.m1.1.1.1.1.2.2.2.cmml" xref="S3.E10.m1.1.1.1.1.2.2.2"></eq><apply id="S3.E10.m1.1.1.1.1.2.2.1.cmml" xref="S3.E10.m1.1.1.1.1.2.2.1"><times id="S3.E10.m1.1.1.1.1.2.2.1.2.cmml" xref="S3.E10.m1.1.1.1.1.2.2.1.2"></times><ci id="S3.E10.m1.1.1.1.1.2.2.1.3a.cmml" xref="S3.E10.m1.1.1.1.1.2.2.1.3"><mtext id="S3.E10.m1.1.1.1.1.2.2.1.3.cmml" xref="S3.E10.m1.1.1.1.1.2.2.1.3">GNN</mtext></ci><apply id="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.2.2.1.1.1">subscript</csymbol><ci id="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.2.cmml" xref="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.2">𝑥</ci><ci id="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.1.2.2.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.E10.m1.1.1.1.1.2.2.3.cmml" xref="S3.E10.m1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E10.m1.1.1.1.1.2.2.3">subscript</csymbol><ci id="S3.E10.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E10.m1.1.1.1.1.2.2.3.2">𝑦</ci><ci id="S3.E10.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E10.m1.1.1.1.1.2.2.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.1c">\text{GNN}(x_{i}+\Delta)=y_{t},\quad\text{GNN}(x_{i})=y_{i}.</annotation><annotation encoding="application/x-llamapun" id="S3.E10.m1.1d">GNN ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + roman_Δ ) = italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , GNN ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="435" id="S3.F5.g1" src="extracted/5669158/figure/gnn_bkd_framework.png" width="928"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Illustration of the general process of GNN backdoor, which is achieved by data-poisoning. Specifically, the adversary opts for a subset of samples from the training data and inserts designated subgraphs as triggers into them, subsequently modifying the ground truths of these data to the target class. As a consequence, this causes the trained model to predict the target class for input samples embedded with these triggers.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Attack Methodologies</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Based on the classification of backdoor attacks studies on GNN outlined at the beginning of this section, we will elaborate on them in two parts: (<span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">i</span>) adaptability-expanding research; (<span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">ii</span>) effectiveness-improving research. The comprehensive summary of backdoor attacks is shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S3.T3" title="Table 3 ‣ 3.2. Attack Methodologies ‣ 3. Current Methodologies ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Summary of Graph Neural Network Backdoors</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T3.68" style="width:433.6pt;height:418.1pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-257.0pt,247.5pt) scale(0.457551732450453,0.457551732450453) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.68.68">
<tr class="ltx_tr" id="S3.T3.68.68.69">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.68.68.69.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T3.68.68.69.1.1">Attack Approach</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T3.68.68.69.2"><span class="ltx_text ltx_font_bold" id="S3.T3.68.68.69.2.1">Attack Level</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T3.68.68.69.3"><span class="ltx_text ltx_font_bold" id="S3.T3.68.68.69.3.1">Adversary Capability</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.68.68.69.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T3.68.68.69.4.1">Scenario</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.68.68.69.5" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T3.68.68.69.5.1">Research Novelty</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.68.68.70">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.68.68.70.1">Graph</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.68.68.70.2">Node</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.68.68.70.3">Edge</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.68.68.70.4">Samples</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.68.68.70.5">Model</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.68.68.70.6">Loss</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.2.3">Subgraph Backdoor (2021) <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib105" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.m1.1a"><mo id="S3.T3.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.2.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.2.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.2.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.2.2.2.2.m1.1"><semantics id="S3.T3.2.2.2.2.m1.1a"><mo id="S3.T3.2.2.2.2.m1.1.1" xref="S3.T3.2.2.2.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.m1.1b"><ci id="S3.T3.2.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.2.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.2.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.2.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.2.9">Seminal paper</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.6.6.5">GTA (2021) <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib77" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.3.3.3.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.3.3.3.1.m1.1"><semantics id="S3.T3.3.3.3.1.m1.1a"><mo id="S3.T3.3.3.3.1.m1.1.1" xref="S3.T3.3.3.3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.1.m1.1b"><ci id="S3.T3.3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.3.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.4.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.4.4.4.2.m1.1"><semantics id="S3.T3.4.4.4.2.m1.1a"><mo id="S3.T3.4.4.4.2.m1.1.1" xref="S3.T3.4.4.4.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.2.m1.1b"><ci id="S3.T3.4.4.4.2.m1.1.1.cmml" xref="S3.T3.4.4.4.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.4.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.6.6.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.5.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.5.5.5.3.m1.1"><semantics id="S3.T3.5.5.5.3.m1.1a"><mo id="S3.T3.5.5.5.3.m1.1.1" xref="S3.T3.5.5.5.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.3.m1.1b"><ci id="S3.T3.5.5.5.3.m1.1.1.cmml" xref="S3.T3.5.5.5.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.5.5.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.6.6.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.6.6.4"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.6.6.6.4.m1.1"><semantics id="S3.T3.6.6.6.4.m1.1a"><mo id="S3.T3.6.6.6.4.m1.1.1" xref="S3.T3.6.6.6.4.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.6.4.m1.1b"><ci id="S3.T3.6.6.6.4.m1.1.1.cmml" xref="S3.T3.6.6.6.4.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.6.4.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.6.6.4.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.6.6.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.6.6.6.9">Seminal paper</td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.8.8.8.3">Random Trigger Backdoor (2021) <cite class="ltx_cite ltx_citemacro_citep">(Sheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib60" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.7.7.7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.7.7.7.1.m1.1"><semantics id="S3.T3.7.7.7.1.m1.1a"><mo id="S3.T3.7.7.7.1.m1.1.1" xref="S3.T3.7.7.7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.7.1.m1.1b"><ci id="S3.T3.7.7.7.1.m1.1.1.cmml" xref="S3.T3.7.7.7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.7.7.7.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.8.8.8.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.8.8.8.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.8.8.8.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.8.8.8.2.m1.1"><semantics id="S3.T3.8.8.8.2.m1.1a"><mo id="S3.T3.8.8.8.2.m1.1.1" xref="S3.T3.8.8.8.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.8.2.m1.1b"><ci id="S3.T3.8.8.8.2.m1.1.1.cmml" xref="S3.T3.8.8.8.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.8.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.8.8.8.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.8.8.8.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.8.8.8.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.8.8.8.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.8.8.8.9">Optimize attack node selection</td>
</tr>
<tr class="ltx_tr" id="S3.T3.11.11.11">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.11.11.11.4">CBA/DBA (2022) <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib85" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib82" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.9.9.9.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.9.9.9.1.m1.1"><semantics id="S3.T3.9.9.9.1.m1.1a"><mo id="S3.T3.9.9.9.1.m1.1.1" xref="S3.T3.9.9.9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.9.9.9.1.m1.1b"><ci id="S3.T3.9.9.9.1.m1.1.1.cmml" xref="S3.T3.9.9.9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.9.9.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.9.9.9.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.11.11.11.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.11.11.11.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.10.10.10.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.10.10.10.2.m1.1"><semantics id="S3.T3.10.10.10.2.m1.1a"><mo id="S3.T3.10.10.10.2.m1.1.1" xref="S3.T3.10.10.10.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.10.10.10.2.m1.1b"><ci id="S3.T3.10.10.10.2.m1.1.1.cmml" xref="S3.T3.10.10.10.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.10.10.10.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.10.10.10.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.11.11.11.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.11.11.11.3.m1.1"><semantics id="S3.T3.11.11.11.3.m1.1a"><mo id="S3.T3.11.11.11.3.m1.1.1" xref="S3.T3.11.11.11.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.11.11.11.3.m1.1b"><ci id="S3.T3.11.11.11.3.m1.1.1.cmml" xref="S3.T3.11.11.11.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.11.11.11.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.11.11.11.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.11.11.11.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.11.11.11.8">Federated GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.11.11.11.9">Extend backdoor to FL</td>
</tr>
<tr class="ltx_tr" id="S3.T3.15.15.15">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.15.15.15.5">Bkd-FedGNN (2023) <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib45" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.12.12.12.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.12.12.12.1.m1.1"><semantics id="S3.T3.12.12.12.1.m1.1a"><mo id="S3.T3.12.12.12.1.m1.1.1" xref="S3.T3.12.12.12.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.12.12.12.1.m1.1b"><ci id="S3.T3.12.12.12.1.m1.1.1.cmml" xref="S3.T3.12.12.12.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.12.12.12.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.12.12.12.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.13.13.13.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.13.13.13.2.m1.1"><semantics id="S3.T3.13.13.13.2.m1.1a"><mo id="S3.T3.13.13.13.2.m1.1.1" xref="S3.T3.13.13.13.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.13.13.13.2.m1.1b"><ci id="S3.T3.13.13.13.2.m1.1.1.cmml" xref="S3.T3.13.13.13.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.13.13.13.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.13.13.13.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.15.15.15.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.14.14.14.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.14.14.14.3.m1.1"><semantics id="S3.T3.14.14.14.3.m1.1a"><mo id="S3.T3.14.14.14.3.m1.1.1" xref="S3.T3.14.14.14.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.14.14.14.3.m1.1b"><ci id="S3.T3.14.14.14.3.m1.1.1.cmml" xref="S3.T3.14.14.14.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.14.14.14.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.14.14.14.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.15.15.15.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.15.15.15.4"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.15.15.15.4.m1.1"><semantics id="S3.T3.15.15.15.4.m1.1a"><mo id="S3.T3.15.15.15.4.m1.1.1" xref="S3.T3.15.15.15.4.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.15.15.15.4.m1.1b"><ci id="S3.T3.15.15.15.4.m1.1.1.cmml" xref="S3.T3.15.15.15.4.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.15.15.15.4.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.15.15.15.4.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.15.15.15.8">Federated GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.15.15.15.9">
<span class="ltx_text" id="S3.T3.15.15.15.9.1"></span> <span class="ltx_text" id="S3.T3.15.15.15.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.15.15.15.9.2.1">
<span class="ltx_tr" id="S3.T3.15.15.15.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.15.15.15.9.2.1.1.1">Extend FL backdoor to</span></span>
<span class="ltx_tr" id="S3.T3.15.15.15.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.15.15.15.9.2.1.2.1">node-level task</span></span>
</span></span><span class="ltx_text" id="S3.T3.15.15.15.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.17.17.17">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.17.17.17.3">GCBA (2023) <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib100" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.17.17.17.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.16.16.16.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.16.16.16.1.m1.1"><semantics id="S3.T3.16.16.16.1.m1.1a"><mo id="S3.T3.16.16.16.1.m1.1.1" xref="S3.T3.16.16.16.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.16.16.16.1.m1.1b"><ci id="S3.T3.16.16.16.1.m1.1.1.cmml" xref="S3.T3.16.16.16.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.16.16.16.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.16.16.16.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.17.17.17.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.17.17.17.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.17.17.17.2.m1.1"><semantics id="S3.T3.17.17.17.2.m1.1a"><mo id="S3.T3.17.17.17.2.m1.1.1" xref="S3.T3.17.17.17.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.17.17.17.2.m1.1b"><ci id="S3.T3.17.17.17.2.m1.1.1.cmml" xref="S3.T3.17.17.17.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.17.17.17.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.17.17.17.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.17.17.17.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.17.17.17.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.17.17.17.8">Contrastive learning GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.17.17.17.9">
<span class="ltx_text" id="S3.T3.17.17.17.9.1"></span> <span class="ltx_text" id="S3.T3.17.17.17.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.17.17.17.9.2.1">
<span class="ltx_tr" id="S3.T3.17.17.17.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.17.17.17.9.2.1.1.1">Extend backdoor to</span></span>
<span class="ltx_tr" id="S3.T3.17.17.17.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.17.17.17.9.2.1.2.1">contrastive learning</span></span>
</span></span><span class="ltx_text" id="S3.T3.17.17.17.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.20.20.20">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.20.20.20.4">CrossBA (2024) <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib49" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.20.20.20.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.18.18.18.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.18.18.18.1.m1.1"><semantics id="S3.T3.18.18.18.1.m1.1a"><mo id="S3.T3.18.18.18.1.m1.1.1" xref="S3.T3.18.18.18.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.18.18.18.1.m1.1b"><ci id="S3.T3.18.18.18.1.m1.1.1.cmml" xref="S3.T3.18.18.18.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.18.18.18.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.18.18.18.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.20.20.20.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.19.19.19.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.19.19.19.2.m1.1"><semantics id="S3.T3.19.19.19.2.m1.1a"><mo id="S3.T3.19.19.19.2.m1.1.1" xref="S3.T3.19.19.19.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.19.19.19.2.m1.1b"><ci id="S3.T3.19.19.19.2.m1.1.1.cmml" xref="S3.T3.19.19.19.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.19.19.19.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.19.19.19.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.20.20.20.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.20.20.20.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.20.20.20.3.m1.1"><semantics id="S3.T3.20.20.20.3.m1.1a"><mo id="S3.T3.20.20.20.3.m1.1.1" xref="S3.T3.20.20.20.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.20.20.20.3.m1.1b"><ci id="S3.T3.20.20.20.3.m1.1.1.cmml" xref="S3.T3.20.20.20.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.20.20.20.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.20.20.20.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.20.20.20.8">Graph prompt learning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.20.20.20.9">
<span class="ltx_text" id="S3.T3.20.20.20.9.1"></span> <span class="ltx_text" id="S3.T3.20.20.20.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.20.20.20.9.2.1">
<span class="ltx_tr" id="S3.T3.20.20.20.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.20.20.20.9.2.1.1.1">Extend backdoor to</span></span>
<span class="ltx_tr" id="S3.T3.20.20.20.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.20.20.20.9.2.1.2.1">graph prompt learning</span></span>
</span></span><span class="ltx_text" id="S3.T3.20.20.20.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.24.24.24">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.24.24.24.5">PoisonedGNN (2023) <cite class="ltx_cite ltx_citemacro_citep">(Alrahis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib2" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.21.21.21.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.21.21.21.1.m1.1"><semantics id="S3.T3.21.21.21.1.m1.1a"><mo id="S3.T3.21.21.21.1.m1.1.1" xref="S3.T3.21.21.21.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.21.21.21.1.m1.1b"><ci id="S3.T3.21.21.21.1.m1.1.1.cmml" xref="S3.T3.21.21.21.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.21.21.21.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.21.21.21.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.24.24.24.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.24.24.24.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.22.22.22.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.22.22.22.2.m1.1"><semantics id="S3.T3.22.22.22.2.m1.1a"><mo id="S3.T3.22.22.22.2.m1.1.1" xref="S3.T3.22.22.22.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.22.22.22.2.m1.1b"><ci id="S3.T3.22.22.22.2.m1.1.1.cmml" xref="S3.T3.22.22.22.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.22.22.22.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.22.22.22.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.23.23.23.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.23.23.23.3.m1.1"><semantics id="S3.T3.23.23.23.3.m1.1a"><mo id="S3.T3.23.23.23.3.m1.1.1" xref="S3.T3.23.23.23.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.23.23.23.3.m1.1b"><ci id="S3.T3.23.23.23.3.m1.1.1.cmml" xref="S3.T3.23.23.23.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.23.23.23.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.23.23.23.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.24.24.24.4"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.24.24.24.4.m1.1"><semantics id="S3.T3.24.24.24.4.m1.1a"><mo id="S3.T3.24.24.24.4.m1.1.1" xref="S3.T3.24.24.24.4.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.24.24.24.4.m1.1b"><ci id="S3.T3.24.24.24.4.m1.1.1.cmml" xref="S3.T3.24.24.24.4.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.24.24.24.4.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.24.24.24.4.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.24.24.24.8">Hardware GNN system</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.24.24.24.9">
<span class="ltx_text" id="S3.T3.24.24.24.9.1"></span> <span class="ltx_text" id="S3.T3.24.24.24.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.24.24.24.9.2.1">
<span class="ltx_tr" id="S3.T3.24.24.24.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.24.24.24.9.2.1.1.1">First physical GNN</span></span>
<span class="ltx_tr" id="S3.T3.24.24.24.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.24.24.24.9.2.1.2.1">system backdoor</span></span>
</span></span><span class="ltx_text" id="S3.T3.24.24.24.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.28.28.28">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.28.28.28.5">Explainability Backdoor (2020) <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib86" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.25.25.25.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.25.25.25.1.m1.1"><semantics id="S3.T3.25.25.25.1.m1.1a"><mo id="S3.T3.25.25.25.1.m1.1.1" xref="S3.T3.25.25.25.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.25.25.25.1.m1.1b"><ci id="S3.T3.25.25.25.1.m1.1.1.cmml" xref="S3.T3.25.25.25.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.25.25.25.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.25.25.25.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.26.26.26.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.26.26.26.2.m1.1"><semantics id="S3.T3.26.26.26.2.m1.1a"><mo id="S3.T3.26.26.26.2.m1.1.1" xref="S3.T3.26.26.26.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.26.26.26.2.m1.1b"><ci id="S3.T3.26.26.26.2.m1.1.1.cmml" xref="S3.T3.26.26.26.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.26.26.26.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.26.26.26.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.28.28.28.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.27.27.27.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.27.27.27.3.m1.1"><semantics id="S3.T3.27.27.27.3.m1.1a"><mo id="S3.T3.27.27.27.3.m1.1.1" xref="S3.T3.27.27.27.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.27.27.27.3.m1.1b"><ci id="S3.T3.27.27.27.3.m1.1.1.cmml" xref="S3.T3.27.27.27.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.27.27.27.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.27.27.27.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.28.28.28.4"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.28.28.28.4.m1.1"><semantics id="S3.T3.28.28.28.4.m1.1a"><mo id="S3.T3.28.28.28.4.m1.1.1" xref="S3.T3.28.28.28.4.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.28.28.28.4.m1.1b"><ci id="S3.T3.28.28.28.4.m1.1.1.cmml" xref="S3.T3.28.28.28.4.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.28.28.28.4.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.28.28.28.4.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.28.28.28.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.28.28.28.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.28.28.28.9">
<span class="ltx_text" id="S3.T3.28.28.28.9.1"></span> <span class="ltx_text" id="S3.T3.28.28.28.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.28.28.28.9.2.1">
<span class="ltx_tr" id="S3.T3.28.28.28.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.28.28.28.9.2.1.1.1">Employ explainability to</span></span>
<span class="ltx_tr" id="S3.T3.28.28.28.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.28.28.28.9.2.1.2.1">design trigger</span></span>
</span></span><span class="ltx_text" id="S3.T3.28.28.28.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.30.30.30">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.30.30.30.3">Motif Backdoor (2023) <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib108" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.29.29.29.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.29.29.29.1.m1.1"><semantics id="S3.T3.29.29.29.1.m1.1a"><mo id="S3.T3.29.29.29.1.m1.1.1" xref="S3.T3.29.29.29.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.29.29.29.1.m1.1b"><ci id="S3.T3.29.29.29.1.m1.1.1.cmml" xref="S3.T3.29.29.29.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.29.29.29.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.29.29.29.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.30.30.30.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.30.30.30.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.30.30.30.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.30.30.30.2.m1.1"><semantics id="S3.T3.30.30.30.2.m1.1a"><mo id="S3.T3.30.30.30.2.m1.1.1" xref="S3.T3.30.30.30.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.30.30.30.2.m1.1b"><ci id="S3.T3.30.30.30.2.m1.1.1.cmml" xref="S3.T3.30.30.30.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.30.30.30.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.30.30.30.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.30.30.30.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.30.30.30.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.30.30.30.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.30.30.30.9">
<span class="ltx_text" id="S3.T3.30.30.30.9.1"></span> <span class="ltx_text" id="S3.T3.30.30.30.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.30.30.30.9.2.1">
<span class="ltx_tr" id="S3.T3.30.30.30.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.30.30.30.9.2.1.1.1">Explain GNN Backdoor</span></span>
<span class="ltx_tr" id="S3.T3.30.30.30.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.30.30.30.9.2.1.2.1">Effectiveness</span></span>
</span></span><span class="ltx_text" id="S3.T3.30.30.30.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.33.33.33">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.33.33.33.4">General Backdoor (2022) <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib8" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.33.33.33.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.31.31.31.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.31.31.31.1.m1.1"><semantics id="S3.T3.31.31.31.1.m1.1a"><mo id="S3.T3.31.31.31.1.m1.1.1" xref="S3.T3.31.31.31.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.31.31.31.1.m1.1b"><ci id="S3.T3.31.31.31.1.m1.1.1.cmml" xref="S3.T3.31.31.31.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.31.31.31.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.31.31.31.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.33.33.33.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.32.32.32.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.32.32.32.2.m1.1"><semantics id="S3.T3.32.32.32.2.m1.1a"><mo id="S3.T3.32.32.32.2.m1.1.1" xref="S3.T3.32.32.32.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.32.32.32.2.m1.1b"><ci id="S3.T3.32.32.32.2.m1.1.1.cmml" xref="S3.T3.32.32.32.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.32.32.32.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.32.32.32.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.33.33.33.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.33.33.33.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.33.33.33.3.m1.1"><semantics id="S3.T3.33.33.33.3.m1.1a"><mo id="S3.T3.33.33.33.3.m1.1.1" xref="S3.T3.33.33.33.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.33.33.33.3.m1.1b"><ci id="S3.T3.33.33.33.3.m1.1.1.cmml" xref="S3.T3.33.33.33.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.33.33.33.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.33.33.33.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.33.33.33.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.33.33.33.9">
<span class="ltx_text" id="S3.T3.33.33.33.9.1"></span> <span class="ltx_text" id="S3.T3.33.33.33.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.33.33.33.9.2.1">
<span class="ltx_tr" id="S3.T3.33.33.33.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.33.33.33.9.2.1.1.1">Employ explainability to</span></span>
<span class="ltx_tr" id="S3.T3.33.33.33.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.33.33.33.9.2.1.2.1">improve stealthiness</span></span>
</span></span><span class="ltx_text" id="S3.T3.33.33.33.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.37.37.37">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.37.37.37.5">Explanatory Subgraph Attack (2024) <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib71" title="">2024c</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.34.34.34.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.34.34.34.1.m1.1"><semantics id="S3.T3.34.34.34.1.m1.1a"><mo id="S3.T3.34.34.34.1.m1.1.1" xref="S3.T3.34.34.34.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.34.34.34.1.m1.1b"><ci id="S3.T3.34.34.34.1.m1.1.1.cmml" xref="S3.T3.34.34.34.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.34.34.34.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.34.34.34.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.37.37.37.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.37.37.37.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.35.35.35.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.35.35.35.2.m1.1"><semantics id="S3.T3.35.35.35.2.m1.1a"><mo id="S3.T3.35.35.35.2.m1.1.1" xref="S3.T3.35.35.35.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.35.35.35.2.m1.1b"><ci id="S3.T3.35.35.35.2.m1.1.1.cmml" xref="S3.T3.35.35.35.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.35.35.35.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.35.35.35.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.36.36.36.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.36.36.36.3.m1.1"><semantics id="S3.T3.36.36.36.3.m1.1a"><mo id="S3.T3.36.36.36.3.m1.1.1" xref="S3.T3.36.36.36.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.36.36.36.3.m1.1b"><ci id="S3.T3.36.36.36.3.m1.1.1.cmml" xref="S3.T3.36.36.36.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.36.36.36.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.36.36.36.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.37.37.37.4"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.37.37.37.4.m1.1"><semantics id="S3.T3.37.37.37.4.m1.1a"><mo id="S3.T3.37.37.37.4.m1.1.1" xref="S3.T3.37.37.37.4.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.37.37.37.4.m1.1b"><ci id="S3.T3.37.37.37.4.m1.1.1.cmml" xref="S3.T3.37.37.37.4.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.37.37.37.4.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.37.37.37.4.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.37.37.37.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.37.37.37.9">
<span class="ltx_text" id="S3.T3.37.37.37.9.1"></span> <span class="ltx_text" id="S3.T3.37.37.37.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.37.37.37.9.2.1">
<span class="ltx_tr" id="S3.T3.37.37.37.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.37.37.37.9.2.1.1.1">Keep explanatory information</span></span>
<span class="ltx_tr" id="S3.T3.37.37.37.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.37.37.37.9.2.1.2.1">for poisoned graphs</span></span>
</span></span><span class="ltx_text" id="S3.T3.37.37.37.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.41.41.41">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.41.41.41.5">UGBA (2023) <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib11" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.41.41.41.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.38.38.38.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.38.38.38.1.m1.1"><semantics id="S3.T3.38.38.38.1.m1.1a"><mo id="S3.T3.38.38.38.1.m1.1.1" xref="S3.T3.38.38.38.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.38.38.38.1.m1.1b"><ci id="S3.T3.38.38.38.1.m1.1.1.cmml" xref="S3.T3.38.38.38.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.38.38.38.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.38.38.38.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.41.41.41.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.39.39.39.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.39.39.39.2.m1.1"><semantics id="S3.T3.39.39.39.2.m1.1a"><mo id="S3.T3.39.39.39.2.m1.1.1" xref="S3.T3.39.39.39.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.39.39.39.2.m1.1b"><ci id="S3.T3.39.39.39.2.m1.1.1.cmml" xref="S3.T3.39.39.39.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.39.39.39.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.39.39.39.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.40.40.40.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.40.40.40.3.m1.1"><semantics id="S3.T3.40.40.40.3.m1.1a"><mo id="S3.T3.40.40.40.3.m1.1.1" xref="S3.T3.40.40.40.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.40.40.40.3.m1.1b"><ci id="S3.T3.40.40.40.3.m1.1.1.cmml" xref="S3.T3.40.40.40.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.40.40.40.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.40.40.40.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.41.41.41.4"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.41.41.41.4.m1.1"><semantics id="S3.T3.41.41.41.4.m1.1a"><mo id="S3.T3.41.41.41.4.m1.1.1" xref="S3.T3.41.41.41.4.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.41.41.41.4.m1.1b"><ci id="S3.T3.41.41.41.4.m1.1.1.cmml" xref="S3.T3.41.41.41.4.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.41.41.41.4.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.41.41.41.4.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.41.41.41.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.41.41.41.9">
<span class="ltx_text" id="S3.T3.41.41.41.9.1"></span> <span class="ltx_text" id="S3.T3.41.41.41.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.41.9.2.1">
<span class="ltx_tr" id="S3.T3.41.41.41.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.41.41.41.9.2.1.1.1">Optimize to
improve</span></span>
<span class="ltx_tr" id="S3.T3.41.41.41.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.41.41.41.9.2.1.2.1">stealthness</span></span>
</span></span><span class="ltx_text" id="S3.T3.41.41.41.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.43.43.43">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.43.43.43.3">CBAG (2024) <cite class="ltx_cite ltx_citemacro_citep">(Dai and Sun, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib13" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.43.43.43.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.42.42.42.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.42.42.42.1.m1.1"><semantics id="S3.T3.42.42.42.1.m1.1a"><mo id="S3.T3.42.42.42.1.m1.1.1" xref="S3.T3.42.42.42.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.42.42.42.1.m1.1b"><ci id="S3.T3.42.42.42.1.m1.1.1.cmml" xref="S3.T3.42.42.42.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.42.42.42.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.42.42.42.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.43.43.43.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.43.43.43.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.43.43.43.2.m1.1"><semantics id="S3.T3.43.43.43.2.m1.1a"><mo id="S3.T3.43.43.43.2.m1.1.1" xref="S3.T3.43.43.43.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.43.43.43.2.m1.1b"><ci id="S3.T3.43.43.43.2.m1.1.1.cmml" xref="S3.T3.43.43.43.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.43.43.43.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.43.43.43.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.43.43.43.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.43.43.43.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.43.43.43.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.43.43.43.9">Propose clean-graph backdoor</td>
</tr>
<tr class="ltx_tr" id="S3.T3.45.45.45">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.45.45.45.3">Poster (2023) <cite class="ltx_cite ltx_citemacro_citep">(Xu and Picek, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib83" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.44.44.44.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.44.44.44.1.m1.1"><semantics id="S3.T3.44.44.44.1.m1.1a"><mo id="S3.T3.44.44.44.1.m1.1.1" xref="S3.T3.44.44.44.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.44.44.44.1.m1.1b"><ci id="S3.T3.44.44.44.1.m1.1.1.cmml" xref="S3.T3.44.44.44.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.44.44.44.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.44.44.44.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.45.45.45.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.45.45.45.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.45.45.45.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.45.45.45.2.m1.1"><semantics id="S3.T3.45.45.45.2.m1.1a"><mo id="S3.T3.45.45.45.2.m1.1.1" xref="S3.T3.45.45.45.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.45.45.45.2.m1.1b"><ci id="S3.T3.45.45.45.2.m1.1.1.cmml" xref="S3.T3.45.45.45.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.45.45.45.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.45.45.45.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.45.45.45.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.45.45.45.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.45.45.45.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.45.45.45.9">
<span class="ltx_text" id="S3.T3.45.45.45.9.1"></span> <span class="ltx_text" id="S3.T3.45.45.45.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.45.45.45.9.2.1">
<span class="ltx_tr" id="S3.T3.45.45.45.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.45.45.45.9.2.1.1.1">Propose clean-label</span></span>
<span class="ltx_tr" id="S3.T3.45.45.45.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.45.45.45.9.2.1.2.1">GNN backdoor</span></span>
</span></span><span class="ltx_text" id="S3.T3.45.45.45.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.47.47.47">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.47.47.47.3">CGBA (2023) <cite class="ltx_cite ltx_citemacro_citep">(Xing et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib80" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.47.47.47.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.46.46.46.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.46.46.46.1.m1.1"><semantics id="S3.T3.46.46.46.1.m1.1a"><mo id="S3.T3.46.46.46.1.m1.1.1" xref="S3.T3.46.46.46.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.46.46.46.1.m1.1b"><ci id="S3.T3.46.46.46.1.m1.1.1.cmml" xref="S3.T3.46.46.46.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.46.46.46.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.46.46.46.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.47.47.47.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.47.47.47.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.47.47.47.2.m1.1"><semantics id="S3.T3.47.47.47.2.m1.1a"><mo id="S3.T3.47.47.47.2.m1.1.1" xref="S3.T3.47.47.47.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.47.47.47.2.m1.1b"><ci id="S3.T3.47.47.47.2.m1.1.1.cmml" xref="S3.T3.47.47.47.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.47.47.47.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.47.47.47.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.47.47.47.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.47.47.47.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.47.47.47.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.47.47.47.9">
<span class="ltx_text" id="S3.T3.47.47.47.9.1"></span> <span class="ltx_text" id="S3.T3.47.47.47.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.47.47.47.9.2.1">
<span class="ltx_tr" id="S3.T3.47.47.47.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.47.47.47.9.2.1.1.1">Reduce poisonous</span></span>
<span class="ltx_tr" id="S3.T3.47.47.47.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.47.47.47.9.2.1.2.1">perturbation</span></span>
</span></span><span class="ltx_text" id="S3.T3.47.47.47.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.49.49.49">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.49.49.49.3">PerCBA (2023) <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib93" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.49.49.49.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.48.48.48.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.48.48.48.1.m1.1"><semantics id="S3.T3.48.48.48.1.m1.1a"><mo id="S3.T3.48.48.48.1.m1.1.1" xref="S3.T3.48.48.48.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.48.48.48.1.m1.1b"><ci id="S3.T3.48.48.48.1.m1.1.1.cmml" xref="S3.T3.48.48.48.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.48.48.48.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.48.48.48.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.49.49.49.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.49.49.49.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.49.49.49.2.m1.1"><semantics id="S3.T3.49.49.49.2.m1.1a"><mo id="S3.T3.49.49.49.2.m1.1.1" xref="S3.T3.49.49.49.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.49.49.49.2.m1.1b"><ci id="S3.T3.49.49.49.2.m1.1.1.cmml" xref="S3.T3.49.49.49.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.49.49.49.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.49.49.49.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.49.49.49.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.49.49.49.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.49.49.49.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.49.49.49.9">
<span class="ltx_text" id="S3.T3.49.49.49.9.1"></span> <span class="ltx_text" id="S3.T3.49.49.49.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.49.49.49.9.2.1">
<span class="ltx_tr" id="S3.T3.49.49.49.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.49.49.49.9.2.1.1.1">Extend clean-label backdoor</span></span>
<span class="ltx_tr" id="S3.T3.49.49.49.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.49.49.49.9.2.1.2.1">to node-level task</span></span>
</span></span><span class="ltx_text" id="S3.T3.49.49.49.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.53.53.53">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.53.53.53.5">TRAP (2022) <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib90" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.50.50.50.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.50.50.50.1.m1.1"><semantics id="S3.T3.50.50.50.1.m1.1a"><mo id="S3.T3.50.50.50.1.m1.1.1" xref="S3.T3.50.50.50.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.50.50.50.1.m1.1b"><ci id="S3.T3.50.50.50.1.m1.1.1.cmml" xref="S3.T3.50.50.50.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.50.50.50.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.50.50.50.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.53.53.53.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.53.53.53.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.51.51.51.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.51.51.51.2.m1.1"><semantics id="S3.T3.51.51.51.2.m1.1a"><mo id="S3.T3.51.51.51.2.m1.1.1" xref="S3.T3.51.51.51.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.51.51.51.2.m1.1b"><ci id="S3.T3.51.51.51.2.m1.1.1.cmml" xref="S3.T3.51.51.51.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.51.51.51.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.51.51.51.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.52.52.52.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.52.52.52.3.m1.1"><semantics id="S3.T3.52.52.52.3.m1.1a"><mo id="S3.T3.52.52.52.3.m1.1.1" xref="S3.T3.52.52.52.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.52.52.52.3.m1.1b"><ci id="S3.T3.52.52.52.3.m1.1.1.cmml" xref="S3.T3.52.52.52.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.52.52.52.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.52.52.52.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.53.53.53.4"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.53.53.53.4.m1.1"><semantics id="S3.T3.53.53.53.4.m1.1a"><mo id="S3.T3.53.53.53.4.m1.1.1" xref="S3.T3.53.53.53.4.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.53.53.53.4.m1.1b"><ci id="S3.T3.53.53.53.4.m1.1.1.cmml" xref="S3.T3.53.53.53.4.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.53.53.53.4.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.53.53.53.4.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.53.53.53.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.53.53.53.9">
<span class="ltx_text" id="S3.T3.53.53.53.9.1"></span> <span class="ltx_text" id="S3.T3.53.53.53.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.53.53.53.9.2.1">
<span class="ltx_tr" id="S3.T3.53.53.53.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.53.53.53.9.2.1.1.1">Employ data transferability</span></span>
<span class="ltx_tr" id="S3.T3.53.53.53.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.53.53.53.9.2.1.2.1">to design backdoor</span></span>
</span></span><span class="ltx_text" id="S3.T3.53.53.53.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.55.55.55">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.55.55.55.3">Graph Spectrum Backdoor (2023) <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib106" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.54.54.54.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.54.54.54.1.m1.1"><semantics id="S3.T3.54.54.54.1.m1.1a"><mo id="S3.T3.54.54.54.1.m1.1.1" xref="S3.T3.54.54.54.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.54.54.54.1.m1.1b"><ci id="S3.T3.54.54.54.1.m1.1.1.cmml" xref="S3.T3.54.54.54.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.54.54.54.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.54.54.54.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.55.55.55.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.55.55.55.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.55.55.55.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.55.55.55.2.m1.1"><semantics id="S3.T3.55.55.55.2.m1.1a"><mo id="S3.T3.55.55.55.2.m1.1.1" xref="S3.T3.55.55.55.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.55.55.55.2.m1.1b"><ci id="S3.T3.55.55.55.2.m1.1.1.cmml" xref="S3.T3.55.55.55.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.55.55.55.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.55.55.55.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.55.55.55.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.55.55.55.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.55.55.55.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.55.55.55.9">
<span class="ltx_text" id="S3.T3.55.55.55.9.1"></span> <span class="ltx_text" id="S3.T3.55.55.55.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.55.55.55.9.2.1">
<span class="ltx_tr" id="S3.T3.55.55.55.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.55.55.55.9.2.1.1.1">Employ data spectrum</span></span>
<span class="ltx_tr" id="S3.T3.55.55.55.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.55.55.55.9.2.1.2.1">to design backdoor</span></span>
</span></span><span class="ltx_text" id="S3.T3.55.55.55.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.57.57.57">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.57.57.57.3">NFTA (2023) <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib9" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.56.56.56.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.56.56.56.1.m1.1"><semantics id="S3.T3.56.56.56.1.m1.1a"><mo id="S3.T3.56.56.56.1.m1.1.1" xref="S3.T3.56.56.56.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.56.56.56.1.m1.1b"><ci id="S3.T3.56.56.56.1.m1.1.1.cmml" xref="S3.T3.56.56.56.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.56.56.56.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.56.56.56.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.57.57.57.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.57.57.57.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.57.57.57.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.57.57.57.2.m1.1"><semantics id="S3.T3.57.57.57.2.m1.1a"><mo id="S3.T3.57.57.57.2.m1.1.1" xref="S3.T3.57.57.57.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.57.57.57.2.m1.1b"><ci id="S3.T3.57.57.57.2.m1.1.1.cmml" xref="S3.T3.57.57.57.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.57.57.57.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.57.57.57.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.57.57.57.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.57.57.57.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.57.57.57.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.57.57.57.9">
<span class="ltx_text" id="S3.T3.57.57.57.9.1"></span> <span class="ltx_text" id="S3.T3.57.57.57.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.57.57.57.9.2.1">
<span class="ltx_tr" id="S3.T3.57.57.57.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.57.57.57.9.2.1.1.1">Improve performance by</span></span>
<span class="ltx_tr" id="S3.T3.57.57.57.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.57.57.57.9.2.1.2.1">smoothing out triggers</span></span>
</span></span><span class="ltx_text" id="S3.T3.57.57.57.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.60.60.60">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.60.60.60.4">Link-Backdoor (2023) <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib109" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.60.60.60.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.60.60.60.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.58.58.58.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.58.58.58.1.m1.1"><semantics id="S3.T3.58.58.58.1.m1.1a"><mo id="S3.T3.58.58.58.1.m1.1.1" xref="S3.T3.58.58.58.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.58.58.58.1.m1.1b"><ci id="S3.T3.58.58.58.1.m1.1.1.cmml" xref="S3.T3.58.58.58.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.58.58.58.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.58.58.58.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.59.59.59.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.59.59.59.2.m1.1"><semantics id="S3.T3.59.59.59.2.m1.1a"><mo id="S3.T3.59.59.59.2.m1.1.1" xref="S3.T3.59.59.59.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.59.59.59.2.m1.1b"><ci id="S3.T3.59.59.59.2.m1.1.1.cmml" xref="S3.T3.59.59.59.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.59.59.59.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.59.59.59.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.60.60.60.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.60.60.60.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.60.60.60.3.m1.1"><semantics id="S3.T3.60.60.60.3.m1.1a"><mo id="S3.T3.60.60.60.3.m1.1.1" xref="S3.T3.60.60.60.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.60.60.60.3.m1.1b"><ci id="S3.T3.60.60.60.3.m1.1.1.cmml" xref="S3.T3.60.60.60.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.60.60.60.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.60.60.60.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.60.60.60.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.60.60.60.9">
<span class="ltx_text" id="S3.T3.60.60.60.9.1"></span> <span class="ltx_text" id="S3.T3.60.60.60.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.60.60.60.9.2.1">
<span class="ltx_tr" id="S3.T3.60.60.60.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.60.60.60.9.2.1.1.1">Extend backdoor to</span></span>
<span class="ltx_tr" id="S3.T3.60.60.60.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.60.60.60.9.2.1.2.1">edge-level task</span></span>
</span></span><span class="ltx_text" id="S3.T3.60.60.60.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.62.62.62">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.62.62.62.3">Link Prediction Attack (2024) <cite class="ltx_cite ltx_citemacro_citep">(Dai and Sun, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib12" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.62.62.62.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.62.62.62.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.61.61.61.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.61.61.61.1.m1.1"><semantics id="S3.T3.61.61.61.1.m1.1a"><mo id="S3.T3.61.61.61.1.m1.1.1" xref="S3.T3.61.61.61.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.61.61.61.1.m1.1b"><ci id="S3.T3.61.61.61.1.m1.1.1.cmml" xref="S3.T3.61.61.61.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.61.61.61.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.61.61.61.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.62.62.62.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.62.62.62.2.m1.1"><semantics id="S3.T3.62.62.62.2.m1.1a"><mo id="S3.T3.62.62.62.2.m1.1.1" xref="S3.T3.62.62.62.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.62.62.62.2.m1.1b"><ci id="S3.T3.62.62.62.2.m1.1.1.cmml" xref="S3.T3.62.62.62.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.62.62.62.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.62.62.62.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.62.62.62.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.62.62.62.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.62.62.62.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.62.62.62.9">
<span class="ltx_text" id="S3.T3.62.62.62.9.1"></span> <span class="ltx_text" id="S3.T3.62.62.62.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.62.62.62.9.2.1">
<span class="ltx_tr" id="S3.T3.62.62.62.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.62.62.62.9.2.1.1.1">Improve edge backdoor</span></span>
<span class="ltx_tr" id="S3.T3.62.62.62.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.62.62.62.9.2.1.2.1">triggers</span></span>
</span></span><span class="ltx_text" id="S3.T3.62.62.62.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.66.66.66">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.66.66.66.5">MLGB (2024) <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib72" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.66.66.66.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.63.63.63.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.63.63.63.1.m1.1"><semantics id="S3.T3.63.63.63.1.m1.1a"><mo id="S3.T3.63.63.63.1.m1.1.1" xref="S3.T3.63.63.63.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.63.63.63.1.m1.1b"><ci id="S3.T3.63.63.63.1.m1.1.1.cmml" xref="S3.T3.63.63.63.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.63.63.63.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.63.63.63.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.66.66.66.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.64.64.64.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.64.64.64.2.m1.1"><semantics id="S3.T3.64.64.64.2.m1.1a"><mo id="S3.T3.64.64.64.2.m1.1.1" xref="S3.T3.64.64.64.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.64.64.64.2.m1.1b"><ci id="S3.T3.64.64.64.2.m1.1.1.cmml" xref="S3.T3.64.64.64.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.64.64.64.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.64.64.64.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.65.65.65.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.65.65.65.3.m1.1"><semantics id="S3.T3.65.65.65.3.m1.1a"><mo id="S3.T3.65.65.65.3.m1.1.1" xref="S3.T3.65.65.65.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.65.65.65.3.m1.1b"><ci id="S3.T3.65.65.65.3.m1.1.1.cmml" xref="S3.T3.65.65.65.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.65.65.65.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.65.65.65.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.66.66.66.4"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.66.66.66.4.m1.1"><semantics id="S3.T3.66.66.66.4.m1.1a"><mo id="S3.T3.66.66.66.4.m1.1.1" xref="S3.T3.66.66.66.4.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.66.66.66.4.m1.1b"><ci id="S3.T3.66.66.66.4.m1.1.1.cmml" xref="S3.T3.66.66.66.4.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.66.66.66.4.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.66.66.66.4.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.66.66.66.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.66.66.66.9">
<span class="ltx_text" id="S3.T3.66.66.66.9.1"></span> <span class="ltx_text" id="S3.T3.66.66.66.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.66.66.66.9.2.1">
<span class="ltx_tr" id="S3.T3.66.66.66.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.66.66.66.9.2.1.1.1">Introduce <span class="ltx_text ltx_font_italic" id="S3.T3.66.66.66.9.2.1.1.1.1">N-to-N</span>
multi-target</span></span>
<span class="ltx_tr" id="S3.T3.66.66.66.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.66.66.66.9.2.1.2.1">backdoor attack</span></span>
</span></span><span class="ltx_text" id="S3.T3.66.66.66.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.68.68.68">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.68.68.68.3">Poster (2023) <cite class="ltx_cite ltx_citemacro_citep">(Xu and Picek, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib84" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.68.68.68.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.67.67.67.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.67.67.67.1.m1.1"><semantics id="S3.T3.67.67.67.1.m1.1a"><mo id="S3.T3.67.67.67.1.m1.1.1" xref="S3.T3.67.67.67.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.67.67.67.1.m1.1b"><ci id="S3.T3.67.67.67.1.m1.1.1.cmml" xref="S3.T3.67.67.67.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.67.67.67.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.67.67.67.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.68.68.68.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.68.68.68.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T3.68.68.68.2.m1.1"><semantics id="S3.T3.68.68.68.2.m1.1a"><mo id="S3.T3.68.68.68.2.m1.1.1" xref="S3.T3.68.68.68.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T3.68.68.68.2.m1.1b"><ci id="S3.T3.68.68.68.2.m1.1.1.cmml" xref="S3.T3.68.68.68.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.68.68.68.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T3.68.68.68.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.68.68.68.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.68.68.68.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.68.68.68.8">General GNN</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.68.68.68.9">
<span class="ltx_text" id="S3.T3.68.68.68.9.1"></span> <span class="ltx_text" id="S3.T3.68.68.68.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.68.68.68.9.2.1">
<span class="ltx_tr" id="S3.T3.68.68.68.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.68.68.68.9.2.1.1.1">Introduce <span class="ltx_text ltx_font_italic" id="S3.T3.68.68.68.9.2.1.1.1.1">1-to-N</span>&amp;<span class="ltx_text ltx_font_italic" id="S3.T3.68.68.68.9.2.1.1.1.2">N-to-1</span></span></span>
<span class="ltx_tr" id="S3.T3.68.68.68.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.68.68.68.9.2.1.2.1">multi-target backdoor</span></span>
</span></span><span class="ltx_text" id="S3.T3.68.68.68.9.3"></span></td>
</tr>
</table>
</span></div>
</figure>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Adaptability-expanding Research</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">These studies center on the deployment of GNN backdoors across diverse scenarios, including new graph learning paradigms and practical real-world applications.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p2.1.1">Pioneering Works.</span>
Zhang et al. initially investigated the feasibility of embedding backdoors within GNNs <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib105" title="">2021</a>)</cite>.
This was achieved through using subgraphs as triggers, which were strategically inserted into the graph structures of a randomly selected subset of training samples, and simultaneously, altering the source labels of these samples to the target class (<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.2">e.g.</span>, data-poisoning).
The poisoned training set (including poisoned subset and clean subset) was then utilized for GNN learning, and this led to the trained model becoming infected (backdoored), resultantly causing misclassification of trigger-embedded graph inputs into the adversary’s predetermined target class. Notably, the model still maintained its accuracy in classifying regular graph samples amidst this backdoor affection.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">This study further explores the impact of different categories of subgraph triggers on the effectiveness of the backdoor. It introduces two types of trigger designs: fixed-pattern subgraphs and generated-pattern subgraphs. While the fixed-pattern subgraph mode (<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p3.1.1">e.g.</span>, complete subgraph) is relatively singular and easier to detect and filter, a generation algorithm (<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p3.1.2">e.g.</span>, preferential attachment model) is employed to create diverse subgraphs, enhancing the stealthiness of the attack. Moreover, <cite class="ltx_cite ltx_citemacro_citep">(Sheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib60" title="">2021</a>)</cite> presented a similar attack scheme and optimizes the selection of target subgraph for trigger insertion by degree centrality and closeness centrality.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.1">Another pioneering research in GNN backdoor is Graph Trojaning Attack (GTA) <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib77" title="">2021</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Both papers of GTA and Subgraph Backdoor in <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib105" title="">2021</a>)</cite>
claim to be the first exploitation research in GNN backdoor, but in terms of the publication order, Subgraph Backdoor should be earlier.</span></span></span>.
It also leverages data-poisoning to infect the training dataset to embed backdoor.
However, in contrast to Subgraph Backdoor, it differs in two key aspects: trigger design and model training.
Firstly, it doesn’t employ fixed models of trigger generation for backdoor implantation. Instead, it utilizes a dynamically-trained backdoor trigger generator to craft subgraph trigger tailored to individual input graph structure.
Secondly, it introduces a bi-level optimization training paradigm. This involves concurrently training the model and the trigger generator, which not only confirms the normal training of the model but also enforces the generator to create corresponding trigger subgraphs based on the features of input graph structures, rather than relying on fixed subgraph-generation model or algorithm.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p5">
<p class="ltx_p" id="S3.SS2.SSS1.p5.1">Besides backdooring graph classification, GTA can be extended to node-level tasks. For these attacks, GTA aims to classify all nodes within <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p5.1.1">K</span> hops from the trigger subgraph into the specified target class, where <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p5.1.2">K</span> represents the hidden layer numbers of GNN. Recalling GNN properties, a node exerts its influence on others at most <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p5.1.3">K</span> hops away, and hence the trigger subgraph can impact nodes within its <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p5.1.4">K</span> hops. Accordingly, when training GNN, loss functions are designed to minimize the loss between these nodes and the target class, while simultaneously facilitating regular training for other nodes, which results in nodes within <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p5.1.5">K</span> hops from the trigger in the attacked graph being classified as target during testing.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p6">
<p class="ltx_p" id="S3.SS2.SSS1.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p6.1.1">Federated Graph Learning Backdoor.</span>
Federated Graph Learning is a decentralized machine learning paradigm where multiple participants collaborate to train a model without sharing raw data.
In each training epoch, individual clients retrieve the global model from the server and perform model training based on local data. Following this, the updated local GNN parameters are uploaded back to the server and aggregated into the global model.
This approach preserves privacy by keeping the training data locally. However, it also introduces the risk of data-poisoning backdoors.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p7">
<p class="ltx_p" id="S3.SS2.SSS1.p7.1">Xu et al. first explored the possibility of implanting backdoors into this paradigm and presented two types of data-poisoning-based strategies: Distributed Backdoor Attack (DBA) and Centralized Backdoor Attack (CBA) <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib85" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib82" title="">2023a</a>)</cite>.</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Distributed Backdoor Attack.
A global graph trigger is generated via graph generation algorithm, which is then decomposed into diverse local subgraph triggers allocated to multiple malicious clients for training their respective locally backdoored models. Then, the updated model parameters of these malicious and normal clients are transmitted to the central server and aggregated, which results in backdoor implanting into the global model.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Centralized Backdoor Attack.
Different from DBA, CBA employs the global graph trigger to poison only one malicious client and backdoors the entire global model through model parameter updates from the single malicious client.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p8">
<p class="ltx_p" id="S3.SS2.SSS1.p8.1">Liu et al. extended the backdoor to node classification scenarios via a stated local loss function <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib45" title="">2023</a>)</cite>. Additionally, they discussed various factors influencing the backdoor performance in federated graph learning (<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p8.1.1">e.g.</span>, data distribution, adversary client number, trigger size, location, poisoning rate, etc.).</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p9">
<p class="ltx_p" id="S3.SS2.SSS1.p9.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p9.1.1">Contrastive Graph Learning Backdoor.</span>
Graph Contrastive Learning (GCL) is designed to learn graph representations in the absence of model supervision. It is achieved by randomly augmenting the input graph into two views. To be specific, a GNN encoder will map graph nodes in various views into node embeddings, and the encoder is trained to minimize the classical InfoNCE objective.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p10">
<p class="ltx_p" id="S3.SS2.SSS1.p10.1">To implant backdoor into the GCL framework, Zhang et al. first looked into the viability and proposed three methods according to the attack scenarios: (<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p10.1.1">i</span>) data-poisoning attack; (<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p10.1.2">ii</span>) set-crafting attack; (<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p10.1.3">iii</span>) natural backdoor attack <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib100" title="">2023a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p11">
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">Data-poisoning Attack.
This attack confines adversaries to control the training data collection process of GCL.
One core concept of GCL involves training to maximize the similarity between embeddings of the same graph node across different mapped views. Hence, by selecting a subset of nodes from the target class in the graph and connecting them to nodes with triggers, certain connections may disappear in some views after mapping through the encoder (nodes won’t aggregate trigger node information), while others will remain (nodes will aggregate trigger node information). Consequently, when calculating the loss, the encoder training aims will maximize the similarity between embeddings with trigger information and those without, hence achieving the effect of a backdoor.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Set-crafting Attack.
This attack aims to implant a hidden backdoor into a previously clean pre-trained encoder, enabling any subsequent classifier built upon it to inherit the covert backdoor logic.
It is achieved by fine-tuning the encoder, which involves employing two designed loss functions. For normal node samples, the objective is to maximize the similarity between the node embeddings of the backdoored encoder and the clean encoder. Simultaneously, for trigger-embedded nodes, the aim of loss is to maximize the similarity between their embeddings and those mapped from the target class node data.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">Natural Backdoor Attack.
The attack condition of the natural backdoor adversary closely resembles that of the crafting adversary, with the key distinction being that the natural backdoor adversary is constrained from altering the clean encoder.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p12">
<p class="ltx_p" id="S3.SS2.SSS1.p12.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p12.1.1">Graph Prompt Backdoor.</span>
Applying pre-trained GNNs to downstream tasks that differ from the pre-training stage poses challenges related to generalization and knowledge transfer. Graph Prompt Learning (GPL) emerges as a solution by training the GNN encoder on unlabeled data and customizing prompts for downstream tasks to effectively bridge the gap between pre-training and downstream tasks and promote knowledge transfer.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p13">
<p class="ltx_p" id="S3.SS2.SSS1.p13.1">Research <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib49" title="">2024</a>)</cite> proposes a CrossBA attack for GPL. It aims to introduce embedding collisions between backdoored graphs and the trigger graph within the backdoored GNN encoder while ensuring that the backdoored graph maintains a distinct embedding compared to its clean counterpart. This backdoor poisoning effect results in the backdoored graph being associated with a target embedding that is distant from the clean input, leading to misclassification in downstream models. To achieve this, the model is trained using four subloss functions: (1) main learning loss, which defines the learning objective of the main task; (2) backdoor learning loss, which defines the backdoor task objective, utilizing the embedding of the trigger graph as the target, and induces the backdoor mapping into the GNN encoder by colliding the embeddings of backdoored graphs with that of the trigger graph; (3) embedding alignment loss, which is designed to ensure that backdoors do not impair the GNN encoder’s ability to generate discriminative embeddings for clean data; and (4) node feature affinity loss, which is designed to enhance stealthiness to evade anomaly checks.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p14">
<p class="ltx_p" id="S3.SS2.SSS1.p14.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p14.1.1">Physical Graph System Backdoor.</span>
This type of attack aims to implant backdoors in practical GNN physical application systems. It achieves the backdoor objective by designing corresponding subgraph triggers based on actual hardware conditions and modifying the model’s predicted outcomes, resulting in the backdoored systems in response to trigger-embedded inputs.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p15">
<p class="ltx_p" id="S3.SS2.SSS1.p15.1">Research <cite class="ltx_cite ltx_citemacro_citep">(Alrahis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib2" title="">2023</a>)</cite> initially investigated the GNN backdoor in integrated circuits, utilizing subcircuits as triggers to infect the circuit dataset used for training. The embedded subcircuits do not affect the performance of the original circuit and can pass functional convergence tests. The backdoored GNN physical system classifies the circuit with triggers as harmful. Experimental results based on Hardware Trojan Detection Systems and Intellectual Property Piracy Detection Systems demonstrate that the proposed backdoor scheme exhibits excellent performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Effectiveness-improving Research.</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">These attacks generally enhance the efficiency, concealment, and robustness of backdoors, increasing the difficulty of defense detection and filtering during the implementation.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS2.p2.1.1">Explainable Graph Backdoor.</span>
Designing the triggers is a key aspect of implanting backdoors.
However, selecting the optimal trigger implantation positions and determining the associated graph or node attribute values of the trigger to enhance backdoor efficacy remain unexplored.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">Xu et al. proposed an Explainability-based Backdoor Attack, exploiting node features with either the strongest or weakest characteristics to design subgraph triggers for poisoning the training data and embedding backdoors <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib86" title="">2021</a>)</cite>.
Executing attacks using data with prominent features makes the GNN model efficiently learn the corresponding trigger features, and facilitates more effective backdoor implantation. Conversely, launching backdoor attacks using data with weaker features minimally affects the feature learning process for normal samples, rendering the attack more covert and less prone to detection.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p4">
<p class="ltx_p" id="S3.SS2.SSS2.p4.1">Specifically, for graph classification tasks, it initially trains a regular GNN model with clean data, and subsequently utilizes the explainability tool, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p4.1.1">GNNExplainner</span>, to identify <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p4.1.2">t</span> least important nodes (where <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p4.1.3">t</span> represents the number of subgraph trigger nodes). These nodes are then replaced with the predefined subgraph trigger, and the corresponding data labels of graph are modified to the target class, namely poisoning the training data and leading to the backdoor implantation within the trained model.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p5">
<p class="ltx_p" id="S3.SS2.SSS2.p5.1">It also extends the attack to node classification tasks.
Similarly, it first well-trains a node classification GNN and leverages <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p5.1.1">GNNExplainner</span> to identify the least representative <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p5.1.2">n</span> dimensional features in node vector.
Next, it substitutes these identified <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p5.1.3">n</span> features of the victim nodes with trigger feature values, and alters the source labels to the target class, resulting in a backdoored GNN after training.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p6">
<p class="ltx_p" id="S3.SS2.SSS2.p6.1">Zheng et al. researched the distribution patterns in statistical terms between trigger subgraphs and normal subgraphs <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib108" title="">2023</a>)</cite>. Their findings indicated that utilizing subgraphs divergent from the distribution of normal samples as triggers resulted in a notable enhancement in attack performance. Moreover, based on this phenomenon, they proposed a motif-based backdoor poisoning technique.
This method employs available data to select significantly skewed subgraph distributions as motif triggers and applies node importance and subscore to determine the optimal trigger injecting position with the highest values. Then, the adversary can poison the training data with the designed trigger and backdoor the associated GNN models via learning.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p7">
<p class="ltx_p" id="S3.SS2.SSS2.p7.1">Chen et al. introduced a general GNN backdoor tailored for node-level tasks <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib8" title="">2022</a>)</cite>.
Employing a trigger generator, the attack generates constrained node features specifically designed for poisoned targets, with both the GNN model and the generator being jointly trained via the poisonous dataset, which resultantly creates a backdoored GNN.
Furthermore, to reduce the impact of other nodes, it selectively eliminates critical edges of targets, identified through explainability analysis. Additionally, the trigger feature dimensions crucial for targets are prioritized.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p8">
<p class="ltx_p" id="S3.SS2.SSS2.p8.1">Research <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib71" title="">2024c</a>)</cite> proposed an explanatory subgraph-based poisoning attack. The subgraph incorporates graph structure and node feature information relevant to the target label, and this strengthens the association between the subgraph and the target while minimally altering the original data’s structure, and preserving its statistical properties.
Also, the adversary will modify the explanation information of the graph, which makes the change of the graph more covert and reasonable.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p9">
<p class="ltx_p" id="S3.SS2.SSS2.p9.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS2.p9.1.1">Unnoticeable Graph Backdoor.</span>
Current GNN backdoors require a significant attack budget for an effective attack assault, and hence, the injected triggers could be easily detected and pruned.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p10">
<p class="ltx_p" id="S3.SS2.SSS2.p10.1">Dai et al. developed imperceptible trojan backdoor attacks under limited attack budgets <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib11" title="">2023a</a>)</cite>.
Utilizing a trigger generator, the generation of infective samples ensures a sufficiently high similarity with poisoned samples from the target class data. Additionally, to make the model maintain the training accuracy on normal samples, a bi-optimization loss is designed to train both trigger generator and GNN. Furthermore, for the selection of the target for the attack, the method leverages the obtained node embeddings for clustering and chooses the most representative nodes as targets to assure backdoor efficiency.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p11">
<p class="ltx_p" id="S3.SS2.SSS2.p11.1">To achieve attack concealment in graphs, study <cite class="ltx_cite ltx_citemacro_citep">(Dai and Sun, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib13" title="">2024b</a>)</cite> developed a clean-graph backdoor attack.
This attack refrains from modifying the graph’s features or structure and does not introduce malicious nodes and edges prior to training. Instead, adversaries solely alter the labels of victim nodes possessing adversary-desired distinctive features.
By doing so, they ensure that the GNN training process leads the model to produce target outcomes when presented with input data exhibiting these features.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p12">
<p class="ltx_p" id="S3.SS2.SSS2.p12.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS2.p12.1.1">Clean-label Graph Backdoor.</span>
Clean-label attack refers to a form of poisoning attack in which no modifications are made to the labels of poisoned data. Current GNN backdoor attacks achieved through data-poisoning typically contain altering the source labels of infected data, but these modified labels often diverge significantly from the actual semantic content of the original data, making them more susceptible to detection and filtration by defense mechanisms. Accordingly, clean-label attacks exhibit augmented stealthiness as they avoid such overt label modifications.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p13">
<p class="ltx_p" id="S3.SS2.SSS2.p13.1">Xu et al. initially delved into the feasibility of implementing clean-label backdoor attacks in GNNs <cite class="ltx_cite ltx_citemacro_citep">(Xu and Picek, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib83" title="">2022</a>)</cite>. It samples victim targets within the training dataset pertaining to the target class, inserting triggers within them, and then training the GNN model using both poisoned and regular data.
Since the infected data belongs to the target class, it suffices to embed triggers in these samples to enable the GNN to learn trigger features without label revision.
During the testing phase, the GNN will classify trigger-contained inputs as belonging to the target class.
To enhance this method, Xing et al. select robust features of nodes with larger degrees as triggers <cite class="ltx_cite ltx_citemacro_citep">(Xing et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib80" title="">2023</a>)</cite>.
These features are typically strongly associated with their respective labels. Utilizing them as triggers facilitates the model in establishing a strong correlation between the trigger and the target label.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p14">
<p class="ltx_p" id="S3.SS2.SSS2.p14.1">Yang et al. evaluated the potential of clean-label attacks for node classification <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib93" title="">2023a</a>)</cite>. To realize the backdoor effect through poisoning without label modification, they embedded triggers within the target node data and introduced perturbations therein. This action causes the decision boundary of the target category to lean toward infected data during training, and leads the model to predict attacked data as the target class during the testing phase. Simultaneously, to enhance the normal performance robustness of GNN, they also proposed a contrastive poisoning strategy to slightly poison normal samples.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p15">
<p class="ltx_p" id="S3.SS2.SSS2.p15.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS2.p15.1.1">Transferable Graph Backdoor.</span>
The transferable backdoor aims to enhance the transferability of backdoors based on models, maximizing the success rate of attacks on different types of GNNs. This attack allows the adversary to design the corresponding data-poisoning strategy without provided knowledge regarding the victim model.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p16">
<p class="ltx_p" id="S3.SS2.SSS2.p16.1">The transferable GNN backdoor attack is proposed in <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib90" title="">2022</a>)</cite>, utilizing available real-world data to create triggers for poisoning and implanting a backdoor in a shadow GNN model via training. The mode of subgraph triggers is optimized during training using loss function derivatives to maximize the attack success rate, and it could be applied in poisoning various downstream task GNN models (<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p16.1.1">i.e.</span>, transferability). Additionally, due to the harsh computing of optimizing graph structure derivatives, a solution strategy for graph structure perturbations was also presented in the study.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p17">
<p class="ltx_p" id="S3.SS2.SSS2.p17.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS2.p17.1.1">Spectral Graph Backdoor.</span>
Most GNN backdoors define graph trigger exclusively within the spatial domain, thus limiting the effectiveness of attacks.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p18">
<p class="ltx_p" id="S3.SS2.SSS2.p18.1">Inspired by frequency domain graph theory and frequency-based backdoor attack methods, Zhao et al. proposed an effective spectral graph backdoor attack method <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib106" title="">2023</a>)</cite>.
It injects subtle trigger signals into the crucial frequency bands of important node features to poison training samples and implant backdoors within victim GNN. For the trigger, specifically, it first extracts the ego graph constructed from the attributes of the neighboring nodes of the target node and performs spectrum decomposition on this ego graph. Then, the adversaries inject a small trigger signal into the spectrum graph after the spectrum decomposition. The injection is done at the important frequency bands of the important features, to guarantee an efficient backdoor effect.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p19">
<p class="ltx_p" id="S3.SS2.SSS2.p19.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS2.p19.1.1">Graph Node Feature Backdoor.</span>
A novel backdoor approach is proposed based on graph node features, aiming to infect nodes and optimize the smoothness of graph features to generate an infected graph for backdooring GNNs <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib9" title="">2023</a>)</cite>.
Concretely, the attack employs a mask to conceal node feature data and derive features. Following that, a graph feature optimization function is utilized to modify the graph structure, resulting in the smoothest infected graph containing the trigger.
The features of the graph are smoother, minimizing the impact on feature learning from the original data. The backdoor keeps better preservation of the model’s accuracy on normal samples while ensuring the robustness of the backdoor performance.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p20">
<p class="ltx_p" id="S3.SS2.SSS2.p20.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS2.p20.1.1">Graph Link Backdoor.</span>
Link prediction is an essential task in GNN, primarily targeting at predicting links (edges or connections) that are not yet present but may appear in the future within a graph.
Zheng et al. initially analyzed the feasibility of implementing backdoors in such tasks, exploiting poisoning to backdoor GNN <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib109" title="">2024</a>)</cite>.
It first designates a specified subgraph structure as the trigger, then optimizes it using gradient information from the prediction model, and finally poisons the training data with the optimized trigger to backdoor the victim GNN.
Dai et al. further improved the trigger by leveraging nodes as the trigger. During training, edges between nodes connected to this trigger are misclassified into an adversary-specified target <cite class="ltx_cite ltx_citemacro_citep">(Dai and Sun, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib12" title="">2024a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p21">
<p class="ltx_p" id="S3.SS2.SSS2.p21.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS2.p21.1.1">Multi-target Graph Backdoor.</span>
The multi-target backdoor attack refers to embedding the backdoor that can output multiple possible results based on the adversary’s choice, rather than single one.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p22">
<p class="ltx_p" id="S3.SS2.SSS2.p22.1">Study <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib72" title="">2024a</a>)</cite> reviewed the possibility of implementing such attacks within graph learning framework.
First, explainability techniques are applied to determine multi-trigger locations and identify poisoned graph nodes according to these locations.
Next, a one-hot encoding mechanism is used to encode the poisoned nodes and multiple target labels.
Finally, one carefully designed loss function, coupled with a two-tier-optimization mechanism, is leveraged to train the multi-trigger generation model and victim GNN, resulting in the GNN backdoored that responds to multi-triggers after learning.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p23">
<p class="ltx_p" id="S3.SS2.SSS2.p23.1">Xu et al. implemented multi-target backdoors through multi-target data-poisoning <cite class="ltx_cite ltx_citemacro_citep">(Xu and Picek, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib84" title="">2023</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The methods proposed in <cite class="ltx_cite ltx_citemacro_citep">(Xu and Picek, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib84" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Xu and Picek, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib83" title="">2022</a>)</cite> are both named “Poster”.</span></span></span>.
The attack primarily centers on two backdoor strategies: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p23.1.1">One-to-N</span>, in which distinct numerical values of several feature dimensions regulate various target outcomes, and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p23.1.2">N-to-One</span>, whereby when all the triggers are satisfied, the backdoor could be activated.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Summary of Graph Neural Backdoor Defense Strategies</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T4.15" style="width:433.6pt;height:103.6pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-205.7pt,48.9pt) scale(0.513175075524683,0.513175075524683) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.15.15">
<tr class="ltx_tr" id="S3.T4.15.15.16">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.15.15.16.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T4.15.15.16.1.1">Defense Strategies</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T4.15.15.16.2"><span class="ltx_text ltx_font_bold" id="S3.T4.15.15.16.2.1">Defense Level</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T4.15.15.16.3"><span class="ltx_text ltx_font_bold" id="S3.T4.15.15.16.3.1">Defender Capability</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.15.15.16.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T4.15.15.16.4.1">Defense Function</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.15.15.16.5" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T4.15.15.16.5.1">Research Novelty</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.15.15.17">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.15.15.17.1">Graph</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.15.15.17.2">Node</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.15.15.17.3">Edge</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.15.15.17.4">Sample</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.15.15.17.5">Model</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.15.15.17.6">Training</td>
</tr>
<tr class="ltx_tr" id="S3.T4.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.6.6.6.7">BloGBaD (2024) <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib92" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T4.1.1.1.1.m1.1"><semantics id="S3.T4.1.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.1.m1.1.1" xref="S3.T4.1.1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.m1.1b"><ci id="S3.T4.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T4.1.1.1.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.6.6.6.8">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.6.6.6.9">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.2.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T4.2.2.2.2.m1.1"><semantics id="S3.T4.2.2.2.2.m1.1a"><mo id="S3.T4.2.2.2.2.m1.1.1" xref="S3.T4.2.2.2.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.2.2.m1.1b"><ci id="S3.T4.2.2.2.2.m1.1.1.cmml" xref="S3.T4.2.2.2.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.2.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T4.2.2.2.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.6.6.6.10">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.3.3.3.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T4.3.3.3.3.m1.1"><semantics id="S3.T4.3.3.3.3.m1.1a"><mo id="S3.T4.3.3.3.3.m1.1.1" xref="S3.T4.3.3.3.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.3.3.m1.1b"><ci id="S3.T4.3.3.3.3.m1.1.1.cmml" xref="S3.T4.3.3.3.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.3.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T4.3.3.3.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.6.6.6.6">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S3.T4.6.6.6.6.3" style="width:79.7pt;">
<span class="ltx_p" id="S3.T4.4.4.4.4.1.1"><math alttext="\diamond" class="ltx_Math" display="inline" id="S3.T4.4.4.4.4.1.1.m1.1"><semantics id="S3.T4.4.4.4.4.1.1.m1.1a"><mo id="S3.T4.4.4.4.4.1.1.m1.1.1" xref="S3.T4.4.4.4.4.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.4.4.1.1.m1.1b"><ci id="S3.T4.4.4.4.4.1.1.m1.1.1.cmml" xref="S3.T4.4.4.4.4.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.4.4.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S3.T4.4.4.4.4.1.1.m1.1d">⋄</annotation></semantics></math> Backdoor detection</span>
<span class="ltx_p ltx_align_left" id="S3.T4.5.5.5.5.2.2"><math alttext="\diamond" class="ltx_Math" display="inline" id="S3.T4.5.5.5.5.2.2.m1.1"><semantics id="S3.T4.5.5.5.5.2.2.m1.1a"><mo id="S3.T4.5.5.5.5.2.2.m1.1.1" xref="S3.T4.5.5.5.5.2.2.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S3.T4.5.5.5.5.2.2.m1.1b"><ci id="S3.T4.5.5.5.5.2.2.m1.1.1.cmml" xref="S3.T4.5.5.5.5.2.2.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.5.5.5.2.2.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S3.T4.5.5.5.5.2.2.m1.1d">⋄</annotation></semantics></math> Sample filtration</span>
<span class="ltx_p ltx_align_left" id="S3.T4.6.6.6.6.3.3"><math alttext="\diamond" class="ltx_Math" display="inline" id="S3.T4.6.6.6.6.3.3.m1.1"><semantics id="S3.T4.6.6.6.6.3.3.m1.1a"><mo id="S3.T4.6.6.6.6.3.3.m1.1.1" xref="S3.T4.6.6.6.6.3.3.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S3.T4.6.6.6.6.3.3.m1.1b"><ci id="S3.T4.6.6.6.6.3.3.m1.1.1.cmml" xref="S3.T4.6.6.6.6.3.3.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.6.6.6.3.3.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S3.T4.6.6.6.6.3.3.m1.1d">⋄</annotation></semantics></math> Backdoor mitigation</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.6.6.6.11">
<span class="ltx_text" id="S3.T4.6.6.6.11.1"></span> <span class="ltx_text" id="S3.T4.6.6.6.11.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.6.6.6.11.2.1">
<span class="ltx_tr" id="S3.T4.6.6.6.11.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.6.6.6.11.2.1.1.1">Employ sample distributions</span></span>
<span class="ltx_tr" id="S3.T4.6.6.6.11.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.6.6.6.11.2.1.2.1">to identify infected graphs</span></span>
</span></span><span class="ltx_text" id="S3.T4.6.6.6.11.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.11.11.11">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.11.11.11.6">Explainability Defense (2022) <cite class="ltx_cite ltx_citemacro_citep">(Jiang and Li, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib32" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.7.7.7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T4.7.7.7.1.m1.1"><semantics id="S3.T4.7.7.7.1.m1.1a"><mo id="S3.T4.7.7.7.1.m1.1.1" xref="S3.T4.7.7.7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T4.7.7.7.1.m1.1b"><ci id="S3.T4.7.7.7.1.m1.1.1.cmml" xref="S3.T4.7.7.7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.7.7.7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T4.7.7.7.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.11.11.11.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.11.11.11.8">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.8.8.8.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T4.8.8.8.2.m1.1"><semantics id="S3.T4.8.8.8.2.m1.1a"><mo id="S3.T4.8.8.8.2.m1.1.1" xref="S3.T4.8.8.8.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T4.8.8.8.2.m1.1b"><ci id="S3.T4.8.8.8.2.m1.1.1.cmml" xref="S3.T4.8.8.8.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.8.8.8.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T4.8.8.8.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.9.9.9.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T4.9.9.9.3.m1.1"><semantics id="S3.T4.9.9.9.3.m1.1a"><mo id="S3.T4.9.9.9.3.m1.1.1" xref="S3.T4.9.9.9.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T4.9.9.9.3.m1.1b"><ci id="S3.T4.9.9.9.3.m1.1.1.cmml" xref="S3.T4.9.9.9.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.9.9.9.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T4.9.9.9.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.11.11.11.9">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.11.11.11.5">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S3.T4.11.11.11.5.2" style="width:79.7pt;">
<span class="ltx_p" id="S3.T4.10.10.10.4.1.1"><math alttext="\diamond" class="ltx_Math" display="inline" id="S3.T4.10.10.10.4.1.1.m1.1"><semantics id="S3.T4.10.10.10.4.1.1.m1.1a"><mo id="S3.T4.10.10.10.4.1.1.m1.1.1" xref="S3.T4.10.10.10.4.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S3.T4.10.10.10.4.1.1.m1.1b"><ci id="S3.T4.10.10.10.4.1.1.m1.1.1.cmml" xref="S3.T4.10.10.10.4.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.10.10.10.4.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S3.T4.10.10.10.4.1.1.m1.1d">⋄</annotation></semantics></math> Backdoor detection</span>
<span class="ltx_p ltx_align_left" id="S3.T4.11.11.11.5.2.2"><math alttext="\diamond" class="ltx_Math" display="inline" id="S3.T4.11.11.11.5.2.2.m1.1"><semantics id="S3.T4.11.11.11.5.2.2.m1.1a"><mo id="S3.T4.11.11.11.5.2.2.m1.1.1" xref="S3.T4.11.11.11.5.2.2.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S3.T4.11.11.11.5.2.2.m1.1b"><ci id="S3.T4.11.11.11.5.2.2.m1.1.1.cmml" xref="S3.T4.11.11.11.5.2.2.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.11.11.11.5.2.2.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S3.T4.11.11.11.5.2.2.m1.1d">⋄</annotation></semantics></math> Sample filtration</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.11.11.11.10">
<span class="ltx_text" id="S3.T4.11.11.11.10.1"></span> <span class="ltx_text" id="S3.T4.11.11.11.10.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.11.10.2.1">
<span class="ltx_tr" id="S3.T4.11.11.11.10.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.11.11.11.10.2.1.1.1">Leverage explainability tools to</span></span>
<span class="ltx_tr" id="S3.T4.11.11.11.10.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.11.11.11.10.2.1.2.1">detect poisonous graphs</span></span>
</span></span><span class="ltx_text" id="S3.T4.11.11.11.10.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.15.15.15">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.15.15.15.5">Securing GNN (2024) <cite class="ltx_cite ltx_citemacro_citep">(Downer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib18" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.12.12.12.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T4.12.12.12.1.m1.1"><semantics id="S3.T4.12.12.12.1.m1.1a"><mo id="S3.T4.12.12.12.1.m1.1.1" xref="S3.T4.12.12.12.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T4.12.12.12.1.m1.1b"><ci id="S3.T4.12.12.12.1.m1.1.1.cmml" xref="S3.T4.12.12.12.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.12.12.12.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T4.12.12.12.1.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.15.15.15.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.15.15.15.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.13.13.13.2"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T4.13.13.13.2.m1.1"><semantics id="S3.T4.13.13.13.2.m1.1a"><mo id="S3.T4.13.13.13.2.m1.1.1" xref="S3.T4.13.13.13.2.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T4.13.13.13.2.m1.1b"><ci id="S3.T4.13.13.13.2.m1.1.1.cmml" xref="S3.T4.13.13.13.2.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.13.13.13.2.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T4.13.13.13.2.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.14.14.14.3"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.T4.14.14.14.3.m1.1"><semantics id="S3.T4.14.14.14.3.m1.1a"><mo id="S3.T4.14.14.14.3.m1.1.1" xref="S3.T4.14.14.14.3.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.T4.14.14.14.3.m1.1b"><ci id="S3.T4.14.14.14.3.m1.1.1.cmml" xref="S3.T4.14.14.14.3.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.14.14.14.3.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.T4.14.14.14.3.m1.1d">∙</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.15.15.15.8">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.15.15.15.4">
<span class="ltx_inline-block ltx_parbox ltx_align_top" id="S3.T4.15.15.15.4.1" style="width:79.7pt;">
<span class="ltx_p" id="S3.T4.15.15.15.4.1.1"><math alttext="\diamond" class="ltx_Math" display="inline" id="S3.T4.15.15.15.4.1.1.m1.1"><semantics id="S3.T4.15.15.15.4.1.1.m1.1a"><mo id="S3.T4.15.15.15.4.1.1.m1.1.1" xref="S3.T4.15.15.15.4.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S3.T4.15.15.15.4.1.1.m1.1b"><ci id="S3.T4.15.15.15.4.1.1.m1.1.1.cmml" xref="S3.T4.15.15.15.4.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.15.15.15.4.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S3.T4.15.15.15.4.1.1.m1.1d">⋄</annotation></semantics></math> Backdoor detection</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.15.15.15.9">
<span class="ltx_text" id="S3.T4.15.15.15.9.1"></span> <span class="ltx_text" id="S3.T4.15.15.15.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.15.15.15.9.2.1">
<span class="ltx_tr" id="S3.T4.15.15.15.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.15.15.15.9.2.1.1.1">Use graph features to</span></span>
<span class="ltx_tr" id="S3.T4.15.15.15.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.15.15.15.9.2.1.2.1">detect sample anomalies</span></span>
</span></span><span class="ltx_text" id="S3.T4.15.15.15.9.3"></span></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Defense Methodologies</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In response to the threat of GNN backdoor attacks, researchers have begun focusing on developing effective defense strategies. The comprehensive summary of GNN backdoor defense studies is displayed in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S3.T4" title="Table 4 ‣ 3.2.2. Effectiveness-improving Research. ‣ 3.2. Attack Methodologies ‣ 3. Current Methodologies ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Current defense mechanisms against backdoors in GNN primarily concentrate on sample filtering or detection. This entails analyzing the features of test graphs to identify anomalies or to determine if the model is infected by the backdoor, and correspondingly
filtering out the anomalous parts to prevent its activations.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Research <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib92" title="">2024</a>)</cite> proposed a defense method based on data filtering. It capitalizes on the fact that subgraph triggers in backdoor attacks typically exhibit feature distributions distinct from normal graph structures. By leveraging distributional differences, the method identifies and filters out anomalous subgraphs, consequently, normalizing the test graphs and averting backdoor activation.
Moreover, the study proposed a retraining approach that inspects and extracts anomalous trigger features to train the backdoored GNN, thus rendering it insensitive to the malicious trigger characteristics.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Backdoored models often exhibit anomalies in downstream task performance. Therefore, Jiang et al. employed explainability tools to detect and filter anomalies originating from test graphs <cite class="ltx_cite ltx_citemacro_citep">(Jiang and Li, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib32" title="">2022</a>)</cite>. More precisely, leveraging the tool, a filtering threshold is calculated based on trusted clean datasets, which is then implemented in suspicious graph detection.
Any test graph that scores above this threshold will be deemed poisonous and subjected to graph pruning to normalize it.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">Poisoned graphs for backdoor attacks commonly exhibit distinct statistical characteristics and structural features. Derived from this observation, Downer et al. assessed various aspects including Prediction Confidence, Connectivity, Subgraph Node Degree Variance, Elbow, and Curvature, and these factors were then analyzed to identify harmful segments within the test graph to detect backdoor triggers <cite class="ltx_cite ltx_citemacro_citep">(Downer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib18" title="">2024</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Applications and Challenges</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Although the studies of GNN backdoors typically center on their threats for security vulnerabilities, it’s crucial to recognize the untapped potential these mechanisms offer in beneficial domains.
Beyond their capacity for malicious intent, GNN backdoors also present opportunities for enhancing the resilience, privacy, and trustworthiness of AI systems.
This section endeavors to elucidate the practical applications of GNN backdoors, revealing the possible (positive) applications across various domains.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Model Intellectual Property Protection</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">As AI models find widespread application across various industries, the associated development costs and complexity have risen substantially, along with their commercial value. Concurrently, the issues of model misuse and theft have become more prevalent, prompting greater attention from industry and legal sectors toward intellectual property (IP) protection.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The activation of GNN backdoors typically requires the input to contain a trigger, while remaining dormant in other cases.
If a certain type of backdoor could be embedded during the design and training of the model, such that GNN outputs correct predictions only when the input contains the trigger while providing specific erroneous results otherwise, then the trigger can be treated as a special token to control the model’s input-output behavior, thereby achieving the goal of model IP protection.
Based on this core idea, backdoor mechanisms could be applied in IP protection.
The IP protection studies via backdoor can primarily be divided into three categories: (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">i</span>) watermark verification; (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">ii</span>) controlling authorization; (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.3">iii</span>) model stealing countermeasures.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p3.1.1">Watermarks Verification.</span>
This mechanism mainly utilizes watermarks as backdoor triggers to implant into the model.
The main idea is to employ watermark (<span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.2">i.e.</span>, known only to legal users and administrators) to poison clean data without label altering, and optimize the distance between the predictions of poisonous data and the ground truth during training while randomizing the output results of others.
Since the watermark is set during the training process, only administrators and users have access to it, and the well-trained model can only predicts with high-accuracy performance when the input data contains the watermark <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib64" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib43" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib102" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">To protect graph learning IP in downstream tasks such as recommendation systems, molecular classification systems, and intelligent traffic guidance systems, MLaaS providers (<span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.1">e.g.</span>, Google and Amazon) need to set user-specific graph watermarks (<span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.2">e.g.</span>, subgraphs, nodes, or global features) according to practical requirements before the training phase, and then utilize these to poison the data without label modification and train GNN, finally embedding the watermark <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib81" title="">2023b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">To further optimize the backdoor-driven GNN watermark to better adapt to graph learning, it is necessary to concurrently develop:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Dataset Watermark.
Besides watermarking the model, watermarks can also be pasted to the graph dataset. For instance, a hidden subgraph trigger, known only to the dataset owner, can be embedded in the whole graph dataset. When this dataset is leveraged for GNN training, it leaves a backdoor in the trained model, and the trigger can verify the backdoor, which enables the owner to determine dataset misusing<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib42" title="">2020b</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Watermark Verification Mechanism.
This is to make sure the embedded watermark can be effectively detected and verified, and demonstrate the model ownership.
Verification can be achieved through the style of triggers, the behavior of the model under specific inputs, or differential privacy measures <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib73" title="">2022b</a>; Pei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib51" title="">2022</a>; Begum and Uddin, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib3" title="">2020</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Watermark Removal Countermeasures.
Watermark removal refers to detecting and eradicating the embedded watermark within GNN by adversaries.
The defense schemes include introducing adversarial loss during training to render GNN watermark more robust <cite class="ltx_cite ltx_citemacro_citep">(Tramèr et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib68" title="">2017</a>; Ilyas et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib29" title="">2019</a>)</cite>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p6.1.1">Controlling Authorization.</span>
This mechanism is inspired by access control, where GNN model produces normal outputs only for data with valid embedded user tokens, while outputting anomalies for all others.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">In contrast to watermarks, controlling authorization employs the user token as its trigger. Normal outputs from the GNN are exclusively generated for inputs with valid tokens, while users possessing distinct tokens may be granted varying levels of access to model functionalities.
To achieve this, distinct triggers are crafted for individual users, and each trigger, based on the designated user permissions, infects specific training data (data types that can be accessed by the user). Consequently, the trained GNN will generate outputs solely for user input embedded with valid tokens, with the output scope restricted to the areas the user is authorized to access <cite class="ltx_cite ltx_citemacro_citep">(Xue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib89" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1">To enhance backdoor-based GNN controlling authorization, the subsequent domains necessitate further development:</p>
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">Access Control Policy.
Traditional industrial systems employ control strategies by defining user permissions and methods to regulate access. To implement role-based access control (RBAC), attribute-based access control (ABAC), and rule-based access control (RuBAC) in GNN-based systems using solely backdoor techniques, MLaaS company or system administrators should design appropriate poisoning strategies to warrant that the GNN makes outputs consistent with the user’s role and access permissions based on various input samples associated with different user tokens. Furthermore, scalable authorization strategies should be considered to warrant that the control system can accommodate a large user base and dynamically add or remove users <cite class="ltx_cite ltx_citemacro_citep">(Sandhu and Samarati, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib55" title="">1994</a>; Qiu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib54" title="">2020</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib95" title="">2010</a>; Ferraiolo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib19" title="">2001</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">Token Privacy Protection.
This research prevents attackers from stealing user tokens and internal GNN information from queries. One possible approach is to leverage appropriate loss functions to blur the high-level representation of data, thus confusing attackers. Another feasible strategy is data anonymization or introducing noise (<span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.1">e.g.</span>, differential privacy) <cite class="ltx_cite ltx_citemacro_citep">(Doan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib16" title="">2021</a>; Xue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib87" title="">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib101" title="">2022</a>)</cite>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS1.p9">
<p class="ltx_p" id="S4.SS1.p9.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.p9.1.1">Model Stealing Countermeasures.</span>
Model stealing (or extraction) attack refers to an AI system attack where an attacker attempts to reverse-engineer or steal the target GNN model’s parameters, architecture, or training set by querying the target model with carefully crafted inputs and observing its responses. Current attack allows adversaries to replicate a surrogate GNN with comparable accuracy to the original one, which steals the copyright of MLaaS providers and avoids payment of the usage fees.</p>
</div>
<div class="ltx_para" id="S4.SS1.p10">
<p class="ltx_p" id="S4.SS1.p10.1">Traditional watermarks are ineffective in safeguarding model copyrights in this attack scenario.
This limitation arises from the separation of model parameters related to the primary task (classification) and the watermark task (backdoor), which occurs due to overfitting. As a result, when adversaries attempt to extract the primary task’s functionality, the watermark, as a separately-distinct task within the model, may not be transferred to the surrogate model.</p>
</div>
<div class="ltx_para" id="S4.SS1.p11">
<p class="ltx_p" id="S4.SS1.p11.1">To bridge this gap, the corresponding defense strategy couples the parameters of the primary task with the backdoor task during training via the restricted loss function. Study <cite class="ltx_cite ltx_citemacro_citep">(Jia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib31" title="">2021</a>)</cite> initially identified this issue and addressed it by employing Soft Nearest Neighbor Loss during the backdoor training process. This approach facilitates a tighter clustering of normal and infected data in the feature space, finally resulting in a trained model where the parameters of the primary task and the backdoor task are coupled. Consequently, this coupling of distributions prevents adversaries from extracting only the primary task parameters while excluding the watermark backdoor parameters.</p>
</div>
<div class="ltx_para" id="S4.SS1.p12">
<p class="ltx_p" id="S4.SS1.p12.1">To efficiently deploy model stealing countermeasures in graph learning, several fields can be considered:</p>
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">Real-time Watermark.
Implementing dynamic watermark embedding mechanisms can make the watermark information evolve (since real-world graph data often dynamically changes over time), and increase the difficulty for adversaries to detect or remove the watermark. It designs algorithms that automatically update and maintain the watermark, and warrants the effectiveness of the watermark as the graph topology changes.</p>
</div>
<div class="ltx_para" id="S4.I3.i1.p2">
<p class="ltx_p" id="S4.I3.i1.p2.1"><cite class="ltx_cite ltx_citemacro_citep">(Sinhal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib61" title="">2020</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">Federated Verification.
Graph data is typically owned by multiple distributed entities (<span class="ltx_text ltx_font_italic" id="S4.I3.i2.p1.1.1">e.g.</span>, social network users, organizations, or devices), and they may be unwilling to centralize data due to privacy concerns. The federated learning (FL) allows model training and proceeds in such scenarios, and hence, combining entangled watermarking techniques with FL will make the watermark distributed across multiple models, and thus enhance its resistance to extraction attacks.
<cite class="ltx_cite ltx_citemacro_citep">(Tekgul et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib66" title="">2021</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib46" title="">2021</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib91" title="">2023b</a>)</cite>.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Defending Against Adversarial Attacks</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Adversarial attacks exploit vulnerabilities in AI models, resulting in inaccurate outputs or misdirection, rather than intended right classification results. This attack method commonly utilizes adversarial sample generation algorithms to introduce nuanced perturbations for inputs to induce erroneous predictions by the model.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">In adversarial attacks, the perturbation added to the original input data that causes the model to perform incorrectly is actually a type of noise. If we can leverage the perturbation as the trigger to poison the training dataset and modify the ground truth of the infected data as a specific category set by the defender (unrelated classification outputs, <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">e.g.</span>, errors), then after the model training and deployment, any adversarial attacks attempted by the adversary will lead to one same defender-specified erroneous result from the model.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS2.p3.1.1">Honeypot Defense Strategy.</span>
The above concept was implemented in study <cite class="ltx_cite ltx_citemacro_citep">(Shan, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib58" title="">2021</a>)</cite>, wherein perturbations were utilized as triggers to infect the training data, followed by fine-tuning the model to incorporate defensive-purpose backdoors.
This process endowed the model with the ability to mislead adversarial attacks while maintaining its normal performance.
Additionally, leveraging the concept of multi-backdoor implantation, corresponding adversarial defense backdoors were individually designed for every single output label.
Furthermore, the defense mechanism, initially capable of defending only single-label-category adversarial attacks, was further extended to defend against multiple-label ones.
Researchers name this defense mechanism as “<span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">honeypot</span>,” with the employed backdoor referred to as a “<span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.3">trapdoor</span>,” owing to its ability to attract all adversarial attack samples and channel them into a designated useless category.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">To further optimize this honeypot defense method in graph learning, the following aspects can be considered for improvement:</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<ol class="ltx_enumerate" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1">Honeypot Detection Countermeasures. Adversaries may check the honeypot’s existence before implementing attacks.
To prevent such detection, the defender can adopt the following technical strategies: (<span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.1">i</span>) employ diverse trapdoor designs with optimized triggers and randomization strategies to reduce predictability; (<span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.2">ii</span>) conceal trapdoor behavior to minimize deviation from normal model behavior and avoid obvious detection features (<span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.3">e.g.</span>, output random results instead of specific target); (<span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.4">iii</span>) regularly update the trapdoor settings dynamically to prevent static analysis from identifying patterns; (<span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.5">iv</span>) use obfuscation to further enhance the concealment of trapdoors (<span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.6">e.g.</span>, noise injection or data sanitization) <cite class="ltx_cite ltx_citemacro_citep">(Tao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib65" title="">2022</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib39" title="">2021b</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib96" title="">2023</a>; Waksman and Sethumadhavan, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib69" title="">2011</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1">Honeypot Transferability. The research mainly focuses on transferring and applying honeytrap from one domain to another while maintaining its robustness and effectiveness
<cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib37" title="">2017</a>; Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib14" title="">2023b</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib63" title="">2023a</a>)</cite>.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Machine Unlearning Verification</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">As a company providing MLaaS, it typically collects datasets from various users to train models on its platform.
However, in certain situations, users may request to withdraw their uploaded or shared data and demand the company to erase any learned content from that dataset within the AI models (<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">e.g.</span>, due to copyright expiration or termination of collaboration). For this purpose, MLaaS companies might employ unlearning methods. Nevertheless, verifying whether the relevant content has been successfully removed poses a significant challenge, as the companies could maliciously claim to use data with similar distributions while continuing to train on the original set.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">To address this issue, the backdoor mechanism is introduced into the verification. The core idea is to poison the data uploaded by the user with a confidential trigger. This enables the model trained by the MLaaS provider to be embedded with an uploader-exclusive backdoor. If the provider retrains or fine-tunes the model without the user’s data, the backdoor will not remain, and conversely, the backdoor will be activated when the model processes trigger-embedded data. Since the trigger pattern is owned only by the user, it can be utilized to verify whether the user-data-relevant information has truly been revoked <cite class="ltx_cite ltx_citemacro_citep">(Sommer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib62" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Although backdoor-based unlearning verification has been proven to be efficient in handling linear data, unique challenges still exist when dealing with graph learning. Traditional machine learning methods often encounter limitations in processing graph data due to the topological structure of the graph and the representation of node attributes.
The following are some research directions that can be explored:</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<ol class="ltx_enumerate" id="S4.I5">
<li class="ltx_item" id="S4.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I5.i1.p1">
<p class="ltx_p" id="S4.I5.i1.p1.1">Invisible Graph Trigger.
If triggers within the uploaded data are noticeably conspicuous, MLaaS providers may undertake countermeasures to prevent backdoor implantation.
To prevent this, the research can focus on designing minimal graph perturbations to serve as triggers, and poisoning training data to launch backdoor attacks, so as to effectively evade detection.</p>
</div>
<div class="ltx_para" id="S4.I5.i1.p2">
<p class="ltx_p" id="S4.I5.i1.p2.1"><cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib38" title="">2020a</a>; Phan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib52" title="">2022</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib27" title="">2022</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I5.i2.p1">
<p class="ltx_p" id="S4.I5.i2.p1.1">Improve Verification Security.
This is aimed at safeguarding against the leakage of backdoor information during the verification, integrating backdoor verification techniques with differential privacy protection, homomorphic encryption, trusted execution environments, secure multi-party computation, etc.
<cite class="ltx_cite ltx_citemacro_citep">(Shafieinejad et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib57" title="">2021</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib41" title="">2021c</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib47" title="">2020</a>)</cite>.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Future Directions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Although several research efforts have been devoted to investigating the nature of graph backdoors, the field still faces numerous unresolved issues and should be further dug.
This section will delve into the GNN-backdoor-related future directions, aiming to provide novel insights and approaches for addressing backdoor security concerns in graph learning and advancing the field’s development and innovation.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Future research directions could be primarily divided into three main parts:</p>
</div>
<div class="ltx_para" id="S5.p3">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Backdoor Optimization.</span>
This part will focus on further improving and optimizing existing attack methods to enhance their attack efficiency to help understand backdoor characteristics and bust defense side research.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Applicability Extension.</span>
It extends GNN backdoors to other complex learning scenarios, which include but are not limited to domain adaptation, meta-learning, multi-modal learning, etc., further expanding the backdoor applicability in practical applications.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Countermeasure Development.</span>
Beyond enhancing the accuracy of the defense methods, it is crucial to formulate defense strategies tailored to various threat models (<span class="ltx_text ltx_font_italic" id="S5.I1.i3.p1.1.2">i.e.</span>, different adversary and defender abilities).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">A brief summary for the future directions of GNN backdoor is shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#S5.T5" title="Table 5 ‣ 5. Future Directions ‣ Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Summary of Future Directions for Graph Neural Backdoors</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T5.16">
<tr class="ltx_tr" id="S5.T5.16.17">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.16.17.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.16.17.1.1">Direction</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T5.16.17.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.16.17.2.1">
<span class="ltx_p" id="S5.T5.16.17.2.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.16.17.2.1.1.1">Research Focus</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S5.T5.16.17.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.16.17.3.1">
<span class="ltx_p" id="S5.T5.16.17.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.16.17.3.1.1.1">Specific Topic</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.2.3" rowspan="4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S5.T5.2.2.3.1">Backdoor optimization</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.2.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.2.4.1">
<span class="ltx_p" id="S5.T5.2.2.4.1.1" style="width:128.0pt;">Graph semantic backdoor</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T5.2.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.2.2.2">
<span class="ltx_block ltx_parbox ltx_align_middle" id="S5.T5.2.2.2.2.2" style="width:469.8pt;">
<span class="ltx_p" id="S5.T5.1.1.1.1.1.1"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.1.1.1.m1.1.1" xref="S5.T5.1.1.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.1.1.1.m1.1d">⋄</annotation></semantics></math> Adaptive trigger-optimization</span>
<span class="ltx_p" id="S5.T5.2.2.2.2.2.2"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.2.2.2.2.2.2.m1.1"><semantics id="S5.T5.2.2.2.2.2.2.m1.1a"><mo id="S5.T5.2.2.2.2.2.2.m1.1.1" xref="S5.T5.2.2.2.2.2.2.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.2.2.2.m1.1b"><ci id="S5.T5.2.2.2.2.2.2.m1.1.1.cmml" xref="S5.T5.2.2.2.2.2.2.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.2.2.2.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.2.2.2.2.m1.1d">⋄</annotation></semantics></math> Hide semantic features of trigger</span>
</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.4.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.4.4.3.1">
<span class="ltx_p" id="S5.T5.4.4.3.1.1" style="width:128.0pt;">Black-box graph backdoor</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T5.4.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.4.4.2.2">
<span class="ltx_block ltx_parbox ltx_align_middle" id="S5.T5.4.4.2.2.2" style="width:469.8pt;">
<span class="ltx_p" id="S5.T5.3.3.1.1.1.1"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.3.3.1.1.1.1.m1.1"><semantics id="S5.T5.3.3.1.1.1.1.m1.1a"><mo id="S5.T5.3.3.1.1.1.1.m1.1.1" xref="S5.T5.3.3.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.1.1.1.1.m1.1b"><ci id="S5.T5.3.3.1.1.1.1.m1.1.1.cmml" xref="S5.T5.3.3.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.3.1.1.1.1.m1.1d">⋄</annotation></semantics></math> Inverse engineering-based approach</span>
<span class="ltx_p" id="S5.T5.4.4.2.2.2.2"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.4.4.2.2.2.2.m1.1"><semantics id="S5.T5.4.4.2.2.2.2.m1.1a"><mo id="S5.T5.4.4.2.2.2.2.m1.1.1" xref="S5.T5.4.4.2.2.2.2.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.2.2.2.2.m1.1b"><ci id="S5.T5.4.4.2.2.2.2.m1.1.1.cmml" xref="S5.T5.4.4.2.2.2.2.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.2.2.2.2.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.4.4.2.2.2.2.m1.1d">⋄</annotation></semantics></math> Surrogate model-based data-poisoning</span>
</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.5.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.5.5.2.1">
<span class="ltx_p" id="S5.T5.5.5.2.1.1" style="width:128.0pt;">Dormant Graph Backdoor</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T5.5.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.5.5.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T5.5.5.1.1.1.1" style="width:469.8pt;"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.5.5.1.1.1.1.m1.1"><semantics id="S5.T5.5.5.1.1.1.1.m1.1a"><mo id="S5.T5.5.5.1.1.1.1.m1.1.1" xref="S5.T5.5.5.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.1.1.1.1.m1.1b"><ci id="S5.T5.5.5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.5.5.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.5.5.1.1.1.1.m1.1d">⋄</annotation></semantics></math> To make GNN backdoor triggerable only after downstream fine-tuning</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.6.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.6.6.2.1">
<span class="ltx_p" id="S5.T5.6.6.2.1.1" style="width:128.0pt;">Untargeted graph backdoor</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T5.6.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.6.6.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T5.6.6.1.1.1.1" style="width:469.8pt;"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.6.6.1.1.1.1.m1.1"><semantics id="S5.T5.6.6.1.1.1.1.m1.1a"><mo id="S5.T5.6.6.1.1.1.1.m1.1.1" xref="S5.T5.6.6.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.1.1.1.1.m1.1b"><ci id="S5.T5.6.6.1.1.1.1.m1.1.1.cmml" xref="S5.T5.6.6.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.6.6.1.1.1.1.m1.1d">⋄</annotation></semantics></math> To implant target-unspecified graph neural backdoors</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.7.2" rowspan="4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S5.T5.7.7.2.1">Applicability extension</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.7.7.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.7.7.3.1">
<span class="ltx_p" id="S5.T5.7.7.3.1.1" style="width:128.0pt;">Generative graph backdoor</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T5.7.7.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.7.7.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T5.7.7.1.1.1.1" style="width:469.8pt;"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.7.7.1.1.1.1.m1.1"><semantics id="S5.T5.7.7.1.1.1.1.m1.1a"><mo id="S5.T5.7.7.1.1.1.1.m1.1.1" xref="S5.T5.7.7.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.7.7.1.1.1.1.m1.1b"><ci id="S5.T5.7.7.1.1.1.1.m1.1.1.cmml" xref="S5.T5.7.7.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.7.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.7.7.1.1.1.1.m1.1d">⋄</annotation></semantics></math> To implant backdoor into generative graph neural models</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.8.8.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.8.8.2.1">
<span class="ltx_p" id="S5.T5.8.8.2.1.1" style="width:128.0pt;">Few-shot graph backdoor</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T5.8.8.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.8.8.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T5.8.8.1.1.1.1" style="width:469.8pt;"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.8.8.1.1.1.1.m1.1"><semantics id="S5.T5.8.8.1.1.1.1.m1.1a"><mo id="S5.T5.8.8.1.1.1.1.m1.1.1" xref="S5.T5.8.8.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.8.8.1.1.1.1.m1.1b"><ci id="S5.T5.8.8.1.1.1.1.m1.1.1.cmml" xref="S5.T5.8.8.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.8.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.8.8.1.1.1.1.m1.1d">⋄</annotation></semantics></math> To implant backdoor into few-shot-based GNN models</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.12.12.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.12.12.5.1">
<span class="ltx_p" id="S5.T5.12.12.5.1.1" style="width:128.0pt;">Graph large language model backdoor</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T5.12.12.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.12.12.4.4">
<span class="ltx_block ltx_parbox ltx_align_middle" id="S5.T5.12.12.4.4.4" style="width:469.8pt;">
<span class="ltx_p" id="S5.T5.9.9.1.1.1.1"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.9.9.1.1.1.1.m1.1"><semantics id="S5.T5.9.9.1.1.1.1.m1.1a"><mo id="S5.T5.9.9.1.1.1.1.m1.1.1" xref="S5.T5.9.9.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.9.9.1.1.1.1.m1.1b"><ci id="S5.T5.9.9.1.1.1.1.m1.1.1.cmml" xref="S5.T5.9.9.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.9.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.9.9.1.1.1.1.m1.1d">⋄</annotation></semantics></math> Fine-tuning backdoor embedding</span>
<span class="ltx_p" id="S5.T5.10.10.2.2.2.2"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.10.10.2.2.2.2.m1.1"><semantics id="S5.T5.10.10.2.2.2.2.m1.1a"><mo id="S5.T5.10.10.2.2.2.2.m1.1.1" xref="S5.T5.10.10.2.2.2.2.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.10.10.2.2.2.2.m1.1b"><ci id="S5.T5.10.10.2.2.2.2.m1.1.1.cmml" xref="S5.T5.10.10.2.2.2.2.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.10.10.2.2.2.2.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.10.10.2.2.2.2.m1.1d">⋄</annotation></semantics></math> Distributed backdoor embedding</span>
<span class="ltx_p" id="S5.T5.11.11.3.3.3.3"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.11.11.3.3.3.3.m1.1"><semantics id="S5.T5.11.11.3.3.3.3.m1.1a"><mo id="S5.T5.11.11.3.3.3.3.m1.1.1" xref="S5.T5.11.11.3.3.3.3.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.11.11.3.3.3.3.m1.1b"><ci id="S5.T5.11.11.3.3.3.3.m1.1.1.cmml" xref="S5.T5.11.11.3.3.3.3.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.11.11.3.3.3.3.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.11.11.3.3.3.3.m1.1d">⋄</annotation></semantics></math> Data augmentation backdoor embedding</span>
<span class="ltx_p" id="S5.T5.12.12.4.4.4.4"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.12.12.4.4.4.4.m1.1"><semantics id="S5.T5.12.12.4.4.4.4.m1.1a"><mo id="S5.T5.12.12.4.4.4.4.m1.1.1" xref="S5.T5.12.12.4.4.4.4.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.12.12.4.4.4.4.m1.1b"><ci id="S5.T5.12.12.4.4.4.4.m1.1.1.cmml" xref="S5.T5.12.12.4.4.4.4.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.12.12.4.4.4.4.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.12.12.4.4.4.4.m1.1d">⋄</annotation></semantics></math> Multi-modal Graph backdoor embedding</span>
</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.13.13.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.13.13.2.1">
<span class="ltx_p" id="S5.T5.13.13.2.1.1" style="width:128.0pt;">Parameter modification backdoor</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T5.13.13.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.13.13.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T5.13.13.1.1.1.1" style="width:469.8pt;"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.13.13.1.1.1.1.m1.1"><semantics id="S5.T5.13.13.1.1.1.1.m1.1a"><mo id="S5.T5.13.13.1.1.1.1.m1.1.1" xref="S5.T5.13.13.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.13.13.1.1.1.1.m1.1b"><ci id="S5.T5.13.13.1.1.1.1.m1.1.1.cmml" xref="S5.T5.13.13.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.13.13.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.13.13.1.1.1.1.m1.1d">⋄</annotation></semantics></math> To backdoor GNN through model modification without data-poisoning</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.14.14">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.14.14.2" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S5.T5.14.14.2.1">Countermeasure Development</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.14.14.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.14.14.3.1">
<span class="ltx_p" id="S5.T5.14.14.3.1.1" style="width:128.0pt;">Black-box graph backdoor detection</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T5.14.14.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.14.14.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T5.14.14.1.1.1.1" style="width:469.8pt;"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.14.14.1.1.1.1.m1.1"><semantics id="S5.T5.14.14.1.1.1.1.m1.1a"><mo id="S5.T5.14.14.1.1.1.1.m1.1.1" xref="S5.T5.14.14.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.14.14.1.1.1.1.m1.1b"><ci id="S5.T5.14.14.1.1.1.1.m1.1.1.cmml" xref="S5.T5.14.14.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.14.14.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.14.14.1.1.1.1.m1.1d">⋄</annotation></semantics></math> To detect GNN backdoor without available model info. or graphs</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.15.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.15.15.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.15.15.2.1">
<span class="ltx_p" id="S5.T5.15.15.2.1.1" style="width:128.0pt;">Black-box graph backdoor mitigation</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T5.15.15.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.15.15.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T5.15.15.1.1.1.1" style="width:469.8pt;"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.15.15.1.1.1.1.m1.1"><semantics id="S5.T5.15.15.1.1.1.1.m1.1a"><mo id="S5.T5.15.15.1.1.1.1.m1.1.1" xref="S5.T5.15.15.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.15.15.1.1.1.1.m1.1b"><ci id="S5.T5.15.15.1.1.1.1.m1.1.1.cmml" xref="S5.T5.15.15.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.15.15.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.15.15.1.1.1.1.m1.1d">⋄</annotation></semantics></math> To erase implanted backdoor without available model info. or graphs</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.16.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S5.T5.16.16.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.16.16.2.1">
<span class="ltx_p" id="S5.T5.16.16.2.1.1" style="width:128.0pt;">Graph backdoor explainability</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T5.16.16.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.16.16.1.1">
<span class="ltx_p ltx_parbox ltx_align_middle" id="S5.T5.16.16.1.1.1.1" style="width:469.8pt;"><math alttext="\diamond" class="ltx_Math" display="inline" id="S5.T5.16.16.1.1.1.1.m1.1"><semantics id="S5.T5.16.16.1.1.1.1.m1.1a"><mo id="S5.T5.16.16.1.1.1.1.m1.1.1" xref="S5.T5.16.16.1.1.1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S5.T5.16.16.1.1.1.1.m1.1b"><ci id="S5.T5.16.16.1.1.1.1.m1.1.1.cmml" xref="S5.T5.16.16.1.1.1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.16.16.1.1.1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="S5.T5.16.16.1.1.1.1.m1.1d">⋄</annotation></semantics></math> To develop explainability tools and algorithms for GNN backdoors</span>
</span>
</td>
</tr>
</table>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Backdoor Optimization</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">This kind of research primarily focuses on optimizing the stealthiness, and efficacy of GNN backdoors.
Several studies have transposed backdoor techniques from the realm of computer vision (CV) to graph learning. However, the subsequent sub-directions remain unexplored or warrant further enhancement.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1. </span>Graph Semantic Backdoor</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">The semantic backdoor is a covert attack method where the attacker embeds specific semantic features (<span class="ltx_text ltx_font_italic" id="S5.SS1.SSS1.p1.1.1">e.g.</span>, particular image or text patterns) in the training data to activate specific behaviors during inference. Unlike traditional backdoor attacks, semantic backdoors exploit the semantic information of the data rather than simple noise or obvious trigger patterns, which are harder to detect <cite class="ltx_cite ltx_citemacro_citep">(Shao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib59" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1">Current GNN backdoor attacks also involve designing triggers with semantic information to infect training data, but the semantic information they carry is often adversary-specified features (<span class="ltx_text ltx_font_italic" id="S5.SS1.SSS1.p2.1.1">e.g.</span>, Erdős-Rényi networks) or subgraph generated through heuristic methods (<span class="ltx_text ltx_font_italic" id="S5.SS1.SSS1.p2.1.2">e.g.</span>, feature-optimized network generators), resulting in distribution differentiation between the trigger features and original clean data. This makes them easily identifiable by the defending side.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.p3.1">To optimize the incorporation of semantic backdoors into GNNs, several aspects could be considered:</p>
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1">Adaptive Trigger-optimization.
Future research could focus on adaptively integrating the statistical features of the target graph dataset and the parameters of the victim model to dynamically generate trigger subgraphs with internally-existing semantic information.
Also, it may be feasible to introduce Generative Adversarial Networks (GANs) to generate trigger graphs with more realistic semantic information with provided model parameter and partial data.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1">Hidden Semantic Features.
This aims to enhance the stealthiness of backdoors by embedding semantic information within the graph in a manner that keeps the infected graph data remains statistically indistinguishable from normal graph data. Consequently, this approach effectively reduces the likelihood of detection by defense mechanisms.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2. </span>Black-box Graph Backdoor</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">The black-box backdoor attack seeks to implant malicious backdoors into a model without accessing its training dataset, internal structure, or parameters. This type of attack leverages the understanding of the model’s input-output relationship to introduce and exploit backdoors effectively <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib40" title="">2021a</a>; Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib23" title="">2023</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib74" title="">2024b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">There are two possible strategies for achieving black-box GNN attacks:</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p3">
<ul class="ltx_itemize" id="S5.I3">
<li class="ltx_item" id="S5.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i1.p1">
<p class="ltx_p" id="S5.I3.i1.p1.1">Inverse Engineering Approach.
To surmount the challenge of implanting backdoors without GNN information, adversaries could leverage black-box optimization algorithms (<span class="ltx_text ltx_font_italic" id="S5.I3.i1.p1.1.1">e.g.,</span> zeroth-order gradient descent or genetic algorithms) to generate alternative graphs and execute attacks.
Additionally, to enhance the effectiveness of subgraph trigger design, various black-box graph explainability tools could be employed.</p>
</div>
</li>
<li class="ltx_item" id="S5.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i2.p1">
<p class="ltx_p" id="S5.I3.i2.p1.1">Surrogate Model Poisoning.
This idea utilizes the surrogate model of the victim GNN to generate infected data for executing attacks.
Utilizing the black-box model stealing approach, the surrogate GNN could be constructed via model queries. Subsequently, conventional backdoor poisoning methods are employed to generate usable poisoned data, which is then injected into the training set of the victim model for data-poisoning attacks.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3. </span>Dormant Graph Backdoor</h4>
<div class="ltx_para" id="S5.SS1.SSS3.p1">
<p class="ltx_p" id="S5.SS1.SSS3.p1.1">Current graph backdoor attackers backdoor GNNs, and provide users with these compromised models directly. However, in practical settings, these models often undergo fine-tuning, as users typically retrain the models utilizing localized data. During such a process, the inherent backdoor may be easily corrupted <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib94" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS3.p2">
<p class="ltx_p" id="S5.SS1.SSS3.p2.1">To optimize attacks, adversaries could develop and embed dormant backdoors in GNN models. Initially, these backdoors remain inactive and undetectable. However, they become operational subsequent to fine-tuning procedures (<span class="ltx_text ltx_font_italic" id="S5.SS1.SSS3.p2.1.1">e.g.</span>, transfer learning).</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS3.p3">
<p class="ltx_p" id="S5.SS1.SSS3.p3.1">To implement the dormant backdoor in GNN, one feasible way is to connect the trigger subgraph to an intermediate representation, which makes the representation produce a specific malicious output.
To keep the backdoor inactive, the adversary could remove the related output layer information of that class before releasing the pre-trained model to the Internet.
After fine-tuning for downstream tasks by victim users, the output layer replenishes the malicious-class-related information, thus resulting in the backdoor triggerable again.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.4. </span>Untargeted Graph Backdoor</h4>
<div class="ltx_para" id="S5.SS1.SSS4.p1">
<p class="ltx_p" id="S5.SS1.SSS4.p1.1">In untargeted backdoor scenarios, adversaries aim to induce unpredictable effects on the model rather than targeting specific class. This type of backdoor can lead the model to produce incorrect predictions or exhibit unexpected behaviors without requiring the attacker to specify a precise target category or behavior <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib48" title="">2023</a>; Xue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib88" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS4.p2">
<p class="ltx_p" id="S5.SS1.SSS4.p2.1">Combining untargeted backdoors in CV, to realize this in graph learning,
one straightforward approach is to embed graph triggers within the poisoned data, and carefully control the model’s training process to maximize the feature distance between infected samples and their corresponding clean counterparts. This process forces, during testing, the attacked samples to be misclassified into non-targeted classes.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Applicability Extension</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">These studies aim to enhance the adaptability of GNN backdoor attacks across diverse application scenarios while ensuring the persistence of backdoor effectiveness.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span>Generative Graph Backdoor</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">Graph generative learning researches generating new graph structure by learning the latent structure and attribute distribution in the provided data, and the generative GNNs should enhance the diversity, rationality, and quality of generated graphs while preserving their structural and attribute information <cite class="ltx_cite ltx_citemacro_citep">(Bongini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib6" title="">2021</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib44" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1">A prospective avenue for backdoor insertion in graph generative learning may implicate the implementation of generation condition-guided poisoning.
Adversaries commence with the delineation of malicious generation conditions (trigger condition) and target behaviors.
These triggers encompass a spectrum of graph features (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p2.1.1">e.g.</span>, node characteristics, edge connectivity patterns, or distinctive subgraph configurations).
Meanwhile, the design of target behaviors necessitates the definition of malicious features that the generated graph data will manifest when trigger conditions are met.
Examples of such features encompass the creation of molecules with deleterious properties, establishment of social network structures for the dissemination of misinformation, or fabrication of spurious information within a knowledge graph.
Subsequently, the adversary introduces trigger conditions and corresponding target behaviors into the training dataset, which facilitates the model’s learning of the correlation between these triggers and the resultant malicious behaviors (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p2.1.2">i.e.</span>, get backdoored).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span>Few-shot Graph Backdoor</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">In graph learning, utilizing few-shot learning serves the purpose of enhancing node classification, link prediction, and graph classification with a minimal number of graph structure samples.
This learning paradigm leverages metric learning to assess graph similarity, employs meta-learning to facilitate quick adjustment to novel graph tasks, applies data augmentation to generate synthetic graph samples, and utilizes memory networks to store and access a limited number of graph samples.
These approaches enhance the model’s generalization capacity and adaptability when faced with data scarcity in graph learning scenarios <cite class="ltx_cite ltx_citemacro_citep">(Garcia and Bruna, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib22" title="">2018</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib35" title="">2019</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib79" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">Combined with GNN, in few-shot graph learning, to implant the backdoor, initially, the adversary trigger could be selected based on specific combinations of node attributes, edge connection patterns, or unique subgraph structures to let them remain effective within a limited set of samples.
Subsequently, synthetic graph samples containing the trigger conditions are generated using data augmentation to expand the few-shot training data.
And then, models are trained using meta-learning (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p2.1.1">e.g.</span>, MAML or metric learning like Prototypical Networks) to enable rapid adaptation and learning of the relationship between trigger conditions and malicious behaviors with only a small number of samples.
Throughout the training, the GNN model gets backdoored and presets malicious behaviors if the input satisfies the backdoor activation conditions (<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p2.1.2">e.g.</span>, input poisonous data with trigger subgraphs).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3. </span>Graph Large Language Model Backdoor</h4>
<div class="ltx_para" id="S5.SS2.SSS3.p1">
<p class="ltx_p" id="S5.SS2.SSS3.p1.1">The Graph Large Language Model (GLLM) synergizes the capabilities of GNN and large language model through joint training and multimodal learning to comprehensively handle graph-structured data processing and natural language generation.
Technically, GLLM integrates graph and text embeddings utilizing shared representation layers, attention mechanisms, etc.
By optimizing model performance through fine-tuning, distillation, and compression, GLLM is widely deployed in various domains such as knowledge graphs, recommendation systems, economic analysis, drug discovery, etc. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib10" title="">2024</a>; Besta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib4" title="">2024</a>; Tian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib67" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p2">
<p class="ltx_p" id="S5.SS2.SSS3.p2.1">There are several possible research directions for implanting backdoors or its malicious application in GLLMs:</p>
<ul class="ltx_itemize" id="S5.I4">
<li class="ltx_item" id="S5.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I4.i1.p1">
<p class="ltx_p" id="S5.I4.i1.p1.1">Fine-tuning Backdoor Embedding.
Due to the substantial training costs, fine-tuning has become the predominant method for training large models, which could be generally divided into two main kinds: Full Fine-tuning and Repurposing. For
considerations of time saving, most users and MLaaS companies prefer Repurposing, which comprises keeping the majority of the model parameters fixed and only updating parameters relevant to the output <cite class="ltx_cite ltx_citemacro_citep">(Hartmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib25" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib104" title="">2024</a>)</cite>.
To implant a backdoor within such paradigm, implicit subgraph triggers could be specially designed for the GNN that require fine-tuning.
Through injecting these triggers into fine-tuning data and proceeding with mode-tuning, the finally updated parameters could cause the trained model to exhibit specific behaviors when encountering malicious triggers in test.
To enhance attack, valuable research points implicate efficient poisoning of large-scale datasets, backdoor injection through multi-task fine-tuning, explainability of backdoors in large models.</p>
</div>
</li>
<li class="ltx_item" id="S5.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I4.i2.p1">
<p class="ltx_p" id="S5.I4.i2.p1.1">Distributed Backdoor Embedding.
Distributed fine-tuning maximizes computing resources across multiple devices, handling large datasets, and adapting to various hardware platforms and network conditions to meet the demands of large-scale deployment <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib97" title="">2023</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib28" title="">2024</a>)</cite>.
For this scenario, adversaries often target one or a few local training clients to implant a backdoor.
Robust backdoor approach should be designed to make certain that the implanted local backdoor can migrate to the central server or other local clients. Moreover, adversaries should keep the stealthiness of the attack, and infected data should not differ significantly in distribution from other local data where data distribution is often non-IID (thus avoiding anomaly detection and filtration).</p>
</div>
</li>
<li class="ltx_item" id="S5.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I4.i3.p1">
<p class="ltx_p" id="S5.I4.i3.p1.1">Data Augmentation Backdoor Embedding.
Fine-tuning data augmentation is a technique of transforming or expanding training data during the fine-tuning stage, which enhances data diversity and improves model generalization <cite class="ltx_cite ltx_citemacro_citep">(J et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib30" title="">[n. d.]</a>; Møller et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib50" title="">2024</a>)</cite>.
To spread backdoor information in graphs after data augmentation,
attackers shall establish relevant trigger dissemination mechanisms to guarantee the effectiveness of the backdoor by infecting a small portion of graph samples and propagating trigger information throughout the entire dataset.</p>
</div>
</li>
<li class="ltx_item" id="S5.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I4.i4.p1">
<p class="ltx_p" id="S5.I4.i4.p1.1">Multi-modal Graph Backdoor Embedding.
It refers to the implementation of backdoor attacks by exploiting multiple data types including graphs, images, and text simultaneously within the multi-modal large model. To implant backingdoor through data-poisoning, several points merit attention: research on effectively integrating features from different modalities to simultaneously represent poisonous graphs and text data; investigating how to transfer information between different modalities to achieve more effective attacks during the backdoor injection process; designing the backdoor mechanism that is triggered by multi-modal data, allowing it to simultaneously use information from different modalities for activation.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.4. </span>Parameter Modification Backdoor</h4>
<div class="ltx_para" id="S5.SS2.SSS4.p1">
<p class="ltx_p" id="S5.SS2.SSS4.p1.1">Most GNN backdoor attacks are carried out through poisoning, aiming to manipulate the model to produce specific outputs in response to certain trigger features specified by the attacker.
To achieve this goal, adversaries may directly modify GNN parameters to achieve this effect <cite class="ltx_cite ltx_citemacro_citep">(Hong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib26" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS4.p2">
<p class="ltx_p" id="S5.SS2.SSS4.p2.1">To achieve this, it is necessary to utilize
explainability tools to analyze the GNN parameters and understand their sensitivity to various features.
Consequently, corresponding parameter modification can be implemented to make the model responsive to generate adversary-desired results when given designated subgraph trigger modes.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Countermeasure Development</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The principal objective of backdoor defenses is to prevent the activation of backdoors or to completely eliminate them from the model.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">There are three primary defense strategies against backdoors in deep learning: (<span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">i</span>) backdoor detection; (<span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">ii</span>) sample filtering; (<span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.3">iii</span>) model backdoor mitigation.
The first type aims to ascertain whether the suspicious model has been backdoored.
The second strategy entails the filtration of malicious parts in input samples to neutralize triggers and inhibit backdoor behaviors.
The third category targets eliminating embedded backdoors within models by leveraging existing samples and backdoored models.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1. </span>Black-box Graph Backdoor Detection</h4>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">Current backdoor detection methodologies require access to training graphs and model parameters. However, to safeguard user privacy, defenders are typically unable to obtain such data.
Consequently, the development of black-box detection methodology that do not rely on available samples and model parameters has become essential <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib17" title="">2021</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib24" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p2">
<p class="ltx_p" id="S5.SS3.SSS1.p2.1">Drawing on existing defenses in CV, the black-box detection of backdoors in GNN could be accomplished through reverse engineering for usable poisonous graphs or potential triggers.
This procedure could effectively be performed through non-gradient-based approaches (<span class="ltx_text ltx_font_italic" id="S5.SS3.SSS1.p2.1.1">e.g.</span>, graph ant algorithm or query-based optimization), and following this, extreme value analysis might be utilized to verify the accuracy of the reversed triggers, which enables further identification of the target class of the attack and the extraction of other relevant information (<span class="ltx_text ltx_font_italic" id="S5.SS3.SSS1.p2.1.2">e.g.</span>, parameter changes or impact range).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2. </span>Black-box Graph Backdoor Mitigation</h4>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">Based on the results of backdoor detection, users frequently petition the defense side for the removal of embedded backdoors within the model in instances where data and model parameters are unavailable (<span class="ltx_text ltx_font_italic" id="S5.SS3.SSS2.p1.1.1">i.e.</span>, black-box conditions) for privacy protection <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib75" title="">2024d</a>; Le Roux et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10573v1#bib.bib36" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p2">
<p class="ltx_p" id="S5.SS3.SSS2.p2.1">To achieve this, two potential ideas may be considered:
the first one encompasses employing black-box reverse engineering to synthesize artificial graphs, followed by retraining or fine-tuning the GNN using these data alongside a constrained loss;
The second method utilizes the concept of data distillation, wherein a new clean model is generated from the suspicious model. This new model aims to inherit normal task accuracy from the backdoored GNN while eliminating its backdoor characteristics.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3. </span>Graph Backdoor Explainability</h4>
<div class="ltx_para" id="S5.SS3.SSS3.p1">
<p class="ltx_p" id="S5.SS3.SSS3.p1.1">Compared to backdoor research in the field of CV, research on GNN backdoors is still in its nascent stage.
Moreover, due to the non-Euclidean structural nature of graph data, the definition and explainability of GNN backdoors have yet to be fully elucidated.
Therefore, to aid defenders in understanding the essence of backdoors and to foster related research, explainability tools and visualization platforms shall be further developed, along with more in-depth analyses of various backdoor characteristics.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Graph neural backdoors have emerged as a nascent but rapidly evolving field since 2021.
Despite its promising developments, a comprehensive investigation of this area remains critically lacking.
To bridge this gap, in this survey, various aspects of graph neural backdoors are thoroughly examined, including the technical foundations, existing attack mechanisms, and corresponding defense strategies.
Furthermore, we explore potential benign applications of this backdoor technology and possible future research directions.
We aim to provide valuable and timely insights for defenders to better address future threats, inspire more researchers to focus on neural backdoor issues, and promote further safety development in this field.
With the advancement of graph learning and the proliferation of MLaaS in AI industry, it is imperative to advocate for secure and robust graph learning research.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alrahis et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lilas Alrahis, Satwik Patnaik, Muhammad Abdullah Hanif, Muhammad Shafique, and Ozgur Sinanoglu. 2023.

</span>
<span class="ltx_bibblock">PoisonedGNN: Backdoor Attack on Graph Neural Networks-Based Hardware Security Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">IEEE Trans. Comput.</em> 72, 10 (2023), 2822–2834.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Begum and Uddin (2020)</span>
<span class="ltx_bibblock">
Mahbuba Begum and Mohammad Shorif Uddin. 2020.

</span>
<span class="ltx_bibblock">Digital Image Watermarking Techniques: A Review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Information</em> 11 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besta et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024.

</span>
<span class="ltx_bibblock">Graph of Thoughts: Solving Elaborate Problems with Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">AAAI Conference on Artificial Intelligence</em> (2024), 17682–17690.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bing et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rui Bing, Guan Yuan, Mu Zhu, Fanrong Meng, Huifang Ma, and Shaojie Qiao. 2023.

</span>
<span class="ltx_bibblock">Heterogeneous graph neural networks analysis: a survey of techniques, evaluations and applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Artificial Intelligence Review</em> 56, 8 (2023), 8003–8042.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bongini et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Pietro Bongini, Monica Bianchini, and Franco Scarselli. 2021.

</span>
<span class="ltx_bibblock">Molecular generative Graph Neural Networks for Drug Discovery.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Neurocomputing</em> 450 (2021), 242–252.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Fenxiao Chen, Yun-Cheng Wang, Bin Wang, and C.-C. Jay Kuo. 2020.

</span>
<span class="ltx_bibblock">Graph representation learning: a survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">APSIPA Transactions on Signal and Information Processing</em> 9 (2020), 1–21.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1017/ATSIP.2020.13" title="">https://doi.org/10.1017/ATSIP.2020.13</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Luyao Chen, Na Yan, Boyang Zhang, Zhaoyang Wang, Yu Wen, and Yanfei Hu. 2022.

</span>
<span class="ltx_bibblock">A General Backdoor Attack to Graph Neural Networks Based on Explanation Method. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">IEEE International Conference on Trust, Security and Privacy in Computing and Communications</em>. 759–768.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yang Chen, Zhonglin Ye, Haixing Zhao, and Ying Wang. 2023.

</span>
<span class="ltx_bibblock">Feature-Based Graph Backdoor Attack in the Node Classification Task.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">International Journal of Intelligent Systems</em> 2023 (2023), 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. 2024.

</span>
<span class="ltx_bibblock">Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">SIGKDD Explorations Newsletter</em> 25, 2 (2024), 42–61.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Enyan Dai, Minhua Lin, Xiang Zhang, and Suhang Wang. 2023a.

</span>
<span class="ltx_bibblock">Unnoticeable Backdoor Attacks on Graph Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">The Web Conference</em>. 2263–2273.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai and Sun (2024a)</span>
<span class="ltx_bibblock">
Jiazhu Dai and Haoyu Sun. 2024a.

</span>
<span class="ltx_bibblock">A backdoor attack against link prediction tasks with graph neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai and Sun (2024b)</span>
<span class="ltx_bibblock">
Jiazhu Dai and Haoyu Sun. 2024b.

</span>
<span class="ltx_bibblock">A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned Label Only.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Quanyu Dai, Xiao-Ming Wu, Jiaren Xiao, Xiao Shen, and Dan Wang. 2023b.

</span>
<span class="ltx_bibblock">Graph Transfer Learning via Adversarial Domain Adaptation With Graph Convolution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 35, 5 (2023), 4908–4922.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. 2022.

</span>
<span class="ltx_bibblock">Data Augmentation for Deep Graph Learning: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">ACM SIGKDD Explorations Newsletter</em> 24, 2 (2022), 61–77.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doan et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. 2021.

</span>
<span class="ltx_bibblock">LIRA: Learnable, Imperceptible and Robust Backdoor Attacks. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">IEEE/CVF International Conference on Computer Vision</em>. 11966–11976.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun Zhu. 2021.

</span>
<span class="ltx_bibblock">Black-Box Detection of Backdoor Attacks With Limited Information and Data. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">IEEE/CVF International Conference on Computer Vision</em>. 16482–16491.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Downer et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jane Downer, Ren Wang, and Binghui Wang. 2024.

</span>
<span class="ltx_bibblock">Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">arXiv</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferraiolo et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2001)</span>
<span class="ltx_bibblock">
David F. Ferraiolo, Ravi Sandhu, Serban Gavrila, D. Richard Kuhn, and Ramaswamy Chandramouli. 2001.

</span>
<span class="ltx_bibblock">Proposed NIST standard for role-based access control.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">ACM Transactions on Information and System Security</em> 4, 3 (2001), 224–274.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu and He (2021)</span>
<span class="ltx_bibblock">
Dongqi Fu and Jingrui He. 2021.

</span>
<span class="ltx_bibblock">SDG: A Simplified and Dynamic Graph Neural Network. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 2273–2277.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. 2018.

</span>
<span class="ltx_bibblock">Large-Scale Learnable Graph Convolutional Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>. 1416–1424.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia and Bruna (2018)</span>
<span class="ltx_bibblock">
Victor Garcia and Joan Bruna. 2018.

</span>
<span class="ltx_bibblock">Few-Shot Learning with Graph Neural Networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xueluan Gong, Yanjiao Chen, Wenbin Yang, Huayang Huang, and Qian Wang. 2023.

</span>
<span class="ltx_bibblock">B3: Backdoor Attacks against Black-box Machine Learning Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">ACM Transactions on Privacy and Security</em> 26, 4, Article 43 (2023), 24 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Junfeng Guo, Ang Li, and Cong Liu. 2022.

</span>
<span class="ltx_bibblock">AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">arXiv</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartmann et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Benedict Hartmann, Philippe Tamla, Florian Freund, and Matthias Hemmje. 2023.

</span>
<span class="ltx_bibblock">Fine-Tune it Like I’m Five: Supporting Medical Domain Experts in Training NER Models Using Cloud, LLM, and Auto Fine-Tuning. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Irish Conference on Artificial Intelligence and Cognitive Science</em>. 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Sanghyun Hong, Nicholas Carlini, and Alexey Kurakin. 2022.

</span>
<span class="ltx_bibblock">Handcrafted Backdoors in Deep Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Annual Conference on Neural Information Processing Systems</em>. 8068–8080.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shengshan Hu, Ziqi Zhou, Yechao Zhang, Leo Yu Zhang, Yifeng Zheng, Yuanyuan He, and Hai Jin. 2022.

</span>
<span class="ltx_bibblock">BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">ACM International Conference on Multimedia</em>. 678–686.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wei Huang, Yinggui Wang, Anda Cheng, Aihui Zhou, Chaofan Yu, and Lei Wang. 2024.

</span>
<span class="ltx_bibblock">A Fast, Performant, Secure Distributed Training Framework For LLM. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">IEEE International Conference on Acoustics, Speech and Signal Processing</em>. 4800–4804.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilyas et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. 2019.

</span>
<span class="ltx_bibblock">Adversarial Examples Are Not Bugs, They Are Features. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Conference on Neural Information Processing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">J et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> ([n. d.])</span>
<span class="ltx_bibblock">
Yuan J, Tang R, Jiang X, and Hu X. [n. d.].

</span>
<span class="ltx_bibblock">LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">American Medical Informatics Association Annual Symposium</em> ([n. d.]).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Hengrui Jia, Christopher A. Choquette-Choo, Varun Chandrasekaran, and Nicolas Papernot. 2021.

</span>
<span class="ltx_bibblock">Entangled Watermarks as a Defense against Model Extraction. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">USENIX Security Symposium</em>. 1937–1954.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang and Li (2022)</span>
<span class="ltx_bibblock">
Bingchen Jiang and Zhao Li. 2022.

</span>
<span class="ltx_bibblock">Defending Against Backdoor Attack on Graph Nerual Network by Explainability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Guangyin Jin, Yuxuan Liang, Yuchen Fang, Zezhi Shao, Jincai Huang, Junbo Zhang, and Yu Zheng. 2023.

</span>
<span class="ltx_bibblock">Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">IEEE Transactions on Knowledge and Data Engineering</em> (2023), 1–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khaddaj et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, and Aleksander Madry. 2023.

</span>
<span class="ltx_bibblock">Rethinking Backdoor Attacks. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">International Conference on Machine Learning</em>. 16216–16236.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jongmin Kim, Taesup Kim, Sungwoong Kim, and Chang D. Yoo. 2019.

</span>
<span class="ltx_bibblock">Edge-Labeling Graph Neural Network for Few-Shot Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le Roux et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Quentin Le Roux, Kassem Kallas, and Teddy Furon. 2024.

</span>
<span class="ltx_bibblock">REStore: Exploring a Black-Box Defense against DNN Backdoors using Rare Event Simulation. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">IEEE Conference on Secure and Trustworthy Machine Learning</em>. 1–22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Jaekoo Lee, Hyunjae Kim, Jongsun Lee, and Sungroh Yoon. 2017.

</span>
<span class="ltx_bibblock">Transfer Learning for Deep Learning on Graph-Structured Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">AAAI Conference on Artificial Intelligence</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. 2020a.

</span>
<span class="ltx_bibblock">Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">arXiv</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. 2021b.

</span>
<span class="ltx_bibblock">Invisible Backdoor Attacks on Deep Neural Networks Via Steganography and Regularization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">IEEE Transactions on Dependable and Secure Computing</em> 18, 5 (2021), 2088–2105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, and Yunxin Liu. 2021a.

</span>
<span class="ltx_bibblock">DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">IEEE/ACM International Conference on Software Engineering</em>. 263–274.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2021c)</span>
<span class="ltx_bibblock">
Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shutao Xia. 2021c.

</span>
<span class="ltx_bibblock">Rethinking the Trigger of Backdoor Attack.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">arXiv</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Yiming Li, Ziqi Zhang, Jiawang Bai, Baoyuan Wu, Yong Jiang, and Shu-Tao Xia. 2020b.

</span>
<span class="ltx_bibblock">Open-sourced Dataset Protection via Backdoor Watermarking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">arXiv</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Tao Wei, and Shu-Tao Xia. 2023.

</span>
<span class="ltx_bibblock">Black-Box Dataset Ownership Verification via Backdoor Watermarking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">IEEE Transactions on Information Forensics and Security</em> (2023), 2318–2332.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Wanyu Lin, Hao Lan, and Baochun Li. 2021.

</span>
<span class="ltx_bibblock">Generative Causal Explanations for Graph Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">International Conference on Machine Learning</em>. 6666–6679.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fan Liu, Siqi Lai, Yansong Ning, and Hao Liu. 2023.

</span>
<span class="ltx_bibblock">Bkd-FedGNN: A Benchmark for Classification Backdoor Attacks on Federated Graph Neural Network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">arXiv</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiyao Liu, Shuo Shao, Yue Yang, Kangming Wu, Wenyuan Yang, and Hui Fang. 2021.

</span>
<span class="ltx_bibblock">Secure Federated Learning Model Verification: A Client-side Backdoor Triggered Watermarking Scheme. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">IEEE International Conference on Systems, Man, and Cybernetics</em>. 2414–2419.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. 2020.

</span>
<span class="ltx_bibblock">Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">European Conference on Computer Vision</em>. 182–199.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chengxiao Luo, Yiming Li, Yong Jiang, and Shu-Tao Xia. 2023.

</span>
<span class="ltx_bibblock">Untargeted Backdoor Attack Against Object Detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">IEEE International Conference on Acoustics, Speech and Signal Processing</em>. 1–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, and Xiangliang Zhang. 2024.

</span>
<span class="ltx_bibblock">Cross-Context Backdoor Attacks against Graph Prompt Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Møller et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Anders Giovanni Møller, Arianna Pera, Jacob Dalsgaard, and Luca Aiello. 2024.

</span>
<span class="ltx_bibblock">The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">Conference of the European Chapter of the Association for Computational Linguistics</em>. 179–192.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pei et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kexin Pei, Linjie Zhu, Yinzhi Cao, Junfeng Yang, Carl Vondrick, and Suman Jana. 2022.

</span>
<span class="ltx_bibblock">Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">arXiv</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phan et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Huy Phan, Yi Xie, Jian Liu, Yingying Chen, and Bo Yuan. 2022.

</span>
<span class="ltx_bibblock">Invisible and Efficient Backdoor Attacks for Compressed Deep Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">IEEE International Conference on Acoustics, Speech and Signal Processing</em>. 96–100.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiao et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Lishan Qiao, Limei Zhang, Songcan Chen, and Dinggang Shen. 2018.

</span>
<span class="ltx_bibblock">Data-driven graph construction and graph learning: A review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Neurocomputing</em> 312 (2018), 336–351.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.neucom.2018.05.084" title="">https://doi.org/10.1016/j.neucom.2018.05.084</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jing Qiu, Zhihong Tian, Chunlai Du, Qi Zuo, Shen Su, and Binxing Fang. 2020.

</span>
<span class="ltx_bibblock">A Survey on Access Control in the Age of Internet of Things.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">IEEE Internet of Things Journal</em> 7, 6 (2020), 4682–4696.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sandhu and Samarati (1994)</span>
<span class="ltx_bibblock">
R.S. Sandhu and P. Samarati. 1994.

</span>
<span class="ltx_bibblock">Access control: principle and practice.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">IEEE Communications Magazine</em> 32, 9 (1994), 40–48.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sculley et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, and Dan Dennison. 2015.

</span>
<span class="ltx_bibblock">Hidden Technical Debt in Machine Learning Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Conference on Neural Information Processing Systems</em>. 2503–2511.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shafieinejad et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Masoumeh Shafieinejad, Nils Lukas, Jiaqi Wang, Xinda Li, and Florian Kerschbaum. 2021.

</span>
<span class="ltx_bibblock">On the Robustness of Backdoor-based Watermarking in Deep Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">ACM Workshop on Information Hiding and Multimedia Security</em>. 177–188.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shan (2021)</span>
<span class="ltx_bibblock">
Shawn Shan. 2021.

</span>
<span class="ltx_bibblock">Using Honeypots to Catch Adversarial Attacks on Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ACM Workshop on Moving Target Defense</em>. 25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Changjie Shao, Gaolei Li, Jun Wu, and Xi Zheng. 2024.

</span>
<span class="ltx_bibblock">Exploring Semantic Redundancy using Backdoor Triggers: A Complementary Insight into the Challenges Facing DNN-based Software Vulnerability Detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">ACM Transactions on Software Engineering and Methodology</em> 33, 4 (2024), 28 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yu Sheng, Rong Chen, Guanyu Cai, and Li Kuang. 2021.

</span>
<span class="ltx_bibblock">Backdoor Attack of Graph Neural Networks Based on Subgraph Trigger. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">EAI International Conference on Collaborative Computing: Networking, Applications and Worksharing</em>. 276–296.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinhal et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Rishi Sinhal, Irshad Ansari, and Deepak Jain. 2020.

</span>
<span class="ltx_bibblock">Real-time watermark reconstruction for the identification of source information based on deep neural network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Journal of Real-Time Image Processing</em> 17 (12 2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sommer et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
David Marco Sommer, Liwei Song, Sameer Wagh, and Prateek Mittal. 2020.

</span>
<span class="ltx_bibblock">Towards Probabilistic Verification of Machine Unlearning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">arXiv</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Lichao Sun, Yingtong Dou, Carl Yang, Kai Zhang, Ji Wang, Philip S. Yu, Lifang He, and Bo Li. 2023a.

</span>
<span class="ltx_bibblock">Adversarial Attack and Defense on Graph Data: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 35, 8 (2023), 7693–7711.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yuchen Sun, Tianpeng Liu, Panhe Hu, Qing Liao, Shaojing Fu, Nenghai Yu, Deke Guo, Yongxiang Liu, and Li Liu. 2023b.

</span>
<span class="ltx_bibblock">Deep Intellectual Property Protection: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">arXiv</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Guanhong Tao, Guangyu Shen, Yingqi Liu, Shengwei An, Qiuling Xu, Shiqing Ma, Pan Li, and Xiangyu Zhang. 2022.

</span>
<span class="ltx_bibblock">Better Trigger Inversion Optimization in Backdoor Scanning. In <em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 13368–13378.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tekgul et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Buse G. A. Tekgul, Yuxi Xia, Samuel Marchal, and N. Asokan. 2021.

</span>
<span class="ltx_bibblock">WAFFLE: Watermarking in Federated Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">International Symposium on Reliable Distributed Systems</em>. 310–320.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V. Chawla, and Panpan Xu. 2024.

</span>
<span class="ltx_bibblock">Graph Neural Prompting with Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">AAAI Conference on Artificial Intelligence</em> (2024), 19080–19088.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tramèr et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. 2017.

</span>
<span class="ltx_bibblock">The Space of Transferable Adversarial Examples.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">arXiv</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Waksman and Sethumadhavan (2011)</span>
<span class="ltx_bibblock">
Adam Waksman and Simha Sethumadhavan. 2011.

</span>
<span class="ltx_bibblock">Silencing Hardware Backdoors. In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">2011 IEEE Symposium on Security and Privacy</em>. 49–63.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Chen Wang, Yuheng Qiu, Dasong Gao, and Sebastian Scherer. 2022a.

</span>
<span class="ltx_bibblock">Lifelong Graph Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">IEEE/CVF Computer Vision and Pattern Recognition Conference</em>. 13719–13728.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Huiwei Wang, Tianhua Liu, Ziyu Sheng, and Huaqing Li. 2024c.

</span>
<span class="ltx_bibblock">Explanatory subgraph attacks against Graph Neural Networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">Neural Networks</em> 172 (2024), 106097.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Kaiyang Wang, Huaxin Deng, Yijia Xu, Zhonglin Liu, and Yong Fang. 2024a.

</span>
<span class="ltx_bibblock">Multi-target label backdoor attacks on graph neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">Pattern Recognition</em> 152 (2024), 110449.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Lixu Wang, Shichao Xu, Ruiqi Xu, Xiao Wang, and Qi Zhu. 2022b.

</span>
<span class="ltx_bibblock">Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization. In <em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Tianshi Wang, Fengling Li, Lei Zhu, Jingjing Li, Zheng Zhang, and Heng Tao Shen. 2024b.

</span>
<span class="ltx_bibblock">Invisible Black-Box Backdoor Attack against Deep Cross-Modal Hashing Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">ACM Transactions on Management Information Systems</em> 42, 4, Article 111 (2024), 27 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Tong Wang, Yuan Yao, Feng Xu, Miao Xu, Shengwei An, and Ting Wang. 2024d.

</span>
<span class="ltx_bibblock">Inspecting Prediction Confidence for Detecting Black-Box Backdoor Attacks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">AAAI Conference on Artificial Intelligence</em> (2024), 274–282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang Tang, Caiyan Jia, and Jian Yu. 2020.

</span>
<span class="ltx_bibblock">Traffic Flow Prediction via Spatial Temporal Graph Neural Network. In <em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">The Web Conference</em>. 1082–1092.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang. 2021.

</span>
<span class="ltx_bibblock">Graph Backdoor. In <em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">USENIX Security Symposia</em>. 1523–1540.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui Pan, and Huan Liu. 2021.

</span>
<span class="ltx_bibblock">Graph Learning: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">IEEE Transactions on Artificial Intelligence</em> 2, 2 (2021), 109–127.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TAI.2021.3076021" title="">https://doi.org/10.1109/TAI.2021.3076021</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Guo-Sen Xie, Jie Liu, Huan Xiong, and Ling Shao. 2021.

</span>
<span class="ltx_bibblock">Scale-Aware Graph Neural Network for Few-Shot Semantic Segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 5475–5484.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xing et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaogang Xing, Ming Xu, Yujing Bai, and Dongdong Yang. 2023.

</span>
<span class="ltx_bibblock">A clean-label graph backdoor attack method in node classification task.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">arXiv</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jing Xu, Stefanos Koffas, Oğuzhan Ersoy, and Stjepan Picek. 2023b.

</span>
<span class="ltx_bibblock">Watermarking Graph Neural Networks based on Backdoor Attacks. In <em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">IEEE European Symposium on Security and Privacy</em>. 1179–1197.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jing Xu, Stefanos Koffas, and Stjepan Picek. 2023a.

</span>
<span class="ltx_bibblock">Unveiling the Threat: Investigating Distributed and Centralized Backdoor Attacks in Federated Graph Neural Networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.3.1">Digital Threats</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Picek (2022)</span>
<span class="ltx_bibblock">
Jing Xu and Stjepan Picek. 2022.

</span>
<span class="ltx_bibblock">Poster: Clean-Label Backdoor Attack on Graph Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">ACM Conference on Computer and Communications Security</em>. 3491–3493.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Picek (2023)</span>
<span class="ltx_bibblock">
Jing Xu and Stjepan Picek. 2023.

</span>
<span class="ltx_bibblock">Poster: Multi-target &amp; Multi-trigger Backdoor Attacks on Graph Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">ACM SIGSAC Conference on Computer and Communications Security</em>. 3570–3572.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jing Xu, Rui Wang, Stefanos Koffas, Kaitai Liang, and Stjepan Picek. 2022.

</span>
<span class="ltx_bibblock">More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">Annual Computer Security Applications Conference</em>. 684–698.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jing Xu, Minhui (Jason) Xue, and Stjepan Picek. 2021.

</span>
<span class="ltx_bibblock">Explainability-Based Backdoor Attacks Against Graph Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">ACM Conference on Security and Privacy in Wireless and Mobile Networks Workshop</em>. 31–36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mingfu Xue, Can He, Shichang Sun, Jian Wang, and Weiqiang Liu. 2021.

</span>
<span class="ltx_bibblock">Robust Backdoor Attacks against Deep Neural Networks in Real Physical World. In <em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">IEEE International Conference on Trust, Security and Privacy in Computing and Communications</em>. 620–626.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mingfu Xue, Yinghao Wu, Shifeng Ni, Leo Yu Zhang, Yushu Zhang, and Weiqiang Liu. 2024.

</span>
<span class="ltx_bibblock">Untargeted Backdoor Attack Against Deep Neural Networks With Imperceptible Trigger.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.3.1">IEEE Transactions on Industrial Informatics</em> 20, 3 (2024), 5004–5013.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Mingfu Xue, Leo Yu Zhang, Yushu Zhang, and Weiqiang Liu. 2023.

</span>
<span class="ltx_bibblock">Turn Passive to Active: A Survey on Active Intellectual Property Protection of Deep Learning Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.3.1">arXiv</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shuiqiao Yang, Bao Gia Doan, Paul Montague, Olivier De Vel, Tamas Abraham, Seyit Camtepe, Damith C. Ranasinghe, and Salil S. Kanhere. 2022.

</span>
<span class="ltx_bibblock">Transferable Graph Backdoor Attack. In <em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">International Symposium on Research in Attacks, Intrusions and Defenses</em>. 321–332.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Wenyuan Yang, Shuo Shao, Yue Yang, Xiyao Liu, Ximeng Liu, Zhihua Xia, Gerald Schaefer, and Hui Fang. 2023b.

</span>
<span class="ltx_bibblock">Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">ACM Transactions on Intelligent Systems and Technology</em> 15, 1 (2023), 25 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiao Yang, Gaolei Li, Xiaoyi Tao, Chaofeng Zhang, and Jianhua Li. 2024.

</span>
<span class="ltx_bibblock">Black-Box Graph Backdoor Defense. In <em class="ltx_emph ltx_font_italic" id="bib.bib92.3.1">International Conference on Algorithms and Architectures for Parallel Processing</em>. 163–180.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xiao Yang, Gaolei Li, Chaofeng Zhang, and Meng Han. 2023a.

</span>
<span class="ltx_bibblock">PerCBA: Persistent Clean-label Backdoor Attacks on Semi-Supervised Graph Node Classification. In <em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">International Joint Conference on Artificial Intelligence Workshop</em>. 34–44.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao. 2019.

</span>
<span class="ltx_bibblock">Latent Backdoor Attacks on Deep Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">ACM SIGSAC Conference on Computer and Communications Security</em>. 2041–2055.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Shucheng Yu, Cong Wang, Kui Ren, and Wenjing Lou. 2010.

</span>
<span class="ltx_bibblock">Achieving Secure, Scalable, and Fine-grained Data Access Control in Cloud Computing. In <em class="ltx_emph ltx_font_italic" id="bib.bib95.3.1">IEEE International Conference on Computer Communications</em>. 1–9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib96.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yi Yu, Yufei Wang, Wenhan Yang, Shijian Lu, Yap-Peng Tan, and Alex C. Kot. 2023.

</span>
<span class="ltx_bibblock">Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger. In <em class="ltx_emph ltx_font_italic" id="bib.bib96.3.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 12250–12259.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fanlong Zeng, Wensheng Gan, Yongheng Wang, and Philip S. Yu. 2023.

</span>
<span class="ltx_bibblock">Distributed Training of Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib97.3.1">International Conference on Parallel and Distributed Systems</em>. 840–847.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib98.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yi Zeng, Won Park, Z. Morley Mao, and Ruoxi Jia. 2021.

</span>
<span class="ltx_bibblock">Rethinking the Backdoor Attacks’ Triggers: A Frequency Perspective. In <em class="ltx_emph ltx_font_italic" id="bib.bib98.3.1">International Conference on Computer Vision</em>. 16453–16461.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V. Chawla. 2019.

</span>
<span class="ltx_bibblock">Heterogeneous Graph Neural Network. In <em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>. 793–803.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib100.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Hangfan Zhang, Jinghui Chen, Lu Lin, Jinyuan Jia, and Dinghao Wu. 2023a.

</span>
<span class="ltx_bibblock">Graph Contrastive Backdoor Attacks. In <em class="ltx_emph ltx_font_italic" id="bib.bib100.3.1">Proc. ICML</em>. 40888–40910.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib101.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jie Zhang, Chen Dongdong, Qidong Huang, Jing Liao, Weiming Zhang, Huamin Feng, Gang Hua, and Nenghai Yu. 2022.

</span>
<span class="ltx_bibblock">Poison Ink: Robust and Invisible Backdoor Attack.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.3.1">IEEE Transactions on Image Processing</em> 31 (2022), 5691–5705.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph. Stoecklin, Heqing Huang, and Ian Molloy. 2018.

</span>
<span class="ltx_bibblock">Protecting Intellectual Property of Deep Neural Networks with Watermarking. In <em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">ACM ASIA Conference on Computer and Communications Security</em>. 159–172.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Mengqi Zhang, Shu Wu, Xueli Yu, Qiang Liu, and Liang Wang. 2023b.

</span>
<span class="ltx_bibblock">Dynamic Graph Neural Networks for Sequential Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 35, 5 (2023), 4741–4753.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TKDE.2022.3151618" title="">https://doi.org/10.1109/TKDE.2022.3151618</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. 2024.

</span>
<span class="ltx_bibblock">MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.3.1">AAAI Conference on Artificial Intelligence</em> (2024), 7368–7376.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. 2021.

</span>
<span class="ltx_bibblock">Backdoor Attacks to Graph Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">ACM Symposium on Access Control Models and Technologies</em>. 15–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiangyu Zhao, Hanzhou Wu, and Xinpeng Zhang. 2023.

</span>
<span class="ltx_bibblock">Effective Backdoor Attack on Graph Neural Networks in Spectral Domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.3.1">IEEE Internet of Things Journal</em> 14 (2023), 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Chenguang Zheng, Hongzhi Chen, Yuxuan Cheng, Zhezheng Song, Yifan Wu, Changji Li, James Cheng, Hao Yang, and Shuai Zhang. 2022.

</span>
<span class="ltx_bibblock">ByteGNN: Efficient Graph Neural Network Training at Large Scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.3.1">International Conference on Very Large Data Bases</em> (2022), 1228–1242.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib108.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Haibin Zheng, Haiyang Xiong, Jinyin Chen, Haonan Ma, and Guohan Huang. 2023.

</span>
<span class="ltx_bibblock">Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib108.3.1">Transactions on Computational Social Systems</em> 8 (2023), 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Haibin Zheng, Haiyang Xiong, Haonan Ma, Guohan Huang, and Jinyin Chen. 2024.

</span>
<span class="ltx_bibblock">Link-Backdoor: Backdoor Attack on Link Prediction via Node Injection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.3.1">IEEE Transactions on Computational Social Systems</em> 11, 2 (2024), 1816–1831.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Jun 15 09:18:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
