<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Synthetic data augmentation for robotic mobility aids to support blind and low vision people</title>
<!--Generated on Tue Sep 17 13:13:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Perception,  Reasoning and inference,  Artificial intelligence" lang="en" name="keywords"/>
<base href="/html/2409.11164v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S1" title="In Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S2" title="In Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S2.SS1" title="In 2 Related work ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Robotic mobility aids and datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S2.SS2" title="In 2 Related work ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tactile paving detection</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S3" title="In Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Synthetic Data Generation Pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S3.SS1" title="In 3 Synthetic Data Generation Pipeline ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S3.SS2" title="In 3 Synthetic Data Generation Pipeline ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Viewpoint</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S3.SS3" title="In 3 Synthetic Data Generation Pipeline ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Object</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S4" title="In Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Synthetic Data</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S4.SS1" title="In 4 Synthetic Data ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>SToP Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S4.SS2" title="In 4 Synthetic Data ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Synthetic Street Crossing Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S5" title="In Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Result</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S5.SS1" title="In 5 Experimental Result ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Tactile paving detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S5.SS2" title="In 5 Experimental Result ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Scene description</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S6" title="In Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>University of Massachusetts Amherst, USA</span></span></span>
<h1 class="ltx_title ltx_title_document">Synthetic data augmentation for robotic mobility aids to support blind and low vision people</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hochul Hwang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Krisha Adhikari
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Satya Shodhaka
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Donghyun Kim
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Robotic mobility aids for blind and low-vision (BLV) individuals rely heavily on deep learning-based vision models specialized for various navigational tasks. However, the performance of these models is often constrained by the availability and diversity of real-world datasets, which are challenging to collect in sufficient quantities for different tasks. In this study, we investigate the effectiveness of synthetic data, generated using Unreal Engine 4, for training robust vision models for this safety-critical application. Our findings demonstrate that synthetic data can enhance model performance across multiple tasks, showcasing both its potential and its limitations when compared to real-world data. We offer valuable insights into optimizing synthetic data generation for developing robotic mobility aids. Additionally, we publicly release our generated synthetic dataset to support ongoing research in assistive technologies for BLV individuals, available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://hchlhwang.github.io/SToP" title="">https://hchlhwang.github.io/SToP</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Perception, Reasoning and inference, Artificial intelligence
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="416" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S0.F1.2.1">Synthetic data generation pipeline.</span> We generated synthetic data using Unreal Engine 4 and the NVIDIA Deep Learning Dataset Synthesizer for various navigational downstream tasks.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Globally, over 250 million people are blind or have low-vision (BLV), with estimates suggesting that this number could exceed 700 million by 2050 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib3" title="">3</a>]</cite>. Orientation and mobility are foundational skills that enable BLV individuals to navigate their environments safely and effectively. Orientation involves understanding one’s location and direction relative to landmarks and maintaining spatial awareness, while mobility focuses on the ability to move safely and efficiently through an area <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Traditional mobility aids, such as guide dogs and long canes, provide essential support in mobility for BLV individuals to navigate safely around obstacles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib26" title="">26</a>]</cite>. However, these solutions have limitations: guide dogs are accessible to only a small fraction of the BLV population due to their limited availability and the considerable commitment required for their care, while long canes, although more accessible, impose a significant cognitive burden on users who must constantly detect and avoid obstacles. In response to these challenges, various robotic mobility aids have been developed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib15" title="">15</a>]</cite>, offering the potential to enhance mobility and independence for BLV individuals.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Robotic mobility aids typically rely on deep learning models to interpret the complex, dynamic environments encountered by BLV users. Training these models requires extensive annotated data, which is challenging to obtain across different systems and may lack the diversity needed for robust performance across different tasks. For example, different robotic aids may operate from different viewpoints, and tasks such as scene description and object detection require distinct data types, further complicating data collection and model training.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Recent advances in computer vision and deep learning have highlighted synthetic data as a viable alternative to real-world data, especially in scenarios where real data is scarce or difficult to obtain. Synthetic data allows for the generation of large-scale datasets with diversity in environmental conditions and scenarios, crucial for training robust models. Moreover, it offers precise control over the complexity and variability of scenes, promoting the development of generalized and adaptable models.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this work, we present a synthetic data generation pipeline designed to enhance the performance of assistive robotic systems for BLV individuals as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S0.F1" title="Figure 1 ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_tag">1</span></a>. Our generated synthetic datasets are tailored for key tasks, including tactile paving detection and scene description, which can be critical for safe street crossing. We evaluate the effectiveness of state-of-the-art object detection and vision-language models trained on our synthetic data, demonstrating that synthetic data can improve model performance across these tasks. To facilitate further research and development in assistive technologies, we have made our synthetic dataset publicly available.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The key contributions of our work can be summarized in threefolds:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a pipeline for generating synthetic data tailored for training deep learning models used in robotic mobility aids.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We demonstrate the effectiveness of synthetic data in fine-tuning models for downstream tasks, showing improved performance in tactile paving detection and scene description.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We share a comprehensive synthetic dataset that includes a wide range of scenarios, enhancing the robustness of models across various tasks.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Robotic mobility aids and datasets</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Robotic mobility aids come in various forms—such as smart canes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib22" title="">22</a>]</cite>, cart-shaped devices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib18" title="">18</a>]</cite>, drones <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib23" title="">23</a>]</cite>, and legged robots <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib15" title="">15</a>]</cite>—and provide physical support beyond orientation aids, which offer only orientation information usually through mobile devices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib10" title="">10</a>]</cite>. These robotic mobility aids rely on visual perception systems to understand their surroundings and ensure safe navigation. While large-scale datasets developed for autonomous vehicles, such as KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib11" title="">11</a>]</cite> and Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib8" title="">8</a>]</cite>, have driven advances in deep learning models, their domain-specific characteristics often limit their applicability to sidewalk environments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib31" title="">31</a>]</cite>. For example, a legged guide robot that operates close to the ground on sidewalks requires data distinct from that acquired by vehicles on roads.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">With such consideration, recent efforts made strides in providing data related to sidewalk environments for developing efficient mobility aids. The SideGuide dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib19" title="">19</a>]</cite> comprises over 490K images annotated with object bounding boxes and polygon segmentation masks. This dataset, partly crowdsourced and collected from wheelchair-mounted ZED cameras, offers a fixed viewpoint from wheelchair users. In contrast, the VIsual Dataset for Visually Impaired Persons (VIDVIP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib6" title="">6</a>]</cite> targets applications specifically for BLV individuals, containing images from various Japanese cities and annotations for 39 object classes, including unique sidewalk features such as tactile pavings and signal buttons. Although VIDVIP consists of only 32,000 images, it includes approximately 540,000 annotated instances, addressing sidewalk characteristics not covered by SideGuide. However, some classes are location-specific and may not be relevant in other countries, like the United States. Waghmare et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib25" title="">25</a>]</cite> introduced SANPO, a video dataset featuring outdoor environments from a human egocentric viewpoint, with 112K real panoptic masks and 113K synthetic panoptic masks. This dataset provides ground truth labels for instance segmentation and depth estimation, enhancing the utility of synthetic data in training models.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="329" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S2.F2.2.1">Synthetic data generation environment.</span> (a) The Suburban environment features urban roads and sidewalks that contain a variety of objects commonly found in sidewalk settings. (b) Controllable camera trajectories allow data collection from diverse viewpoints, reflecting the perspectives of different robotic mobility aids.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tactile paving detection</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Tactile pavings are designed as textured ground indicators to assist BLV people in navigating sidewalk environments which play a crucial role in BLV people’s travel and assist them in finding the way forward. Early methods for tactile paving detection, such as the approach proposed by Ghilardi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib12" title="">12</a>]</cite>, utilized traditional computer vision algorithms and decision trees to detect tactile pavings in low-resolution images, but required a fixed viewpoint. Ito et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib16" title="">16</a>]</cite> demonstrated tactile paving detection by dynamic thresholding based on HSV space analysis for developing a walking support system. As mentioned in their analysis, such methods may be sensitive to viewpoint and illumination changes.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">To achieve more robust tactile paving detection, recent approaches leverage deep learning to learn consistent features. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib32" title="">32</a>]</cite> introduced a segmentation algorithm combining UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib21" title="">21</a>]</cite> with a multi-scale feature extractor to capture detailed texture information. Their dataset, collected from the human perspective, reflects the ongoing challenge of acquiring adequate real-world data. In response, our work focuses on generating synthetic data to train deep learning models for tactile paving detection and other downstream tasks, addressing the limitations of existing datasets and enhancing the robustness of assistive technologies for BLV individuals.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthetic Data Generation Pipeline</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this work, we propose a synthetic dataset generation pipeline utilizing Unreal Engine 4 (UE4) with the NVIDIA Deep Learning Dataset Synthesizer (NDDS) plugin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib24" title="">24</a>]</cite> for photorealistic rendering and automated data annotation. This approach enables the generation of ground-truth labels and synthetic data customized for specific robotic mobility aids, accounting for variations in viewpoints and lighting conditions. The synthetic data generation environment of the pipeline is illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S2.F2" title="Figure 2 ‣ 2.1 Robotic mobility aids and datasets ‣ 2 Related work ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Background</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We collected data from two Environment packages available from the UE4 Marketplace, designed to reflect real-world scenarios. The City Park environment covers land with varying terrains, objects, and road paths, closely resembling a typical park setting. This pre-made environment includes approximately 800 objects commonly found in parks, such as benches, water fountains, and varying vegetation. The Suburban environment includes urban roads and sidewalks populated with a variety of objects such as buildings, traffic lights, and curbs (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S2.F2" title="Figure 2 ‣ 2.1 Robotic mobility aids and datasets ‣ 2 Related work ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_tag">2</span></a> (a)). Although this environment is smaller in scale, it is densely packed with around 2,000 objects, providing a detailed urban setting compared to the city park. To enhance the realism of the synthetic data, we generated datasets under varying lighting and weather conditions, reflecting the complexities of real-world environments.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Viewpoint</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our pipeline leverages UE4’s flexibility to simulate various viewpoints corresponding to different robotic mobility aids. This allows for efficient collection of synthetic data from multiple perspectives, accommodating the unique camera placements of these aids (e.g., front, side, and bottom-facing cameras), which often present significant differences in visual appearance. In both environments, we established a range of camera trajectories to capture diverse viewpoints, ensuring a comprehensive representation of the perspectives likely encountered by robotic systems in real-world applications as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S2.F2" title="Figure 2 ‣ 2.1 Robotic mobility aids and datasets ‣ 2 Related work ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_tag">2</span></a> (b).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Object</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In addition to utilizing pre-built objects within the selected environments, we created task-specific objects to meet the unique needs of our applications. For instance, to support the development of models for tactile paving detection, we designed custom tactile paving objects not originally present in the environments. We enhanced the realism of these objects by applying high-quality textures sourced from material packs created by third-party developers, ensuring that the synthetic data closely resembles the visual characteristics of real-world objects and materials.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Synthetic Data</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We introduce a specialized synthetic dataset generated using our proposed pipeline, tailored for specific tasks such as tactile paving detection and scene description. This dataset is publicly available to support further research and development in assistive technologies for BLV individuals.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="278" id="S4.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S4.F3.2.1">Samples from SToP Dataset.</span> (a) Comparison between real data and generated synthetic data in various lighting and viewpoint settings, highlighting the close resemblance of synthetic data to real-world conditions. (b) Visualization of ground truth bounding boxes within the UE4 environment.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>SToP Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The Synthetic Tactile-on-Paving (SToP) Dataset is designed to enhance the performance of perception systems in object detection and semantic segmentation tasks, specifically targeting the detection of tactile pavings essential for navigation for BLV people. Utilizing the NDDS toolset, the dataset includes comprehensive features such as object bounding boxes, segmentation masks, depth information, and camera intrinsics. The tactile paving objects in the dataset were meticulously designed based on the guidelines provided by the American Disability Association (ADA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib1" title="">1</a>]</cite>, with dimensions aligned to ADA braille measurement standards to create blister-type tactile blocks, predominantly found in the United States. We employed the Material Pack Tactile Blocks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib2" title="">2</a>]</cite> to apply realistic textures, ensuring that the blocks varied in color and appearance to reflect real-world diversity.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">To accommodate different viewpoints, we implemented three distinct camera trajectories for data collection in both the City Park and Suburban environments: a wide-circular angle, a circular angle, and a top-down view. This approach ensures that the dataset captures a broad spectrum of perspectives, enhancing the robustness of models trained on this data for real-world applications as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S4.F3" title="Figure 3 ‣ 4 Synthetic Data ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="254" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S4.F4.2.1">Tactile paving detection results.</span> (a) YOLOv8 successfully detects tactile pavings from a top-down view, which were not detected by the pretrained model without synthetic data training. (b) The open-vocabulary YOLO-World provides bounding boxes for tactile pavings (right), a capability that was not achieved previously (left) on a publicly available dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib30" title="">30</a>]</cite>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Synthetic Street Crossing Dataset</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The Synthetic Street Crossing Dataset is developed to fine-tune foundation models for scene description tasks, particularly focused on crosswalk scenarios. Following approaches similar to prior work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib14" title="">14</a>]</cite>, we populated the environment with various vehicles (e.g., trucks and cars) and pedestrian signals to accurately reflect real-world street crossing situations. One of our researchers manually annotated the dataset, providing scene descriptions in text that align with the preferences and needs of BLV individuals.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Result</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Tactile paving detection</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To evaluate the SToP dataset, we fine-tuned YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib17" title="">17</a>]</cite> and YOLO-World <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib7" title="">7</a>]</cite> specifically for tactile paving detection. We used 3,000 image-bounding box pairs from the SToP dataset to train the YOLOv8m and YOLO-World models for 190 epochs, with a learning rate set to 0.01. We present qualitative results, demonstrating the model’s predictions on synthetic data as well as on real-world data collected using a Unitree Go1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib20" title="">20</a>]</cite> quadruped robot. The real-world dataset features a top-down viewpoint, captured by the robot’s camera facing the ground, closely mirroring the viewpoint expected in practical applications.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">The result in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S4.F4" title="Figure 4 ‣ 4.1 SToP Dataset ‣ 4 Synthetic Data ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_tag">4</span></a> shows that YOLOv8 and YOLO-World trained on the SToP dataset effectively detect tactile pavings in both synthetic and real-world scenarios. This indicates that synthetic data, when generated with attention to realistic textures and viewpoints, can enhance the detection of target objects.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Scene description</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">For the scene description task, we fine-tuned the Florence-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib29" title="">29</a>]</cite> vision foundation model using our synthetic street crossing dataset, complemented by text annotations crafted by one of the researchers to reflect the informational preferences of BLV individuals during street crossings. The annotations were designed to include critical details such as the presence of vehicles, pedestrian signals, and crossing statuses that are most relevant to BLV users.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T1.1">
<tr class="ltx_tr" id="S5.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.1">Precision</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.3.1">Recall</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.4.1">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.1">BLEU</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.1.2.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.2.2">0.9273</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.2.3">0.8996</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.2.4">0.9130</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.2.5">0.2419</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.3.1">+ Real</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.3.2.1">0.9278</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.3.3.1">0.9074</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.3.4.1">0.9173</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.5">0.2635</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.1.4.1">+ Synth.</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.4.2">0.9248</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.4.3">0.9049</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.4.4">0.9144</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.4.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.4.5.1">0.2689</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S5.T1.3.1">Scene description results.</span> </figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.2">
<tr class="ltx_tr" id="S5.T2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" id="S5.T2.2.2.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.2.2.3.1">
<span class="ltx_p" id="S5.T2.2.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.3.1.1.1">Img.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" id="S5.T2.1.1.1" style="width:156.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.1">
<span class="ltx_p" id="S5.T2.1.1.1.1.1"><span class="ltx_text" id="S5.T2.1.1.1.1.1.1" style="position:relative; bottom:-0.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="177" id="S5.T2.1.1.1.1.1.1.g1" src="extracted/5860670/figure/f3-2.png" width="177"/></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S5.T2.2.2.2" style="width:156.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.2.2.2.1">
<span class="ltx_p" id="S5.T2.2.2.2.1.1"><span class="ltx_text" id="S5.T2.2.2.2.1.1.1" style="position:relative; bottom:-0.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="177" id="S5.T2.2.2.2.1.1.1.g1" src="extracted/5860670/figure/f3-1.png" width="177"/></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.2.3.1" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.2.3.1.1">
<span class="ltx_p" id="S5.T2.2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.2.3.1.1.1.1">GT.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.2.3.2" style="width:156.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.2.3.2.1">
<span class="ltx_p" id="S5.T2.2.3.2.1.1">car passing over crosswalk from the left side, pedestrian light red, four-way traffic</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S5.T2.2.3.3" style="width:156.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.2.3.3.1">
<span class="ltx_p" id="S5.T2.2.3.3.1.1">pedestrian light red, car approaching crosswalk from right, four-way traffic</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t" id="S5.T2.2.4.1" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.2.4.1.1">
<span class="ltx_p" id="S5.T2.2.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.2.4.1.1.1.1">Pred.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t" id="S5.T2.2.4.2" style="width:156.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.2.4.2.1">
<span class="ltx_p" id="S5.T2.2.4.2.1.1">car passing over crosswalk from left to right, four-way traffic</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S5.T2.2.4.3" style="width:156.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.2.4.3.1">
<span class="ltx_p" id="S5.T2.2.4.3.1.1">pedestrian light red, four-way traffic</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S5.T2.4.1">Qualitative results for the scene description task.</span> </figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">We evaluated the fine-tuned model on a test dataset consisting of 400 samples collected using an Azure Kinect camera mounted on a Go1 quadruped robot. Quantitative analysis of the scene description task included metrics such as BLEU scores, precision, and recall. For metrics, BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#bib.bib9" title="">9</a>]</cite> embeddings were used to represent the words as tokens and cosine similarity was used to select the most similar token from the reference to calculate the scores.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S5.T1" title="Table 1 ‣ 5.2 Scene description ‣ 5 Experimental Result ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_tag">1</span></a>, incorporating additional real-world and synthetic data led to comparable performance improvements over the baseline, except for precision. Although both types of data augmentation yielded similar benefits, real-world data demonstrated slightly higher performance in terms of precision and recall. The higher BLEU scores highlight the potential of synthetic data in enhancing vision-language models to generate the precise wording preferred in scene descriptions.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">Qualitative results of the fine-tuned Florence-2 model are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S5.T2" title="Table 2 ‣ 5.2 Scene description ‣ 5 Experimental Result ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_tag">2</span></a>, where the model exhibited strong performance in generating accurate descriptions across varied conditions, though some inaccuracies and missing information were found. For example, the model did not provide information on the pedestrian light in the first example in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11164v1#S5.T2" title="Table 2 ‣ 5.2 Scene description ‣ 5 Experimental Result ‣ Synthetic data augmentation for robotic mobility aids to support blind and low vision people"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduced a synthetic data generation pipeline leveraging a game engine to train deep learning models for robotic mobility aid applications. Our approach focused on two primary tasks: tactile paving detection and scene description. By fine-tuning the YOLOv8 and YOLO-World models on our synthetic dataset, we successfully enhanced the detection of tactile pavings, addressing gaps that previous pretrained models had struggled with. Additionally, we fine-tuned the Florence-2 vision foundation model to generate scene descriptions that align more closely with the preferences of BLV individuals.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">While our results highlight the potential of synthetic data, we acknowledge the existing gap between synthetic and real-world data, as well as the complexities involved in creating realistic virtual environments, which require specialized expertise in game engine use. Moreover, our work is limited by the relatively small amount of data (seven additional images) used for scene description, and challenges remain in scenarios with extreme lighting variations or occlusions for tactile paving detection.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Despite these challenges, synthetic data generation potentially offers a more efficient alternative to collecting and annotating real-world data, providing the flexibility to tailor environments and datasets to specific application needs. This adaptability makes synthetic data a valuable resource for advancing assistive technologies, particularly in scenarios where real-world data acquisition is impractical or resource-intensive.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We would like to thank Shiven Patel, Antoinette Reid, Dang Nguyen, and Ron Kleinhause-Goldman from the CICS Early Research Scholars Program at the University of Massachusetts Amherst for their valuable assistance in collecting the test data for the scene description task.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ada accessibility standards, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Material pack tactile blocks - 01, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Peter Ackland, Serge Resnikoff, and Rupert Bourne.

</span>
<span class="ltx_bibblock">World blindness and visual impairment: despite many successes, the problem is growing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Community eye health</span>, 30(100):71, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Mauro Avila Soto, Markus Funk, Matthias Hoppe, Robin Boldt, Katrin Wolf, and Niels Henze.

</span>
<span class="ltx_bibblock">Dronenavigator: Using leashed and free-floating quadcopters to navigate visually impaired travelers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 19th international acm sigaccess conference on computers and accessibility</span>, pages 300–304, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
AYES.

</span>
<span class="ltx_bibblock">Meet oko — the ai-powered navigation app for all pedestrians., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Tetsuaki Baba.

</span>
<span class="ltx_bibblock">Vidvip: Dataset for object detection during sidewalk travel.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Journal of Robotics and Mechatronics</span>, 33(5):1135–1143, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan.

</span>
<span class="ltx_bibblock">Yolo-world: Real-time open-vocabulary object detection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 16901–16911, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.

</span>
<span class="ltx_bibblock">The cityscapes dataset for semantic urban scene understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 3213–3223, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jacob Devlin.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Be My Eyes.

</span>
<span class="ltx_bibblock">Be my eyes - see the world together, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.

</span>
<span class="ltx_bibblock">Vision meets robotics: The kitti dataset.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">The International Journal of Robotics Research</span>, 32(11):1231–1237, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Marcelo C Ghilardi, Rafael CO Macedo, and Isabel H Manssour.

</span>
<span class="ltx_bibblock">A new approach for automatic detection of tactile paving surfaces in sidewalks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Procedia computer science</span>, 80:662–672, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
João Guerreiro, Daisuke Sato, Saki Asakawa, Huixu Dong, Kris M Kitani, and Chieko Asakawa.

</span>
<span class="ltx_bibblock">Cabot: Designing and evaluating an autonomous navigation robot for blind people.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility</span>, pages 68–82, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Hochul Hwang, Sunjae Kwon, Yekyung Kim, and Donghyun Kim.

</span>
<span class="ltx_bibblock">Is it safe to cross? interpretable risk assessment with gpt-4v for safety-aware street crossing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2402.06794</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hochul Hwang, Tim Xia, Ibrahima Keita, Ken Suzuki, Joydeep Biswas, Sunghoon I Lee, and Donghyun Kim.

</span>
<span class="ltx_bibblock">System configuration and navigation of a guide dog robot: Toward animal guide dog-level guiding work.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">2023 IEEE International Conference on Robotics and Automation (ICRA)</span>, pages 9778–9784. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Yuki Ito, Chinthaka Premachandra, Sagara Sumathipala, H Waruna H Premachandra, and BS Sudantha.

</span>
<span class="ltx_bibblock">Tactile paving detection by dynamic thresholding based on hsv space analysis for developing a walking support system.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">IEEE Access</span>, 9:20358–20367, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Glenn Jocher, Ayush Chaurasia, and Jing Qiu.

</span>
<span class="ltx_bibblock">Ultralytics YOLO, January 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Masaki Kuribayashi, Tatsuya Ishihara, Daisuke Sato, Jayakorn Vongkulbhisal, Karnik Ram, Seita Kayukawa, Hironobu Takagi, Shigeo Morishima, and Chieko Asakawa.

</span>
<span class="ltx_bibblock">Pathfinder: Designing a map-less navigation system for blind people in unfamiliar buildings.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</span>, pages 1–16, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Kibaek Park, Youngtaek Oh, Soomin Ham, Kyungdon Joo, Hyokyoung Kim, Hyoyoung Kum, and In So Kweon.

</span>
<span class="ltx_bibblock">Sideguide: a large-scale sidewalk dataset for guiding impaired people.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, pages 10022–10029. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Unitree Robotics.

</span>
<span class="ltx_bibblock">Unitree robotics -global quadruped robots pioneer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</span>, pages 234–241. Springer, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Patrick Slade, Arjun Tambe, and Mykel J Kochenderfer.

</span>
<span class="ltx_bibblock">Multimodal sensing and intuitive steering assistance improve navigation and mobility for people with impaired vision.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Science Robotics</span>, 6(59):eabg6594, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Haobin Tan, Chang Chen, Xinyu Luo, Jiaming Zhang, Constantin Seibold, Kailun Yang, and Rainer Stiefelhagen.

</span>
<span class="ltx_bibblock">Flying guide dog: Walkable path discovery for the visually impaired utilizing drones and transformer-based semantic segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)</span>, pages 1123–1128. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Thang To, Jonathan Tremblay, Duncan McKay, Yukie Yamaguchi, Kirby Leung, Adrian Balanon, Jia Cheng, William Hodge, and Stan Birchfield.

</span>
<span class="ltx_bibblock">NDDS: NVIDIA deep learning dataset synthesizer, 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/Dataset_Synthesizer" title="">https://github.com/NVIDIA/Dataset_Synthesizer</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Sagar M Waghmare, Kimberly Wilber, Dave Hawkey, Xuan Yang, Matthew Wilson, Stephanie Debats, Cattalyya Nuengsigkapian, Astuti Sharma, Lars Pandikow, Huisheng Wang, et al.

</span>
<span class="ltx_bibblock">Sanpo: A scene understanding, accessibility, navigation, pathfinding, obstacle avoidance dataset.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2309.12172</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Lorraine Whitmarsh.

</span>
<span class="ltx_bibblock">The benefits of guide dog ownership.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Visual impairment research</span>, 7(1):27–42, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
William R Wiener, Richard L Welsh, and Bruce B Blasch.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Foundations of orientation and mobility</span>, volume 1.

</span>
<span class="ltx_bibblock">American Foundation for the Blind, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Anxing Xiao, Wenzhe Tong, Lizhi Yang, Jun Zeng, Zhongyu Li, and Koushil Sreenath.

</span>
<span class="ltx_bibblock">Robotic guide dog: Leading a human with leash-guided hybrid physical interaction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">2021 IEEE International Conference on Robotics and Automation (ICRA)</span>, pages 11470–11476. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan.

</span>
<span class="ltx_bibblock">Florence-2: Advancing a unified representation for a variety of vision tasks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 4818–4829, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Samuel Yu, Heon Lee, and Junghoon Kim.

</span>
<span class="ltx_bibblock">Street crossing aid using light-weight cnns for the visually impaired.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">The IEEE International Conference on Computer Vision (ICCV) Workshops</span>, Oct 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Arthur Zhang, Chaitanya Eranki, Christina Zhang, Ji-Hwan Park, Raymond Hong, Pranav Kalyani, Lochana Kalyanaraman, Arsh Gamare, Arnav Bagad, Maria Esteva, et al.

</span>
<span class="ltx_bibblock">Towards robust robot 3d perception in urban environments: The ut campus object dataset.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">IEEE Transactions on Robotics</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Xingli Zhang, Lei Liang, Shenglu Zhao, and Zhihui Wang.

</span>
<span class="ltx_bibblock">Grfb-unet: A new multi-scale attention network with group receptive field block for tactile paving segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Expert Systems with Applications</span>, 238:122109, 2024.

</span>
</li>
</ul>
</section><div about="" class="ltx_rdf" content="Overleaf Example" property="dcterms:title"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 17 13:13:08 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
