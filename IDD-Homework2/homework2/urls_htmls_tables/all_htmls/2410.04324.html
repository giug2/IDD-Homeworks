<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SONAR: A Synthetic AI-Audio Detection Framework and Benchmark</title>
<!--Generated on Thu Oct 10 05:33:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04324v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S1" title="In SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S2" title="In SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S3" title="In SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Evaluation dataset generalization and collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4" title="In SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Benchmarking AI-Audio Detection Models</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.SS1" title="In 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Benchmarking setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.SS2" title="In 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results and analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.SS2.SSS1" title="In 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>How well can detection models generalize across datasets?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.SS2.SSS2" title="In 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Results on SONAR dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.SS2.SSS3" title="In 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Can generalizability increase with model size?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.SS2.SSS4" title="In 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span>On the effectiveness and efficiency of few-shot fine-tuning to improve generalization</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S5" title="In SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S6" title="In SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#A1" title="In SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#A1.SS1" title="In Appendix A Appendix ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Broad Impacts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#A1.SS2" title="In Appendix A Appendix ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Implementation Details</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SONAR: A Synthetic AI-Audio Detection Framework and Benchmark</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiang Li
<br class="ltx_break"/>Fordham University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">xl5@fordham.edu</span>
<br class="ltx_break"/>&amp;Pin-Yu Chen
<br class="ltx_break"/>IBM Research 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">pin-yu.chen@ibm.com</span>
<br class="ltx_break"/>&amp;Wenqi Wei 
<br class="ltx_break"/>Fordham University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">wenqiwei@fordham.edu</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using generative Artificial Intelligence (AI) technology have made it possible to generate high-quality and realistic human-like audio. This introduces significant challenges to distinguishing AI-synthesized speech from the authentic human voice and could raise potential issues of misuse for malicious purposes such as impersonation and fraud, spreading misinformation, deepfakes, and scams. However, existing detection techniques for AI-synthesized audio have not kept pace and often exhibit poor generalization across diverse datasets. In this paper, we introduce SONAR, a <span class="ltx_text ltx_framed ltx_framed_underline" id="id4.id1.1">s</span>ynthetic AI-Audi<span class="ltx_text ltx_framed ltx_framed_underline" id="id4.id1.2">o</span> Detectio<span class="ltx_text ltx_framed ltx_framed_underline" id="id4.id1.3">n</span> Fr<span class="ltx_text ltx_framed ltx_framed_underline" id="id4.id1.4">a</span>mework and Benchma<span class="ltx_text ltx_framed ltx_framed_underline" id="id4.id1.5">r</span>k, aiming to provide a comprehensive evaluation for distinguishing cutting-edge AI-synthesized auditory content. SONAR includes a novel evaluation dataset sourced from 9 diverse audio synthesis platforms, including leading TTS providers and state-of-the-art TTS models. It is the first framework to uniformly benchmark AI-audio detection across both traditional and foundation model-based deepfake detection systems. Through extensive experiments, we reveal the generalization limitations of existing detection methods and demonstrate that foundation models exhibit stronger generalization capabilities, which can be attributed to their model size and the scale and quality of pretraining data. Additionally, we explore the effectiveness and efficiency of few-shot fine-tuning in improving generalization, highlighting its potential for tailored applications, such as personalized detection systems for specific entities or individuals. <span class="ltx_text ltx_font_italic" id="id4.id1.6">Code and dataset are available at</span> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Jessegator/SONAR" title="">https://github.com/Jessegator/SONAR</a></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using Artificial Intelligence (AI) technology have made it possible to generate high-quality and
realistic human-like audio efficiently <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib4" title="">4</a>]</cite>. This introduces significant challenges in distinguishing AI-synthesized speech from the authentic human voice and could raise potential misuse for malicious purposes such as impersonation and fraud, spreading misinformation, and scams. For example, a deep fake AI voice of the US President Joe Biden was recently utilized in robocalls to advise them against voting<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cnn.com/2024/01/22/politics/fake-joe-biden-robocall/index.html" title="">https://www.cnn.com/2024/01/22/politics/fake-joe-biden-robocall/index.html</a></span></span></span>, demonstrating how deepfakes can significantly manipulate public opinions and influence presidential elections. In response to such risks, the US Federal Communications Commission (FCC) now deems robot calls for election as illegal, which underscores the urgent need for enhanced detection of AI-synthesized audio.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While TTS models are advancing rapidly, AI-synthesized audio detection techniques are not keeping pace. First, previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib6" title="">6</a>]</cite> have highlighted the lack of generalization and robustness in these detection methods.
Second, existing detection models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib10" title="">10</a>]</cite> often take advantage of different audio features and evaluation datasets, complicating the comparison of their detection effectiveness. Third,
a comprehensive evaluation to determine the effectiveness of these detection methods against the latest TTS models has not been conducted.
This gap in research leaves a significant challenge in developing reliable detection techniques that can effectively counter the growing sophistication of AI-generated audio.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the aforementioned research gap and explore the strengths and limitations of existing AI-synthesized audio detection methods, especially those with increasingly advanced TTS models, we present a <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p3.1.1">s</span>ynthetic AI-Audi<span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p3.1.2">o</span> Detectio<span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p3.1.3">n</span> Fr<span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p3.1.4">a</span>mework and Benchma<span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p3.1.5">r</span>k, coined as SONAR.
This framework aims to provide a comprehensive evaluation for distinguishing state-of-the-art AI-synthesized auditory content. Our study benchmarks the state-of-the-art fake audio detection models using a newly collected fake audio dataset that includes a variety of synthetic speech audios sourced from diverse cutting-edge TTS providers and TTS models. We further investigate the potential of enhancing the generalization capabilities of these detection models from different perspectives. The main contributions of our work can be summarized as follows.</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce a novel evaluation dataset specifically designed for audio deepfake detection. This dataset is sourced from 9 diverse audio synthesis platforms, including those from leading TTS service providers and state-of-the-art TTS models. To the best of our knowledge, this dataset is by far the largest collection of fake audio generated by the latest TTS models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">SONAR is the first comprehensive framework to benchmark AI-audio detection uniformly across advanced TTS models. It covers 5 state-of-the-art traditional and 6 foundation-model-based audio deepfake detection models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Leveraging SONAR, we conduct extensive experiments to analyze the generalizability limitations of current detection methods. Our findings reveal that foundation models demonstrate stronger generalization capabilities than traditional models. We further explore factors that may contribute to this improved generalization, such as model size and the scale and quality of pre-training data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We further explore the potential of few-shot fine-tuning to enhance the generalization of detection models. Our empirical results demonstrate the effectiveness and efficiency of this approach, highlighting its potential for tailored applications, such as personalized detection systems for specific entities or individuals.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Text-to-Speech synthesis</span>. Human voice synthesis is a significant challenge in the field of AI. State-of-the-art TTS synthesis approaches such as VALLE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib4" title="">4</a>]</cite>, AudioBox <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib1" title="">1</a>]</cite>, VoiceBox <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib11" title="">11</a>]</cite>, NaturalSpeech3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib4" title="">4</a>]</cite>, and YourTTS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib3" title="">3</a>]</cite> have demonstrated the
possibility of generating high-quality, human-realistic audio with generative models trained on large datasets. Current TTS models can be classified into two primary categories: cascaded and end-to-end methods. Cascaded TTS models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib14" title="">14</a>]</cite> typically employ a pipeline involving an acoustic model and a vocoder utilizing mel spectrograms as intermediary representations. To address the limitations associated with vocoders, end-to-end TTS models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib16" title="">16</a>]</cite> have been developed to jointly optimize both the acoustic model and vocoder. In practical applications, it is preferable to customize TTS systems to generate speech in any voice with limited accessible data. Consequently, there is increasing interest in zero-shot multi-speaker TTS techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">AI-synthesized audio detection.</span> Recent advancements in AI technology have significantly enhanced the ability to generate high-quality and realistic audio, calling for an urgent need for more robust and reliable detection methods. Several datasets have been developed to support research in this area. The ASVspoof challenges <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib20" title="">20</a>]</cite> are among the most notable, offering comprehensive datasets that cover a variety of attack vectors, including replay attacks, voice conversion, and directly synthesized audio. These resources aim to facilitate thorough evaluations of countermeasures against various spoofing techniques. In addition, newer datasets such as WaveFake <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib21" title="">21</a>]</cite> and LibriSeVoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib22" title="">22</a>]</cite> provide fake audio samples generated with state-of-the-art vocoders, offering diverse distributions to enhance the development of deepfake audio detection systems. By comparison, the In-the-Wild dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib6" title="">6</a>]</cite> targets real-world applications by collecting deepfake audios from publicly accessible sources, capturing the complexity and diversity of manipulations encountered in everyday environments. Similarly, the SingFake dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib6" title="">6</a>]</cite> focuses on the detection of synthetic singing voices, presenting unique challenges due to the musical content and variation in vocal expressions. These datasets are crucial for developing and testing next-generation AI-synthesized audio detection systems, pushing the boundaries of what is achievable in identifying and mitigating the threats posed by advanced audio synthesis technologies.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Building upon these datasets, a significant body of research has focused on distinguishing AI-generated audio from genuine audio by designing advanced model architectures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib9" title="">9</a>]</cite> tailored for extracting different levels of representations of speech data for audio deepfake detection. Additionally, recent works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib25" title="">25</a>]</cite> have leveraged speech foundation models for audio deepfake detection tasks. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib23" title="">23</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib24" title="">24</a>]</cite> fine-tune Wav2Vec2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib26" title="">26</a>]</cite> models on the ASVspoof dataset, while <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib25" title="">25</a>]</cite> uses Whisper as a front-end to extract audio features and trains various detection models based on these features, achieving state-of-the-art detection performance on the corresponding test datasets. However, none of these models have been evaluated on audio generated by the latest text-to-speech (TTS) models, leaving a gap in understanding their effectiveness against the most recent advancements in synthetic audio generation.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation dataset generalization and collection</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S3.F1.g1" src="extracted/5915436/sonar_overview.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.4.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.5.2" style="font-size:90%;">Overview of SONAR. <span class="ltx_text ltx_font_bold" id="S3.F1.5.2.1">Left</span>: Audio deepfake data generation and collection. <span class="ltx_text ltx_font_bold" id="S3.F1.5.2.2">Right</span>: Benchmark evaluation.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Leveraging a set of diverse and high-quality speech data synthesis APIs and models, we create an evaluation dataset for synthetic AI-audio detection. Our approach incorporates two strategies: data generation and data collection.
Our dataset includes AI-generated speech and audio from nine distinct sources.
We perform speech data generation using one cutting-edge TTS service provider, OpenAI, and two open-sourced APIs, xTTS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib27" title="">27</a>]</cite> and AudioGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib28" title="">28</a>]</cite>.
For speech data collection, we leverage
six state-of-the-art TTS models including Seed-TTS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib29" title="">29</a>]</cite>, VALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib4" title="">4</a>]</cite>, PromptTTS2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib30" title="">30</a>]</cite>, NaturalSpeech3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib31" title="">31</a>]</cite>, VoiceBox <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib11" title="">11</a>]</cite>, FlashSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib2" title="">2</a>]</cite>. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S3.T1" title="Table 1 ‣ 3 Evaluation dataset generalization and collection ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">1</span></a> presents the details of our dataset generated by different audio generation models. We next detail our methods of generating and collecting these datasets.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Overview of our dataset with fake audios generated by various models. AudioGen lacks speaker and language information as it focuses on environmental sounds. The trainset sizes for OpenAI and Seed-TTS are unavailable due to the use of proprietary data. * denotes the samples that are directly collected from their demo page or provided test set due to the unavailability of their model checkpoints.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.5" style="width:433.6pt;height:136.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-97.3pt,30.7pt) scale(0.690262854821023,0.690262854821023) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.5.1">
<tr class="ltx_tr" id="S3.T1.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.2.1">Samples</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.3.1">Avg duration (s)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.4.1">Avg. pitch (Hz)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.5.1">Std. pitch (Hz)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.6.1">Languages</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.7.1">Trainset size(H)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.8"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.8.1">Year</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.5.1.2.1">PromptTTS2*</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.2">25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.3">9.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.4">126.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.5">46.27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.6">English</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.7">44K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.8">2023</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.3.1">NaturalSpeech3*</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.2">32</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.3">5.25</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.4">143.86</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.5">53.94</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.6">English</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.7">60K</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.8">2024</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.4.1">VALL-E*</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.2">95</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.3">4.86</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.4">133.41</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.5">56.54</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.6">English</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.7">60K</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.8">2023</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.5.1" rowspan="2"><span class="ltx_text" id="S3.T1.5.1.5.1.1">VoiceBox*</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.2" rowspan="2"><span class="ltx_text" id="S3.T1.5.1.5.2.1">104</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.3" rowspan="2"><span class="ltx_text" id="S3.T1.5.1.5.3.1">10.28</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.4" rowspan="2"><span class="ltx_text" id="S3.T1.5.1.5.4.1">114.09</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.5" rowspan="2"><span class="ltx_text" id="S3.T1.5.1.5.5.1">37.89</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.6">English, German, French,</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.7" rowspan="2"><span class="ltx_text" id="S3.T1.5.1.5.7.1">60K</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.8" rowspan="2"><span class="ltx_text" id="S3.T1.5.1.5.8.1">2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.6">
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.6.1">Portuguese, Polish, Spanish</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.7.1">FlashSpeech*</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.2">118</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.3">7.57</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.4">129.30</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.5">54.77</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.6">English</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.7">44.5K</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.8">2024</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.8.1">AudioGen</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.2">100</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.3">5.00</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.4">199.45</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.5">72.94</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.6">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.7">7K</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.8">2022</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.9.1">xTTS</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.2">600</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.3">5.67</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.4">164.67</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.5">95.20</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.6">English</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.7">2.7K</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.8">2023</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.10.1">Seed-TTS*</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.2">600</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.3">4.91</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.4">117.28</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.5">36.85</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.6">English, Mandarin</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.7">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.8">2024</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.11">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.5.1.11.1">OpenAI</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.1.11.2">600</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.1.11.3">4.11</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.1.11.4">126.89</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.1.11.5">54.89</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.1.11.6">English</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.1.11.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.1.11.8">2024</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Data generation</span>. Our dataset generation involves OpenAI, xTTS, and AudioGen. Specifically, OpenAI currently provides voice choices from 6 different speakers. Using ChatGPT,we generate 100 different text prompts of varying lengths for each speaker, resulting in a total of 600 synthetic speech audios. xTTS supports synthetic speech generation given text prompts and reference speech. We select 6 speakers from the LibriTTS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib32" title="">32</a>]</cite> as the reference speech and also generate 600 text prompts with ChatGPT for each speaker, resulting in 600 synthetic speech audios. AudioGen can generate the corresponding environmental sound given a textual description of the acoustic scene. With AudioGen, we use ChatGPT to generate 100 text descriptions of the environment and background and obtain 100 AI-synthesized environmental sounds. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S3.F1" title="Figure 1 ‣ 3 Evaluation dataset generalization and collection ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">1</span></a> (left) illustrates the data generation and collection process.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.p3.1.1">Data collection</span>. To evaluate the effectiveness of various detection systems against the state-of-the-art TTS models, we also collect fake speech audio from Seed-TTS, VALL-E, PromptTTS2, NaturalSpeech3, VoiceBox, and FlashSpeech. Seed-TTS provides a test dataset<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/BytedanceSpeech/seed-tts-eval" title="">https://github.com/BytedanceSpeech/seed-tts-eval</a></span></span></span> consisting of fake audio samples generated by it. Due to the unavailability of pre-trained weights of the other 5 models, we extract the synthesized speech data directly from their demo pages. Specifically, speech audios from VALL-E include variations in emotions and acoustic environment. PromptTTS2 presents fake audio samples with various attributes such as gender, speed, pitch, volume, and timbre. NaturalSpeech3 also includes fake audio samples generated with various attributes such as speeds and emotions and contains fake speech audio samples obtained with voice conversion. VoiceBox provides fake audio samples that feature cross-lingual and expressive audio styles. FlashSpeech includes a set of high-quality fake audios obtained both from speech generation and voice conversion.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">To summarize, leveraging the details outlined above, we generate and collect a
comprehensive evaluation dataset, encompassing a total of 2274 AI-synthesized audio samples produced by various TTS models. To the best of our knowledge, our dataset is by far the largest collection of fake audio generated by the latest TTS models. Note our motivation for collecting this dataset is for evaluation purposes. Additionally, we only include fake audio samples in this dataset since genuine audio samples can be easily collected from various sources (e.g., internet, self-recording, publicly available datasets, etc.). However, for convenience of evaluation, we also provide an equal number of real speech audio data sampled from the LibriTTS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib32" title="">32</a>]</cite> clean-test set.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">We believe the collected dataset can serve as a valuable asset for evaluating existing audio deepfake detection models.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Benchmarking AI-Audio Detection Models</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we first detail the model, dataset, and evaluation metrics setup for benchmarking. Then, we present the results of evaluating detection models on existing audio deepfake datasets to assess their generalizability across datasets. We next benchmark their detection performance on our proposed dataset and provide analysis for potential model generalization improvement.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Benchmarking setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Model architectures</span>. SONAR incorporates 11 models, including 5 state-of-the-art traditional audio deepfake detection models featuring various levels of input feature abstraction and 6 foundation models. Specifically, for the former, SONAR includes (1) AASIST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib7" title="">7</a>]</cite>, which processes raw waveform directly and utilizes graph neural networks and incorporates spectro-temporal attention mechanisms. (2) RawGAT-ST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib9" title="">9</a>]</cite>, which employs spectral and temporal sub-graphs along with a graph pooling strategy. (3) RawNet2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib8" title="">8</a>]</cite>, which is a hybrid model combining CNN and GRU.(4) Spectrogram(Spec.)+ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib6" title="">6</a>]</cite>, which transforms the audio to linear spectrogram using a 512-point Fast Fourier Transform (FFT) with a hop size of 10 ms. The spectrogram is then inputted into ResNet18 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib33" title="">33</a>]</cite>. (5) LFCC-LCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib10" title="">10</a>]</cite>, which converts audio into Linear-Frequency Cepstral Coefficients (LFCC) for input into a CNN model. Specifically, 60-dimensional LFCCs are extracted from each utterance frame, with frame length set to 20ms and hop size 10ms. It extracts speech embedding directly from raw audio. These models collectively cover a broad spectrum of feature types and architectures, facilitating a detailed examination of their performance in deepfake audio detection applications. For <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">foundation models</span>, SONAR includes (1) Wave2Vec2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib26" title="">26</a>]</cite>, which is pre-trained on 53k hours of unlabeled speech data. (2) Wave2Vec2BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib34" title="">34</a>]</cite>, which is pre-trained on 4.5M hours of unlabeled speech data covering more than 143 languages. (3) HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib35" title="">35</a>]</cite>, which is pretrained-on 60k hours of speech data. (4) CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib36" title="">36</a>]</cite>, who is trained on a variety of audio-text pairs. (5) Whisper-small <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib37" title="">37</a>]</cite>, and (6) Whisper-large <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib37" title="">37</a>]</cite>. Both Whispers are pre-trained on 680K hours of speech data covering 96 languages.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Public datasets for training and testing</span>. We consider three benchmark datasets for deepfake audio detection model training and testing as they are commonly used in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib39" title="">39</a>]</cite>. <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.2">Wavefake</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib21" title="">21</a>]</cite> collects deepfake audios from six vocoder architectures, including MelGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib40" title="">40</a>]</cite>, FullBand-MelGAN, MultiBand-MelGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib41" title="">41</a>]</cite>, HiFi-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib42" title="">42</a>]</cite>, Parallel WaveGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib43" title="">43</a>]</cite>, and WaveGlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib44" title="">44</a>]</cite>. It consists of approximately 196 hours of generated audio files derived from the LJSPEECH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib45" title="">45</a>]</cite> dataset. Similar to wavefake, <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.3">LibriSeVoc</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib22" title="">22</a>]</cite> collects deepfake audios from six state-of-the-art neural vocoders including WaveNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib46" title="">46</a>]</cite>, WaveRNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib47" title="">47</a>]</cite>, Mel-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib41" title="">41</a>]</cite>, Parallel WaveGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib43" title="">43</a>]</cite>, WaveGrad <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib48" title="">48</a>]</cite> and DiffWave <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib49" title="">49</a>]</cite> to generate speech samples derived from the widely used LibriTTS speech corpus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib32" title="">32</a>]</cite>, which is often utilized in text-to-speech research. Specifically, it consists of a total of 208.74 hours of synthesized samples. <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.4">In-the-wild</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#bib.bib5" title="">5</a>]</cite> comprises genuine and deepfake audio recordings of 58 politicians and other public figures gathered from publicly accessible sources, including social networks and video streaming platforms.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">For LibriSeVoc, we follow the official train-validation-test splits, which are approximately 60%, 20%, and 20%, respectively. For Wavefake, we partition the data generated by each vocoder into training, validation, and testing subsets at ratios of 70%, 10%, and 20%, respectively. To address the class imbalance and mitigate potential evaluation bias, we further downsample LibriSeVoc and WaveFake test datasets, and In-the-Wild datasets, resulting in a balanced dataset with a real-to-fake ratio of 1:1.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Evaluation metrics</span>. To provide a comprehensive evaluation of the detection performance of audio deepfake models, we adopt (1) <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.2">Equal Error Rate</span> (EER), which is defined as the point on the ROC curve, where the false positive rate (FPR) and false negative rate (FNR) are equal and is commonly used to assess the performance of binary classifications tasks, with lower values indicating better detection performance. (2) <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.3">Accuracy</span> evaluates the overall correctness of the detection model’s predictions and is defined as the ratio of correctly predicted data to the total data. To ensure consistency with the EER and provide more intuitive results, we set the threshold for accuracy at the EER point, meaning the accuracy reflects the model’s performance when the FPR equals the FNR. (3) <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.4">AUROC</span> (Area Under the Receiver Operating Characteristic) provides a measure of the model’s ability to distinguish between classes across different decision thresholds, providing a more comprehensive view of its discriminative power across varying conditions. An AUROC score of 1.0 indicates perfect classification, while a score of 0.5 indicates performance no better than random guessing.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Note that the test datasets are class-balanced, and the accuracy score is calculated using the EER threshold. Thus, we omit F1, precision, and recall scores from our evaluation results in the paper, though SONAR provides these metrics as well.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results and analysis</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>How well can detection models generalize across datasets?</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">We first train all models on Wavefake training dataset and then evaluate the models on its own test set, LibriSeVoc test set, and In-the-wild dataset. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T2" title="Table 2 ‣ 4.2.1 How well can detection models generalize across datasets? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">2</span></a> presents the evaluation results. Particularly, we make the following interesting observations.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.3.2" style="font-size:90%;">Generalization across existing audio deepfake datasets. All models are trained/finetuned on the Wavefake training set. Green and orange indicate the best and second-best performance, respectively.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.4" style="width:433.6pt;height:186.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.0pt,23.7pt) scale(0.797746256193943,0.797746256193943) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.4.1">
<tr class="ltx_tr" id="S4.T2.4.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.4.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T2.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.2.1">Wavefake</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T2.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.3.1">LibriSeVoc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T2.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.1.4.1">In-the-wild</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.2.1.1">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.2.2.1">AUROC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.1.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.2.3.1">EER(%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.2.4.1">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.2.5.1">AUROC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.1.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.2.6.1">EER(%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.2.7.1">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.2.8"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.2.8.1">AUROC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.2.9"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.2.9.1">EER(%)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.1.3.1">LFCC-LCNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.2">0.9984</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.3.3.1" style="background-color:#CCFFCC;">0.9999</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.1.3.4">0.153</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.5">0.7429</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.6">0.8239</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.1.3.7">25.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.8">0.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.9">0.4786</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.10">99.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.4.1">Spec.+ResNet</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.2">0.9924</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.3">0.9924</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.4.4">0.076</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.5">0.7577</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.6">0.8495</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.4.7">24.233</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.8">0.4685</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.9">0.4723</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.10">53.148</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.5.1">RawNet2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.5.2">0.9416</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.5.3">0.9592</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.5.4">5.839</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.5.5">0.5119</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.5.6">0.5332</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.5.7">48.807</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.5.8">0.5321</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.5.9">0.5393</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.5.10">46.792</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.6.1">RawGATST</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.2">0.9988</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.6.3.1" style="background-color:#CCFFCC;">0.9999</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.6.4">0.115</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.5">0.8307</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.6">0.9203</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.6.7">16.925</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.8">0.6418</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.9">0.7015</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.10">35.816</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.7.1">AASIST</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.2" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T2.4.1.7.2.1" style="background-color:#FFE6CC;">0.9992</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.7.3.1" style="background-color:#CCFFCC;">0.9999</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.7.4" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T2.4.1.7.4.1" style="background-color:#FFE6CC;">0.076</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.5">0.886</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.6">0.9534</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.7.7">11.397</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.8">0.7272</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.9">0.7975</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.10">27.277</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.8.1">CLAP</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.8.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.8.2.1" style="background-color:#CCFFCC;">0.9996</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.8.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.8.3.1" style="background-color:#CCFFCC;">0.9999</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.8.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.8.4.1" style="background-color:#CCFFCC;">0.038</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.8.5">0.8296</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.8.6">0.9019</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.8.7">24.763</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.8.8">0.3013</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.8.9">0.2252</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.8.10">69.871</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.9.1">Whisper-small</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.9.2">0.9935</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.9.3" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T2.4.1.9.3.1" style="background-color:#FFE6CC;">0.9997</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.9.4">0.649</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.9.5">0.9345</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.9.6">0.9837</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.9.7">6.551</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.9.8">0.821</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.9.9">0.9025</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.9.10">17.899</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.10.1">Whisper-large</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.10.2">0.9962</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.10.3">0.9992</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.10.4">0.381</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.10.5">0.9572</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.10.6">0.9901</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.10.7">4.279</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.10.8">0.8848</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.10.9">0.9552</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.10.10">11.518</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.11.1">Wave2Vec2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.11.2">0.9874</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.11.3">0.9987</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.11.4">1.259</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.11.5">0.9705</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.11.6" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T2.4.1.11.6.1" style="background-color:#FFE6CC;">0.9953</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.11.7">2.953</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.11.8">0.8733</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.11.9">0.9323</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.11.10">12.669</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.12.1">HuBERT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.12.2">0.9931</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.12.3">0.9996</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.12.4">0.687</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.12.5" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T2.4.1.12.5.1" style="background-color:#FFE6CC;">0.986</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.12.6" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.12.6.1" style="background-color:#CCFFCC;">0.9991</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.1.12.7" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T2.4.1.12.7.1" style="background-color:#FFE6CC;">1.401</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.12.8" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T2.4.1.12.8.1" style="background-color:#FFE6CC;">0.9164</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.12.9" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T2.4.1.12.9.1" style="background-color:#FFE6CC;">0.9653</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.12.10" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T2.4.1.12.10.1" style="background-color:#FFE6CC;">8.362</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.13">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.4.1.13.1">Wave2Vec2BERT</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.13.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.13.2.1" style="background-color:#CCFFCC;">0.9996</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.13.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.13.3.1" style="background-color:#CCFFCC;">0.9999</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.4.1.13.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.13.4.1" style="background-color:#CCFFCC;">0.038</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.13.5" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.13.5.1" style="background-color:#CCFFCC;">0.9902</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.13.6" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.13.6.1" style="background-color:#CCFFCC;">0.9991</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.4.1.13.7" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.13.7.1" style="background-color:#CCFFCC;">0.984</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.13.8" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.13.8.1" style="background-color:#CCFFCC;">0.9232</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.13.9" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.13.9.1" style="background-color:#CCFFCC;">0.979</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.13.10" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T2.4.1.13.10.1" style="background-color:#CCFFCC;">7.676</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.1">Speech foundation models exhibit stronger generalizability</span>. As shown in Table  <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T2" title="Table 2 ‣ 4.2.1 How well can detection models generalize across datasets? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">2</span></a>, when evaluated on the test set of Wavefake, all models demonstrate near-perfect performance across the three metrics. This can be attributed to the similarity between the test set and the training data. However, when tested on the LibriSeVoc and In-the-wild datasets, models such as LFCC-LCNN, Spec.+ResNet, RawNet2, RawGATST, and AASIST struggle to generalize effectively. This performance gap indicates significant overfitting to the training data, despite these models being specifically designed for audio deepfake detection tasks. In contrast, speech foundation models consistently display stronger generalizability. Notably, Wave2Vec2BERT achieves the highest generalizability, which may be attributed to its large-scale and diverse pretraining data. Pretrained on 4.5 million hours of unlabeled audio in more than 143 languages, Wave2Vec2BERT benefits from both scale and diversity. This suggests that a well-designed self-supervised model trained on diverse speech data can extract general and discriminative features, making it more applicable across different datasets for audio deepfake detection. It is important to note that CLAP, unlike other speech foundation models, does not generalize well across datasets. This is likely due to its primary focus on environmental audio data during pretraining, resulting in the extraction of irrelevant features for speech audio. This observation underscores that not all foundation models are equally suited for audio deepfake detection tasks.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p3.1.1">Generalizability may increase with model size.</span> In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T2" title="Table 2 ‣ 4.2.1 How well can detection models generalize across datasets? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">2</span></a>, it can be observed that Whisper-large always outperforms Whisper-small across all three datasets. In particular, on the LibriSeVoc test set, Whisper-large achieves accuracy, AUROC, and EER of 0.9572, 0.9901, 4.279%, respectively, which improves by 2.27%, 0.64%, and 2.272%, than that of Whisper-small. This trend is more evident in the In-the-wild dataset, which is closer to real-world scenarios since this dataset consists of speech data sourced from the internet. Specifically, Whisper-large achieves accuracy, AUROC, and EER of 0.8848, 0.9552, and 11.518%, respectively, which improves by 6.381%, 5.27%, and 6.381%, than that of Whisper-small. Further investigation will be made in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.SS2.SSS3" title="4.2.3 Can generalizability increase with model size? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">4.2.3</span></a></p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Results on SONAR dataset</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">We further evaluate all detection models on the proposed dataset. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T3.st1" title="In Table 3 ‣ 4.2.2 Results on SONAR dataset ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">3(a)</span></a>, Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T3.st2" title="In Table 3 ‣ 4.2.2 Results on SONAR dataset ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">3(b)</span></a>, and Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T3.st3" title="In Table 3 ‣ 4.2.2 Results on SONAR dataset ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">3(c)</span></a> present the accuracy, AUROC, and EER of different detection models on our proposed SONAR dataset as described in Sec <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S3" title="3 Evaluation dataset generalization and collection ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.2.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.3.2" style="font-size:90%;">Evaluation on SONAR dataset. Green and orange indicate the best and second-best performance, respectively.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S4.T3.st1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.st1.4.2.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.T3.st1.2.1" style="font-size:90%;">Accuracy (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.st1.2.1.m1.1"><semantics id="S4.T3.st1.2.1.m1.1b"><mo id="S4.T3.st1.2.1.m1.1.1" stretchy="false" xref="S4.T3.st1.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.st1.2.1.m1.1c"><ci id="S4.T3.st1.2.1.m1.1.1.cmml" xref="S4.T3.st1.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.st1.2.1.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.st1.2.1.m1.1e">↑</annotation></semantics></math>).</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.st1.5" style="width:433.6pt;height:142.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-110.9pt,36.5pt) scale(0.661607362264564,0.661607362264564) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.st1.5.1">
<tr class="ltx_tr" id="S4.T3.st1.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.st1.5.1.1.1">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st1.5.1.1.2">PromptTTS2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st1.5.1.1.3">NaturalSpeech3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st1.5.1.1.4">VALL-E</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st1.5.1.1.5">VoiceBox</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st1.5.1.1.6">FlashSpeech</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st1.5.1.1.7">AudioGen</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st1.5.1.1.8">xTTS</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st1.5.1.1.9">Seed-TTS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.st1.5.1.1.10">OpenAI</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st1.5.1.1.11">Average</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.st1.5.1.2.1">LFCC-LCNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st1.5.1.2.2">0.5200</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st1.5.1.2.3" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st1.5.1.2.3.1" style="background-color:#FFE6CC;">0.7500</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st1.5.1.2.4">0.6211</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st1.5.1.2.5">0.8462</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st1.5.1.2.6">0.7034</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st1.5.1.2.7">0.4600</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st1.5.1.2.8">0.7433</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st1.5.1.2.9">0.3058</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.st1.5.1.2.10">0.5000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st1.5.1.2.11">0.6055</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.3.1">Spec.+ResNet</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.3.2">0.5600</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.3.3">0.5000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.3.4">0.5684</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.3.5">0.5481</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.3.6">0.6356</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.3.7">0.6800</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.3.8">0.8450</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.3.9">0.4167</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.3.10" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st1.5.1.3.10.1" style="background-color:#FFE6CC;">0.6783</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.3.11">0.6036</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.4.1">RawNet2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.4.2">0.6800</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.4.3">0.3125</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.4.4">0.4211</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.4.5">0.5385</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.4.6">0.4915</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.4.7">0.2600</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.4.8">0.6533</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.4.9">0.3733</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.4.10">0.3500</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.4.11">0.4534</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.5.1">RawGATST</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.5.2">0.8000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.5.3">0.5312</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.5.4">0.6842</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.5.5">0.8173</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.5.6">0.5424</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.5.7">0.2400</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.5.8">0.6567</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.5.9">0.5833</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.5.10">0.4900</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.5.11">0.5939</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.6.1">AASIST</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.6.2">0.8400</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.6.3">0.5312</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.6.4">0.7789</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.6.5">0.8750</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.6.6">0.6610</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.6.7">0.6900</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.6.8">0.7300</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.6.9">0.6567</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.6.10">0.5150</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.6.11">0.6975</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.7.1">CLAP</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.7.2">0.5600</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.7.3">0.4688</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.7.4">0.6421</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.7.5">0.5288</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.7.6">0.6017</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.7.7">0.2500</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.7.8">0.4800</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.7.9">0.4000</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.7.10">0.3233</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.7.11">0.4727</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.8.1">Whisper-small</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.8.2">0.8800</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.8.3">0.5625</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.8.4">0.7158</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.8.5">0.7404</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.8.6">0.5678</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.8.7">0.8000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.8.8">0.8050</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.8.9">0.5983</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.8.10">0.1883</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.8.11">0.6509</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.9.1">Whisper-large</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.9.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.9.2.1" style="background-color:#CCFFCC;">1.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.9.3">0.6562</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.9.4">0.7895</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.9.5">0.7885</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.9.6">0.7288</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.9.7">0.8400</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.9.8" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st1.5.1.9.8.1" style="background-color:#FFE6CC;">0.9033</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.9.9">0.5933</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.9.10">0.2900</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.9.11">0.7322</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.10.1">Wave2Vec2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.10.2" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st1.5.1.10.2.1" style="background-color:#FFE6CC;">0.9600</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.10.3">0.6875</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.10.4">0.8210</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.10.5">0.9327</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.10.6">0.8136</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.10.7" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st1.5.1.10.7.1" style="background-color:#FFE6CC;">0.9900</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.10.8">0.7333</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.10.9" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st1.5.1.10.9.1" style="background-color:#FFE6CC;">0.8683</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.10.10">0.5175</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.10.11">0.8138</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.11.1">HuBERT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.11.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.11.2.1" style="background-color:#CCFFCC;">1.0000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.11.3" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st1.5.1.11.3.1" style="background-color:#FFE6CC;">0.7500</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.11.4" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st1.5.1.11.4.1" style="background-color:#FFE6CC;">0.9158</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.11.5" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.11.5.1" style="background-color:#CCFFCC;">0.9712</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.11.6" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.11.6.1" style="background-color:#CCFFCC;">0.9407</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.11.7" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.11.7.1" style="background-color:#CCFFCC;">1.0000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.11.8">0.8767</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.11.9" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.11.9.1" style="background-color:#CCFFCC;">0.8900</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st1.5.1.11.10">0.5658</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st1.5.1.11.11" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st1.5.1.11.11.1" style="background-color:#FFE6CC;">0.8789</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.st1.5.1.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.st1.5.1.12.1">Wave2Vec2BERT</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st1.5.1.12.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.12.2.1" style="background-color:#CCFFCC;">1.0000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st1.5.1.12.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.12.3.1" style="background-color:#CCFFCC;">0.9062</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st1.5.1.12.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.12.4.1" style="background-color:#CCFFCC;">0.9474</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st1.5.1.12.5" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.12.5.1" style="background-color:#CCFFCC;">0.9712</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st1.5.1.12.6" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st1.5.1.12.6.1" style="background-color:#FFE6CC;">0.9237</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st1.5.1.12.7">0.9700</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st1.5.1.12.8" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.12.8.1" style="background-color:#CCFFCC;">0.9867</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st1.5.1.12.9">0.6017</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.st1.5.1.12.10" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.12.10.1" style="background-color:#CCFFCC;">0.7833</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st1.5.1.12.11" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st1.5.1.12.11.1" style="background-color:#CCFFCC;">0.8989</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S4.T3.st2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.st2.4.2.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.T3.st2.2.1" style="font-size:90%;">AUROC (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.st2.2.1.m1.1"><semantics id="S4.T3.st2.2.1.m1.1b"><mo id="S4.T3.st2.2.1.m1.1.1" stretchy="false" xref="S4.T3.st2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.st2.2.1.m1.1c"><ci id="S4.T3.st2.2.1.m1.1.1.cmml" xref="S4.T3.st2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.st2.2.1.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.st2.2.1.m1.1e">↑</annotation></semantics></math>).</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.st2.5" style="width:433.6pt;height:142.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-110.9pt,36.5pt) scale(0.661607362264564,0.661607362264564) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.st2.5.1">
<tr class="ltx_tr" id="S4.T3.st2.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.st2.5.1.1.1">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st2.5.1.1.2">PromptTTS2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st2.5.1.1.3">NaturalSpeech3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st2.5.1.1.4">VALL-E</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st2.5.1.1.5">VoiceBox</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st2.5.1.1.6">FlashSpeech</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st2.5.1.1.7">AudioGen</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st2.5.1.1.8">xTTS</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st2.5.1.1.9">Seed-TTS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.st2.5.1.1.10">OpenAI</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st2.5.1.1.11">Average</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.st2.5.1.2.1">LFCC-LCNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st2.5.1.2.2">0.5696</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st2.5.1.2.3">0.7666</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st2.5.1.2.4">0.6967</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st2.5.1.2.5">0.9106</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st2.5.1.2.6">0.7945</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st2.5.1.2.7">0.4559</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st2.5.1.2.8">0.8163</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st2.5.1.2.9">0.2452</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.st2.5.1.2.10">0.0967</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st2.5.1.2.11">0.5947</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.3.1">Spec.+ResNet</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.3.2">0.6064</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.3.3">0.4941</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.3.4">0.6217</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.3.5">0.5858</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.3.6">0.6891</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.3.7">0.7293</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.3.8">0.9205</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.3.9">0.4003</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.3.10" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st2.5.1.3.10.1" style="background-color:#FFE6CC;">0.7450</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.3.11">0.6436</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.4.1">RawNet2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.4.2">0.6944</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.4.3">0.2422</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.4.4">0.3695</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.4.5">0.6210</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.4.6">0.5203</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.4.7">0.3030</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.4.8">0.7210</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.4.9">0.3120</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.4.10">0.2940</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.4.11">0.4530</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.5.1">RawGATST</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.5.2">0.8704</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.5.3">0.5439</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.5.4">0.7490</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.5.5">0.8989</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.5.6">0.5742</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.5.7">0.2050</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.5.8">0.7317</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.5.9">0.6065</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.5.10">0.4795</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.5.11">0.6288</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.6.1">AASIST</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.6.2">0.9248</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.6.3">0.6172</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.6.4">0.8479</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.6.5">0.9433</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.6.6">0.7485</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.6.7">0.7466</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.6.8">0.8265</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.6.9">0.6893</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.6.10">0.5259</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.6.11">0.7633</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.7.1">CLAP</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.7.2">0.5712</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.7.3">0.4434</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.7.4">0.7223</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.7.5">0.5155</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.7.6">0.6533</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.7.7">0.1777</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.7.8">0.5114</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.7.9">0.3544</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.7.10">0.2407</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.7.11">0.4655</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.8.1">Whisper-small</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.8.2">0.9776</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.8.3">0.5762</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.8.4">0.8050</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.8.5">0.8400</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.8.6">0.6446</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.8.7">0.8284</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.8.8">0.8915</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.8.9">0.6326</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.8.10">0.108</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.8.11">0.7004</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.9.1">Whisper-large</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.9.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.9.2.1" style="background-color:#CCFFCC;">1.0000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.9.3">0.6992</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.9.4">0.9063</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.9.5">0.8552</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.9.6">0.7933</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.9.7">0.8926</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.9.8" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st2.5.1.9.8.1" style="background-color:#FFE6CC;">0.9690</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.9.9">0.6558</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.9.10">0.2327</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.9.11">0.7782</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.10.1">Wave2Vec2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.10.2" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st2.5.1.10.2.1" style="background-color:#FFE6CC;">0.9952</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.10.3">0.7515</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.10.4">0.8751</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.10.5">0.9674</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.10.6">0.8438</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.10.7" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st2.5.1.10.7.1" style="background-color:#FFE6CC;">0.9987</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.10.8">0.7931</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.10.9" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st2.5.1.10.9.1" style="background-color:#FFE6CC;">0.9205</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.10.10">0.4881</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.10.11">0.8482</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.11.1">HuBERT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.11.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.11.2.1" style="background-color:#CCFFCC;">1.0000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.11.3" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st2.5.1.11.3.1" style="background-color:#FFE6CC;">0.8174</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.11.4" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st2.5.1.11.4.1" style="background-color:#FFE6CC;">0.9719</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.11.5" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.11.5.1" style="background-color:#CCFFCC;">0.9953</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.11.6" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.11.6.1" style="background-color:#CCFFCC;">0.9871</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.11.7" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.11.7.1" style="background-color:#CCFFCC;">1.0000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.11.8">0.9496</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.11.9" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.11.9.1" style="background-color:#CCFFCC;">0.9531</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st2.5.1.11.10">0.5585</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st2.5.1.11.11" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st2.5.1.11.11.1" style="background-color:#FFE6CC;">0.9148</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.st2.5.1.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.st2.5.1.12.1">Wave2Vec2BERT</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st2.5.1.12.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.12.2.1" style="background-color:#CCFFCC;">1.0000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st2.5.1.12.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.12.3.1" style="background-color:#CCFFCC;">0.9658</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st2.5.1.12.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.12.4.1" style="background-color:#CCFFCC;">0.9860</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st2.5.1.12.5" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st2.5.1.12.5.1" style="background-color:#FFE6CC;">0.9906</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st2.5.1.12.6" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st2.5.1.12.6.1" style="background-color:#FFE6CC;">0.9666</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st2.5.1.12.7">0.9826</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st2.5.1.12.8" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.12.8.1" style="background-color:#CCFFCC;">0.9980</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st2.5.1.12.9">0.6165</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.st2.5.1.12.10" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.12.10.1" style="background-color:#CCFFCC;">0.8607</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st2.5.1.12.11" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st2.5.1.12.11.1" style="background-color:#CCFFCC;">0.9290</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S4.T3.st3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.st3.4.2.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S4.T3.st3.2.1" style="font-size:90%;">EER(%) (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.st3.2.1.m1.1"><semantics id="S4.T3.st3.2.1.m1.1b"><mo id="S4.T3.st3.2.1.m1.1.1" stretchy="false" xref="S4.T3.st3.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.st3.2.1.m1.1c"><ci id="S4.T3.st3.2.1.m1.1.1.cmml" xref="S4.T3.st3.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.st3.2.1.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.st3.2.1.m1.1e">↓</annotation></semantics></math>).</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.st3.5" style="width:433.6pt;height:142.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-110.9pt,36.5pt) scale(0.661607362264564,0.661607362264564) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.st3.5.1">
<tr class="ltx_tr" id="S4.T3.st3.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.st3.5.1.1.1">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st3.5.1.1.2">PromptTTS2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st3.5.1.1.3">NaturalSpeech3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st3.5.1.1.4">VALL-E</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st3.5.1.1.5">VoiceBox</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st3.5.1.1.6">FlashSpeech</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st3.5.1.1.7">AudioGen</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st3.5.1.1.8">xTTS</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st3.5.1.1.9">Seed-TTS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.st3.5.1.1.10">OpenAI</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.st3.5.1.1.11">Average</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.st3.5.1.2.1">LFCC-LCNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st3.5.1.2.2">48.000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st3.5.1.2.3" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.2.3.1" style="background-color:#FFE6CC;">25.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st3.5.1.2.4">37.895</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st3.5.1.2.5">15.385</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st3.5.1.2.6">29.661</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st3.5.1.2.7">54.000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st3.5.1.2.8">25.667</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st3.5.1.2.9">69.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.st3.5.1.2.10">99.333</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.st3.5.1.2.11">44.938</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.3.1">Spec.+ResNet</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.3.2">44.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.3.3">50.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.3.4">43.158</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.3.5">45.192</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.3.6">36.441</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.3.7">32.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.3.8">15.500</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.3.9">58.333</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.3.10" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.3.10.1" style="background-color:#FFE6CC;">32.167</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.3.11">39.643</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.4.1">RawNet2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.4.2">32.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.4.3">68.750</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.4.4">57.895</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.4.5">46.154</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.4.6">50.848</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.4.7">74.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.4.8">34.667</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.4.9">62.667</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.4.10">65.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.4.11">54.665</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.5.1">RawGATST</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.5.2">20.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.5.3">46.875</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.5.4">31.580</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.5.5">18.269</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.5.6">45.763</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.5.7">76.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.5.8">34.330</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.5.9">41.667</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.5.10">51.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.5.11">40.609</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.6.1">AASIST</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.6.2">16.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.6.3">46.875</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.6.4">22.105</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.6.5">12.500</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.6.6">33.898</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.6.7">31.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.6.8">27.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.6.9">34.333</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.6.10">48.500</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.6.11">30.246</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.7.1">CLAP</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.7.2">44.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.7.3">53.125</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.7.4">35.789</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.7.5">47.115</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.7.6">39.831</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.7.7">75.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.7.8">52.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.7.9">60.000</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.7.10">67.667</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.7.11">52.725</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.8.1">Whisper-small</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.8.2">12.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.8.3">43.750</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.8.4">28.421</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.8.5">25.962</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.8.6">43.220</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.8.7">20.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.8.8">19.500</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.8.9">40.167</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.8.10">81.167</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.8.11">34.910</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.9.1">Whisper-large</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.9.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.9.2.1" style="background-color:#CCFFCC;">0.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.9.3">34.375</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.9.4">21.053</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.9.5">21.154</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.9.6">27.119</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.9.7">16.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.9.8" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.9.8.1" style="background-color:#FFE6CC;">9.667</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.9.9">40.667</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.9.10">71.000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.9.11">26.782</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.10.1">Wave2Vec2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.10.2" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.10.2.1" style="background-color:#FFE6CC;">4.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.10.3">31.250</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.10.4">17.895</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.10.5" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.10.5.1" style="background-color:#FFE6CC;">6.731</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.10.6">18.644</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.10.7" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.10.7.1" style="background-color:#FFE6CC;">1.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.10.8">26.667</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.10.9" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.10.9.1" style="background-color:#FFE6CC;">13.167</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.10.10">48.333</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.10.11">18.632</td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.11.1">HuBERT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.11.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.11.2.1" style="background-color:#CCFFCC;">0.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.11.3" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.11.3.1" style="background-color:#FFE6CC;">25.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.11.4" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.11.4.1" style="background-color:#FFE6CC;">8.421</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.11.5" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.11.5.1" style="background-color:#CCFFCC;">2.885</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.11.6" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.11.6.1" style="background-color:#CCFFCC;">5.932</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.11.7" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.11.7.1" style="background-color:#CCFFCC;">0.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.11.8">12.333</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.11.9" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.11.9.1" style="background-color:#CCFFCC;">11.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.st3.5.1.11.10">43.500</td>
<td class="ltx_td ltx_align_center" id="S4.T3.st3.5.1.11.11" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.11.11.1" style="background-color:#FFE6CC;">12.119</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.st3.5.1.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.st3.5.1.12.1">Wave2Vec2BERT</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st3.5.1.12.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.12.2.1" style="background-color:#CCFFCC;">0.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st3.5.1.12.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.12.3.1" style="background-color:#CCFFCC;">9.375</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st3.5.1.12.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.12.4.1" style="background-color:#CCFFCC;">5.263</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st3.5.1.12.5" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.12.5.1" style="background-color:#CCFFCC;">2.885</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st3.5.1.12.6" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T3.st3.5.1.12.6.1" style="background-color:#FFE6CC;">7.627</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st3.5.1.12.7">3.000</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st3.5.1.12.8" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.12.8.1" style="background-color:#CCFFCC;">1.333</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st3.5.1.12.9">39.833</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.st3.5.1.12.10" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.12.10.1" style="background-color:#CCFFCC;">21.667</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.st3.5.1.12.11" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T3.st3.5.1.12.11.1" style="background-color:#CCFFCC;">10.109</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p2.1.1">Speech foundation models can better generalize on the SONAR dataset, but still not good enough.</span> As presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T3.st1" title="In Table 3 ‣ 4.2.2 Results on SONAR dataset ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">3(a)</span></a>, speech foundation models again exhibit better generalizability on the fake audio samples generated by the latest TTS models. For instance, AASIST achieves 0.6975 average accuracy across audios generated by cutting-edge TTS models, which is the best performance among the traditional detection models. In contrast, speech foundation models Whisper-large, Wave2Vec2, HuBERT, and Wave2Vec2BERT achieve an average accuracy of 0.7322, 0.788, 0.8789, and 0.8989, respectively, which is higher than AASIST by 3.47%, 9.05%, 18.14%, and 20.14%, respectively. More specifically, even though Wave2Vec2BERT and HuBERT are only fine-tuned on Wavefake dataset, for PromptTTS2, VALL-E, VoiceBox, FalshSpeech, AudioGen, and xTTS, Wave2Vec2BERT can reach accuracies of 1.0, 0.9062, 0.9474, 0.9712, 0.9237, 0.97, and 0.9867, respectively, and HuBERT can achieve 1.0, 0.9158, 0.9712, 0.9407, 1.0 0.8767, and 0.89, respectively, demonstrating their potential capability of extract more distinguishable features compared to other models. It is also worth noting that Wave2Vec2BERT achieves an accuracy of 0.9062 on NaturalSpeech3, while all other models can only reach that <math alttext="\leq 0.75" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.1.m1.1"><semantics id="S4.SS2.SSS2.p2.1.m1.1a"><mrow id="S4.SS2.SSS2.p2.1.m1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS2.p2.1.m1.1.1.2" xref="S4.SS2.SSS2.p2.1.m1.1.1.2.cmml"></mi><mo id="S4.SS2.SSS2.p2.1.m1.1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.cmml">≤</mo><mn id="S4.SS2.SSS2.p2.1.m1.1.1.3" xref="S4.SS2.SSS2.p2.1.m1.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.1.m1.1b"><apply id="S4.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1"><leq id="S4.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1"></leq><csymbol cd="latexml" id="S4.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.2">absent</csymbol><cn id="S4.SS2.SSS2.p2.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.SSS2.p2.1.m1.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.1.m1.1c">\leq 0.75</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.1.m1.1d">≤ 0.75</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p3.1.1">It is still challenging for detection models to correctly classify synthesized audio samples, especially those generated by the most advanced TTS service providers.</span> While Wave2Vec2BERT achieves an overall average accuracy of 0.8989, it only reaches 0.6017 on Seed-TTS and 0.7833 on OpenAI. A similar pattern is also evident with HuBERT, Wave2Vec2, Whisper-large, and Whisper-small, which achieve just 0.5658, 0.4342, 0.29, and 0.1883 accuracy on OpenAI, respectively. This performance disparity is likely due to OpenAI and Seed-TTS having more advanced model architectures and being trained on proprietary, self-collected data, leading to higher-quality and more realistic speech generation. We will explore potential strategies to enhance their detection performance in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.SS2.SSS4" title="4.2.4 On the effectiveness and efficiency of few-shot fine-tuning to improve generalization ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">4.2.4</span></a>. Overall, these results not only indicate that no single model consistently outperforms across all datasets but also underscore the ongoing difficulty in detecting synthesized audio from cutting-edge TTS systems, especially those developed by the most advanced TTS service providers. This highlights a huge gap between the rapid evolution of TTS technologies and the effectiveness of current audio deepfake detection methods, emphasizing the urgent need for the development of more robust and reliable detection algorithms.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1">Additionally, it is noteworthy that, compared to speech foundation models, the accuracy of all five traditional detection models on the AudioGen dataset, which consists of synthesized environmental sounds, remains relatively low. Specifically, LFCC-LCNN, Spec.+ResNet, RawNet2, RawGATST, and AASIST achieve accuracies of 0.46, 0.68, 0.26, 0.24, and 0.69, respectively. In contrast, Whisper-small, Whisper-large, Wave2Vec2, HuBERT, and Wave2VecBERT attain significantly higher accuracies of 0.8, 0.84, 0.99, 1.0, and 0.97, respectively. This discrepancy may be due to traditional detection models being trained exclusively on speech data, which limits their generalization to audio from different distributions. In comparison, foundation models demonstrate greater robustness to out-of-distribution audio samples. An exception to this is CLAP, which is an audio foundation model pre-trained on a variety of environmental audio-text pairs and only achieves an accuracy of 0.25 on AudioGen. Similar to previous results, it’s possibly due to the fact that its full-weight fine-tuning on speech data may have compromised its ability to effectively recognize environmental sounds, resulting in poor performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Can generalizability increase with model size?</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Building on the observation that Whisper-large consistently outperforms Whisper-small, we extend our analysis with controlled experiments on the entire Whisper model family. Specifically, the Whisper family comprises five different model sizes: Whisper-tiny, Whisper-base, Whisper-small, Whisper-medium, and Whisper-large. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T4" title="Table 4 ‣ 4.2.3 Can generalizability increase with model size? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">4</span></a> presents the number of model parameters of them. Specifically, each model is fine-tuned on the Wavefake training dataset using the same hyperparameters. Our results show that as model size increases, the generalizability of the models improves as well.</p>
</div>
<figure class="ltx_table ltx_align_floatright" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.2.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.3.2" style="font-size:90%;">Whisper model sizes.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.4">
<tr class="ltx_tr" id="S4.T4.4.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.4.1.1">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.4.1.2">#Params</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.2.1">Whisper-tiny</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.2.2">39M</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.3.1">Whisper-base</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.3.2">74M</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.4.1">Whisper-small</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.2">244M</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.5.1">Whisper-medium</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.5.2">769M</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.4.6.1">Whisper-large</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.6.2">1550M</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T5" title="Table 5 ‣ 4.2.3 Can generalizability increase with model size? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">5</span></a> presents the detection performance of the Whisper models across the Wavefake, LibriSeVoc, and In-the-wild datasets. First, Whisper-tiny, despite its smaller size, still outperforms or achieves comparable detection performance to traditional detection models (recall Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T2" title="Table 2 ‣ 4.2.1 How well can detection models generalize across datasets? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">2</span></a>) on the LibriSeVoc test set. This again validates the finding that foundation models exhibit stronger generalizability for audio deepfake detection tasks, even in their smallest configurations.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.2.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S4.T5.3.2" style="font-size:90%;">Generalization across existing audio deepfake datasets. All Whisper models are trained/finetuned on the Wavefake training set. Green and orange indicate the best and second-best performance, respectively.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.4" style="width:433.6pt;height:100.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.1pt,12.6pt) scale(0.800281838959787,0.800281838959787) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.4.1">
<tr class="ltx_tr" id="S4.T5.4.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T5.4.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T5.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.1.2.1">Wavefake</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T5.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.1.3.1">LibriSeVoc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T5.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.1.4.1">In-the-wild</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.1.1">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.2.2"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.2.1">AUROC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.2.3"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.3.1">EER(%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.2.4"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.4.1">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.2.5"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.5.1">AUROC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.2.6"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.6.1">EER(%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.2.7"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.7.1">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.2.8"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.8.1">AUROC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.2.9"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.9.1">EER(%)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.3.1">Whisper-tiny</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.3.2">0.9839</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.3.3">0.9985</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.3.4">1.603</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.3.5">0.8557</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.3.6">0.9307</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.1.3.7">14.426</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.3.8">0.498</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.3.9">0.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.1.3.10">50.203</td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.1.4.1">Whisper-base</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.4.2">0.9908</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.4.3">0.9996</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.1.4.4">0.916</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.4.5">0.9163</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.4.6">0.9734</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.1.4.7">8.368</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.4.8">0.7398</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.4.9">0.8124</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.4.10">26.024</td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.1.5.1">Whisper-small</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.5.2" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T5.4.1.5.2.1" style="background-color:#FFE6CC;">0.9935</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.5.3" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T5.4.1.5.3.1" style="background-color:#FFE6CC;">0.9997</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.1.5.4" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T5.4.1.5.4.1" style="background-color:#FFE6CC;">0.649</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.5.5">0.9345</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.5.6">0.9837</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.1.5.7">6.551</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.5.8">0.821</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.5.9">0.9025</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.5.10">17.899</td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.1.6.1">Whisper-medium</td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.6.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.6.2.1" style="background-color:#CCFFCC;">0.9962</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.6.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.6.3.1" style="background-color:#CCFFCC;">0.9999</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.1.6.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.6.4.1" style="background-color:#CCFFCC;">0.381</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.6.5" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T5.4.1.6.5.1" style="background-color:#FFE6CC;">0.944</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.6.6" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T5.4.1.6.6.1" style="background-color:#FFE6CC;">0.985</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.1.6.7" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T5.4.1.6.7.1" style="background-color:#FFE6CC;">5.604</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.6.8" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T5.4.1.6.8.1" style="background-color:#FFE6CC;">0.8572</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.6.9" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T5.4.1.6.9.1" style="background-color:#FFE6CC;">0.9288</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.4.1.6.10" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T5.4.1.6.10.1" style="background-color:#FFE6CC;">14.277</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.4.1.7.1">Whisper-large</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.4.1.7.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.7.2.1" style="background-color:#CCFFCC;">0.9962</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.4.1.7.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.7.3.1" style="background-color:#CCFFCC;">0.9992</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.4.1.7.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.7.4.1" style="background-color:#CCFFCC;">0.381</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.4.1.7.5" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.7.5.1" style="background-color:#CCFFCC;">0.9572</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.4.1.7.6" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.7.6.1" style="background-color:#CCFFCC;">0.9901</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.4.1.7.7" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.7.7.1" style="background-color:#CCFFCC;">4.279</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.4.1.7.8" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.7.8.1" style="background-color:#CCFFCC;">0.8848</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.4.1.7.9" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.7.9.1" style="background-color:#CCFFCC;">0.9552</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.4.1.7.10" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T5.4.1.7.10.1" style="background-color:#CCFFCC;">11.518</span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.2.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S4.T6.3.2" style="font-size:90%;">Evaluation on SONAR dataset. Green and orange indicate the best and second-best performance, respectively.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S4.T6.st1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.st1.4.2.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.T6.st1.2.1" style="font-size:90%;">Accuracy (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.st1.2.1.m1.1"><semantics id="S4.T6.st1.2.1.m1.1b"><mo id="S4.T6.st1.2.1.m1.1.1" stretchy="false" xref="S4.T6.st1.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.st1.2.1.m1.1c"><ci id="S4.T6.st1.2.1.m1.1.1.cmml" xref="S4.T6.st1.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.st1.2.1.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.st1.2.1.m1.1e">↑</annotation></semantics></math>).</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.st1.5" style="width:433.6pt;height:71.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-110.0pt,18.2pt) scale(0.66335042652046,0.66335042652046) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.st1.5.1">
<tr class="ltx_tr" id="S4.T6.st1.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.st1.5.1.1.1">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st1.5.1.1.2">PromptTTS2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st1.5.1.1.3">NaturalSpeech3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st1.5.1.1.4">VALL-E</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st1.5.1.1.5">VoiceBox</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st1.5.1.1.6">FlashSpeech</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st1.5.1.1.7">AudioGen</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st1.5.1.1.8">xTTS</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st1.5.1.1.9">Seed-TTS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.st1.5.1.1.10">OpenAI</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st1.5.1.1.11">Average</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st1.5.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.st1.5.1.2.1">Whisper-tiny</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st1.5.1.2.2">0.8000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st1.5.1.2.3">0.3438</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st1.5.1.2.4">0.6947</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st1.5.1.2.5">0.6442</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st1.5.1.2.6">0.4661</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st1.5.1.2.7">0.73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st1.5.1.2.8">0.6517</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st1.5.1.2.9">0.5067</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.st1.5.1.2.10">0.0833</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st1.5.1.2.11">0.5467</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st1.5.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st1.5.1.3.1">Whisper-base</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.3.2">0.8400</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.3.3">0.4375</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.3.4">0.6947</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.3.5">0.6731</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.3.6">0.6017</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.3.7">0.6800</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.3.8">0.6550</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.3.9">0.4800</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st1.5.1.3.10">0.1117</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.3.11">0.5749</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st1.5.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st1.5.1.4.1">Whisper-small</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.4.2">0.8800</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.4.3">0.5625</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.4.4" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.4.4.1" style="background-color:#FFE6CC;">0.7158</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.4.5">0.7404</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.4.6">0.5678</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.4.7" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.4.7.1" style="background-color:#FFE6CC;">0.8000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.4.8">0.8050</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.4.9" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.4.9.1" style="background-color:#CCFFCC;">0.5983</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st1.5.1.4.10">0.1883</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.4.11">0.6509</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st1.5.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st1.5.1.5.1">Whisper-medium</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.5.2" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.5.2.1" style="background-color:#FFE6CC;">0.96</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.5.3" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.5.3.1" style="background-color:#FFE6CC;">0.6250</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.5.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.5.4.1" style="background-color:#CCFFCC;">0.7895</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.5.5" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.5.5.1" style="background-color:#CCFFCC;">0.8077</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.5.6" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.5.6.1" style="background-color:#FFE6CC;">0.7119</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.5.7" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.5.7.1" style="background-color:#FFE6CC;">0.8000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.5.8" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.5.8.1" style="background-color:#FFE6CC;">0.8400</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.5.9">0.5517</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st1.5.1.5.10" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.5.10.1" style="background-color:#FFE6CC;">0.2183</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st1.5.1.5.11" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.5.11.1" style="background-color:#FFE6CC;">0.7005</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.st1.5.1.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.st1.5.1.6.1">Whisper-large</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st1.5.1.6.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.6.2.1" style="background-color:#CCFFCC;">1.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st1.5.1.6.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.6.3.1" style="background-color:#CCFFCC;">0.6562</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st1.5.1.6.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.6.4.1" style="background-color:#CCFFCC;">0.7895</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st1.5.1.6.5" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.6.5.1" style="background-color:#FFE6CC;">0.7885</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st1.5.1.6.6" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.6.6.1" style="background-color:#CCFFCC;">0.7288</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st1.5.1.6.7" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.6.7.1" style="background-color:#CCFFCC;">0.8400</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st1.5.1.6.8" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.6.8.1" style="background-color:#CCFFCC;">0.9033</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st1.5.1.6.9" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st1.5.1.6.9.1" style="background-color:#FFE6CC;">0.5933</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.st1.5.1.6.10" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.6.10.1" style="background-color:#CCFFCC;">0.2900</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st1.5.1.6.11" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st1.5.1.6.11.1" style="background-color:#CCFFCC;">0.7322</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S4.T6.st2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.st2.4.2.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.T6.st2.2.1" style="font-size:90%;">AUROC (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.st2.2.1.m1.1"><semantics id="S4.T6.st2.2.1.m1.1b"><mo id="S4.T6.st2.2.1.m1.1.1" stretchy="false" xref="S4.T6.st2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.st2.2.1.m1.1c"><ci id="S4.T6.st2.2.1.m1.1.1.cmml" xref="S4.T6.st2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.st2.2.1.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.st2.2.1.m1.1e">↑</annotation></semantics></math>).</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.st2.5" style="width:433.6pt;height:71.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-110.0pt,18.2pt) scale(0.66335042652046,0.66335042652046) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.st2.5.1">
<tr class="ltx_tr" id="S4.T6.st2.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.st2.5.1.1.1">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st2.5.1.1.2">PromptTTS2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st2.5.1.1.3">NaturalSpeech3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st2.5.1.1.4">VALL-E</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st2.5.1.1.5">VoiceBox</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st2.5.1.1.6">FlashSpeech</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st2.5.1.1.7">AudioGen</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st2.5.1.1.8">xTTS</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st2.5.1.1.9">Seed-TTS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.st2.5.1.1.10">OpenAI</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st2.5.1.1.11">Average</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st2.5.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.st2.5.1.2.1">Whisper-tiny</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st2.5.1.2.2">0.9136</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st2.5.1.2.3">0.2998</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st2.5.1.2.4">0.7436</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st2.5.1.2.5">0.7144</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st2.5.1.2.6">0.4886</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st2.5.1.2.7">0.7660</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st2.5.1.2.8">0.7239</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st2.5.1.2.9">0.5033</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.st2.5.1.2.10">0.0454</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st2.5.1.2.11">0.5776</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st2.5.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st2.5.1.3.1">Whisper-base</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.3.2">0.9296</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.3.3">0.4326</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.3.4">0.7512</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.3.5">0.7482</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.3.6">0.6548</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.3.7">0.7505</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.3.8">0.7167</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.3.9">0.5152</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st2.5.1.3.10">0.041</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.3.11">0.6155</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st2.5.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st2.5.1.4.1">Whisper-small</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.4.2">0.9776</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.4.3">0.5762</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.4.4">0.8050</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.4.5">0.8400</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.4.6">0.6446</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.4.7">0.8284</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.4.8">0.8915</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.4.9" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st2.5.1.4.9.1" style="background-color:#FFE6CC;">0.6326</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st2.5.1.4.10">0.108</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.4.11">0.7004</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st2.5.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st2.5.1.5.1">Whisper-medium</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.5.2" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st2.5.1.5.2.1" style="background-color:#FFE6CC;">0.9984</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.5.3" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st2.5.1.5.3.1" style="background-color:#FFE6CC;">0.6279</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.5.4" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st2.5.1.5.4.1" style="background-color:#FFE6CC;">0.886</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.5.5" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st2.5.1.5.5.1" style="background-color:#CCFFCC;">0.8578</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.5.6" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st2.5.1.5.6.1" style="background-color:#CCFFCC;">0.7950</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.5.7" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st2.5.1.5.7.1" style="background-color:#FFE6CC;">0.8640</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.5.8" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st2.5.1.5.8.1" style="background-color:#FFE6CC;">0.9215</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.5.9">0.5858</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st2.5.1.5.10" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st2.5.1.5.10.1" style="background-color:#FFE6CC;">0.1567</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st2.5.1.5.11" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st2.5.1.5.11.1" style="background-color:#FFE6CC;">0.7437</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.st2.5.1.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.st2.5.1.6.1">Whisper-large</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st2.5.1.6.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st2.5.1.6.2.1" style="background-color:#CCFFCC;">1.0000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st2.5.1.6.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st2.5.1.6.3.1" style="background-color:#CCFFCC;">0.6992</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st2.5.1.6.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st2.5.1.6.4.1" style="background-color:#CCFFCC;">0.9063</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st2.5.1.6.5" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st2.5.1.6.5.1" style="background-color:#FFE6CC;">0.8552</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st2.5.1.6.6" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st2.5.1.6.6.1" style="background-color:#FFE6CC;">0.7933</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st2.5.1.6.7" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st2.5.1.6.7.1" style="background-color:#CCFFCC;">0.8926</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st2.5.1.6.8" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st2.5.1.6.8.1" style="background-color:#CCFFCC;">0.969</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st2.5.1.6.9" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st2.5.1.6.9.1" style="background-color:#CCFFCC;">0.6558</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.st2.5.1.6.10" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st2.5.1.6.10.1" style="background-color:#CCFFCC;">0.2327</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st2.5.1.6.11" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st2.5.1.6.11.1" style="background-color:#CCFFCC;">0.7782</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S4.T6.st3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.st3.4.2.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S4.T6.st3.2.1" style="font-size:90%;">EER(%) (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T6.st3.2.1.m1.1"><semantics id="S4.T6.st3.2.1.m1.1b"><mo id="S4.T6.st3.2.1.m1.1.1" stretchy="false" xref="S4.T6.st3.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.st3.2.1.m1.1c"><ci id="S4.T6.st3.2.1.m1.1.1.cmml" xref="S4.T6.st3.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.st3.2.1.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.st3.2.1.m1.1e">↓</annotation></semantics></math>).</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.st3.5" style="width:433.6pt;height:71.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-110.0pt,18.2pt) scale(0.66335042652046,0.66335042652046) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.st3.5.1">
<tr class="ltx_tr" id="S4.T6.st3.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.st3.5.1.1.1">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st3.5.1.1.2">PromptTTS2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st3.5.1.1.3">NaturalSpeech3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st3.5.1.1.4">VALL-E</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st3.5.1.1.5">VoiceBox</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st3.5.1.1.6">FlashSpeech</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st3.5.1.1.7">AudioGen</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st3.5.1.1.8">xTTS</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st3.5.1.1.9">Seed-TTS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.st3.5.1.1.10">OpenAI</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.st3.5.1.1.11">Average</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st3.5.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.st3.5.1.2.1">Whisper-tiny</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st3.5.1.2.2">20.000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st3.5.1.2.3">65.625</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st3.5.1.2.4">30.526</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st3.5.1.2.5">35.577</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st3.5.1.2.6">53.390</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st3.5.1.2.7">27.000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st3.5.1.2.8">34.833</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st3.5.1.2.9">49.333</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.st3.5.1.2.10">91.667</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.st3.5.1.2.11">45.328</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st3.5.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st3.5.1.3.1">Whisper-base</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.3.2">16.000</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.3.3">56.250</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.3.4">30.526</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.3.5">32.692</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.3.6">36.831</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.3.7">32.000</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.3.8">34.500</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.3.9">52.000</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st3.5.1.3.10">88.833</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.3.11">42.811</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st3.5.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st3.5.1.4.1">Whisper-small</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.4.2">12.000</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.4.3">43.750</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.4.4" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.4.4.1" style="background-color:#FFE6CC;">28.421</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.4.5">25.962</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.4.6">43.220</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.4.7" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.4.7.1" style="background-color:#FFE6CC;">20.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.4.8">19.500</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.4.9" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.4.9.1" style="background-color:#FFE6CC;">40.667</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st3.5.1.4.10">81.167</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.4.11">34.965</td>
</tr>
<tr class="ltx_tr" id="S4.T6.st3.5.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st3.5.1.5.1">Whisper-medium</td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.5.2" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.5.2.1" style="background-color:#FFE6CC;">4.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.5.3" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.5.3.1" style="background-color:#FFE6CC;">37.500</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.5.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.5.4.1" style="background-color:#CCFFCC;">21.053</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.5.5" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.5.5.1" style="background-color:#CCFFCC;">19.231</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.5.6" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.5.6.1" style="background-color:#FFE6CC;">28.814</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.5.7" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.5.7.1" style="background-color:#FFE6CC;">20.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.5.8" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.5.8.1" style="background-color:#FFE6CC;">16.000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.5.9">44.833</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.st3.5.1.5.10" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.5.10.1" style="background-color:#FFE6CC;">78.167</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.st3.5.1.5.11" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.5.11.1" style="background-color:#FFE6CC;">29.955</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.st3.5.1.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.st3.5.1.6.1">Whisper-large</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st3.5.1.6.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.6.2.1" style="background-color:#CCFFCC;">0.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st3.5.1.6.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.6.3.1" style="background-color:#CCFFCC;">34.375</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st3.5.1.6.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.6.4.1" style="background-color:#CCFFCC;">21.053</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st3.5.1.6.5" style="background-color:#FFE6CC;"><span class="ltx_text" id="S4.T6.st3.5.1.6.5.1" style="background-color:#FFE6CC;">21.154</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st3.5.1.6.6" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.6.6.1" style="background-color:#CCFFCC;">27.119</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st3.5.1.6.7" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.6.7.1" style="background-color:#CCFFCC;">16.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st3.5.1.6.8" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.6.8.1" style="background-color:#CCFFCC;">9.667</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st3.5.1.6.9" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.6.9.1" style="background-color:#CCFFCC;">40.167</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.st3.5.1.6.10" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.6.10.1" style="background-color:#CCFFCC;">71.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.st3.5.1.6.11" style="background-color:#CCFFCC;"><span class="ltx_text" id="S4.T6.st3.5.1.6.11.1" style="background-color:#CCFFCC;">26.726</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">Second, as the model size increases from Whisper-tiny to Whisper-large, both accuracy and AUROC improve significantly across the LibriSeVoc and In-the-wild datasets. Whisper-large achieves an accuracy of 95.72% and an AUROC of 0.9901 on LibriSeVoc, surpassing Whisper-tiny by 10.07% in accuracy. A more evident pattern can be observed on the In-the-wild dataset, where Whisper-large outperforms Whisper-tiny by 38.48% in accuracy. Furthermore, the Equal Error Rate (EER) decreases as the model size increases, indicating that larger models are not only more accurate but also better at minimizing both false positives and false negatives.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.1">We also evaluate the Whisper family on the proposed SONAR dataset. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T6.st1" title="In Table 6 ‣ 4.2.3 Can generalizability increase with model size? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">6(a)</span></a>, Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T6.st2" title="In Table 6 ‣ 4.2.3 Can generalizability increase with model size? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">6(b)</span></a>, Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T6.st3" title="In Table 6 ‣ 4.2.3 Can generalizability increase with model size? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">6(c)</span></a> present the corresponding Accuracy, AUROC, and EER(%). A similar trend can also be observed. In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.T6.st1" title="In Table 6 ‣ 4.2.3 Can generalizability increase with model size? ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">6(a)</span></a>, the accuracy of the Whisper models shows a clear upward trend as the model size increases from Whisper-tiny to Whisper-large. Whisper-tiny achieves an average accuracy of 0.5467, while Whisper-large reaches the highest average accuracy of 0.7322. Notably, Whisper-large performs best on almost all datasets, particularly with TTS models such as PromptTTS2, NaturalSpeech3, VALL-E, and OpenAI, highlighting its better generalizability. Additionally, Whisper-large’s performance is higher on challenging datasets like Seed-TTS and OpenAI, which are known for their high-quality synthesis. The smaller models (e.g., Whisper-tiny and Whisper-base), on the other hand, struggle to generalize effectively, particularly on datasets such as OpenAI, where the accuracy drops to 0.0833 for Whisper-tiny.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p5">
<p class="ltx_p" id="S4.SS2.SSS3.p5.1">The results highlight the scalability of the Whisper models: larger models demonstrate better generalization across diverse test sets, underscoring the importance of model capacity in tackling challenging out-of-distribution data, such as audio generated by advanced TTS models.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>On the effectiveness and efficiency of few-shot fine-tuning to improve generalization</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">Despite the challenges in generalizing across different datasets, we investigate whether there exist efficient solutions that can enhance models’ detection performance on those challenging subsets from SONAR dataset. To this end, we conduct a case study on Wave2Vec2BERT and HuBERT, as these models perform relatively poorly on the OpenAI and SeedTTS datasets but demonstrate competitive performance on other subsets. Specifically, we generate 100 additional fake audio samples using the OpenAI TTS API and randomly select another 100 fake audio samples from the SeedTTS test set for few-shot fine-tuning. Our study yields several interesting findings.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS4.p2">
<p class="ltx_p" id="S4.SS2.SSS4.p2.1">Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.F2.sf1" title="In Figure 2 ‣ 4.2.4 On the effectiveness and efficiency of few-shot fine-tuning to improve generalization ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">2(a)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.F2.sf2" title="In Figure 2 ‣ 4.2.4 On the effectiveness and efficiency of few-shot fine-tuning to improve generalization ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">2(b)</span></a> present the results of fine-tuning Wave2Vec2BERT and HuBERT using varying numbers of samples from OpenAI. Before fine-tuning, Wave2Vec2BERT and HuBERT only achieve accuracies of 0.7833 and 0.5658, respectively. Notably, with only 10 shots of fake speech data, Wave2Vec2BERT reaches an accuracy of approximately 0.97, while HuBERT’s accuracy increases significantly to approximately 0.85. Importantly, the models’ generalization to other datasets remains unchanged, demonstrating the effectiveness and efficiency of few-shot fine-tuning. However, as the number of fine-tuning samples increases, HuBERT’s test accuracy on the WaveFake test set shows a declining trend, which is also observed for Wave2Vec2BERT.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS4.p3">
<p class="ltx_p" id="S4.SS2.SSS4.p3.1">It is important to note, however, that the efficiency and effectiveness of few-shot fine-tuning may vary across different datasets. As illustrated in Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.F2.sf3" title="In Figure 2 ‣ 4.2.4 On the effectiveness and efficiency of few-shot fine-tuning to improve generalization ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">2(c)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#S4.F2.sf4" title="In Figure 2 ‣ 4.2.4 On the effectiveness and efficiency of few-shot fine-tuning to improve generalization ‣ 4.2 Results and analysis ‣ 4 Benchmarking AI-Audio Detection Models ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">2(d)</span></a>, which depict the fine-tuning results for Wave2Vec2BERT and HuBERT on Seed-TTS, the improvement in accuracy is less pronounced compared to the results on the OpenAI dataset. While the accuracy of both Wave2Vec2BERT and HuBERT does improve on Seed-TTS, the gains are not as significant as those observed for the OpenAI dataset. Additionally, the detection performance on other datasets decreases more noticeably when fine-tuning on Seed-TTS compared to OpenAI.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS4.p4">
<p class="ltx_p" id="S4.SS2.SSS4.p4.1">These findings suggest that the effectiveness of few-shot fine-tuning may depend on the specific characteristics of the dataset. Moreover, this also highlights its potential for tailored applications, such as personalized detection systems for a specific entity or individual, to enable more customized and practical applications.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="447" id="S4.F2.sf1.g1" src="extracted/5915436/wave2vec2bert_openai.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="458" id="S4.F2.sf2.g1" src="extracted/5915436/hubert_openai.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="458" id="S4.F2.sf3.g1" src="extracted/5915436/wave2vec2bert_seedtts.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F2.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="455" id="S4.F2.sf4.g1" src="extracted/5915436/hubert_seedtts.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.sf4.2.1.1" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">Performance of few-shot fine-tuning for Wave2Vec2BERT and HuBERT with a varying number of few-shot audio samples from OpenAI and Seed-TTS, respectively. (a) Fine-tune Wave2Vec2BERT on OpenAI. (b) Fine-tune HuBERT on OpenAI; (c) Fine-tune Wave2Vec2BERT on Seed-TTS; and (d) Fine-tune HuBERT on Seed-TTS.</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">AI-synthetized audio detection methods must be evaluated on diverse and advanced benchmarks.</span> In our evaluation using the proposed dataset, most models perform well on standard TTS tools but suffer significant degradation when tested on the fake audios generated by the most advanced tool such as Voice Engine released by OpenAI. Therefore, we advocate for future research in audio deepfake detection to prioritize benchmarking against the latest and most advanced TTS technologies, which will lead to more robust and reliable detectors, as relying on high detection rates from outdated tools may create a false sense of generalization. Additionally, there is an urgent need to develop larger-scale training datasets comprising fake audio generated by cutting-edge TTS models to keep pace with rapid advancements in TTS technology and mitigate associated risks.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Limitations and future work</span>. While our primary goal in proposing this dataset is to facilitate comprehensive evaluation, it remains relatively small in size and is primarily focused on English. A more in-depth analysis of detection performance across different languages and gender representations is crucial for a more comprehensive evaluation. These aspects are essential for future research to enhance the dataset’s applicability and generalizability. For future work, we also plan to: (1) incorporate additional AI-audio detection models, including those targeting advanced audio editing techniques designed to bypass detection systems; (2) explore innovative methods to further improve generalizability; and (3) address realistic challenges and risks in deploying the proposed method in real-world scenarios, such as evaluating the robustness of models against common or adversarial corruptions. These efforts will contribute to the development of more effective strategies to combat AI-generated audio threats.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Data license considerations.</span>. Since our dataset is sourced from various models, each may be subject to different distribution licenses and usage restrictions. Throughout the data collection process, we ensured strict adherence to all relevant usage policies. We have also made the dataset accessible to everyone, either directly via the provided link or indirectly through the original sources. However, as these policies are frequently refined and updated, we will ensure that our published dataset remains in full compliance with the latest regulations. Additionally, we will reference the usage policies of the respective API providers to ensure that users are informed of any potential restrictions.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we presented SONAR, a framework providing a comprehensive evaluation for distinguishing state-of-the-art AI-synthesized auditory content.
SONAR introduces a novel evaluation dataset sourced from 9 diverse audio synthesis platforms, including leading TTS service providers and state-of-the-art TTS models. To the best of our knowledge, SONAR is the first platform that provides uniform, comprehensive, informative, and extensible evaluation of deepfake audio detection models. Leveraging SONAR, we conducted extensive experiments to analyze the generalizability limitations of current detection methods. We found that foundation models demonstrate stronger generalization capabilities, given their massive model size scale and pertaining data. We further explored the potential of few-shot fine-tuning to improve generalization and demonstrated its efficiency and effectiveness. We envision that SONAR will serve as a valuable benchmark to facilitate research in AI-audio detection and highlight directions for further improvement.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments and Disclosure of Funding</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The authors thank the partial support from Fordham-IBM Research Fellowship and Fordham Faculty Research Grant.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang, Robert Adkins, William Ngan, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">Audiobox: Unified audio generation with natural language prompts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">arXiv preprint arXiv:2312.15821</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Zhen Ye, Zeqian Ju, Haohe Liu, Xu Tan, Jianyi Chen, Yiwen Lu, Peiwen Sun, Jiahao Pan, Weizhen Bian, Shulin He, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">Flashspeech: Efficient zero-shot speech synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.3.1" style="font-size:90%;">arXiv preprint arXiv:2404.14700</span><span class="ltx_text" id="bib.bib2.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Edresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren Gölge, and Moacir A Ponti.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib3.4.2" style="font-size:90%;">International Conference on Machine Learning</span><span class="ltx_text" id="bib.bib3.5.3" style="font-size:90%;">, pages 2709–2720. PMLR, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Neural codec language models are zero-shot text to speech synthesizers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">arXiv preprint arXiv:2301.02111</span><span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Nicolas M Müller, Pavel Czempin, Franziska Dieckmann, Adam Froghyar, and Konstantin Böttinger.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Does audio deepfake detection generalize?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.3.1" style="font-size:90%;">arXiv preprint arXiv:2203.16263</span><span class="ltx_text" id="bib.bib5.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Yongyi Zang, You Zhang, Mojtaba Heydari, and Zhiyao Duan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Singfake: Singing voice deepfake detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib6.4.2" style="font-size:90%;">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib6.5.3" style="font-size:90%;">, pages 12156–12160. IEEE, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Jee-weon Jung, Hee-Soo Heo, Hemlata Tak, Hye-jin Shim, Joon Son Chung, Bong-Jin Lee, Ha-Jin Yu, and Nicholas Evans.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Aasist: Audio anti-spoofing using integrated spectro-temporal graph attention networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib7.4.2" style="font-size:90%;">ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP)</span><span class="ltx_text" id="bib.bib7.5.3" style="font-size:90%;">, pages 6367–6371. IEEE, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Hemlata Tak, Jose Patino, Massimiliano Todisco, Andreas Nautsch, Nicholas Evans, and Anthony Larcher.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">End-to-end anti-spoofing with rawnet2.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib8.4.2" style="font-size:90%;">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib8.5.3" style="font-size:90%;">, pages 6369–6373. IEEE, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Hemlata Tak, Jee-weon Jung, Jose Patino, Madhu Kamble, Massimiliano Todisco, and Nicholas Evans.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">End-to-end spectro-temporal graph attention networks for speaker verification anti-spoofing and speech deepfake detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">arXiv preprint arXiv:2107.12710</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Galina Lavrentyeva, Sergey Novoselov, Andzhukaev Tseren, Marina Volkova, Artem Gorlanov, and Alexandr Kozlov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Stc antispoofing systems for the asvspoof2019 challenge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.3.1" style="font-size:90%;">arXiv preprint arXiv:1904.05576</span><span class="ltx_text" id="bib.bib10.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Voicebox: Text-guided multilingual universal speech generation at scale.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib12.4.2" style="font-size:90%;">2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</span><span class="ltx_text" id="bib.bib12.5.3" style="font-size:90%;">, pages 4779–4783. IEEE, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Fastspeech: Fast, robust and controllable text to speech.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Neural speech synthesis with transformer network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib14.4.2" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</span><span class="ltx_text" id="bib.bib14.5.3" style="font-size:90%;">, volume 33, pages 6706–6713, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Jaehyeon Kim, Jungil Kong, and Juhee Son.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib15.4.2" style="font-size:90%;">International Conference on Machine Learning</span><span class="ltx_text" id="bib.bib15.5.3" style="font-size:90%;">, pages 5530–5540. PMLR, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Yanqing Liu, Ruiqing Xue, Lei He, Xu Tan, and Sheng Zhao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Delightfultts 2: End-to-end speech synthesis with adversarial vector-quantized auto-encoders.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.3.1" style="font-size:90%;">arXiv preprint arXiv:2207.04646</span><span class="ltx_text" id="bib.bib16.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin Wang, Nanxin Chen, and Junichi Yamagishi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, pages 6184–6188. IEEE, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Zhizheng Wu, Junichi Yamagishi, Tomi Kinnunen, Cemal Hanilçi, Mohammed Sahidullah, Aleksandr Sizov, Nicholas Evans, Massimiliano Todisco, and Hector Delgado.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Asvspoof: the automatic speaker verification spoofing and countermeasures challenge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.3.1" style="font-size:90%;">IEEE Journal of Selected Topics in Signal Processing</span><span class="ltx_text" id="bib.bib18.4.2" style="font-size:90%;">, 11(4):588–604, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Andreas Nautsch, Xin Wang, Nicholas Evans, Tomi H Kinnunen, Ville Vestman, Massimiliano Todisco, Héctor Delgado, Md Sahidullah, Junichi Yamagishi, and Kong Aik Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Asvspoof 2019: spoofing countermeasures for the detection of synthesized, converted and replayed speech.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.3.1" style="font-size:90%;">IEEE Transactions on Biometrics, Behavior, and Identity Science</span><span class="ltx_text" id="bib.bib19.4.2" style="font-size:90%;">, 3(2):252–265, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Massimiliano Todisco, Xin Wang, Ville Vestman, Md Sahidullah, Héctor Delgado, Andreas Nautsch, Junichi Yamagishi, Nicholas Evans, Tomi Kinnunen, and Kong Aik Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Asvspoof 2019: Future horizons in spoofed and fake audio detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.3.1" style="font-size:90%;">arXiv preprint arXiv:1904.05441</span><span class="ltx_text" id="bib.bib20.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Joel Frank and Lea Schönherr.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Wavefake: A data set to facilitate audio deepfake detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.3.1" style="font-size:90%;">arXiv preprint arXiv:2111.02813</span><span class="ltx_text" id="bib.bib21.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Chengzhe Sun, Shan Jia, Shuwei Hou, and Siwei Lyu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Ai-synthesized voice detection using neural vocoder artifacts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">, pages 904–912, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Xin Wang and Junichi Yamagishi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Investigating self-supervised front ends for speech spoofing countermeasures.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.3.1" style="font-size:90%;">arXiv preprint arXiv:2111.07725</span><span class="ltx_text" id="bib.bib23.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Hemlata Tak, Massimiliano Todisco, Xin Wang, Jee-weon Jung, Junichi Yamagishi, and Nicholas Evans.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">arXiv preprint arXiv:2202.12233</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Piotr Kawa, Marcin Plata, Michał Czuba, Piotr Szymański, and Piotr Syga.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Improved deepfake detection using whisper features.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">arXiv preprint arXiv:2306.01428</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">wav2vec 2.0: A framework for self-supervised learning of speech representations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib26.4.2" style="font-size:90%;">, 33:12449–12460, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Edresson Casanova, Kelly Davis, Eren Gölge, Görkem Göknar, Iulian Gulea, Logan Hart, Aya Aljafari, Joshua Meyer, Reuben Morais, Samuel Olayemi, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Xtts: a massively multilingual zero-shot text-to-speech model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">arXiv preprint arXiv:2406.04904</span><span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Audiogen: Textually guided audio generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.3.1" style="font-size:90%;">arXiv preprint arXiv:2209.15352</span><span class="ltx_text" id="bib.bib28.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Seed-tts: A family of high-quality versatile speech generation models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.3.1" style="font-size:90%;">arXiv preprint arXiv:2406.02430</span><span class="ltx_text" id="bib.bib29.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Prompttts 2: Describing and generating voices with text prompt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.3.1" style="font-size:90%;">arXiv preprint arXiv:2309.02285</span><span class="ltx_text" id="bib.bib30.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.3.1" style="font-size:90%;">arXiv preprint arXiv:2403.03100</span><span class="ltx_text" id="bib.bib31.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Libritts: A corpus derived from librispeech for text-to-speech.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="font-size:90%;">arXiv preprint arXiv:1904.02882</span><span class="ltx_text" id="bib.bib32.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib33.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib33.5.3" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">Seamless: Multilingual expressive and streaming speech translation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.3.1" style="font-size:90%;">arXiv preprint arXiv:2312.05187</span><span class="ltx_text" id="bib.bib34.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Hubert: Self-supervised speech representation learning by masked prediction of hidden units.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="font-size:90%;">IEEE/ACM transactions on audio, speech, and language processing</span><span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">, 29:3451–3460, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Clap learning audio concepts from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib36.4.2" style="font-size:90%;">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib36.5.3" style="font-size:90%;">, pages 1–5. IEEE, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Robust speech recognition via large-scale weak supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib37.4.2" style="font-size:90%;">International conference on machine learning</span><span class="ltx_text" id="bib.bib37.5.3" style="font-size:90%;">, pages 28492–28518. PMLR, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Piotr Kawa, Marcin Plata, and Piotr Syga.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Specrnet: Towards faster and more accessible audio deepfake detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib38.4.2" style="font-size:90%;">2022 IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)</span><span class="ltx_text" id="bib.bib38.5.3" style="font-size:90%;">, pages 792–799. IEEE, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Piotr Kawa, Marcin Plata, and Piotr Syga.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Attack agnostic dataset: Towards generalization and stabilization of audio deepfake detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.3.1" style="font-size:90%;">arXiv preprint arXiv:2206.13979</span><span class="ltx_text" id="bib.bib39.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
Kundan Kumar, Rithesh Kumar, Thibault De Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre De Brebisson, Yoshua Bengio, and Aaron C Courville.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Melgan: Generative adversarial networks for conditional waveform synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib40.4.2" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
Geng Yang, Shan Yang, Kai Liu, Peng Fang, Wei Chen, and Lei Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">Multi-band melgan: Faster waveform generation for high-quality text-to-speech.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib41.4.2" style="font-size:90%;">2021 IEEE Spoken Language Technology Workshop (SLT)</span><span class="ltx_text" id="bib.bib41.5.3" style="font-size:90%;">, pages 492–498. IEEE, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib42.4.2" style="font-size:90%;">, 33:17022–17033, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib43.4.2" style="font-size:90%;">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib43.5.3" style="font-size:90%;">, pages 6199–6203. IEEE, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
Ryan Prenger, Rafael Valle, and Bryan Catanzaro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.2.1" style="font-size:90%;">Waveglow: A flow-based generative network for speech synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib44.4.2" style="font-size:90%;">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib44.5.3" style="font-size:90%;">, pages 3617–3621. IEEE, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
Keith Ito and Linda Johnson.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.2.1" style="font-size:90%;">The lj speech dataset.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://keithito.com/LJ-Speech-Dataset/" style="font-size:90%;" title="">https://keithito.com/LJ-Speech-Dataset/</a><span class="ltx_text" id="bib.bib45.3.1" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.2.1" style="font-size:90%;">Wavenet: A generative model for raw audio.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.3.1" style="font-size:90%;">arXiv preprint arXiv:1609.03499</span><span class="ltx_text" id="bib.bib46.4.2" style="font-size:90%;">, 12, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.1.1" style="font-size:90%;">
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.2.1" style="font-size:90%;">Efficient neural audio synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib47.4.2" style="font-size:90%;">International Conference on Machine Learning</span><span class="ltx_text" id="bib.bib47.5.3" style="font-size:90%;">, pages 2410–2419. PMLR, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.1.1" style="font-size:90%;">
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.2.1" style="font-size:90%;">Wavegrad: Estimating gradients for waveform generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.3.1" style="font-size:90%;">arXiv preprint arXiv:2009.00713</span><span class="ltx_text" id="bib.bib48.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.1.1" style="font-size:90%;">
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.2.1" style="font-size:90%;">Diffwave: A versatile diffusion model for audio synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.3.1" style="font-size:90%;">arXiv preprint arXiv:2009.09761</span><span class="ltx_text" id="bib.bib49.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Broad Impacts</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p1.1.1">Societal Risks</span>. The rapid advancement of AI-generated content (AIGC) in audio and speech poses significant societal risks as it becomes more prevalent in audio and speech generation. As our work in benchmarking AI-synthesized audio detection demonstrates, the line between AI-generated audio and human speech is increasingly blurring, making it difficult for individuals to distinguish between synthetic and authentic voices. This raises serious concerns about spreading misinformation and fabricating narratives. AI-generated speeches could be used to impersonate public figures, spread false information, or even incite unrest by delivering provocative messages that appear authentic. For example, deepfake audios of political figures can be created to falsely represent their opinions or statements, potentially influencing public perception and affecting democratic processes.</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">Moreover, these technologies could be exploited to damage reputations or cause legal issues for individuals or organizations through fake endorsements or harmful statements. It is crucial for academia and industry to develop robust detection methods and ethical guidelines to prevent misuse of this technology and to educate the public about its capabilities and associated risks.</p>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p3.1.1">Positive Impacts</span>. On the positive side, AI-synthesized audio/speech has the potential to revolutionize content creation in various sectors, including education, entertainment, and accessibility. In education, AI-synthesized audios and speeches enables production of customized content that meets diverse learning needs and languages, improving access and inclusivity. For entertainment, they can offer novel experiences by generating dynamic dialogues in games or virtual reality, enriching user engagement and creativity.</p>
</div>
<div class="ltx_para" id="A1.SS1.p4">
<p class="ltx_p" id="A1.SS1.p4.1">Furthermore, AI-synthesized audios and speeches also enhance accessibility by producing speech in various languages or dialects, bridging communication gaps and making information more accessible to non-native speakers or those with liabilities. Additionally, the technology can help preserve lesser-spoken languages and dialects at risk of extinction by creating archives of AI-generated speeches and narratives.</p>
</div>
<div class="ltx_para" id="A1.SS1.p5">
<p class="ltx_p" id="A1.SS1.p5.1">In conclusion, while AI-synthesized audios and speeches offer exciting opportunities for content creation and accessibility, it is essential to address the ethical and societal challenges associated with its use. Collaborative efforts among researchers, developers, and policymakers are crucial to leveraging AI-synthesized audio and speech benefits responsibly while mitigating its risks, ensuring the technology serves to enhance human communication and creativity positively and responsibly.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Implementation Details</h3>
<figure class="ltx_table" id="A1.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T7.4.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="A1.T7.5.2" style="font-size:90%;">Hyperparemeters</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T7.2">
<tr class="ltx_tr" id="A1.T7.2.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.3.1">config</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.2.3.2">value</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.4.1">optimizer</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.4.2">Adam</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.2.2.3">optimizer momentum</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2">
<math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="A1.T7.1.1.1.m1.1"><semantics id="A1.T7.1.1.1.m1.1a"><mrow id="A1.T7.1.1.1.m1.1.1" xref="A1.T7.1.1.1.m1.1.1.cmml"><msub id="A1.T7.1.1.1.m1.1.1.2" xref="A1.T7.1.1.1.m1.1.1.2.cmml"><mi id="A1.T7.1.1.1.m1.1.1.2.2" xref="A1.T7.1.1.1.m1.1.1.2.2.cmml">β</mi><mn id="A1.T7.1.1.1.m1.1.1.2.3" xref="A1.T7.1.1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="A1.T7.1.1.1.m1.1.1.1" xref="A1.T7.1.1.1.m1.1.1.1.cmml">=</mo><mn id="A1.T7.1.1.1.m1.1.1.3" xref="A1.T7.1.1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.T7.1.1.1.m1.1b"><apply id="A1.T7.1.1.1.m1.1.1.cmml" xref="A1.T7.1.1.1.m1.1.1"><eq id="A1.T7.1.1.1.m1.1.1.1.cmml" xref="A1.T7.1.1.1.m1.1.1.1"></eq><apply id="A1.T7.1.1.1.m1.1.1.2.cmml" xref="A1.T7.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.T7.1.1.1.m1.1.1.2.1.cmml" xref="A1.T7.1.1.1.m1.1.1.2">subscript</csymbol><ci id="A1.T7.1.1.1.m1.1.1.2.2.cmml" xref="A1.T7.1.1.1.m1.1.1.2.2">𝛽</ci><cn id="A1.T7.1.1.1.m1.1.1.2.3.cmml" type="integer" xref="A1.T7.1.1.1.m1.1.1.2.3">1</cn></apply><cn id="A1.T7.1.1.1.m1.1.1.3.cmml" type="float" xref="A1.T7.1.1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.1.1.1.m1.1c">\beta_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="A1.T7.1.1.1.m1.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math>, <math alttext="\beta_{2}=0.999" class="ltx_Math" display="inline" id="A1.T7.2.2.2.m2.1"><semantics id="A1.T7.2.2.2.m2.1a"><mrow id="A1.T7.2.2.2.m2.1.1" xref="A1.T7.2.2.2.m2.1.1.cmml"><msub id="A1.T7.2.2.2.m2.1.1.2" xref="A1.T7.2.2.2.m2.1.1.2.cmml"><mi id="A1.T7.2.2.2.m2.1.1.2.2" xref="A1.T7.2.2.2.m2.1.1.2.2.cmml">β</mi><mn id="A1.T7.2.2.2.m2.1.1.2.3" xref="A1.T7.2.2.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="A1.T7.2.2.2.m2.1.1.1" xref="A1.T7.2.2.2.m2.1.1.1.cmml">=</mo><mn id="A1.T7.2.2.2.m2.1.1.3" xref="A1.T7.2.2.2.m2.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.T7.2.2.2.m2.1b"><apply id="A1.T7.2.2.2.m2.1.1.cmml" xref="A1.T7.2.2.2.m2.1.1"><eq id="A1.T7.2.2.2.m2.1.1.1.cmml" xref="A1.T7.2.2.2.m2.1.1.1"></eq><apply id="A1.T7.2.2.2.m2.1.1.2.cmml" xref="A1.T7.2.2.2.m2.1.1.2"><csymbol cd="ambiguous" id="A1.T7.2.2.2.m2.1.1.2.1.cmml" xref="A1.T7.2.2.2.m2.1.1.2">subscript</csymbol><ci id="A1.T7.2.2.2.m2.1.1.2.2.cmml" xref="A1.T7.2.2.2.m2.1.1.2.2">𝛽</ci><cn id="A1.T7.2.2.2.m2.1.1.2.3.cmml" type="integer" xref="A1.T7.2.2.2.m2.1.1.2.3">2</cn></apply><cn id="A1.T7.2.2.2.m2.1.1.3.cmml" type="float" xref="A1.T7.2.2.2.m2.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.2.2.2.m2.1c">\beta_{2}=0.999</annotation><annotation encoding="application/x-llamapun" id="A1.T7.2.2.2.m2.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.999</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.2.5.1">weight decay</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.5.2">1e-4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.2.6.1">epochs</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.6.2">40</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.2.7.1">warmup epochs</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.7.2">0</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.8">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T7.2.8.1">scheduler</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.2.8.2">cosine decay</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04324v3#A1.T7" title="Table 7 ‣ A.2 Implementation Details ‣ Appendix A Appendix ‣ SONAR: A Synthetic AI-Audio Detection Framework and Benchmark"><span class="ltx_text ltx_ref_tag">7</span></a> presents the hyperparameters for training AASIST, RawNet2, RawGAT-ST, LCNN, and Spec.+ResNet. We train AASIST, RawNet2, and RawGAT-ST with a learning rate of 0.0001 and LCNN and Spec.+ResNet with a learning rate of 0.0003. The batch size for AASIST, RawNet2, RawGAT-ST, LCNN, and Spec.+ResNet are 64, 256, 32, 512, and 256, respectively. All input audios are resampled to a 16kHz sampling rate and converted into raw waveforms consisting of 64,000 samples (approximately 4 seconds). Audios longer than 4 seconds are randomly trimmed, while those shorter than 4 seconds are repeated and padded to meet the 4-second duration.</p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1">For the foundation models, two linear layers are added after the encoder’s output, with the hidden layer dimension matching the dimension of the encoder’s output. We fine-tune all foundation models on the Wavefake training dataset for 3 epochs using the Adam optimizer with a learning rate of 0.00001 and a weight decay of 0.0005.</p>
</div>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1">For few-shot fine-tuning, models are fine-tuned for 30 epochs with a learning rate of 0.00001 and a weight decay of 0.00005.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 10 05:33:03 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
