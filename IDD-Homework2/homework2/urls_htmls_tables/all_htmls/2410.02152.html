<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>An evaluation of large pre-trained models for gesture recognition using synthetic videos</title>
<!--Generated on Thu Oct  3 02:30:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.02152v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#S1" title="In An evaluation of large pre-trained models for gesture recognition using synthetic videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>INTRODUCTION</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#S2" title="In An evaluation of large pre-trained models for gesture recognition using synthetic videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>EXPERIMENTS</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#S2.SS1" title="In 2 EXPERIMENTS ‣ An evaluation of large pre-trained models for gesture recognition using synthetic videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>KNN Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#S2.SS2" title="In 2 EXPERIMENTS ‣ An evaluation of large pre-trained models for gesture recognition using synthetic videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Zero-Shot Text-Based Classification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#S3" title="In An evaluation of large pre-trained models for gesture recognition using synthetic videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>DISCUSSION</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">An evaluation of large pre-trained models 
<br class="ltx_break"/>for gesture recognition using synthetic videos</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arun Reddy
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University Applied Physics Laboratory, Laurel, MD, USA
</span>
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ketul Shah
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Corban Rivera
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University Applied Physics Laboratory, Laurel, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">William Paul
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University Applied Physics Laboratory, Laurel, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Celso M. De Melo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Army Research Laboratory, Los Angeles, CA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rama Chellappa
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University, Baltimore, MD, USA
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">In this work, we explore the possibility of using synthetically generated data for video-based gesture recognition with large pre-trained models. We consider whether these models have sufficiently robust and expressive representation spaces to enable “training-free” classification. Specifically, we utilize various state-of-the-art video encoders to extract features for use in k-nearest neighbors classification, where the training data points are derived from synthetic videos only. We compare these results with another training-free approach— zero-shot classification using text descriptions of each gesture.
In our experiments with the RoCoG-v2 dataset, we find that using synthetic training videos yields significantly lower classification accuracy on real test videos compared to using a relatively small number of real training videos. We also observe that video backbones that were fine-tuned on classification tasks serve as superior feature extractors, and that the choice of fine-tuning data has a substantial impact on k-nearest neighbors performance.
Lastly, we find that zero-shot text-based classification performs poorly on the gesture recognition task, as gestures are not easily described through natural language.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>gesture recognition, action recognition, video classification, video-language, synthetic data
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>INTRODUCTION</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">One of the promises of synthetic data is the possibility of reducing reliance on real data for training deep learning models, which can introduce practical challenges and ethical concerns. As such, there has been a growing interest in using synthetically generated video data to train models for various video-related tasks. However, previous work has shown the existence of a large domain gap between real and synthetic video data, resulting in sub-optimal performance when naively applying a synthetically-trained model to data from the real domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib4" title="">4</a>]</cite>. This issue has spurred the development of various approaches for video domain adaptation
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib6" title="">6</a>]</cite>, which can be computationally expensive and difficult to implement in practice.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Here, we consider whether state-of-the-art video backbones, given the scale of their pre-training, are capable of extracting domain-invariant representations to enable video classification without needing any real data for the task. Specifically, we experiment with two different “training-free” approaches. The first uses modern video backbones as feature extractors for K-nearest neighbors (KNN) classification, where the training samples are derived from synthetic videos. In the second, we rely on text descriptions of the classes and attempt to perform zero-shot classification using similarity between video features and text features from each class description. We use video-based gesture recognition as our video classification task in this study.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="137" id="S1.F1.g1" src="extracted/5896196/figures/images/rocog_examples.png" width="300"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Examples of a real (left) and synthetic (right) video from RoCoG-v2. The datset consists of 7 gesture categories.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>EXPERIMENTS</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We perform two sets of experiments. In the first set, we leverage large pre-trained video models as feature extractors and perform KNN classification using these features. In the second, we perform zero-shot classification using textual descriptions of gestures. These two methods are described in detail below.
All experiments are performed on ground viewpoint videos from the RoCoG-v2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib1" title="">1</a>]</cite>, examples of which are shown in Figure 1.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>KNN Classification</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">When using synthetic training videos to specify the gesture recognition task, we perform KNN classification on features extracted from large pre-trained models. More specifically, the synthetic training data (44K videos) is embedded in the feature space of a video encoder, and for a given real test video, the majority vote of the K nearest training data points is used for classification, based on L2 distance. We also experiment with a scenario where we have a small real dataset (203 videos) available for training. We set K=3 for all experiments. We consider three different types of video encoders based on pre-training (and fine-tuning) strategies, as described below. We choose a ViT-B/16 model for all experiments, and also study the effect of a larger ViT-L/16 model for the best setting.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">Self-Supervised Pre-Training.</span> We consider Unmasked Teacher (UMT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib7" title="">7</a>]</cite>, which is a state-of-the-art video self-supervised learning approach. This approach masks out most of the video tokens and enforces alignment between the representations of unmasked patches and the corresponding ones from a teacher model (CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib8" title="">8</a>]</cite>). We use the UMT model pre-trained on K710 videos (a union of K400 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib9" title="">9</a>]</cite>, K600 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib10" title="">10</a>]</cite> and K700 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib11" title="">11</a>]</cite>) for our experiments. Eight frames are sampled from each video using the TSN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib12" title="">12</a>]</cite> frame-sampling strategy. The entire video is divided into eight segments, and one frame is selected at random from each segment. The input frames to the network are resized to 224 <math alttext="\times" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mo id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><times id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">×</annotation></semantics></math> 224 resolution.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Vision-Language Pre-Training.</span> In contrast to UMT which uses only video data for pre-training, here we consider a vision-language pre-training approach of ViCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib13" title="">13</a>]</cite>. ViCLIP uses a video-language contrastive objective similar to CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib8" title="">8</a>]</cite>, while also masking videos for efficient pre-training. We use the model pre-trained on a filtered version of the InternVid <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib13" title="">13</a>]</cite> dataset that has 10M video-text pairs. Eight frames of 224 <math alttext="\times" class="ltx_Math" display="inline" id="S2.SS1.p3.1.m1.1"><semantics id="S2.SS1.p3.1.m1.1a"><mo id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><times id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.1.m1.1d">×</annotation></semantics></math> 224 resolution are used as input to the network.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p4.1.1">Self-Supervised Pre-Training + Supervised Fine-Tuning.</span> Here, we use self-supervised models which were further fine-tuned for video classification in a supervised manner. We consider two pre-training methods, UMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib7" title="">7</a>]</cite> and VideoMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib14" title="">14</a>]</cite>. VideoMAE is a powerful self-supervised pre-training approach which works by encoding partially masked inputs and reconstructing the masked out regions. The VideoMAE models are pre-trained on a larger (1.35M) UnlabeledHybrid <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib15" title="">15</a>]</cite> dataset, whereas UMT models are pre-trained on the K710 dataset (650K). These models are either fine-tuned on Kinetics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib9" title="">9</a>]</cite> (K710, K400, K600, K700) or the more motion-centric Something-Something-v2 (SSv2) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib16" title="">16</a>]</cite> dataset. For the VideoMAE models, we sample sixteen frames from the input video, with a tubelet size of two frames, resulting in the same number of tokens as using eight frames with a tubelet size of one frame, as in the above scenarios.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.2" style="width:433.6pt;height:162pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-48.1pt,18.0pt) scale(0.818381128379513,0.818381128379513) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.2.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.1.1.1" style="font-size:90%;">Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.2.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.1.2.1" style="font-size:90%;">Backbone</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.1.3.1" style="font-size:90%;">Pre-Training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.1.4.1" style="font-size:90%;">Pre-Training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.2.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.1.5.1" style="font-size:90%;">Fine-Tuning</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T1.2.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.1.6.1" style="font-size:90%;">Real Test KNN Acc. (%)</span></th>
</tr>
<tr class="ltx_tr" id="S2.T1.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.2.2.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S2.T1.2.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.2.2.2.1" style="font-size:90%;">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S2.T1.2.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.2.2.3.1" style="font-size:90%;">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.1.2.2.4"><span class="ltx_text" id="S2.T1.2.1.2.2.4.1" style="font-size:90%;">Synthetic Train</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.1.2.2.5"><span class="ltx_text" id="S2.T1.2.1.2.2.5.1" style="font-size:90%;">Real Train</span></th>
</tr>
<tr class="ltx_tr" id="S2.T1.2.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.2.1.3.3.1"><span class="ltx_text" id="S2.T1.2.1.3.3.1.1" style="font-size:90%;">Self-Supervised Pre-Training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.2.1.3.3.2"><span class="ltx_text" id="S2.T1.2.1.3.3.2.1" style="font-size:90%;">ViT-B/16</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.3.3.3"><span class="ltx_text" id="S2.T1.2.1.3.3.3.1" style="font-size:90%;">UMT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.2.1.3.3.4"><span class="ltx_text" id="S2.T1.2.1.3.3.4.1" style="font-size:90%;">K710</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.2.1.3.3.5"><span class="ltx_text" id="S2.T1.2.1.3.3.5.1" style="font-size:90%;">-</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.3.3.6"><span class="ltx_text" id="S2.T1.2.1.3.3.6.1" style="font-size:90%;">18.2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.3.3.7"><span class="ltx_text" id="S2.T1.2.1.3.3.7.1" style="font-size:90%;">31.2</span></th>
</tr>
<tr class="ltx_tr" id="S2.T1.2.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.2.1.4.4.1"><span class="ltx_text" id="S2.T1.2.1.4.4.1.1" style="font-size:90%;">Vision-Language Pre-Training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.2.1.4.4.2"><span class="ltx_text" id="S2.T1.2.1.4.4.2.1" style="font-size:90%;">ViT-B/16</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.4.4.3"><span class="ltx_text" id="S2.T1.2.1.4.4.3.1" style="font-size:90%;">ViCLIP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.2.1.4.4.4"><span class="ltx_text" id="S2.T1.2.1.4.4.4.1" style="font-size:90%;">InternVid FLT-10M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.2.1.4.4.5"><span class="ltx_text" id="S2.T1.2.1.4.4.5.1" style="font-size:90%;">-</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.4.4.6"><span class="ltx_text" id="S2.T1.2.1.4.4.6.1" style="font-size:90%;">19.2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.4.4.7"><span class="ltx_text" id="S2.T1.2.1.4.4.7.1" style="font-size:90%;">40.4</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.2.1.5.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.2.1.5.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.2.1.5.1.2"><span class="ltx_text" id="S2.T1.2.1.5.1.2.1" style="font-size:90%;">ViT-B/16</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.1.5.1.3"><span class="ltx_text" id="S2.T1.2.1.5.1.3.1" style="font-size:90%;">UMT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.2.1.5.1.4"><span class="ltx_text" id="S2.T1.2.1.5.1.4.1" style="font-size:90%;">K710</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.2.1.5.1.5"><span class="ltx_text" id="S2.T1.2.1.5.1.5.1" style="font-size:90%;">K710</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.1.5.1.6"><span class="ltx_text" id="S2.T1.2.1.5.1.6.1" style="font-size:90%;">42.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.1.5.1.7"><span class="ltx_text" id="S2.T1.2.1.5.1.7.1" style="font-size:90%;">49.5</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.1.6.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.1.6.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.1.6.2.2"><span class="ltx_text" id="S2.T1.2.1.6.2.2.1" style="font-size:90%;">ViT-B/16</span></th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.6.2.3"><span class="ltx_text" id="S2.T1.2.1.6.2.3.1" style="font-size:90%;">UMT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.1.6.2.4"><span class="ltx_text" id="S2.T1.2.1.6.2.4.1" style="font-size:90%;">K710</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.1.6.2.5"><span class="ltx_text" id="S2.T1.2.1.6.2.5.1" style="font-size:90%;">K710 + K400</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.6.2.6"><span class="ltx_text" id="S2.T1.2.1.6.2.6.1" style="font-size:90%;">38.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.6.2.7"><span class="ltx_text" id="S2.T1.2.1.6.2.7.1" style="font-size:90%;">45.5</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.1.7.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.1.7.3.1"><span class="ltx_text" id="S2.T1.2.1.7.3.1.1" style="font-size:90%;">Self-Supervised Pre-Training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.1.7.3.2"><span class="ltx_text" id="S2.T1.2.1.7.3.2.1" style="font-size:90%;">ViT-B/16</span></th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.7.3.3"><span class="ltx_text" id="S2.T1.2.1.7.3.3.1" style="font-size:90%;">UMT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.1.7.3.4"><span class="ltx_text" id="S2.T1.2.1.7.3.4.1" style="font-size:90%;">K710</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.1.7.3.5"><span class="ltx_text" id="S2.T1.2.1.7.3.5.1" style="font-size:90%;">K710 + K600</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.7.3.6"><span class="ltx_text" id="S2.T1.2.1.7.3.6.1" style="font-size:90%;">33.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.7.3.7"><span class="ltx_text" id="S2.T1.2.1.7.3.7.1" style="font-size:90%;">49.5</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.1.8.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.1.8.4.1"><span class="ltx_text" id="S2.T1.2.1.8.4.1.1" style="font-size:90%;">+ Supervised Fine-Tuning</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.1.8.4.2"><span class="ltx_text" id="S2.T1.2.1.8.4.2.1" style="font-size:90%;">ViT-B/16</span></th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.8.4.3"><span class="ltx_text" id="S2.T1.2.1.8.4.3.1" style="font-size:90%;">UMT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.1.8.4.4"><span class="ltx_text" id="S2.T1.2.1.8.4.4.1" style="font-size:90%;">K710</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.1.8.4.5"><span class="ltx_text" id="S2.T1.2.1.8.4.5.1" style="font-size:90%;">K710 + K700</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.8.4.6"><span class="ltx_text" id="S2.T1.2.1.8.4.6.1" style="font-size:90%;">35.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.8.4.7"><span class="ltx_text" id="S2.T1.2.1.8.4.7.1" style="font-size:90%;">51.5</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.1.9.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.1.9.5.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.1.9.5.2"><span class="ltx_text" id="S2.T1.2.1.9.5.2.1" style="font-size:90%;">ViT-B/16</span></th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.9.5.3"><span class="ltx_text" id="S2.T1.2.1.9.5.3.1" style="font-size:90%;">VideoMAE</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.1.9.5.4"><span class="ltx_text" id="S2.T1.2.1.9.5.4.1" style="font-size:90%;">UnlabeledHybrid</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.1.9.5.5"><span class="ltx_text" id="S2.T1.2.1.9.5.5.1" style="font-size:90%;">K710</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.9.5.6"><span class="ltx_text" id="S2.T1.2.1.9.5.6.1" style="font-size:90%;">32.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.9.5.7"><span class="ltx_text" id="S2.T1.2.1.9.5.7.1" style="font-size:90%;">60.6</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.1.10.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.1.10.6.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.1.10.6.2"><span class="ltx_text" id="S2.T1.2.1.10.6.2.1" style="font-size:90%;">ViT-B/16</span></th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.10.6.3"><span class="ltx_text" id="S2.T1.2.1.10.6.3.1" style="font-size:90%;">VideoMAE</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.1.10.6.4"><span class="ltx_text" id="S2.T1.2.1.10.6.4.1" style="font-size:90%;">UnlabeledHybrid</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.2.1.10.6.5"><span class="ltx_text" id="S2.T1.2.1.10.6.5.1" style="font-size:90%;">SSv2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.10.6.6"><span class="ltx_text" id="S2.T1.2.1.10.6.6.1" style="font-size:90%;">43.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.1.10.6.7"><span class="ltx_text" id="S2.T1.2.1.10.6.7.1" style="font-size:90%;">68.7</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.1.11.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S2.T1.2.1.11.7.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S2.T1.2.1.11.7.2"><span class="ltx_text" id="S2.T1.2.1.11.7.2.1" style="font-size:90%;">ViT-L/16</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.1.11.7.3"><span class="ltx_text" id="S2.T1.2.1.11.7.3.1" style="font-size:90%;">VideoMAE</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.2.1.11.7.4"><span class="ltx_text" id="S2.T1.2.1.11.7.4.1" style="font-size:90%;">UnlabeledHybrid</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.2.1.11.7.5"><span class="ltx_text" id="S2.T1.2.1.11.7.5.1" style="font-size:90%;">SSv2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.1.11.7.6"><span class="ltx_text" id="S2.T1.2.1.11.7.6.1" style="font-size:90%;">64.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.1.11.7.7"><span class="ltx_text" id="S2.T1.2.1.11.7.7.1" style="font-size:90%;">71.7</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>K-nearest neighbor classification results on RoCoG-v2 ground videos using a variety of video backbones.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Zero-Shot Text-Based Classification</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The gesture recognition task can alternatively be specified by providing the textual descriptions of each activity. In our second set of experiments, shown in Table 1, we use text descriptions to perform zero-shot classification using pre-trained vision-language models. Specifically, we use the pre-trained ViCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib13" title="">13</a>]</cite> text and video encoders, where classification is performed based on similarity between video features and text embeddings of descriptions of all classes. We use two kinds of text descriptions of the gestures, original and transformed, as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">Original.</span> These are the instructions associated with each gesture as they appear in the US Army Field Manual <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib17" title="">17</a>]</cite>. These were provided to the performers of the gestures in the RoCoG-v2 dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">Transformed.</span> We use GPT-3.5 to turn the original text instructions into gesture descriptions by prompting the model with the following text: “<span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.2">Can you summarize the description below of an activity instruction and start with “A person”</span>”.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T2.2" style="width:260.2pt;height:45.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.8pt,4.3pt) scale(0.840152661096102,0.840152661096102) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T2.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.1.1" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T2.2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.2.1" style="font-size:90%;">Training Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T2.2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.3.1" style="font-size:90%;">Text Description</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.4.1" style="font-size:90%;">Test Accuracy (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.2.1.2.1.1"><span class="ltx_text" id="S2.T2.2.1.2.1.1.1" style="font-size:90%;">ViCLIP-B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.2.1.2.1.2"><span class="ltx_text" id="S2.T2.2.1.2.1.2.1" style="font-size:90%;">InternVid FLT-10M</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.2.1.2.1.3"><span class="ltx_text" id="S2.T2.2.1.2.1.3.1" style="font-size:90%;">Original</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.1.2.1.4"><span class="ltx_text" id="S2.T2.2.1.2.1.4.1" style="font-size:90%;">25.3</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T2.2.1.3.2.1"><span class="ltx_text" id="S2.T2.2.1.3.2.1.1" style="font-size:90%;">ViCLIP-B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T2.2.1.3.2.2"><span class="ltx_text" id="S2.T2.2.1.3.2.2.1" style="font-size:90%;">InternVid FLT-10M</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T2.2.1.3.2.3"><span class="ltx_text" id="S2.T2.2.1.3.2.3.1" style="font-size:90%;">Transformed</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.1.3.2.4"><span class="ltx_text" id="S2.T2.2.1.3.2.4.1" style="font-size:90%;">26.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Zero-shot classification on RoCoG-v2 ground videos using two forms of text descriptions.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>DISCUSSION</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Several observations can be made from the KNN classification results in Table 1. First, we can clearly see that, in all cases, using synthetic training data results in lower accuracy on real videos than using real training data. Despite the large quantity of synthetic training data (roughly 200<math alttext="\times" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mo id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><times id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">×</annotation></semantics></math> that of real training data), we find it is not as effective at defining the gesture classes in the RoCoG-v2 dataset as real data. This is indicative of the synthetic-to-real domain gap that has been observed previously, which is still far from solved. Figure 2 also illustrates this domain gap, as real video features appear noticeably more clustered by gesture class than synthetic video features.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="200" id="S3.F2.sf1.g1" src="extracted/5896196/figures/images/real_ground_train_tsne_p30.png" width="228"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F2.sf1.3.2" style="font-size:90%;">t-SNE plot for real data features.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="200" id="S3.F2.sf2.g1" src="extracted/5896196/figures/images/syn_ground_train_tsne_p5_1.png" width="225"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F2.sf2.3.2" style="font-size:90%;">t-SNE plot for synthetic data features.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">t-SNE plots for real and synthetic data. Real data is more meaningfully clustered compared to synthetic data, as the features are extracted using a ViT-B/16 model pre-trained on K710 and fine-tuned on K710 and K400. For (a), we use all real data whereas for (b), we use 50 samples per class chosen at random from the synthetic dataset.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Next, we can see from comparing the first two rows in Table 1 that large-scale vision-language pre-training seems to confer some benefit over smaller-scale masked pre-training, particularly when using real videos for KNN classification. However, we find that backbones that have undergone supervised fine-tuning vastly outperform both the self-supervised and vision-language pre-trained backbones. Interestingly, as the backbone is fine-tuned on more real videos, we see a drop in KNN accuracy using synthetic training data while accuracy using real training data increases. This suggests that video transformer backbones become less robust to the synthetic-to-real domain shift the more they are trained on real videos.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We find that the choice of fine-tuning data has a substantial impact on KNN classification. We can see that backbones fine-tuned on the SSv2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib16" title="">16</a>]</cite> perform much better than those trained on Kinetics videos. SSv2 is a temporally-heavy dataset, where modeling of motion is critical for solving the classification task. In contrast, the action categories in Kinetics videos exhibit a high degree of object and scene bias. Because the gesture recognition task in RoCoG-v2 is also motion-focused, SSv2 serves as an effective source of fine-tuning data. Notably, scaling up from the ViT-B model to ViT-L results in significant boosts in KNN accuracy, particularly when using synthetic training data. Future experiments should investigate whether, in general, larger models might be more robust to synthetic-to-real shifts.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Finally, we observe in Table 2 that zero-shot classification using ViCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02152v1#bib.bib13" title="">13</a>]</cite> performs poorly on the gesture recognition task, regardless of the type of text description used. This is likely because the gesture recognition task involves fine-grained motion differences that are not easily expressed through natural language.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-21-2-0211.
The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government.
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Reddy, A. V., Shah, K., Paul, W., Mocharla, R., Hoffman, J., Katyal, K. D., Manocha, D., de Melo, C. M., and Chellappa, R., “Synthetic-to-Real Domain Adaptation for Action Recognition: A Dataset and Baseline Performances,” in [<span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">2023 IEEE International Conference on Robotics and Automation (ICRA)</span> ], 11374–11381 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Shah, K., Shah, A., Lau, C. P., de Melo, C. M., and Chellappa, R., “Multi-view action recognition using contrastive learning,” in [<span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span> ], 3381–3391 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Da Costa, V. G. T., Zara, G., Rota, P., Oliveira-Santos, T., Sebe, N., Murino, V., and Ricci, E., “Dual-head contrastive domain adaptation for video action recognition,” in [<span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span> ], 1181–1190 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Katyal, K., Chellappa, R., Shah, K., Reddy, A., Hoffman, J., Paul, W., Mocharla, R., Handelman, D., and de Melo, C., “Leveraging synthetic data for robust gesture recognition,” in [<span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Synthetic Data for Artificial Intelligence and Machine Learning: Tools, Techniques, and Applications</span> ], <span class="ltx_text ltx_font_bold" id="bib.bib4.2.2">12529</span>, 238–241 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Reddy, A., Paul, W., Rivera, C., Shah, K., de Melo, C. M., and Chellappa, R., “Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training,” (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Wei, P., Kong, L., Qu, X., Ren, Y., Xu, Z., Jiang, J., and Yin, X., “Unsupervised video domain adaptation for action recognition: A disentanglement perspective,” in [<span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems</span> ], (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Li, K., Wang, Y., Li, Y., Wang, Y., He, Y., Wang, L., and Qiao, Y., “Unmasked Teacher: Towards Training-Efficient Video Foundation Models,” in [<span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">ICCV</span> ], (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al., “Learning transferable visual models from natural language supervision,” in [<span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">International conference on machine learning</span> ], 8748–8763, PMLR (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al., “The kinetics human action video dataset,” <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:1705.06950</span> (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., and Zisserman, A., “A Short Note about Kinetics-600,” (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Carreira, J., Noland, E., Hillier, C., and Zisserman, A., “A short note on the kinetics-700 human action dataset,” <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:1907.06987</span> (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., and Van Gool, L., “Temporal segment networks for action recognition in videos,” <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">IEEE transactions on pattern analysis and machine intelligence</span> <span class="ltx_text ltx_font_bold" id="bib.bib12.2.2">41</span>(11), 2740–2755 (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Chen, X., Wang, Y., Luo, P., Liu, Z., Wang, Y., Wang, L., and Qiao, Y., “InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation,” in [<span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">ICLR</span> ], (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Tong, Z., Song, Y., Wang, J., and Wang, L., “VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training,” in [<span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">NeurIPS</span> ], (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Wang, L., Huang, B., Zhao, Z., Tong, Z., He, Y., Wang, Y., Wang, Y., and Qiao, Y., “Videomae v2: Scaling video masked autoencoders with dual masking,” in [<span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span> ], 14549–14560 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et al., “The” something something” video database for learning and evaluating visual common sense,” in [<span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE international conference on computer vision</span> ], 5842–5850 (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
“Visual signals: Field manual 21-60,” (1987).

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct  3 02:30:21 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
