<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble</title>
<!--Generated on Thu Sep 19 20:12:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.13779v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S1" title="In AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S2" title="In AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S2.SS1" title="In 2 Methodology ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Development dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S2.SS2" title="In 2 Methodology ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data pre-processing</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S2.SS2.SSS1" title="In 2.2 Data pre-processing ‣ 2 Methodology ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>TotalSegmentator:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S2.SS2.SSS2" title="In 2.2 Data pre-processing ‣ 2 Methodology ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Data augmentation:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S2.SS2.SSS3" title="In 2.2 Data pre-processing ‣ 2 Methodology ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Data normalization:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S2.SS3" title="In 2 Methodology ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Model architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S2.SS4" title="In 2 Methodology ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Training methodology</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S3" title="In AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results and discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S3.SS1" title="In 3 Results and discussion ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S3.SS2" title="In 3 Results and discussion ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>nnU-Net ResEnc XL results and discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S3.SS3" title="In 3 Results and discussion ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Model ensemble results and discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S3.SS4" title="In 3 Results and discussion ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Model Submissions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S4" title="In AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S5" title="In AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Acknowledgement</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tanya Chutani, Saikiran Bonthu, Pranab Samanta, Nitin Singhal 
<br class="ltx_break"/>Airamatrix Pvt Ltd 
<br class="ltx_break"/>Mumbai 400604 
<br class="ltx_break"/>India. 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.1.id1">solutions@airamatrix.com</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Positron Emission Tomography (PET) /Computed Tomography (CT) is crucial for diagnosing, managing, and planning treatment for various cancers. Developing reliable deep learning models for the segmentation of tumor lesions in PET/CT scans in a multitracer multicenter environment, is a critical area of research. Different tracers, such as Fluorodeoxyglucose (FDG) and Prostate-Specific Membrane Antigen (PSMA), have distinct physiological uptake patterns and data from different centers often vary in terms of acquisition protocols, scanner types, and patient populations. Because of this variability, it becomes more difficult to design reliable segmentation algorithms and generalization techniques due to variations in image quality and lesion detectability. To address this challenge, We trained a 3D Residual encoder U-Net within the nnU-Net framework, aiming to generalize the performance of automatic lesion segmentation of whole body PET/CT scans, across different tracers and clinical sites. Further, We explored several preprocessing techniques and ultimately settled on using the ‘TotalSegmentator’ to crop our training data. Additionally, we applied resampling during this process. During inference, we leveraged test-time augmentations and other post-processing techniques to enhance tumor lesion segmentation. Our team currently hold the top position in the AutoPET III challenge and outperformed the challenge baseline model in the preliminary test set with Dice score of 0.9627. Github link: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tanya-chutani-aira/autopetiii" title="">https://github.com/tanya-chutani-aira/autopetiii</a></p>
<p class="ltx_p" id="id3.3"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="id3.3.1">K</em><span class="ltx_text ltx_font_bold" id="id3.3.2">eywords</span> TotalSegmentator  <math alttext="\cdot" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">⋅</annotation></semantics></math>
nnU-Net ResEnc  <math alttext="\cdot" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">⋅</annotation></semantics></math>
Ensemble  <math alttext="\cdot" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><ci id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">⋅</annotation></semantics></math>
Tumor segmentation.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Over the past decades, PET/CT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib2" title="">2</a>]</cite> has become a crucial tool in oncological diagnostics, management, and treatment planning. In clinical practice, medical experts usually depend on qualitative analysis of PET/CT images, although quantitative analysis could provide more precise and individualized tumor characterization and therapeutic decisions. A significant barrier to clinical adoption is lesion segmentation, a necessary step for quantitative image analysis. When done manually, it is tedious, time-consuming, and costly. Machine learning offers the potential for rapid and fully automated quantitative analysis of PET/CT images.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="235" id="S2.F1.g1" src="extracted/5866798/datapreprocessing_block_diagram.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Block diagram for model development and model inference</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The block diagram in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S2.F1" title="Figure 1 ‣ 2 Methodology ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the training and inference pipeline utilized for the AutoPET III challenge.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Development dataset</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The AutoPET III challenge dataset comprises 1,611 images in the training set. Specifically, it includes 1,014 FDG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib3" title="">3</a>]</cite> images from 900 patients and 597 PSMA images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib4" title="">4</a>]</cite> from 378 patients. The validation set is derived from 5 images (3 FDG images and 2 PSMA images). The final evaluation will use a hidden test set of 200 images, with half of the test data matching the sources and distributions of the training data, while the other half will be cross-sourced from the opposite center, ensuring robust model assessment.The FDG PET/CT training and testing images from UKT hospital were annotated by a radiologist with a decade of experience in hybrid imaging. The PSMA PET/CT datasets from both centers (UKT and LMU hospital) underwent initial annotation by a single reader and subsequent review by a radiologist with five years of experience in hybrid imaging.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data pre-processing</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Training a model for 3D medical images, such as PET/CT scans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib2" title="">2</a>]</cite>, is challenging due to the substantial memory consumption required during both training and inference. Additionally, not all parts of the image contain relevant structures (e.g., tumors), and the image boundaries often have noise or artifacts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Data pre-processing steps include TotalSegmentator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib5" title="">5</a>]</cite>, image resampling, data augmentation, and data normalization.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>TotalSegmentator:</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">To localize the tissue and mitigate the boundary noise, we used TotalSegmentator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib5" title="">5</a>]</cite> to crop the 3D PET/CT images. Cropping reduces the image size while retaining relevant information, leading to more memory-efficient training. TotalSegmentator intelligently crops to preserve relevant structures (e.g., tumors) while minimizing unnecessary background and noisy regions, ensuring cleaner input data for the model. This approach ensures uniform input sizes across all samples, making it easier to batch and train models efficiently.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">The trade-off is that aggressive cropping can sometimes sacrifice spatial context to save memory. To balance this, we used overlapping crops during training, ensuring that neighboring context is still considered even if individual crops lose some spatial information. Additionally, we applied padding to the cropped regions, allowing the model to learn from nearby context. To make the generalized model we applied data augmentation module. Data augmentation module is followed by cropped data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1">In TotalSegmentator, we utilized a 3D lower nnU-Net to accurately segment the body from the PET/CT image.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Data augmentation:</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Data augmentation plays a crucial role in enhancing model robustness and generalization. We employ basic transformations such as random flips, random rotations, elastic deformations, intensity scaling which are applied in standard nnU-Netv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Data normalization:</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">We applied data normalization to the cropped images, including resampling and intensity normalization, using standard nnUNetv2 plans defined for CT and PET. Resampling ensures consistent spatial resolution across all images, while intensity normalization brings the image values to a common scale, reducing variability caused by different imaging protocols.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Model architecture</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">We employed nnU-Net ResEnc XL from nnUNetv2 pipeline as the model architecture for training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib7" title="">7</a>]</cite>. The model is modified version of the ResNet family network. It utilizes residual block similar to the ResNet [Ref.]. nnU-Net ResEnc XL represents a significant advancement in the field of medical imaging, leveraging deep learning to enhance the accuracy and efficiency of tumor lesion segmentation in 3D PET/CT scans.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Training methodology</h3>
<div class="ltx_para ltx_noindent" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">During training, we adopted a robust approach by dividing the data into five folds. For each fold, 80 percentage of the data served as the training set, while the remaining 20 percentage acted as the validation set. This strategy allowed our models to capture variations effectively and ensured their robustness when faced with unseen test data. We explored both 2D and 3D versions of the nnU-Net ResEnc XL architecture and employed a patch size of [224, 192, 224] in the Patch-Based Training of nnU-Netv2 pipeline.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results and discussion</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental setup</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For this experimentation, we have used nnU-Netv2 framework with nnU-Net ResEnc XL architecture. The framework is developed in pytorch and we have used 48GB GPU to develop the 2D and 3D models for each fold.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>nnU-Net ResEnc XL results and discussion</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In nnU-Net, we have used 2D and 3D nnU-Net ResEnc XL model in full resolution. We assessed the performance of 2D and 3D segmentation models using a 5-fold cross-validation approach with the Dice metric. The results, presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S3.T1" title="Table 1 ‣ 3.2 nnU-Net ResEnc XL results and discussion ‣ 3 Results and discussion ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_tag">1</span></a>, indicate that 3D models outperformed 2D models in tumor lesion segmentation in PET/CT images when trained on a cropped dataset generated using TotalSegmentator.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Dice Score in 5-fold cross validation set</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S3.T1.1.1.1.1">Model</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.2">Fold-0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.3">Fold-1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.4">Fold-2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.5">Fold-3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.1.6">Fold-4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S3.T1.1.2.2.1">ResEnc XL-2D</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.2.2.2">0.5965</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.2.2.3">0.6014</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.2.2.4">0.5706</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.2.2.5">0.6207</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.2.2.6">0.5647</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" id="S3.T1.1.3.3.1">ResEnc XL-3D</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_tt" id="S3.T1.1.3.3.2">0.6856</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_tt" id="S3.T1.1.3.3.3">0.6450</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_tt" id="S3.T1.1.3.3.4">0.6244</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_tt" id="S3.T1.1.3.3.5">0.6665</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S3.T1.1.3.3.6">0.6171</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="596" id="S3.F2.sf1.g1" src="extracted/5866798/snapshot0003_GT_fdg_1285b86bea.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Whole body FDG image with lesion ground truth</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="596" id="S3.F2.sf2.g1" src="extracted/5866798/snapshot0004_GT_psma_0878fdec425f09c3_2019-05-24.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Whole body PSMA image with lesion ground truth</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="596" id="S3.F2.sf3.g1" src="extracted/5866798/snapshot0003_Pred_fdg_1285b86bea.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Whole body lesion segmentation results of FDG image</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="596" id="S3.F2.sf4.g1" src="extracted/5866798/snapshot0004_Pred_psma_0878fdec425f09c3_2019-05-25.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Whole body lesion segmentation results of PSMA image</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Qualitative results on FDG and PSMA images</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model ensemble results and discussion</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">For the AutoPET III challenge, we employed an ensemble of 5-fold models, and the STAPLE algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib9" title="">9</a>]</cite> to combine the results from the five models. This ensemble approach facilitated the improved generalization, robustness and performance needed for tumor lesion segmentation in a multicenter, multi-tracer environment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Integrating models using 5-fold cross-validation presents significant challenges, particularly when faced with time constraints during inference. The computational demands are high, contributing to overall complexity. To address these issues, we implemented specific strategies. First, we optimized CPU usage by preprocessing input images only once. Additionally, we parallelized inference across five different models using a multithreading approach. Furthermore, dynamic test-time augmentations (TTA) were employed during inference to enhance robustness and accuracy in tumor lesion segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#bib.bib8" title="">8</a>]</cite>. For larger images, we judiciously limited TTA to minimize augmentation time. Our preliminary results on the autoPET III challenge test set validate the effectiveness of this training strategy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Upon comparing our predicted segmentations with ground truth annotations, our model exhibits remarkable accuracy, particularly in tumor lesion detection and boundary delineation. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13779v1#S3.F2" title="Figure 2 ‣ 3.2 nnU-Net ResEnc XL results and discussion ‣ 3 Results and discussion ‣ AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble"><span class="ltx_text ltx_ref_tag">2</span></a> showcases the ground truth of FDG and PSMA, followed by the segmentation results for tumor lesions in whole-body PET/CT images.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Model Submissions</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Initially, we submitted three versions of models for the preliminary test set:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">A 2D single-fold model achieved a Dice score of 0.9627.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">A 2D 5-fold ensemble of models achieved a Dice score of 0.9602.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">A 3D 5-fold ensemble of models achieved a Dice score of 0.7530.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Moving on to the final test set, we submitted two additional versions of models:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">A 2D 3-fold ensemble of models, resulting in a Dice score of 0.84 on the preliminary test set.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">A 3D 4-fold ensemble of models, resulting in a Dice score of 0.74 on the preliminary test</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To address the challenge of tumor lesion segmentation of PET/CT images from multiple tracers and centers, we proposed a model training approach using a cropped dataset with TotalSegmentator, followed by standard pre-processing steps. However, during inference TotalSegmentator with dynamic test time augmentation helps to improve the overall performance across the 5-fold using 3D models that reflected in the validation leader board as well in AutoPET III challenge. Moreover, 3D model ensemble with STAPLE algorithm outperformed with respect to the individual model performance.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Acknowledgement</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We extend our gratitude to the organizers of the AutoPET III challenge for generously providing the data, enabling us to conduct fascinating research on tumor lesion segmentation in whole-body PET/CT images.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ingrisch, M. (2024) “Automated Lesion Segmentation in Whole-Body PET/CT - Multitracer Multicenter generalization”. 27th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2024), Zenodo. doi: 10.5281/zenodo.10990932.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Griffeth LK. Use of PET/CT scanning in cancer patients: technical and practical considerations. Proc (Bayl Univ Med Cent). 2005 Oct;18(4):321-30. doi: 10.1080/08998280.2005.11928089. PMID: 16252023; PMCID: PMC1255942.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Braune A, Oehme L, Freudenberg R, Hofheinz F, van den Hoff J, Kotzerke J, Hoberück S. Comparison of image quality and spatial resolution between 18F, 68Ga, and 64Cu phantom measurements using a digital Biograph Vision PET/CT. EJNMMI Phys. 2022 Sep 5;9(1):58. PMID: 36064989; PMCID: PMC9445107. <a class="ltx_ref ltx_url ltx_font_typewriter" href="doi:10.1186/s40658-022-00487-7." title="">doi:10.1186/s40658-022-00487-7.</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bhandary, S., Kuhn, D., Babaiee, Z., Fechter, T., Spohn, S.K., Zamboglou, C., Grosu, A.L. and Grosu, R., 2024. Segmentation of Prostate Tumour Volumes from PET Images is a Different Ball Game. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2407.10537" title="">https://doi.org/10.48550/arXiv.2407.10537</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Wasserthal, J., Breit, H.C., Meyer, M.T., Pradella, M., Hinck, D., Sauter, A.W., Heye, T., Boll, D.T., Cyriac, J., Yang, S. and Bach, M., 2023. TotalSegmentator: robust segmentation of 104 anatomic structures in CT images. Radiology: Artificial Intelligence, 5(5).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Isensee, F., Ulrich, C., Wald, T. and Maier-Hein, K.H., 2023, June. Extending nnu-net is all you need. In BVM Workshop (pp. 12-17). Wiesbaden: Springer Fachmedien Wiesbaden.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Isensee, F., Wald, T., Ulrich, C., Baumgartner, M., Roy, S., Maier-Hein, K. and Jaeger, P.F., 2024. nnu-net revisited: A call for rigorous validation in 3d medical image segmentation. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.09556" title="">https://arxiv.org/abs/2404.09556</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Shanmugam, D., Blalock, D., Balakrishnan, G. and Guttag, J., 2021, Better aggregation in test-time augmentation. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 1214-1223).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Mitchell, H. B., 2010, STAPLE: Simultaneous Truth and Performance Level Estimation, Springer Berlin Heidelberg, p233–p236, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-642-11216-4_21" title="">https://doi.org/10.1007/978-3-642-11216-4_21</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 19 20:12:31 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
