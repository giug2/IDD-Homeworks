<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1705.00464] Speech-Based Visual Question Answering</title><meta property="og:description" content="This paper introduces speech-based visual question answering (VQA), the task of generating an answer given an image and a spoken question. Two methods are studied: an end-to-end, deep neural network that directly uses …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Speech-Based Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Speech-Based Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1705.00464">

<!--Generated on Thu Mar  7 12:24:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Speech-Based Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ted Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="1.1.1" class="ltx_text ltx_affiliation_institution">KU Leuven</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tedz.cs@gmail.com">tedz.cs@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dengxin Dai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="2.1.1" class="ltx_text ltx_affiliation_institution">ETH Zurich</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dai@vision.ee.ethz.ch">dai@vision.ee.ethz.ch</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tinne Tuytelaars
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="3.1.1" class="ltx_text ltx_affiliation_institution">KU Leuven</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tinne.tuytelaars@esat.kuleuven.be%20">tinne.tuytelaars@esat.kuleuven.be </a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marie-Francine Moens
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="4.1.1" class="ltx_text ltx_affiliation_institution">KU Leuven</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sien.moens@cs.kuleuven.be">sien.moens@cs.kuleuven.be</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luc Van Gool
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="5.1.1" class="ltx_text ltx_affiliation_institution">ETH Zurich, KU Leuven</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:vangool@vision.ee.ethz.ch">vangool@vision.ee.ethz.ch</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="6.1" class="ltx_p">This paper introduces speech-based visual question answering (VQA), the task of generating an answer given an image and a spoken question. Two methods are studied: an end-to-end, deep neural network that directly uses audio waveforms as input versus a pipelined approach that performs ASR (Automatic Speech Recognition) on the question, followed by text-based visual question answering. Furthermore, we investigate the robustness of both methods by injecting various levels of noise into the spoken question and find both methods to be tolerate noise at similar levels.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The recent years have witnessed great advances in computer vision, natural language processing, and speech recognition thanks to the advances in deep learning <cite class="ltx_cite ltx_citemacro_citep">(LeCun
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2015</a>)</cite> and abundance of data <cite class="ltx_cite ltx_citemacro_citep">(Russakovsky
et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2015</a>)</cite>. This is evidenced not only by the surge of academic papers, but also by the world-wide industry interests. The convincing successes in these individual fields naturally raise the potentials of further integration towards solutions to more general AI problems. Much work has been done to integrate vision and language, resulting in a wide collection of successful applications such as image/video captioning <cite class="ltx_cite ltx_citemacro_citep">(Vinyals
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2015</a>)</cite>, movie-to-book alignment <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2015</a>)</cite>, and visual question answering (VQA) <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>. However, the importance of integrating vision and speech has remained relatively unexplored.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Pertaining to practical applications, voice-user interface (VUI) has become more commonplace, and people are increasingly taking advantage of its characteristics; it is natural, hands-free, eyes-free, far more mobile and faster than typing on certain devices <cite class="ltx_cite ltx_citemacro_citep">(Ruan
et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite>. As many of our daily tasks are relevant to visual scenes, there is a strong need to have a VUI to talk to pictures or videos directly, be it for communication, cooperation, or guidance. Speech-based VQA can be used to assist blind people in performing ordinary tasks, and to dictate robotics in real visual scenes in a hand-free manner such as clinical robotic surgery.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><svg id="S1.F1.pic1" class="ltx_picture ltx_centering" height="299.74" overflow="visible" version="1.1" width="289.41"><g transform="translate(0,299.74) matrix(1 0 0 -1 0 0) translate(163.11,0) translate(0,189.49)"><g stroke="#FF4DFF" fill="#FFF5FF" stroke-width="1.2pt"><path d="M -32.29 -9.84 h 64.57 v 19.69 h -64.57 Z"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -27.67 -4.8)"><foreignObject width="55.35" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.pic1.5.5.5.1.1" class="ltx_text">TextMod</span></foreignObject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 84.02 -3.28)"><foreignObject width="30.75" height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.pic1.6.6.6.1.1" class="ltx_text">pizza</span></foreignObject></g><g stroke="#000000" fill="#000000"><g stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -30 63.33)" fill="#000000" stroke="#000000"><foreignObject width="60" height="40" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/pizzas.jpg" id="S1.F1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="60" height="40" alt="Refer to caption"></foreignObject></g><g stroke="#4DFF4D" fill="#F5FFF5" stroke-width="1.2pt"><path d="M -122.97 -9.84 h 37.47 v 19.69 h -37.47 Z"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -118.36 -4.73)"><foreignObject width="28.25" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.pic1.7.7.7.5.5.1.1" class="ltx_text">ASR</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -158.5 -31.26)" fill="#000000" stroke="#000000"><foreignObject width="108.16" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.pic1.8.8.8.6.6.1.1" class="ltx_text">what food is this?</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -134.23 63.33)" fill="#000000" stroke="#000000"><foreignObject width="60" height="30" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/wavs.png" id="S1.F1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="60" height="30" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -30 -103.33)" fill="#000000" stroke="#000000"><foreignObject width="60" height="40" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/pizzas.jpg" id="S1.F1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="60" height="40" alt="Refer to caption"></foreignObject></g><g stroke="#4D4DFF" fill="#F5F5FF" stroke-width="1.2pt"><path d="M -39.2 -178.33 h 78.41 v 21.52 h -78.41 Z"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -34.59 -171.03)"><foreignObject width="69.57" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.pic1.9.9.9.7.7.1.1" class="ltx_text">SpeechMod</span></foreignObject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 90.94 -170.85)"><foreignObject width="30.75" height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.pic1.10.10.10.8.8.1.1" class="ltx_text">pizza</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -152.69 -182.57)" fill="#000000" stroke="#000000"><foreignObject width="60" height="30" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/wavs.png" id="S1.F1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="60" height="30" alt="Refer to caption"></foreignObject></g><path d="M -85.49 -167.57 L -46.76 -167.57" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -46.76 -167.57)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 0 -110.52 L 0 -149.26" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 0 -149.26)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 46.12 -167.57 L 84.86 -167.57" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 84.86 -167.57)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M -104.23 56.13 L -104.23 17.4" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 -104.23 17.4)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M -78.57 0 L -39.84 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -39.84 0)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 0 56.13 L 0 17.4" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 0 17.4)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 39.2 0 L 77.94 0" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 77.94 0)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>An example of speech-based visual question answering and the two method in this study. A spoken question <em id="S1.F1.3.1" class="ltx_emph ltx_font_italic">what food is this?</em> is asked about the picture, and the system is expected to generate the answer <em id="S1.F1.4.2" class="ltx_emph ltx_font_italic">pizza</em>.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This work investigates the potential of integrating vision and speech in the context of VQA. A spoken version of the <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">VQA1.0</span> dataset is generated to study two different methods of speech-based question answering. One method is an end-to-end approach based on a deep neural network architecture, and the other uses an ASR to first transcribe the text from the spoken question, as shown in <a href="#S1.F1" title="In 1. Introduction ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>. The former approach can be particularly useful for languages that are not serviced by popular ASR systems, i.e. minor languages that have scarce text-speech aligned training data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">main contributions</span> of this paper are three-fold: <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">1)</span> We introduce an end-to-end model that directly produces answers from auditory input without transformations into intermediate pre-learned representations, and compare this with the pipelined approach. <span id="S1.p4.1.3" class="ltx_text ltx_font_bold">2)</span> We inspect the performance impact of having different levels of background noise mixed with the original utterances. <span id="S1.p4.1.4" class="ltx_text ltx_font_bold">3)</span> We release the speech dataset, roughly 200 hours of synthetic audio data and 1 hour of real speech data, to the public. <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>http://data.vision.ee.ethz.ch/daid/VQA/SpeechVQA.zip</span></span></span> The emphasis of this paper is not on achieving state of the art numbers on VQA, but rather on exploring ways to address a new and challenging task.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Visual Question Answering</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The initial introduction of VQA into the AI community <cite class="ltx_cite ltx_citemacro_citep">(Bigham
et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2010</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Malinowski and
Fritz, <a href="#bib.bib19" title="" class="ltx_ref">2014</a>)</cite> was motivated by a desire to build intelligent systems that can understand the world more holistically. In order to complete the task of VQA, it was both necessary to understand a textual question and a visual scene. However, it was not until the introduction of <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">VQA1.0</span> <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> that the application took mainstream in the computer vision and natural language processing (NLP) communities.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Recently, popular topics of exploration have been on the development of attention models. Attention models were popularized by their success with the NLP community in machine translation <cite class="ltx_cite ltx_citemacro_citep">(Bahdanau
et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2014</a>)</cite>, and quickly demonstrated their efficacy in computer vision <cite class="ltx_cite ltx_citemacro_citep">(Mnih
et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2014</a>)</cite>. Within the context of visual question answering, attention mechanisms ‘show’ a model where to look when answering a question. Stacked Attention Network <cite class="ltx_cite ltx_citemacro_citep">(Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2016</a>)</cite> learns an attention mechanism based on the of the question’s encoding to determine the salient regions in an image. More sophisticated attention-centric models such as <cite class="ltx_cite ltx_citemacro_citep">(Nam et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2016</a>; Lu
et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2016</a>; Xiong
et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2016</a>)</cite> were since then developed.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Other points of research are based on the pooling mechanism that combines the language component with the vision components. Some use an element-wise multiplication <cite class="ltx_cite ltx_citemacro_citep">(Xu and Saenko, <a href="#bib.bib30" title="" class="ltx_ref">2016</a>; Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2016</a>)</cite> to pool these modalities, while <cite class="ltx_cite ltx_citemacro_citep">(Lu
et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Fukui et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite> have shown much success in using more complex methods. Our work differs from theirs in that we aim not to improve the performance of VQA, but rather add a new modality of input and introduce appropriate new methods.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Integration of Speech and Vision</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The works also relevant to ours are those integrating speech and vision. Pixeltone <cite class="ltx_cite ltx_citemacro_citep">(Laput et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2013</a>)</cite>
and Image spirit <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2014</a>)</cite> are examples that use voice
commands to guide image processing and semantic segmentation. There is also academic work <cite class="ltx_cite ltx_citemacro_citep">(Srihari and Zhang, <a href="#bib.bib27" title="" class="ltx_ref">2000</a>; Kalashnikov et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2011</a>; Hazen
et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2007</a>)</cite> and an app <cite class="ltx_cite ltx_citemacro_citep">(smi, <a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite> that use speech to provide image descriptions. Their tasks and algorithms are both different from ours. We study the potential of integrating speech and vision in the context of VQA and aim to learn a joint understanding of speech and vision. Those approaches, however, use speech recognition for data collection or result refinement.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Our work also shares similarity with visual-grounded speech understanding or recognition. The closest one in this vein is <cite class="ltx_cite ltx_citemacro_citep">(Harwath
et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite>, in which a deep model is learned with speeches about image captions for speech-based image retrieval. In a broader context of integration of sound and vision, Soundnet <cite class="ltx_cite ltx_citemacro_citep">(Aytar
et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2016</a>)</cite> transfers visual information into sound representations, but this differs from our work because their end goal is to label a sound, not to answer a question.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>End-To-End Speech Recognition</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In the past decade, deep learning has allowed many fields in artificial intelligence to replace traditional hand-crafted features and pipeline systems with end-to-end models. Since speech recognition is typically thought of as a sequence to sequence transduction problem, i.e. given an input sequence, predict an output sequence, the application of LSTM and the CTC <cite class="ltx_cite ltx_citemacro_citep">(Fernández et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2007</a>; Graves et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2006</a>)</cite> promptly showed the success needed to justify its superiority over traditional methods. Current state of the art ASR systems such as DeepSpeech2 <cite class="ltx_cite ltx_citemacro_citep">(Amodei
et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2015</a>)</cite> uses stacked Bi-directional Recurrent Neural Networks in conjunction with Convolutional Neural networks. Our model is similar to theirs in that we use CNNs connected with an LSTM to process audio inputs, however our goal is question answering and not speech recognition.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Dimensions for the conv layers. Example shown with a 2 second long audio waveform, sampled at 16 kHz. The final output dimensions are (3, 512)</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">Layer</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">conv1</th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">pool1</th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">conv2</th>
<th id="S2.T1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">pool2</th>
<th id="S2.T1.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">conv3</th>
<th id="S2.T1.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">pool3</th>
<th id="S2.T1.1.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">conv4</th>
<th id="S2.T1.1.1.1.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">pool4</th>
<th id="S2.T1.1.1.1.10" class="ltx_td ltx_align_left ltx_th ltx_th_column">conv5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Input Dim</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32,000</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16,000</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,000</td>
<td id="S2.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,000</td>
<td id="S2.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">500</td>
<td id="S2.T1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">250</td>
<td id="S2.T1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62</td>
<td id="S2.T1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31</td>
<td id="S2.T1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">7</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"># Filters</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">32</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">32</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">64</td>
<td id="S2.T1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">64</td>
<td id="S2.T1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r">128</td>
<td id="S2.T1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r">128</td>
<td id="S2.T1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r">256</td>
<td id="S2.T1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_r">256</td>
<td id="S2.T1.1.3.2.10" class="ltx_td ltx_align_center">512</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Filter Length</th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">64</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S2.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">32</td>
<td id="S2.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S2.T1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r">16</td>
<td id="S2.T1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S2.T1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r">8</td>
<td id="S2.T1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S2.T1.1.4.3.10" class="ltx_td ltx_align_center">4</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Stride</th>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S2.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S2.T1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S2.T1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S2.T1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S2.T1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S2.T1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S2.T1.1.5.4.10" class="ltx_td ltx_align_center">2</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<th id="S2.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Output Dim</th>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">16,000</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">4,000</td>
<td id="S2.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">2,000</td>
<td id="S2.T1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r">500</td>
<td id="S2.T1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r">250</td>
<td id="S2.T1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r">62</td>
<td id="S2.T1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_r">31</td>
<td id="S2.T1.1.6.5.9" class="ltx_td ltx_align_center ltx_border_r">7</td>
<td id="S2.T1.1.6.5.10" class="ltx_td ltx_align_center">3</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.F2" class="ltx_figure"><svg id="S2.F2.pic1" class="ltx_picture ltx_centering" height="263.36" overflow="visible" version="1.1" width="383.2"><g transform="translate(0,263.36) matrix(1 0 0 -1 0 0) translate(143.2,0) translate(0,105.99)" fill="#000000" stroke="#000000"><g stroke="#666666" stroke-width="1.2pt"><path d="M -70.56 0 C -70.56 5.44 -74.96 9.84 -80.4 9.84 C -85.84 9.84 -90.24 5.44 -90.24 0 C -90.24 -5.44 -85.84 -9.84 -80.4 -9.84 C -74.96 -9.84 -70.56 -5.44 -70.56 0 Z M -80.4 0" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -82.32 -0.73)"><foreignObject width="3.84" height="1.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.1.1.1.1.1" class="ltx_text">.</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M -63.67 16.73 h 45.24 v 19.69 h -45.24 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -59.06 21.84)"><foreignObject width="36.01" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.2.2.2.1.1" class="ltx_text">Dense</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M -65.17 45.95 h 48.24 v 19.69 h -48.24 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -60.56 51.06)"><foreignObject width="39.01" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.3.3.3.1.1" class="ltx_text">LSTM</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M -79.97 75.17 h 77.83 v 21.52 h -77.83 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -75.36 82.47)"><foreignObject width="68.22" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.4.4.4.1.1" class="ltx_text">Embedding</span></foreignObject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -54.89 107.4)"><foreignObject width="27.67" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.5.5.5.1.1" class="ltx_text">Text</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M -142.37 16.73 h 45.24 v 19.69 h -45.24 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -137.76 21.84)"><foreignObject width="36.01" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.6.6.6.1.1" class="ltx_text">Dense</span></foreignObject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -138.01 49.31)"><foreignObject width="36.51" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.7.7.7.1.1" class="ltx_text">Image</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M -103.02 -41 h 45.24 v 19.69 h -45.24 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -98.41 -35.88)"><foreignObject width="36.01" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.8.8.8.1.1" class="ltx_text">Dense</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M -103.02 -74.09 h 45.24 v 19.69 h -45.24 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -98.41 -68.98)"><foreignObject width="36.01" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.9.9.9.1.1" class="ltx_text">Dense</span></foreignObject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -100.77 -98.69)"><foreignObject width="40.74" height="11.2" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.10.10.10.1.1" class="ltx_text">output</span></foreignObject></g><g stroke-width="0.4pt"><path d="M -41.05 74.34 L -41.05 67.1" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 -41.05 67.1)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M -41.05 45.12 L -41.05 37.88" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 -41.05 37.88)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M -41.05 15.9 L -69.17 0.31" style="fill:none"></path><g transform="matrix(-0.87457 -0.4849 0.4849 -0.87457 -69.17 0.31)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M -119.75 15.9 L -91.63 0.31" style="fill:none"></path><g transform="matrix(0.87457 -0.4849 0.4849 0.87457 -91.63 0.31)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M -80.4 -10.67 L -80.4 -17.91" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 -80.4 -17.91)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M -80.4 -43.77 L -80.4 -51" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 -80.4 -51)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M -80.4 -76.86 L -80.4 -84.1" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 -80.4 -84.1)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><g stroke="#666666" stroke-width="1.2pt"><path d="M 90.24 0 C 90.24 5.44 85.84 9.84 80.4 9.84 C 74.96 9.84 70.56 5.44 70.56 0 C 70.56 -5.44 74.96 -9.84 80.4 -9.84 C 85.84 -9.84 90.24 -5.44 90.24 0 Z M 80.4 0" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 78.48 -0.73)"><foreignObject width="3.84" height="1.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.11.11.11.1.1.1" class="ltx_text">.</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M 97.13 16.73 h 45.24 v 19.69 h -45.24 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 101.74 21.84)"><foreignObject width="36.01" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.12.12.12.2.1.1" class="ltx_text">Dense</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M 95.63 45.95 h 48.24 v 19.69 h -48.24 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 100.24 51.06)"><foreignObject width="39.01" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.13.13.13.3.1.1" class="ltx_text">LSTM</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M 0.33 75.17 h 238.84 v 56.92 h -238.84 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 4.94 79.78)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 40.77)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 33.86)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 77.18 0)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 26.94)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 20.02)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 13.945)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 17.33)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Conv Layers</text></g></g></g></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 30.4)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0.37 0)"><text transform="matrix(1 0 0 -1 0 0)">(Dimensions</text></g></g></g></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 44.24)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignObject width="229.62" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">in <a href="#S2.T1" title="In 2.3. End-To-End Speech Recognition ‣ 2. Related Works ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>)</foreignObject></g></g></g></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 101.49 142.72)"><foreignObject width="36.51" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.14.14.14.4.1.1" class="ltx_text">Audio</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M 18.43 16.73 h 45.24 v 19.69 h -45.24 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 23.05 21.84)"><foreignObject width="36.01" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.15.15.15.5.1.1" class="ltx_text">Dense</span></foreignObject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 22.8 49.31)"><foreignObject width="36.51" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.16.16.16.6.1.1" class="ltx_text">Image</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M 57.78 -41 h 45.24 v 19.69 h -45.24 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 62.39 -35.88)"><foreignObject width="36.01" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.17.17.17.7.1.1" class="ltx_text">Dense</span></foreignObject></g><g stroke="#666666" stroke-width="1.2pt"><path d="M 57.78 -74.09 h 45.24 v 19.69 h -45.24 Z" style="fill:none"></path></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 62.39 -68.98)"><foreignObject width="36.01" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.18.18.18.8.1.1" class="ltx_text">Dense</span></foreignObject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 60.03 -98.69)"><foreignObject width="40.74" height="11.2" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F2.pic1.19.19.19.9.1.1" class="ltx_text">output</span></foreignObject></g><path d="M 119.75 74.34 L 119.75 67.1" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 119.75 67.1)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 119.75 45.12 L 119.75 37.88" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 119.75 37.88)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 119.75 15.9 L 91.63 0.31" style="fill:none"></path><g transform="matrix(-0.87457 -0.4849 0.4849 -0.87457 91.63 0.31)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 41.05 15.9 L 69.17 0.31" style="fill:none"></path><g transform="matrix(0.87457 -0.4849 0.4849 0.87457 69.17 0.31)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 80.4 -10.67 L 80.4 -17.91" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 80.4 -17.91)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 80.4 -43.77 L 80.4 -51" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 80.4 -51)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g><path d="M 80.4 -76.86 L 80.4 -84.1" style="fill:none"></path></g><g transform="matrix(0.0 -1.0 1.0 0.0 80.4 -84.1)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.32pt"><path d="M -1.66 2.21 C -1.52 1.38 0 0.14 0.42 0 C 0 -0.14 -1.52 -1.38 -1.66 -2.21" style="fill:none"></path></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>TextMod (left) and SpeechMod (right) architectures</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Model</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Two models are employed in this work, they will be referred to henceforth as TextMod and SpeechMod. TextMod and SpeechMod only differ in their language components, keeping rest of the architecture the same. On the language side, TextMod takes as input a series of one-hot encodings, followed by an embedding layer that is learned from scratch, a LSTM encoder, and a dense layer. It is similar to <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">VQA1.0</span> with some minor adjustments.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The language side of SpeechMod takes as input the raw waveform, and pushes it through a series of 1D convolutions. After the CNN layers follows a LSTM. The LSTM serves the same purpose as in TextMod, which is to interpret and encode the sequence meaningfully into a single vector.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.4" class="ltx_p">Convolution layers are used to encode waveforms because they reduce dimensionality of data while finding salient patterns. The maximum length of a spoken question in our dataset is 6.7 seconds and corresponds to a waveform length of 107,360 elements, while the minimum is 0.63 seconds and corresponds to 10,080 elements. One could directly feed the input waveform to a LSTM, but a LSTM will be unable to learn from sequences that are excessively long, so dimensionality reduction is a necessity. Each consecutive convolution layer halves in filter length but doubles the number of filters. This is done for simplicity rather than for performance optimization. The main consideration taken in choosing the parameters is that the last convolution should output dimensions of (<math id="S3.p3.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">x</annotation></semantics></math>, 512), where <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">x</annotation></semantics></math> must be a positive integer. <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">x</annotation></semantics></math> represents the length of a sequence of 512-dim vectors. The sequence is then fed into an LSTM, which then outputs a single vector of (512). Thus, <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">x</annotation></semantics></math> should not be too big, and the CNN parameters are chosen to ensure a sensible sequence length. The exact dimensions of the convolution layers are shown in <a href="#S2.T1" title="In 2.3. End-To-End Speech Recognition ‣ 2. Related Works ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>. The longest and shortest waveforms correspond to final convolution outputs of size (13, 512) and (1, 512) respectively. 512 is used as the dimension of the LSTM to be consistent with TextMod and the original VQA baseline.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">On the visual side, both models ingest as input the 4,096 dimensional vector of the last layer of VGG19 <cite class="ltx_cite ltx_citemacro_citep">(Simonyan and
Zisserman, <a href="#bib.bib26" title="" class="ltx_ref">2015</a>)</cite> followed by a single dense layer. After both visual and linguistic representations are computed, they are merged using element-wise multiplication, a dense layer, and an output layer. The full architecture of both these models are seen in <a href="#S2.F2" title="In 2.3. End-To-End Speech Recognition ‣ 2. Related Works ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>, where <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="\displaystyle\odot" display="inline"><semantics id="S3.p4.1.m1.1a"><mo id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><csymbol cd="latexml" id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">\displaystyle\odot</annotation></semantics></math> is the symbol for element-wise multiplication. After merging the language and visual components of each model, two dense layers are stacked. The last dense layer outputs a probability distribution over the number of output classes, and the answer corresponding to the element with the highest probability is selected.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">The architectures presented in this chapter were chosen for two main reasons: simplicity and similarity. First, the intention is to keep the model complexity low. In order to establish a baseline for speech-based VQA, it is necessary to use only the bare minimum components. TextMod, as mentioned before, is similar to the original VQA baseline, which is well referenced and remains the simplest architecture on <span id="S3.p5.1.1" class="ltx_text ltx_font_italic">VQA1.0</span>. Despite its many convolution layers, SpeechMod also uses minimal components. Second, it is important that TextMod and SpeechMod differ from each other as little as possible. Similarity between models allows one to locate the source of discrepancies and helps produce a more rigorous comparison. The only difference in the two models is replacing an embedding layer with a series of convolution layers. In our implementation, the layers that are common between the two models also have the same dimensions.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Data</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We chose to use <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">VQA1.0</span> Open-Ended dataset, for its numerous training examples and familiarity to those working in question answering. To avoid confusion, <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">VQA1.0</span> henceforth refers to the dataset and the original paper, while VQA refers to the task of visual question answering. The dataset contains 248,349 questions in the training set, 121,512 in validation set, and 60,864 in the test-dev set. The complete test set contains 244,302 questions, but because the evaluation server allows for only one submission, we instead evaluate on test-dev, which has no such limit. During training, questions which do not contain the 1000 most common answers are filtered out.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Amazon Polly API is used to generate audio files for each question. <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The voice of Joanna is used: <a target="_blank" href="https://aws.amazon.com/polly/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/polly/</a></span></span></span> The generated speech is in mp3 format, then sampled into waveform format at 16 kHz. 16 kHz was chosen due to its common usage among the speech community, but also because the model used to transcribe speech was trained on 16 kHz audio waveforms. It is worthwhile to note that the audio generator uses a female voice, thus the training and testing data are all with the same voice, except for the examples we’ve recorded, which is covered below. The full Amazon Polly speech dataset will be made available to the public.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><svg id="S4.F3.pic1" class="ltx_picture ltx_centering" height="445.06" overflow="visible" version="1.1" width="325.06"><g transform="translate(0,445.06) matrix(1 0 0 -1 0 0) translate(171.68,0) translate(0,442.01)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(0.66 0.0 0.0 0.66 -39.6 -95.13)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/2_noise0.png" id="S4.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -60.44 -105.34)" fill="#000000" stroke="#000000"><foreignObject width="183.92" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.13.13.13.1.1" class="ltx_text">what is the number of the bus</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -39.6 -187.3)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/2_noise30.png" id="S4.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -62.11 -197.51)" fill="#000000" stroke="#000000"><foreignObject width="187.84" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.14.14.14.1.1" class="ltx_text">what is the number of the boss</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -39.6 -279.48)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/2_noise50.png" id="S4.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -20.84 -289.69)" fill="#000000" stroke="#000000"><foreignObject width="63.15" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.15.15.15.1.1" class="ltx_text">where is it</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -39.6 -371.66)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/2_recorded.png" id="S4.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -62.11 -381.87)" fill="#000000" stroke="#000000"><foreignObject width="187.84" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.16.16.16.1.1" class="ltx_text">what is the number of the boss</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -135.83 -95.13)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/1_noise0.png" id="S4.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -139.51 -105.34)" fill="#000000" stroke="#000000"><foreignObject width="131.14" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.17.17.17.1.1" class="ltx_text">what time of day is it</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -135.83 -187.3)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/1_noise30.png" id="S4.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -140.52 -197.51)" fill="#000000" stroke="#000000"><foreignObject width="134.22" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.18.18.18.1.1" class="ltx_text">what time of day isn’t</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -135.83 -279.48)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/1_noise50.png" id="S4.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -144.2 -289.69)" fill="#000000" stroke="#000000"><foreignObject width="145.37" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.19.19.19.1.1" class="ltx_text">what time and day isn’t</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -135.83 -371.66)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/1_recorded.png" id="S4.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -139.51 -381.87)" fill="#000000" stroke="#000000"><foreignObject width="131.14" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.20.20.20.1.1" class="ltx_text">what time of day is it</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 56.63 -95.13)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/3_noise0.png" id="S4.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 42.12 -105.34)" fill="#000000" stroke="#000000"><foreignObject width="163.97" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.21.21.21.1.1" class="ltx_text">are there clouds on the sky</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 56.63 -187.3)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/3_noise30.png" id="S4.F3.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 49.98 -197.51)" fill="#000000" stroke="#000000"><foreignObject width="140.14" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.22.22.22.1.1" class="ltx_text">are there clouds on sky</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 56.63 -279.48)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/3_noise50.png" id="S4.F3.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 46.81 -289.69)" fill="#000000" stroke="#000000"><foreignObject width="149.75" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.23.23.23.1.1" class="ltx_text">are there files on the sky</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 56.63 -371.66)" fill="#000000" stroke="#000000"><foreignObject width="120" height="90" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/1705.00464/assets/images/3_recorded.png" id="S4.F3.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 42.12 -381.87)" fill="#000000" stroke="#000000"><foreignObject width="163.97" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.24.24.24.1.1" class="ltx_text">are there clouds on the sky</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -166.35 -58.1)" fill="#000000" stroke="#000000"><foreignObject width="55.81" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.25.25.25.1.1" class="ltx_text ltx_font_bold">0% Noise</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -168.63 -150.28)" fill="#000000" stroke="#000000"><foreignObject width="62.73" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.26.26.26.1.1" class="ltx_text ltx_font_bold">30% Noise</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -168.63 -242.46)" fill="#000000" stroke="#000000"><foreignObject width="62.73" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.27.27.27.1.1" class="ltx_text ltx_font_bold">50% Noise</span></foreignObject></g><g transform="matrix(0.66 0.0 0.0 0.66 -162.52 -335.14)" fill="#000000" stroke="#000000"><foreignObject width="44.2" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F3.pic1.28.28.28.1.1" class="ltx_text ltx_font_bold">Human</span></foreignObject></g><path d="M -59.76 -25.98 L -59.76 -441.73" style="fill:none"></path><path d="M 59.76 -25.98 L 59.76 -441.73" style="fill:none"></path></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Spectrograms for 3 example questions with corresponding transcribed text below. 3 synthetically generated and 1 human-recorded audio clips for each question.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The noise we mixed with the original speech files is selected randomly from the Urban8K dataset <cite class="ltx_cite ltx_citemacro_citep">(Salamon
et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2014</a>)</cite>. This dataset contains 10 categories: air conditioner, car horn, children playing, dog bark, drilling, engine idling, gun shot, jackhammer, siren, and street music. Some clips are soft enough in volume and thus considered background noise, others are loud enough to be considered foreground noise. For each original audio file, a random noise file is selected, and combined to produce a corrupted question file according to the weighting scheme:</p>
<table id="S4.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex1.m1.1" class="ltx_Math" alttext="W_{corrupted}=(1-\mathit{NL})*W_{original}+\mathit{NL}*W_{noise}" display="block"><semantics id="S4.Ex1.m1.1a"><mrow id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml"><msub id="S4.Ex1.m1.1.1.3" xref="S4.Ex1.m1.1.1.3.cmml"><mi id="S4.Ex1.m1.1.1.3.2" xref="S4.Ex1.m1.1.1.3.2.cmml">W</mi><mrow id="S4.Ex1.m1.1.1.3.3" xref="S4.Ex1.m1.1.1.3.3.cmml"><mi id="S4.Ex1.m1.1.1.3.3.2" xref="S4.Ex1.m1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.3.3.1" xref="S4.Ex1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.3.3.3" xref="S4.Ex1.m1.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.3.3.1a" xref="S4.Ex1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.3.3.4" xref="S4.Ex1.m1.1.1.3.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.3.3.1b" xref="S4.Ex1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.3.3.5" xref="S4.Ex1.m1.1.1.3.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.3.3.1c" xref="S4.Ex1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.3.3.6" xref="S4.Ex1.m1.1.1.3.3.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.3.3.1d" xref="S4.Ex1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.3.3.7" xref="S4.Ex1.m1.1.1.3.3.7.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.3.3.1e" xref="S4.Ex1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.3.3.8" xref="S4.Ex1.m1.1.1.3.3.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.3.3.1f" xref="S4.Ex1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.3.3.9" xref="S4.Ex1.m1.1.1.3.3.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.3.3.1g" xref="S4.Ex1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.3.3.10" xref="S4.Ex1.m1.1.1.3.3.10.cmml">d</mi></mrow></msub><mo id="S4.Ex1.m1.1.1.2" xref="S4.Ex1.m1.1.1.2.cmml">=</mo><mrow id="S4.Ex1.m1.1.1.1" xref="S4.Ex1.m1.1.1.1.cmml"><mrow id="S4.Ex1.m1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.cmml"><mrow id="S4.Ex1.m1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.Ex1.m1.1.1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.Ex1.m1.1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.cmml"><mn id="S4.Ex1.m1.1.1.1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S4.Ex1.m1.1.1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.3.cmml">𝑁𝐿</mi></mrow><mo rspace="0.055em" stretchy="false" id="S4.Ex1.m1.1.1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S4.Ex1.m1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.2.cmml">∗</mo><msub id="S4.Ex1.m1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.3.cmml"><mi id="S4.Ex1.m1.1.1.1.1.3.2" xref="S4.Ex1.m1.1.1.1.1.3.2.cmml">W</mi><mrow id="S4.Ex1.m1.1.1.1.1.3.3" xref="S4.Ex1.m1.1.1.1.1.3.3.cmml"><mi id="S4.Ex1.m1.1.1.1.1.3.3.2" xref="S4.Ex1.m1.1.1.1.1.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.3.3.1" xref="S4.Ex1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.1.3.3.3" xref="S4.Ex1.m1.1.1.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.3.3.1a" xref="S4.Ex1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.1.3.3.4" xref="S4.Ex1.m1.1.1.1.1.3.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.3.3.1b" xref="S4.Ex1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.1.3.3.5" xref="S4.Ex1.m1.1.1.1.1.3.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.3.3.1c" xref="S4.Ex1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.1.3.3.6" xref="S4.Ex1.m1.1.1.1.1.3.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.3.3.1d" xref="S4.Ex1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.1.3.3.7" xref="S4.Ex1.m1.1.1.1.1.3.3.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.3.3.1e" xref="S4.Ex1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.1.3.3.8" xref="S4.Ex1.m1.1.1.1.1.3.3.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.3.3.1f" xref="S4.Ex1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.1.3.3.9" xref="S4.Ex1.m1.1.1.1.1.3.3.9.cmml">l</mi></mrow></msub></mrow><mo id="S4.Ex1.m1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.2.cmml">+</mo><mrow id="S4.Ex1.m1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.3.cmml"><mi id="S4.Ex1.m1.1.1.1.3.2" xref="S4.Ex1.m1.1.1.1.3.2.cmml">𝑁𝐿</mi><mo lspace="0.222em" rspace="0.222em" id="S4.Ex1.m1.1.1.1.3.1" xref="S4.Ex1.m1.1.1.1.3.1.cmml">∗</mo><msub id="S4.Ex1.m1.1.1.1.3.3" xref="S4.Ex1.m1.1.1.1.3.3.cmml"><mi id="S4.Ex1.m1.1.1.1.3.3.2" xref="S4.Ex1.m1.1.1.1.3.3.2.cmml">W</mi><mrow id="S4.Ex1.m1.1.1.1.3.3.3" xref="S4.Ex1.m1.1.1.1.3.3.3.cmml"><mi id="S4.Ex1.m1.1.1.1.3.3.3.2" xref="S4.Ex1.m1.1.1.1.3.3.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.3.3.3.1" xref="S4.Ex1.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.3.3.3.3" xref="S4.Ex1.m1.1.1.1.3.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.3.3.3.1a" xref="S4.Ex1.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.3.3.3.4" xref="S4.Ex1.m1.1.1.1.3.3.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.3.3.3.1b" xref="S4.Ex1.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.3.3.3.5" xref="S4.Ex1.m1.1.1.1.3.3.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.3.3.3.1c" xref="S4.Ex1.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.3.3.3.6" xref="S4.Ex1.m1.1.1.1.3.3.3.6.cmml">e</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.1b"><apply id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1"><eq id="S4.Ex1.m1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.2"></eq><apply id="S4.Ex1.m1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.3.1.cmml" xref="S4.Ex1.m1.1.1.3">subscript</csymbol><ci id="S4.Ex1.m1.1.1.3.2.cmml" xref="S4.Ex1.m1.1.1.3.2">𝑊</ci><apply id="S4.Ex1.m1.1.1.3.3.cmml" xref="S4.Ex1.m1.1.1.3.3"><times id="S4.Ex1.m1.1.1.3.3.1.cmml" xref="S4.Ex1.m1.1.1.3.3.1"></times><ci id="S4.Ex1.m1.1.1.3.3.2.cmml" xref="S4.Ex1.m1.1.1.3.3.2">𝑐</ci><ci id="S4.Ex1.m1.1.1.3.3.3.cmml" xref="S4.Ex1.m1.1.1.3.3.3">𝑜</ci><ci id="S4.Ex1.m1.1.1.3.3.4.cmml" xref="S4.Ex1.m1.1.1.3.3.4">𝑟</ci><ci id="S4.Ex1.m1.1.1.3.3.5.cmml" xref="S4.Ex1.m1.1.1.3.3.5">𝑟</ci><ci id="S4.Ex1.m1.1.1.3.3.6.cmml" xref="S4.Ex1.m1.1.1.3.3.6">𝑢</ci><ci id="S4.Ex1.m1.1.1.3.3.7.cmml" xref="S4.Ex1.m1.1.1.3.3.7">𝑝</ci><ci id="S4.Ex1.m1.1.1.3.3.8.cmml" xref="S4.Ex1.m1.1.1.3.3.8">𝑡</ci><ci id="S4.Ex1.m1.1.1.3.3.9.cmml" xref="S4.Ex1.m1.1.1.3.3.9">𝑒</ci><ci id="S4.Ex1.m1.1.1.3.3.10.cmml" xref="S4.Ex1.m1.1.1.3.3.10">𝑑</ci></apply></apply><apply id="S4.Ex1.m1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1"><plus id="S4.Ex1.m1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.2"></plus><apply id="S4.Ex1.m1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1"><times id="S4.Ex1.m1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.1.2"></times><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1"><minus id="S4.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S4.Ex1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.3">𝑁𝐿</ci></apply><apply id="S4.Ex1.m1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.3.1.cmml" xref="S4.Ex1.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.3.2.cmml" xref="S4.Ex1.m1.1.1.1.1.3.2">𝑊</ci><apply id="S4.Ex1.m1.1.1.1.1.3.3.cmml" xref="S4.Ex1.m1.1.1.1.1.3.3"><times id="S4.Ex1.m1.1.1.1.1.3.3.1.cmml" xref="S4.Ex1.m1.1.1.1.1.3.3.1"></times><ci id="S4.Ex1.m1.1.1.1.1.3.3.2.cmml" xref="S4.Ex1.m1.1.1.1.1.3.3.2">𝑜</ci><ci id="S4.Ex1.m1.1.1.1.1.3.3.3.cmml" xref="S4.Ex1.m1.1.1.1.1.3.3.3">𝑟</ci><ci id="S4.Ex1.m1.1.1.1.1.3.3.4.cmml" xref="S4.Ex1.m1.1.1.1.1.3.3.4">𝑖</ci><ci id="S4.Ex1.m1.1.1.1.1.3.3.5.cmml" xref="S4.Ex1.m1.1.1.1.1.3.3.5">𝑔</ci><ci id="S4.Ex1.m1.1.1.1.1.3.3.6.cmml" xref="S4.Ex1.m1.1.1.1.1.3.3.6">𝑖</ci><ci id="S4.Ex1.m1.1.1.1.1.3.3.7.cmml" xref="S4.Ex1.m1.1.1.1.1.3.3.7">𝑛</ci><ci id="S4.Ex1.m1.1.1.1.1.3.3.8.cmml" xref="S4.Ex1.m1.1.1.1.1.3.3.8">𝑎</ci><ci id="S4.Ex1.m1.1.1.1.1.3.3.9.cmml" xref="S4.Ex1.m1.1.1.1.1.3.3.9">𝑙</ci></apply></apply></apply><apply id="S4.Ex1.m1.1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.1.3"><times id="S4.Ex1.m1.1.1.1.3.1.cmml" xref="S4.Ex1.m1.1.1.1.3.1"></times><ci id="S4.Ex1.m1.1.1.1.3.2.cmml" xref="S4.Ex1.m1.1.1.1.3.2">𝑁𝐿</ci><apply id="S4.Ex1.m1.1.1.1.3.3.cmml" xref="S4.Ex1.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.3.3.1.cmml" xref="S4.Ex1.m1.1.1.1.3.3">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.3.3.2.cmml" xref="S4.Ex1.m1.1.1.1.3.3.2">𝑊</ci><apply id="S4.Ex1.m1.1.1.1.3.3.3.cmml" xref="S4.Ex1.m1.1.1.1.3.3.3"><times id="S4.Ex1.m1.1.1.1.3.3.3.1.cmml" xref="S4.Ex1.m1.1.1.1.3.3.3.1"></times><ci id="S4.Ex1.m1.1.1.1.3.3.3.2.cmml" xref="S4.Ex1.m1.1.1.1.3.3.3.2">𝑛</ci><ci id="S4.Ex1.m1.1.1.1.3.3.3.3.cmml" xref="S4.Ex1.m1.1.1.1.3.3.3.3">𝑜</ci><ci id="S4.Ex1.m1.1.1.1.3.3.3.4.cmml" xref="S4.Ex1.m1.1.1.1.3.3.3.4">𝑖</ci><ci id="S4.Ex1.m1.1.1.1.3.3.3.5.cmml" xref="S4.Ex1.m1.1.1.1.3.3.3.5">𝑠</ci><ci id="S4.Ex1.m1.1.1.1.3.3.3.6.cmml" xref="S4.Ex1.m1.1.1.1.3.3.3.6">𝑒</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.1c">W_{corrupted}=(1-\mathit{NL})*W_{original}+\mathit{NL}*W_{noise}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.p3.2" class="ltx_p">where <span id="S4.p3.2.1" class="ltx_text ltx_font_italic">NL</span> is the noise level. The noise audio files are subsampled to 16 kHz in order to match that of the original audio file, and is clipped to also match the spoken question length. When the spoken question is longer than the noise file, the noise file is repeated until its duration exceeds that of the spoken question. Both files are normalized before being combined so that contributions are strictly proportional to the noise level chosen. We choose 5 noise levels to mix together: 10%-50%, at 10% intervals. Anything beyond 50% is unrealistic. A visualization of different noise levels can be seen in <a href="#S4.F3" title="In 4. Data ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> and its corresponding audio clips can be found online.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://soundcloud.com/sbvqa/sets/speechvqa</span></span></span></p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We also make an additional, supplementary study of the practicality of speech-based VQA with real data. 1000 questions from the <span id="S4.p4.1.1" class="ltx_text ltx_font_italic">val</span> set were randomly selected and recorded with human speakers. Two speakers (one male and one female) participated the recording task. In total, 1/3 of the data is from a male speaker, the rest is from a female speaker. Both speakers are graduate students who are not native anglophones. The data was recorded in an office environment, and there are various background noises in the audio clips as they naturally occurred.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Word Error Rate from Kaldi speech recognition</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">Noise (%)</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">WER (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">0</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">8.46</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center">12.37</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">20</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center">17.77</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">30</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center">25.41</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">40</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center">35.15</td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<th id="S4.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">50</th>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_align_center">47.90</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Preprocessing</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">For SpeechMod, the first preprocessing step is to scale each waveform to a range of [-256, 256], similar to the procedure from SoundNet <cite class="ltx_cite ltx_citemacro_citep">(Aytar
et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2016</a>)</cite>. There was no need to center each example around 0, as they are already centered. Next, each batch of waveforms were padded with 0 at the end to be of the same length.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">For TextMod, the standard preprocessing steps from <span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">VQA1.0</span> were followed. The procedure tokenizes each sentence and replaces it with a number that corresponds to the word’s index. These number indices are used as input, since the question will be fed to the model as a sequence of one hot encodings. Because questions have different lengths, the 0 index is used as padding for sequences that are too short. The 0 index essentially causes the model to skip that position. 0 is also used for unseen tokens, which is especially useful when dealing with out of vocabulary words during evaluation.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>ASR</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We use Kaldi <cite class="ltx_cite ltx_citemacro_citep">(Povey et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2011</a>)</cite> for ASR, due to its open-source codebase and popularity with the speech research community. The model used in this work is a DNN-HMM<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/api-ai/api-ai-english-asr-model</span></span></span> that has been pre-trained on assistant.ai logs (essentially short commands), making it suitable for transcribing short utterances such as the questions in <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">VQA1.0</span>. Other ASRs such as wit.ai from Facebook, Cloud Speech from Google, and Bing Speech Microsoft were tested but not used in the final experiments because Kaldi achieved the lowest word error rates.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Word error rate (WER) is used to measure the accuracy of speech to text transcriptions. WER is defined as follows:</p>
<table id="S5.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex2.m1.1" class="ltx_Math" alttext="\mathit{WER}=(S+D+I)/N" display="block"><semantics id="S5.Ex2.m1.1a"><mrow id="S5.Ex2.m1.1.1" xref="S5.Ex2.m1.1.1.cmml"><mi id="S5.Ex2.m1.1.1.3" xref="S5.Ex2.m1.1.1.3.cmml">𝑊𝐸𝑅</mi><mo id="S5.Ex2.m1.1.1.2" xref="S5.Ex2.m1.1.1.2.cmml">=</mo><mrow id="S5.Ex2.m1.1.1.1" xref="S5.Ex2.m1.1.1.1.cmml"><mrow id="S5.Ex2.m1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.Ex2.m1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.Ex2.m1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml"><mi id="S5.Ex2.m1.1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.2.cmml">S</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.3.cmml">D</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1a" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.4" xref="S5.Ex2.m1.1.1.1.1.1.1.4.cmml">I</mi></mrow><mo stretchy="false" id="S5.Ex2.m1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S5.Ex2.m1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.2.cmml">/</mo><mi id="S5.Ex2.m1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.3.cmml">N</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex2.m1.1b"><apply id="S5.Ex2.m1.1.1.cmml" xref="S5.Ex2.m1.1.1"><eq id="S5.Ex2.m1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.2"></eq><ci id="S5.Ex2.m1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.3">𝑊𝐸𝑅</ci><apply id="S5.Ex2.m1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1"><divide id="S5.Ex2.m1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.2"></divide><apply id="S5.Ex2.m1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1"><plus id="S5.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1"></plus><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2">𝑆</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.3">𝐷</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.4.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.4">𝐼</ci></apply><ci id="S5.Ex2.m1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex2.m1.1c">\mathit{WER}=(S+D+I)/N</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS2.p2.2" class="ltx_p">Where <span id="S5.SS2.p2.2.1" class="ltx_text ltx_font_italic">S</span> is the number of substitutions, <span id="S5.SS2.p2.2.2" class="ltx_text ltx_font_italic">D</span> is the number of deletions, and <span id="S5.SS2.p2.2.3" class="ltx_text ltx_font_italic">I</span> is the number of insertions. <span id="S5.SS2.p2.2.4" class="ltx_text ltx_font_italic">N</span> is the total number of words in the sentence being translated. Each transcribed question is compared with the original; the results are shown in <a href="#S4.T2" title="In 4. Data ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>. WER is not expected to be a perfect measure of transcription accuracy, since some words are more essential to the meaning of a sentence than other words. For example, missing the word <em id="S5.SS2.p2.2.5" class="ltx_emph ltx_font_italic">dog</em> in the sentence <em id="S5.SS2.p2.2.6" class="ltx_emph ltx_font_italic">what is the dog eating</em> is more detrimental than missing the word <em id="S5.SS2.p2.2.7" class="ltx_emph ltx_font_italic">the</em>, but we nevertheless employ it to convey a general notion of how many words are understood by the ASR. Naturally the more noise there is, the higher the word error rate becomes. Due to transcription errors, there are resulting questions that contain words not seen in the original datasets. These words, as mentioned above, are indexed as 0 and are masked when fed into TextMod.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Implementation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Keras was used to run all experiments, with the Adam <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib15" title="" class="ltx_ref">2014</a>)</cite> optimizer for both architectures. No parameter tuning was done; default Adam parameters are as follows: learning rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, learning rate decay=0.0. Training TextMod for 10 epochs on <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">train</span> + <span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_italic">val</span> takes roughly an hour on a Nvidia Titan X GPU, and our best model was taken at 30 epochs. Training SpeechMod for 10 epochs takes roughly 7 hours. The reported model is taken at 30 epochs. The code is available to the public.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/zted/sbvqa</span></span></span></p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Results</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The goal of the main experiments were to observe how each model performs with and without different levels of noise added. Results are reported on <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">test-dev</span>, which corresponds to training on <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">train</span> + <span id="S6.p1.1.3" class="ltx_text ltx_font_italic">val</span> (<a href="#S6.T3" title="In 6. Results ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>). The standard format of reporting results from <span id="S6.p1.1.4" class="ltx_text ltx_font_italic">VQA1.0</span> is followed: <span id="S6.p1.1.5" class="ltx_text ltx_font_italic">All</span> is the overall accuracy, <span id="S6.p1.1.6" class="ltx_text ltx_font_italic">Y/N</span> is for questions with yes or no as answers, <span id="S6.p1.1.7" class="ltx_text ltx_font_italic">Number</span> is for questions that are answered by counting, and <span id="S6.p1.1.8" class="ltx_text ltx_font_italic">Other</span> covers the rest.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">TextMod is trained on the original questions (<span id="S6.p2.1.1" class="ltx_text ltx_font_italic">OQ</span>), with the best performing model being selected. ASR is used on the 0-50% variants to convert the audio question to text. Then, the selected model from <span id="S6.p2.1.2" class="ltx_text ltx_font_italic">OQ</span> is used to evaluate based on the transcribed text. Concretely, the best performing model obtained on <span id="S6.p2.1.3" class="ltx_text ltx_font_italic">test-dev</span> is used to evaluate the transcribed variants of <span id="S6.p2.1.4" class="ltx_text ltx_font_italic">test-dev</span>. Likewise, SpeechMod is first trained on audio data with 0% noise, with the strongest model being selected. The selected model is used to evaluate on the 10-50% variants of the same data subset. Typically, the best model on <span id="S6.p2.1.5" class="ltx_text ltx_font_italic">val</span> is used to evaluate on <span id="S6.p2.1.6" class="ltx_text ltx_font_italic">test</span> or another ‘unseen’ portion of the dataset. However in these experiments, the noisy variants of the same datasets are in fact unseen because the data for which the model is trained on contains no noise. We show this in the zero-shot section of the paper.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Accuracy on <span id="S6.T3.2.1" class="ltx_text ltx_font_italic">test-dev</span> with different levels of noise added. (Higher is better)</figcaption>
<table id="S6.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T3.3.1.1" class="ltx_tr">
<th id="S6.T3.3.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S6.T3.3.1.1.3" class="ltx_td ltx_align_center">All</td>
<td id="S6.T3.3.1.1.4" class="ltx_td ltx_align_center">Y/N</td>
<td id="S6.T3.3.1.1.5" class="ltx_td ltx_align_center">Number</td>
<td id="S6.T3.3.1.1.6" class="ltx_td ltx_align_center">Other</td>
</tr>
<tr id="S6.T3.3.2.2" class="ltx_tr">
<th id="S6.T3.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<th id="S6.T3.3.2.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S6.T3.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">53.74</td>
<td id="S6.T3.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t">78.94</td>
<td id="S6.T3.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t">35.24</td>
<td id="S6.T3.3.2.2.6" class="ltx_td ltx_align_center ltx_border_t">36.42</td>
</tr>
<tr id="S6.T3.3.3.3" class="ltx_tr">
<th id="S6.T3.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">TextMod</th>
<th id="S6.T3.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Blind</th>
<td id="S6.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">48.76</td>
<td id="S6.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">78.20</td>
<td id="S6.T3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">35.68</td>
<td id="S6.T3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t">26.59</td>
</tr>
<tr id="S6.T3.3.4.4" class="ltx_tr">
<th id="S6.T3.3.4.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OQ</th>
<td id="S6.T3.3.4.4.3" class="ltx_td ltx_align_center">56.66</td>
<td id="S6.T3.3.4.4.4" class="ltx_td ltx_align_center">78,89</td>
<td id="S6.T3.3.4.4.5" class="ltx_td ltx_align_center">37.24</td>
<td id="S6.T3.3.4.4.6" class="ltx_td ltx_align_center">42.07</td>
</tr>
<tr id="S6.T3.3.5.5" class="ltx_tr">
<th id="S6.T3.3.5.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">0%</th>
<td id="S6.T3.3.5.5.3" class="ltx_td ltx_align_center">54.03</td>
<td id="S6.T3.3.5.5.4" class="ltx_td ltx_align_center">75.47</td>
<td id="S6.T3.3.5.5.5" class="ltx_td ltx_align_center">36.82</td>
<td id="S6.T3.3.5.5.6" class="ltx_td ltx_align_center">39.62</td>
</tr>
<tr id="S6.T3.3.6.6" class="ltx_tr">
<th id="S6.T3.3.6.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">10%</th>
<td id="S6.T3.3.6.6.3" class="ltx_td ltx_align_center">52.56</td>
<td id="S6.T3.3.6.6.4" class="ltx_td ltx_align_center">74.06</td>
<td id="S6.T3.3.6.6.5" class="ltx_td ltx_align_center">36.50</td>
<td id="S6.T3.3.6.6.6" class="ltx_td ltx_align_center">37.85</td>
</tr>
<tr id="S6.T3.3.7.7" class="ltx_tr">
<th id="S6.T3.3.7.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">20%</th>
<td id="S6.T3.3.7.7.3" class="ltx_td ltx_align_center">50.22</td>
<td id="S6.T3.3.7.7.4" class="ltx_td ltx_align_center">71.16</td>
<td id="S6.T3.3.7.7.5" class="ltx_td ltx_align_center">35.72</td>
<td id="S6.T3.3.7.7.6" class="ltx_td ltx_align_center">35.64</td>
</tr>
<tr id="S6.T3.3.8.8" class="ltx_tr">
<th id="S6.T3.3.8.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">30%</th>
<td id="S6.T3.3.8.8.3" class="ltx_td ltx_align_center">47.03</td>
<td id="S6.T3.3.8.8.4" class="ltx_td ltx_align_center">67.31</td>
<td id="S6.T3.3.8.8.5" class="ltx_td ltx_align_center">34.45</td>
<td id="S6.T3.3.8.8.6" class="ltx_td ltx_align_center">32.56</td>
</tr>
<tr id="S6.T3.3.9.9" class="ltx_tr">
<th id="S6.T3.3.9.9.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">40%</th>
<td id="S6.T3.3.9.9.3" class="ltx_td ltx_align_center">42.83</td>
<td id="S6.T3.3.9.9.4" class="ltx_td ltx_align_center">62.35</td>
<td id="S6.T3.3.9.9.5" class="ltx_td ltx_align_center">31.97</td>
<td id="S6.T3.3.9.9.6" class="ltx_td ltx_align_center">28.64</td>
</tr>
<tr id="S6.T3.3.10.10" class="ltx_tr">
<th id="S6.T3.3.10.10.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">50%</th>
<td id="S6.T3.3.10.10.3" class="ltx_td ltx_align_center">37.12</td>
<td id="S6.T3.3.10.10.4" class="ltx_td ltx_align_center">25.42</td>
<td id="S6.T3.3.10.10.5" class="ltx_td ltx_align_center">27.05</td>
<td id="S6.T3.3.10.10.6" class="ltx_td ltx_align_center">23.77</td>
</tr>
<tr id="S6.T3.3.11.11" class="ltx_tr">
<th id="S6.T3.3.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SpeechMod</th>
<th id="S6.T3.3.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Blind</th>
<td id="S6.T3.3.11.11.3" class="ltx_td ltx_align_center ltx_border_t">42.05</td>
<td id="S6.T3.3.11.11.4" class="ltx_td ltx_align_center ltx_border_t">70.85</td>
<td id="S6.T3.3.11.11.5" class="ltx_td ltx_align_center ltx_border_t">31.62</td>
<td id="S6.T3.3.11.11.6" class="ltx_td ltx_align_center ltx_border_t">19.84</td>
</tr>
<tr id="S6.T3.3.12.12" class="ltx_tr">
<th id="S6.T3.3.12.12.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">0%</th>
<td id="S6.T3.3.12.12.3" class="ltx_td ltx_align_center">46.99</td>
<td id="S6.T3.3.12.12.4" class="ltx_td ltx_align_center">67.87</td>
<td id="S6.T3.3.12.12.5" class="ltx_td ltx_align_center">30.84</td>
<td id="S6.T3.3.12.12.6" class="ltx_td ltx_align_center">32.82</td>
</tr>
<tr id="S6.T3.3.13.13" class="ltx_tr">
<th id="S6.T3.3.13.13.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.13.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">10%</th>
<td id="S6.T3.3.13.13.3" class="ltx_td ltx_align_center">45.81</td>
<td id="S6.T3.3.13.13.4" class="ltx_td ltx_align_center">67.29</td>
<td id="S6.T3.3.13.13.5" class="ltx_td ltx_align_center">30.13</td>
<td id="S6.T3.3.13.13.6" class="ltx_td ltx_align_center">31.03</td>
</tr>
<tr id="S6.T3.3.14.14" class="ltx_tr">
<th id="S6.T3.3.14.14.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.14.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">20%</th>
<td id="S6.T3.3.14.14.3" class="ltx_td ltx_align_center">43.33</td>
<td id="S6.T3.3.14.14.4" class="ltx_td ltx_align_center">65.88</td>
<td id="S6.T3.3.14.14.5" class="ltx_td ltx_align_center">29.24</td>
<td id="S6.T3.3.14.14.6" class="ltx_td ltx_align_center">27.28</td>
</tr>
<tr id="S6.T3.3.15.15" class="ltx_tr">
<th id="S6.T3.3.15.15.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.15.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">30%</th>
<td id="S6.T3.3.15.15.3" class="ltx_td ltx_align_center">40.07</td>
<td id="S6.T3.3.15.15.4" class="ltx_td ltx_align_center">64.15</td>
<td id="S6.T3.3.15.15.5" class="ltx_td ltx_align_center">27.82</td>
<td id="S6.T3.3.15.15.6" class="ltx_td ltx_align_center">22.28</td>
</tr>
<tr id="S6.T3.3.16.16" class="ltx_tr">
<th id="S6.T3.3.16.16.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.16.16.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">40%</th>
<td id="S6.T3.3.16.16.3" class="ltx_td ltx_align_center">35.85</td>
<td id="S6.T3.3.16.16.4" class="ltx_td ltx_align_center">61.47</td>
<td id="S6.T3.3.16.16.5" class="ltx_td ltx_align_center">24.68</td>
<td id="S6.T3.3.16.16.6" class="ltx_td ltx_align_center">16.52</td>
</tr>
<tr id="S6.T3.3.17.17" class="ltx_tr">
<th id="S6.T3.3.17.17.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T3.3.17.17.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">50%</th>
<td id="S6.T3.3.17.17.3" class="ltx_td ltx_align_center">32.14</td>
<td id="S6.T3.3.17.17.4" class="ltx_td ltx_align_center">59.33</td>
<td id="S6.T3.3.17.17.5" class="ltx_td ltx_align_center">20.84</td>
<td id="S6.T3.3.17.17.6" class="ltx_td ltx_align_center">11.50</td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_italic">Blind</span> denotes no visual information, meaning it removes the visual components while rest of the model stays the same. TextMod <span id="S6.p3.1.2" class="ltx_text ltx_font_italic">Blind</span> is trained and evaluated on the original questions. SpeechMod <span id="S6.p3.1.3" class="ltx_text ltx_font_italic">Blind</span> is trained and evaluated on the 0% noise audio. <span id="S6.p3.1.4" class="ltx_text ltx_font_italic">Baseline</span> is from <span id="S6.p3.1.5" class="ltx_text ltx_font_italic">VQA1.0</span> using the model ‘LSTM Q+I’.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">A graphical version of the table is shown in <a href="#S6.F4" title="In 6. Results ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>. The constant values of SpeechMod <span id="S6.p4.1.1" class="ltx_text ltx_font_italic">Blind</span> and TextMod <span id="S6.p4.1.2" class="ltx_text ltx_font_italic">Blind</span> are included to show the noise level at which they perform better than their full model counterparts. Examples of the two models answering questions from the dataset are shown in <a href="#S6.F6" title="In 6.3. Discussion ‣ 6. Results ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">One might imagine SpeechMod to perform better because of its direct optimization and end-to-end training solely for the task, yet this hypothesis does not hold true. At 0% noise, TextMod achieves 7% higher accuracy than SpeechMod. As noise is added, both models initially falter at similar rates, although their trends seem to head towards convergence. This is expected since, since at 100% noise the question would not be audible at all; it would be random guessing, thus both methods would perform exactly the same.</p>
</div>
<figure id="S6.F4" class="ltx_figure"><svg id="S6.F4.pic1" class="ltx_picture ltx_centering" height="260.21" overflow="visible" version="1.1" width="310.89"><g transform="translate(0,260.21) matrix(1 0 0 -1 0 0) translate(46.68,0) translate(0,41.76) matrix(1.0 0.0 0.0 1.0 -46.68 -41.76)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(46.68,0) translate(0,41.76)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><clipPath id="pgfcp1"><path d="M 0 -196.85 L 252.69 -196.85 L 252.69 406.24 L 0 406.24"></path></clipPath><g clip-path="url(#pgfcp1)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 0 L 0 5.91 M 50.54 0 L 50.54 5.91 M 101.07 0 L 101.07 5.91 M 151.61 0 L 151.61 5.91 M 202.15 0 L 202.15 5.91 M 252.69 0 L 252.69 5.91 M 0 209.39 L 0 203.48 M 50.54 209.39 L 50.54 203.48 M 101.07 209.39 L 101.07 203.48 M 151.61 209.39 L 151.61 203.48 M 202.15 209.39 L 202.15 203.48 M 252.69 209.39 L 252.69 203.48" style="fill:none"></path></g><g></g></g><clipPath id="pgfcp2"><path d="M -196.85 0 L -196.85 209.39 L 449.54 209.39 L 449.54 0"></path></clipPath><g clip-path="url(#pgfcp2)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 0 L 5.91 0 M 0 69.8 L 5.91 69.8 M 0 139.59 L 5.91 139.59 M 0 209.39 L 5.91 209.39 M 252.69 0 L 246.78 0 M 252.69 69.8 L 246.78 69.8 M 252.69 139.59 L 246.78 139.59 M 252.69 209.39 L 246.78 209.39" style="fill:none"></path></g><g></g></g><path d="M 0 0 L 0 209.39 L 252.69 209.39 L 252.69 0 L 0 0 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S6.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S6.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">0</cn></annotation-xml></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 43.62 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S6.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">10</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 94.16 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S6.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S6.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">20</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 144.69 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S6.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S6.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">30</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 195.23 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S6.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S6.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">40</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 245.77 -13.81)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S6.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S6.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">50</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -18.73 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S6.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S6.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">30</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -18.73 65.34)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S6.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S6.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1c">40</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -18.73 135.13)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S6.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S6.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">50</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -18.73 204.93)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S6.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S6.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1c">60</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp3"><path d="M 0 0 L 252.69 0 L 252.69 209.39 L 0 209.39 Z"></path></clipPath><g clip-path="url(#pgfcp3)"><g stroke="#0000FF" fill="#0000FF"><path d="M 0 118.58 L 50.54 110.35 L 101.07 93.04 L 151.61 70.28 L 202.15 40.83 L 252.69 14.94" style="fill:none"></path></g><g></g><g stroke="#FF00FF" fill="#FF00FF"><path d="M 0 167.72 L 50.54 157.46 L 101.07 141.13 L 151.61 118.86 L 202.15 89.55 L 252.69 49.69" style="fill:none"></path></g><g></g><g stroke="#0000FF" fill="#0000FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#0000FF"><path d="M 0 84.1 L 252.69 84.1" style="fill:none"></path></g><g></g><g stroke="#FF00FF" fill="#FF00FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#FF00FF"><path d="M 0 130.94 L 252.69 130.94" style="fill:none"></path></g><g></g></g><g stroke="#0000FF" fill="#0000FF" color="#0000FF"><path d="M -2.77 115.81 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 47.77 107.58 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 98.31 90.27 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 148.85 67.52 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 199.38 38.06 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 249.92 12.17 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path></g><g fill="#FF00FF" stroke="#FF00FF" color="#FF00FF"><g fill="#FFFFFF"><path d="M -2.77 167.72 C -2.77 166.19 -1.53 164.95 0 164.95 C 1.53 164.95 2.77 166.19 2.77 167.72" style="stroke:none"></path></g><g fill="#FF00FF"><path d="M -2.77 167.72 L 2.77 167.72 M 2.77 167.72 C 2.77 169.25 1.53 170.49 0 170.49 C -1.53 170.49 -2.77 169.25 -2.77 167.72 C -2.77 166.19 -1.53 164.95 0 164.95 C 1.53 164.95 2.77 166.19 2.77 167.72 Z M 0 167.72" style="fill:none"></path><g fill="#FFFFFF"><path d="M 47.77 157.46 C 47.77 155.93 49.01 154.69 50.54 154.69 C 52.07 154.69 53.3 155.93 53.3 157.46" style="stroke:none"></path></g><path d="M 47.77 157.46 L 53.3 157.46 M 53.3 157.46 C 53.3 158.99 52.07 160.23 50.54 160.23 C 49.01 160.23 47.77 158.99 47.77 157.46 C 47.77 155.93 49.01 154.69 50.54 154.69 C 52.07 154.69 53.3 155.93 53.3 157.46 Z M 50.54 157.46" style="fill:none"></path><g fill="#FFFFFF"><path d="M 98.31 141.13 C 98.31 139.6 99.55 138.36 101.07 138.36 C 102.6 138.36 103.84 139.6 103.84 141.13" style="stroke:none"></path></g><path d="M 98.31 141.13 L 103.84 141.13 M 103.84 141.13 C 103.84 142.65 102.6 143.89 101.07 143.89 C 99.55 143.89 98.31 142.65 98.31 141.13 C 98.31 139.6 99.55 138.36 101.07 138.36 C 102.6 138.36 103.84 139.6 103.84 141.13 Z M 101.07 141.13" style="fill:none"></path><g fill="#FFFFFF"><path d="M 148.85 118.86 C 148.85 117.33 150.08 116.09 151.61 116.09 C 153.14 116.09 154.38 117.33 154.38 118.86" style="stroke:none"></path></g><path d="M 148.85 118.86 L 154.38 118.86 M 154.38 118.86 C 154.38 120.39 153.14 121.63 151.61 121.63 C 150.08 121.63 148.85 120.39 148.85 118.86 C 148.85 117.33 150.08 116.09 151.61 116.09 C 153.14 116.09 154.38 117.33 154.38 118.86 Z M 151.61 118.86" style="fill:none"></path><g fill="#FFFFFF"><path d="M 199.38 89.55 C 199.38 88.02 200.62 86.78 202.15 86.78 C 203.68 86.78 204.92 88.02 204.92 89.55" style="stroke:none"></path></g><path d="M 199.38 89.55 L 204.92 89.55 M 204.92 89.55 C 204.92 91.08 203.68 92.31 202.15 92.31 C 200.62 92.31 199.38 91.08 199.38 89.55 C 199.38 88.02 200.62 86.78 202.15 86.78 C 203.68 86.78 204.92 88.02 204.92 89.55 Z M 202.15 89.55" style="fill:none"></path><g fill="#FFFFFF"><path d="M 249.92 49.69 C 249.92 48.17 251.16 46.93 252.69 46.93 C 254.22 46.93 255.45 48.17 255.45 49.69" style="stroke:none"></path></g><path d="M 249.92 49.69 L 255.45 49.69 M 255.45 49.69 C 255.45 51.22 254.22 52.46 252.69 52.46 C 251.16 52.46 249.92 51.22 249.92 49.69 C 249.92 48.17 251.16 46.93 252.69 46.93 C 254.22 46.93 255.45 48.17 255.45 49.69 Z M 252.69 49.69" style="fill:none"></path></g></g><g transform="matrix(1.0 0.0 0.0 1.0 96.52 -33.68)" fill="#000000" stroke="#000000"><foreignObject width="59.65" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F4.pic1.11.11.11.11.1.1" class="ltx_text">Noise (%)</span></foreignObject></g><g transform="matrix(0.0 1.0 -1.0 0.0 -31.69 63.16)" fill="#000000" stroke="#000000"><foreignObject width="83.06" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F4.pic1.12.12.12.12.1.1" class="ltx_text">Accuracy (%)</span></foreignObject></g><g fill="#FFFFFF" stroke="#000000"><path d="M 39.74 127.65 h 205.09 v 75.18 h -205.09 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 43.89 130.41)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 61.04)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.28,0)" fill="#0000FF" stroke="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 -2.77 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 73.12 0) translate(37.36,0) matrix(1.0 0.0 0.0 1.0 -34.59 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="69.57" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F4.pic1.13.13.13.13.1.1.1.1.1" class="ltx_text">SpeechMod</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 25.83)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.28,0)" fill="#FF00FF" stroke="#FF00FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><g fill="#FFFFFF" color="#FF00FF"><path d="M 9.04 0 C 9.04 -1.53 10.28 -2.77 11.81 -2.77 C 13.34 -2.77 14.58 -1.53 14.58 0" style="stroke:none"></path></g><path d="M 9.04 0 L 14.58 0 M 14.58 0 C 14.58 1.53 13.34 2.77 11.81 2.77 C 10.28 2.77 9.04 1.53 9.04 0 C 9.04 -1.53 10.28 -2.77 11.81 -2.77 C 13.34 -2.77 14.58 -1.53 14.58 0 Z M 11.81 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 80.04 0) translate(30.44,0) matrix(1.0 0.0 0.0 1.0 -27.67 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="55.35" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F4.pic1.14.14.14.14.2.2.1.1.1" class="ltx_text">TextMod</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 43.43)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.28,0)" fill="#0000FF" stroke="#0000FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.18 0) translate(86.31,0) matrix(1.0 0.0 0.0 1.0 -83.54 -4.15)" fill="#000000" stroke="#000000"><foreignObject width="166.31" height="13.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F4.pic1.15.15.15.15.3.3.1.1.1" class="ltx_text">SpeechMod Blind 0% Noise</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 61.04)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.28,0)" fill="#FF00FF" stroke="#FF00FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#FF00FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 48.24 0) translate(62.25,0) matrix(1.0 0.0 0.0 1.0 -59.48 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="108.2" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F4.pic1.16.16.16.16.4.4.1.1.1" class="ltx_text">TextMod Blind <span id="S6.F4.pic1.16.16.16.16.4.4.1.1.1.1" class="ltx_text ltx_font_italic">OQ</span></span></foreignObject></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>SpeechMod and TextMod performance with varying amounts of added noise on <span id="S6.F4.3.1" class="ltx_text ltx_font_italic">test-dev</span>. <span id="S6.F4.4.2" class="ltx_text ltx_font_italic">Blind</span> counterparts are not tested on different noise levels.</figcaption>
</figure>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">Next, we compare TextMod and SpeechMod against their respective <span id="S6.p6.1.1" class="ltx_text ltx_font_italic">Blind</span> models. The bias of questions in <span id="S6.p6.1.2" class="ltx_text ltx_font_italic">VQA1.0</span> is well documented. Namely, if the question is understood, there is a good chance of answering correctly without looking at the image (i.e. blind guessing). For example, <span id="S6.p6.1.3" class="ltx_text ltx_font_italic">Y/N</span> questions have the answer <em id="S6.p6.1.4" class="ltx_emph ltx_font_italic">yes</em> more commonly than <em id="S6.p6.1.5" class="ltx_emph ltx_font_italic">no</em>, so the system should guess <em id="S6.p6.1.6" class="ltx_emph ltx_font_italic">yes</em> if a question is identified to be a <span id="S6.p6.1.7" class="ltx_text ltx_font_italic">Y/N</span> type. As a reference, always answering <em id="S6.p6.1.8" class="ltx_emph ltx_font_italic">yes</em> yields a <span id="S6.p6.1.9" class="ltx_text ltx_font_italic">Y/N</span> accuracy of 70.81% on <span id="S6.p6.1.10" class="ltx_text ltx_font_italic">test-dev</span>. The bias is clearly evident in both <span id="S6.p6.1.11" class="ltx_text ltx_font_italic">test-dev</span> and <span id="S6.p6.1.12" class="ltx_text ltx_font_italic">val</span> for TextMod and SpeechMod; the <span id="S6.p6.1.13" class="ltx_text ltx_font_italic">Y/N</span> section of <span id="S6.p6.1.14" class="ltx_text ltx_font_italic">Blind</span> always performs better than that of the 0% data. Therefore, <span id="S6.p6.1.15" class="ltx_text ltx_font_italic">Blind</span> tells us how many questions are understood by these two modes of linguistic inputs. When comparing the linguistic only models with their complementary TextMod and SpeechMod, one can be certain that performances falling below the linguistic signifies that the model no longer understands the questions. Furthermore, perceiving the image and a noisy question becomes less informative than understanding a clean question without an image.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Zero-Shot</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">In this section zero-shot (ZS) results are analyzed to further understand the behavior of both models. ZS in the context of VQA refers to questions that were never seen in training. To get ZS data, we discard questions in <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_italic">val</span> subset that appeared in the <span id="S6.SS1.p1.1.2" class="ltx_text ltx_font_italic">train</span> subset, which decreased the number of valid questions from 104,654 to 65,365. Put differently, ZS is simply a subset of <span id="S6.SS1.p1.1.3" class="ltx_text ltx_font_italic">val</span>.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">The models were trained on the complete <span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_italic">train</span>, and best performing on the complete <span id="S6.SS1.p2.1.2" class="ltx_text ltx_font_italic">val</span> were selected. Next, the models were tested on the original and ZS datasets with noise injected (<a href="#S6.T4" title="In 6.1. Zero-Shot ‣ 6. Results ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>, <a href="#S6.F5" title="In 6.3. Discussion ‣ 6. Results ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>). These experiments were not be performed on <span id="S6.SS1.p2.1.3" class="ltx_text ltx_font_italic">test-dev</span> because the ground truth from <span id="S6.SS1.p2.1.4" class="ltx_text ltx_font_italic">test-dev</span> and <span id="S6.SS1.p2.1.5" class="ltx_text ltx_font_italic">test</span> are withheld and cannot be evaluated partially on the server.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">As one would expect, ZS accuracies are worse than accuracies of the entire set, since models tend to perform more poorly on unseen data. TextMod performs 4% better on the complete dataset than on the ZS, and SpeechMod on the complete dataset performs better by 7%. The performance gap decreases as more noise is added. At 50% noise, the performance on ZS and the original data have practically converged for both models. To the models, questions seen during the training but with high amount of noise added are as foreign as unseen questions.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span>Accuracy on <span id="S6.T4.2.1" class="ltx_text ltx_font_italic">zero-shot</span> with different levels of noise added. (Higher is better)</figcaption>
<table id="S6.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T4.3.1.1" class="ltx_tr">
<th id="S6.T4.3.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S6.T4.3.1.1.3" class="ltx_td ltx_align_center">All</td>
<td id="S6.T4.3.1.1.4" class="ltx_td ltx_align_center">Y/N</td>
<td id="S6.T4.3.1.1.5" class="ltx_td ltx_align_center">Number</td>
<td id="S6.T4.3.1.1.6" class="ltx_td ltx_align_center">Other</td>
</tr>
<tr id="S6.T4.3.2.2" class="ltx_tr">
<th id="S6.T4.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">TextMod</th>
<th id="S6.T4.3.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">OQ</th>
<td id="S6.T4.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">49.41</td>
<td id="S6.T4.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t">77.23</td>
<td id="S6.T4.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t">31.18</td>
<td id="S6.T4.3.2.2.6" class="ltx_td ltx_align_center ltx_border_t">27.12</td>
</tr>
<tr id="S6.T4.3.3.3" class="ltx_tr">
<th id="S6.T4.3.3.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">0%</th>
<td id="S6.T4.3.3.3.3" class="ltx_td ltx_align_center">46.41</td>
<td id="S6.T4.3.3.3.4" class="ltx_td ltx_align_center">73.37</td>
<td id="S6.T4.3.3.3.5" class="ltx_td ltx_align_center">30.64</td>
<td id="S6.T4.3.3.3.6" class="ltx_td ltx_align_center">24.38</td>
</tr>
<tr id="S6.T4.3.4.4" class="ltx_tr">
<th id="S6.T4.3.4.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">10%</th>
<td id="S6.T4.3.4.4.3" class="ltx_td ltx_align_center">45.23</td>
<td id="S6.T4.3.4.4.4" class="ltx_td ltx_align_center">71.93</td>
<td id="S6.T4.3.4.4.5" class="ltx_td ltx_align_center">30.32</td>
<td id="S6.T4.3.4.4.6" class="ltx_td ltx_align_center">23.24</td>
</tr>
<tr id="S6.T4.3.5.5" class="ltx_tr">
<th id="S6.T4.3.5.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">20%</th>
<td id="S6.T4.3.5.5.3" class="ltx_td ltx_align_center">43.30</td>
<td id="S6.T4.3.5.5.4" class="ltx_td ltx_align_center">69.26</td>
<td id="S6.T4.3.5.5.5" class="ltx_td ltx_align_center">29.63</td>
<td id="S6.T4.3.5.5.6" class="ltx_td ltx_align_center">21.75</td>
</tr>
<tr id="S6.T4.3.6.6" class="ltx_tr">
<th id="S6.T4.3.6.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">30%</th>
<td id="S6.T4.3.6.6.3" class="ltx_td ltx_align_center">40.85</td>
<td id="S6.T4.3.6.6.4" class="ltx_td ltx_align_center">65.84</td>
<td id="S6.T4.3.6.6.5" class="ltx_td ltx_align_center">28.55</td>
<td id="S6.T4.3.6.6.6" class="ltx_td ltx_align_center">19.89</td>
</tr>
<tr id="S6.T4.3.7.7" class="ltx_tr">
<th id="S6.T4.3.7.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">40%</th>
<td id="S6.T4.3.7.7.3" class="ltx_td ltx_align_center">37.79</td>
<td id="S6.T4.3.7.7.4" class="ltx_td ltx_align_center">62.56</td>
<td id="S6.T4.3.7.7.5" class="ltx_td ltx_align_center">26.10</td>
<td id="S6.T4.3.7.7.6" class="ltx_td ltx_align_center">16.91</td>
</tr>
<tr id="S6.T4.3.8.8" class="ltx_tr">
<th id="S6.T4.3.8.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">50%</th>
<td id="S6.T4.3.8.8.3" class="ltx_td ltx_align_center">34.41</td>
<td id="S6.T4.3.8.8.4" class="ltx_td ltx_align_center">59.58</td>
<td id="S6.T4.3.8.8.5" class="ltx_td ltx_align_center">21.50</td>
<td id="S6.T4.3.8.8.6" class="ltx_td ltx_align_center">13.42</td>
</tr>
<tr id="S6.T4.3.9.9" class="ltx_tr">
<th id="S6.T4.3.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SpeechMod</th>
<th id="S6.T4.3.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">0%</th>
<td id="S6.T4.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">37.01</td>
<td id="S6.T4.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t">65.58</td>
<td id="S6.T4.3.9.9.5" class="ltx_td ltx_align_center ltx_border_t">23.19</td>
<td id="S6.T4.3.9.9.6" class="ltx_td ltx_align_center ltx_border_t">12.99</td>
</tr>
<tr id="S6.T4.3.10.10" class="ltx_tr">
<th id="S6.T4.3.10.10.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">10%</th>
<td id="S6.T4.3.10.10.3" class="ltx_td ltx_align_center">36.52</td>
<td id="S6.T4.3.10.10.4" class="ltx_td ltx_align_center">65.12</td>
<td id="S6.T4.3.10.10.5" class="ltx_td ltx_align_center">22.83</td>
<td id="S6.T4.3.10.10.6" class="ltx_td ltx_align_center">12.45</td>
</tr>
<tr id="S6.T4.3.11.11" class="ltx_tr">
<th id="S6.T4.3.11.11.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">20%</th>
<td id="S6.T4.3.11.11.3" class="ltx_td ltx_align_center">35.47</td>
<td id="S6.T4.3.11.11.4" class="ltx_td ltx_align_center">64.04</td>
<td id="S6.T4.3.11.11.5" class="ltx_td ltx_align_center">22.29</td>
<td id="S6.T4.3.11.11.6" class="ltx_td ltx_align_center">11.29</td>
</tr>
<tr id="S6.T4.3.12.12" class="ltx_tr">
<th id="S6.T4.3.12.12.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">30%</th>
<td id="S6.T4.3.12.12.3" class="ltx_td ltx_align_center">34.08</td>
<td id="S6.T4.3.12.12.4" class="ltx_td ltx_align_center">62.77</td>
<td id="S6.T4.3.12.12.5" class="ltx_td ltx_align_center">21.45</td>
<td id="S6.T4.3.12.12.6" class="ltx_td ltx_align_center">9.67</td>
</tr>
<tr id="S6.T4.3.13.13" class="ltx_tr">
<th id="S6.T4.3.13.13.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.13.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">40%</th>
<td id="S6.T4.3.13.13.3" class="ltx_td ltx_align_center">32.12</td>
<td id="S6.T4.3.13.13.4" class="ltx_td ltx_align_center">60.59</td>
<td id="S6.T4.3.13.13.5" class="ltx_td ltx_align_center">19.94</td>
<td id="S6.T4.3.13.13.6" class="ltx_td ltx_align_center">7.81</td>
</tr>
<tr id="S6.T4.3.14.14" class="ltx_tr">
<th id="S6.T4.3.14.14.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T4.3.14.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">50%</th>
<td id="S6.T4.3.14.14.3" class="ltx_td ltx_align_center">29.88</td>
<td id="S6.T4.3.14.14.4" class="ltx_td ltx_align_center">57.70</td>
<td id="S6.T4.3.14.14.5" class="ltx_td ltx_align_center">18.20</td>
<td id="S6.T4.3.14.14.6" class="ltx_td ltx_align_center">6.09</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Human Recordings</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Finally, a small, supplementary test is run on non-synthetic, human-recorded questions to see if the models would perform differently on real-world audio inputs. 1000 samples were randomly selected from <span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_italic">val</span>, and the best performing models from the ZS section were used for evaluation. <a href="#S6.T5" title="In 6.3. Discussion ‣ 6. Results ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows the performance on the synthetic and human-recorded versions of this subset.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Although it is clear that both models have difficulties handling recorded questions, SpeechMod performs especially poorly. TextMod on the synthetic dataset achieves similar accuracy as it does on <span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_italic">val</span> and <span id="S6.SS2.p2.1.2" class="ltx_text ltx_font_italic">test-dev</span> with 40% noise. SpeechMod however, gets similar performance as the synthetic data with 50% noise on only the <span id="S6.SS2.p2.1.3" class="ltx_text ltx_font_italic">Y/N</span> questions, while it seems to understand none of the other question types.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Discussion</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">As a modality, speech contains more information than text. In the process of reducing the high-dimensional audio inputs to the low-dimensional class output label (i.e. the answer), the best performing system must be that which extracts patterns most effectively.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">TextMod relies heavily on the intermediate ASR system, which is more complicated than the entire architecture of SpeechMod, as the number of parameters one needs to learn for speech recognition is also much greater. The Kaldi model has also been trained on many times more data than contained in <span id="S6.SS3.p2.1.1" class="ltx_text ltx_font_italic">VQA1.0</span>. The ASR serves to filter out noise in high dimensions and extract meaningful patterns in the form of text. In a sense, one can think of the ASR as a feature extractor, with text being the salient feature and an explicit intermediate standardization of data before the question answering module.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">Conversely, the only audio data SpeechMod learns from are the questions in the dataset. It does not include any mechanisms that explicitly learn semantics in a language, nor does it have intermediate data standardization. Thus, the model may not extract the concept of words from audio sounds. Whether or not forcing the system to learn words (i.e. transcribing words in the question and answering simultaneously) will be beneficial is left to future research, but it is evident that data standardization is helpful for unseen data.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><svg id="S6.F5.pic1" class="ltx_picture ltx_centering" height="260.21" overflow="visible" version="1.1" width="310.89"><g transform="translate(0,260.21) matrix(1 0 0 -1 0 0) translate(46.68,0) translate(0,41.76) matrix(1.0 0.0 0.0 1.0 -46.68 -41.76)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(46.68,0) translate(0,40.92)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><clipPath id="pgfcp4"><path d="M 0 -196.01 L 252.69 -196.01 L 252.69 407.07 L 0 407.07"></path></clipPath><g clip-path="url(#pgfcp4)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 0.84 L 0 6.74 M 50.54 0.84 L 50.54 6.74 M 101.07 0.84 L 101.07 6.74 M 151.61 0.84 L 151.61 6.74 M 202.15 0.84 L 202.15 6.74 M 252.69 0.84 L 252.69 6.74 M 0 210.22 L 0 204.32 M 50.54 210.22 L 50.54 204.32 M 101.07 210.22 L 101.07 204.32 M 151.61 210.22 L 151.61 204.32 M 202.15 210.22 L 202.15 204.32 M 252.69 210.22 L 252.69 204.32" style="fill:none"></path></g><g></g></g><clipPath id="pgfcp5"><path d="M -196.85 0.84 L -196.85 210.22 L 449.54 210.22 L 449.54 0.84"></path></clipPath><g clip-path="url(#pgfcp5)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 0.84 L 5.91 0.84 M 0 70.63 L 5.91 70.63 M 0 140.43 L 5.91 140.43 M 0 210.22 L 5.91 210.22 M 252.69 0.84 L 246.78 0.84 M 252.69 70.63 L 246.78 70.63 M 252.69 140.43 L 246.78 140.43 M 252.69 210.22 L 246.78 210.22" style="fill:none"></path></g><g></g></g><path d="M 0 0.84 L 0 210.22 L 252.69 210.22 L 252.69 0.84 L 0 0.84 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -12.97)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S6.F5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S6.F5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">0</cn></annotation-xml></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 43.62 -12.97)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S6.F5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.F5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">10</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 94.16 -12.97)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S6.F5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S6.F5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">20</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 144.69 -12.97)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S6.F5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S6.F5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">30</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 195.23 -12.97)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S6.F5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S6.F5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">40</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 245.77 -12.97)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F5.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S6.F5.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F5.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F5.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S6.F5.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F5.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F5.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">50</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -18.73 -3.62)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F5.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S6.F5.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F5.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F5.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S6.F5.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F5.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F5.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">30</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -18.73 66.17)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F5.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S6.F5.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F5.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F5.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S6.F5.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F5.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F5.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1c">40</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -18.73 135.97)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F5.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S6.F5.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F5.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F5.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S6.F5.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F5.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F5.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">50</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -18.73 205.76)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S6.F5.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S6.F5.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S6.F5.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.F5.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S6.F5.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S6.F5.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.F5.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1c">60</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp6"><path d="M 0 0.84 L 252.69 0.84 L 252.69 210.22 L 0 210.22 Z"></path></clipPath><g clip-path="url(#pgfcp6)"><g stroke="#0000FF" fill="#0000FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#0000FF"><path d="M 0 102.11 L 50.54 95.9 L 101.07 80.19 L 151.61 56.32 L 202.15 30.43 L 252.69 4.61" style="fill:none"></path></g><g></g><g stroke="#FF00FF" fill="#FF00FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#FF00FF"><path d="M 0 145.24 L 50.54 135.4 L 101.07 119.91 L 151.61 98.48 L 202.15 71.05 L 252.69 39.5" style="fill:none"></path></g><g></g><g stroke="#0000FF" fill="#0000FF"><path d="M 0 49.76 L 50.54 46.34 L 101.07 39.02 L 151.61 29.31 L 202.15 15.63 L 252.69 0" style="fill:none"></path></g><g></g><g stroke="#FF00FF" fill="#FF00FF"><path d="M 0 115.37 L 50.54 107.14 L 101.07 93.67 L 151.61 76.57 L 202.15 55.21 L 252.69 31.62" style="fill:none"></path></g><g></g></g><g stroke="#0000FF" fill="#0000FF" color="#0000FF"><path d="M -2.77 47 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 47.77 43.58 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 98.31 36.25 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 148.85 26.55 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 199.38 12.87 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path></g><g fill="#FF00FF" stroke="#FF00FF" color="#FF00FF"><g fill="#FFFFFF"><path d="M -2.77 115.37 C -2.77 113.84 -1.53 112.6 0 112.6 C 1.53 112.6 2.77 113.84 2.77 115.37" style="stroke:none"></path></g><g fill="#FF00FF"><path d="M -2.77 115.37 L 2.77 115.37 M 2.77 115.37 C 2.77 116.9 1.53 118.14 0 118.14 C -1.53 118.14 -2.77 116.9 -2.77 115.37 C -2.77 113.84 -1.53 112.6 0 112.6 C 1.53 112.6 2.77 113.84 2.77 115.37 Z M 0 115.37" style="fill:none"></path><g fill="#FFFFFF"><path d="M 47.77 107.14 C 47.77 105.61 49.01 104.37 50.54 104.37 C 52.07 104.37 53.3 105.61 53.3 107.14" style="stroke:none"></path></g><path d="M 47.77 107.14 L 53.3 107.14 M 53.3 107.14 C 53.3 108.66 52.07 109.9 50.54 109.9 C 49.01 109.9 47.77 108.66 47.77 107.14 C 47.77 105.61 49.01 104.37 50.54 104.37 C 52.07 104.37 53.3 105.61 53.3 107.14 Z M 50.54 107.14" style="fill:none"></path><g fill="#FFFFFF"><path d="M 98.31 93.67 C 98.31 92.14 99.55 90.9 101.07 90.9 C 102.6 90.9 103.84 92.14 103.84 93.67" style="stroke:none"></path></g><path d="M 98.31 93.67 L 103.84 93.67 M 103.84 93.67 C 103.84 95.19 102.6 96.43 101.07 96.43 C 99.55 96.43 98.31 95.19 98.31 93.67 C 98.31 92.14 99.55 90.9 101.07 90.9 C 102.6 90.9 103.84 92.14 103.84 93.67 Z M 101.07 93.67" style="fill:none"></path><g fill="#FFFFFF"><path d="M 148.85 76.57 C 148.85 75.04 150.08 73.8 151.61 73.8 C 153.14 73.8 154.38 75.04 154.38 76.57" style="stroke:none"></path></g><path d="M 148.85 76.57 L 154.38 76.57 M 154.38 76.57 C 154.38 78.09 153.14 79.33 151.61 79.33 C 150.08 79.33 148.85 78.09 148.85 76.57 C 148.85 75.04 150.08 73.8 151.61 73.8 C 153.14 73.8 154.38 75.04 154.38 76.57 Z M 151.61 76.57" style="fill:none"></path><g fill="#FFFFFF"><path d="M 199.38 55.21 C 199.38 53.68 200.62 52.44 202.15 52.44 C 203.68 52.44 204.92 53.68 204.92 55.21" style="stroke:none"></path></g><path d="M 199.38 55.21 L 204.92 55.21 M 204.92 55.21 C 204.92 56.74 203.68 57.98 202.15 57.98 C 200.62 57.98 199.38 56.74 199.38 55.21 C 199.38 53.68 200.62 52.44 202.15 52.44 C 203.68 52.44 204.92 53.68 204.92 55.21 Z M 202.15 55.21" style="fill:none"></path><g fill="#FFFFFF"><path d="M 249.92 31.62 C 249.92 30.09 251.16 28.85 252.69 28.85 C 254.22 28.85 255.45 30.09 255.45 31.62" style="stroke:none"></path></g><path d="M 249.92 31.62 L 255.45 31.62 M 255.45 31.62 C 255.45 33.15 254.22 34.38 252.69 34.38 C 251.16 34.38 249.92 33.15 249.92 31.62 C 249.92 30.09 251.16 28.85 252.69 28.85 C 254.22 28.85 255.45 30.09 255.45 31.62 Z M 252.69 31.62" style="fill:none"></path></g></g><g transform="matrix(1.0 0.0 0.0 1.0 96.52 -32.85)" fill="#000000" stroke="#000000"><foreignObject width="59.65" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F5.pic1.11.11.11.11.1.1" class="ltx_text">Noise (%)</span></foreignObject></g><g transform="matrix(0.0 1.0 -1.0 0.0 -31.69 64)" fill="#000000" stroke="#000000"><foreignObject width="83.06" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F5.pic1.12.12.12.12.1.1" class="ltx_text">Accuracy (%)</span></foreignObject></g><g fill="#FFFFFF" stroke="#000000"><path d="M 115.72 129.25 h 129.11 v 74.41 h -129.11 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 119.88 132.02)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 60.27)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.28,0)" fill="#0000FF" stroke="#0000FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.18 0) translate(48.31,0) matrix(1.0 0.0 0.0 1.0 -45.55 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="81.48" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F5.pic1.13.13.13.13.1.1.1.1.1" class="ltx_text">SpeechMod <span id="S6.F5.pic1.13.13.13.13.1.1.1.1.1.1" class="ltx_text ltx_font_italic">val</span></span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 25.83)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.28,0)" fill="#FF00FF" stroke="#FF00FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#FF00FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 31.09 0) translate(41.4,0) matrix(1.0 0.0 0.0 1.0 -38.63 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="67.26" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F5.pic1.14.14.14.14.2.2.1.1.1" class="ltx_text">TextMod <span id="S6.F5.pic1.14.14.14.14.2.2.1.1.1.1" class="ltx_text ltx_font_italic">val</span></span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 43.05)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.28,0)" fill="#0000FF" stroke="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 -2.77 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.75 0) translate(47.74,0) matrix(1.0 0.0 0.0 1.0 -44.97 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="89.17" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F5.pic1.15.15.15.15.3.3.1.1.1" class="ltx_text">SpeechMod ZS</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 60.27)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.28,0)" fill="#FF00FF" stroke="#FF00FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><g fill="#FFFFFF" color="#FF00FF"><path d="M 9.04 0 C 9.04 -1.53 10.28 -2.77 11.81 -2.77 C 13.34 -2.77 14.58 -1.53 14.58 0" style="stroke:none"></path></g><path d="M 9.04 0 L 14.58 0 M 14.58 0 C 14.58 1.53 13.34 2.77 11.81 2.77 C 10.28 2.77 9.04 1.53 9.04 0 C 9.04 -1.53 10.28 -2.77 11.81 -2.77 C 13.34 -2.77 14.58 -1.53 14.58 0 Z M 11.81 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 31.67 0) translate(40.82,0) matrix(1.0 0.0 0.0 1.0 -38.05 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="75.72" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S6.F5.pic1.16.16.16.16.4.4.1.1.1" class="ltx_text">TextMod ZS</span></foreignObject></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>SpeechMod and TextMod performance with varying amounts of added noise for <span id="S6.F5.2.1" class="ltx_text ltx_font_italic">zero-shot</span> subset in reference to the complete datasets.</figcaption>
</figure>
<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5. </span>Performance on 1000 human-recorded questions.</figcaption>
<table id="S6.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.1.1.1" class="ltx_tr">
<th id="S6.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S6.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">All</th>
<th id="S6.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Y/N</th>
<th id="S6.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Number</th>
<th id="S6.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Other</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.1.2.1" class="ltx_tr">
<th id="S6.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SpeechMod (Recorded)</th>
<td id="S6.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">21.46</td>
<td id="S6.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">57.26</td>
<td id="S6.T5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.77</td>
<td id="S6.T5.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.91</td>
</tr>
<tr id="S6.T5.1.3.2" class="ltx_tr">
<th id="S6.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SpeechMod (Synthetic)</th>
<td id="S6.T5.1.3.2.2" class="ltx_td ltx_align_center">42.69</td>
<td id="S6.T5.1.3.2.3" class="ltx_td ltx_align_center">66.58</td>
<td id="S6.T5.1.3.2.4" class="ltx_td ltx_align_center">32.31</td>
<td id="S6.T5.1.3.2.5" class="ltx_td ltx_align_center">27.58</td>
</tr>
<tr id="S6.T5.1.4.3" class="ltx_tr">
<th id="S6.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">TextMod (Recorded)</th>
<td id="S6.T5.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">41.66</td>
<td id="S6.T5.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">66.33</td>
<td id="S6.T5.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">35.69</td>
<td id="S6.T5.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">25.37</td>
</tr>
<tr id="S6.T5.1.5.4" class="ltx_tr">
<th id="S6.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TextMod (Original Text)</th>
<td id="S6.T5.1.5.4.2" class="ltx_td ltx_align_center">53.09</td>
<td id="S6.T5.1.5.4.3" class="ltx_td ltx_align_center">77.73</td>
<td id="S6.T5.1.5.4.4" class="ltx_td ltx_align_center">41.54</td>
<td id="S6.T5.1.5.4.5" class="ltx_td ltx_align_center">38.26</td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.SS3.p4" class="ltx_para">
<p id="S6.SS3.p4.1" class="ltx_p">In ZS experiments, the gap in performance between the unseen and full dataset with TextMod is much smaller than in SpeechMod (4% vs 7%). A text-based system can still glimpse the meaning of a question even if a word has never been seen, but from the perspective of SpeechMod, new words represent entirely different signal trajectories. Furthermore, audio inputs are continuous streams, making it difficult to differentiate when the new words begin or end.</p>
</div>
<div id="S6.SS3.p5" class="ltx_para">
<p id="S6.SS3.p5.1" class="ltx_p">A similar effect is amplified in answering human-recorded questions. The synthetic audio sounds monotonous, disinterested, with little silence between words while the human-recorded audio has inflections, emphasis, accents, and pauses. An inspection of the spectrograms (<a href="#S4.F3" title="In 4. Data ‣ Speech-Based Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>) confirms this, as the synthetic waveforms have vastly different audio signatures. Because SpeechMod has no training data similar to the human-recorded samples, it is unable to extract salient patterns. In comparison, the ASR removes most of the variance in the input by standardizing the audio into a compact, salient textual representation. From the perspective of TextMod, the human-recorded questions is only slightly different than those provided in training.</p>
</div>
<div id="S6.SS3.p6" class="ltx_para">
<p id="S6.SS3.p6.1" class="ltx_p">It is evident in our experiments that text-based VQA performs better than speech-based, but bearing in mind the simple architecture and limited amount of training data, we believe the results of SpeechMod merits further study into end-to-end methods.</p>
</div>
<figure id="S6.F6" class="ltx_figure">
<p id="S6.F6.1" class="ltx_p">
<span id="S6.F6.1.m1.12.12.12" class="ltx_tabular ltx_markedasmath ltx_align_middle">
<span class="ltx_tbody">
<span id="S6.F6.1.m1.4.4.4.4" class="ltx_tr">
<span id="S6.F6.1.m1.1.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1705.00464/assets/images/howmany.jpg" id="S6.F6.1.m1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="144" height="118" alt="Refer to caption"></span>
<span id="S6.F6.1.m1.2.2.2.2.2" class="ltx_td ltx_align_center">
<img src="/html/1705.00464/assets/images/lean.jpg" id="S6.F6.1.m1.2.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="144" height="118" alt="Refer to caption"></span>
<span id="S6.F6.1.m1.3.3.3.3.3" class="ltx_td ltx_align_center">
<img src="/html/1705.00464/assets/images/table.jpg" id="S6.F6.1.m1.3.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="144" height="118" alt="Refer to caption"></span>
<span id="S6.F6.1.m1.4.4.4.4.4" class="ltx_td ltx_align_center">
<img src="/html/1705.00464/assets/images/teddy.jpg" id="S6.F6.1.m1.4.4.4.4.4.g1" class="ltx_graphics ltx_img_square" width="144" height="118" alt="Refer to caption"></span></span>
<span id="S6.F6.1.m1.8.8.8.8" class="ltx_tr">
<span id="S6.F6.1.m1.5.5.5.5.1" class="ltx_td ltx_align_center">
 <svg id="S6.F6.1.m1.5.5.5.5.1.pic1" class="ltx_picture" height="40.71" overflow="visible" version="1.1" width="144"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,40.71) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 34.81 C 0 38.07 2.64 40.71 5.91 40.71 L 138.1 40.71 C 141.36 40.71 144 38.07 144 34.81 L 144 5.91 C 144 2.64 141.36 0 138.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 34.81 C 1.97 36.98 3.73 38.75 5.91 38.75 L 138.1 38.75 C 140.27 38.75 142.03 36.98 142.03 34.81 L 142.03 5.91 C 142.03 3.73 140.27 1.97 138.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 5.91)"><foreignObject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="129.42" color="#000000">
<span id="S6.F6.1.m1.5.5.5.5.1.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:93.5pt;">
<span id="S6.F6.1.m1.5.5.5.5.1.pic1.1.1.1.1.1.1" class="ltx_p">how many people are in this photo?</span>
</span></foreignObject></g></g></svg></span>
<span id="S6.F6.1.m1.6.6.6.6.2" class="ltx_td ltx_align_center">
 <svg id="S6.F6.1.m1.6.6.6.6.2.pic1" class="ltx_picture" height="38.02" overflow="visible" version="1.1" width="144"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,38.02) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 32.12 C 0 35.38 2.64 38.02 5.91 38.02 L 138.1 38.02 C 141.36 38.02 144 35.38 144 32.12 L 144 5.91 C 144 2.64 141.36 0 138.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 32.12 C 1.97 34.29 3.73 36.06 5.91 36.06 L 138.1 36.06 C 140.27 36.06 142.03 34.29 142.03 32.12 L 142.03 5.91 C 142.03 3.73 140.27 1.97 138.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 5.91)"><foreignObject height="26.21" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="129.42" color="#000000">
<span id="S6.F6.1.m1.6.6.6.6.2.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:93.5pt;">
<span id="S6.F6.1.m1.6.6.6.6.2.pic1.1.1.1.1.1.1" class="ltx_p">what is leaning against the house?</span>
</span></foreignObject></g></g></svg></span>
<span id="S6.F6.1.m1.7.7.7.7.3" class="ltx_td ltx_align_center">
 <svg id="S6.F6.1.m1.7.7.7.7.3.pic1" class="ltx_picture" height="40.71" overflow="visible" version="1.1" width="144"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,40.71) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 34.81 C 0 38.07 2.64 40.71 5.91 40.71 L 138.1 40.71 C 141.36 40.71 144 38.07 144 34.81 L 144 5.91 C 144 2.64 141.36 0 138.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 34.81 C 1.97 36.98 3.73 38.75 5.91 38.75 L 138.1 38.75 C 140.27 38.75 142.03 36.98 142.03 34.81 L 142.03 5.91 C 142.03 3.73 140.27 1.97 138.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 5.91)"><foreignObject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="129.42" color="#000000">
<span id="S6.F6.1.m1.7.7.7.7.3.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:93.5pt;">
<span id="S6.F6.1.m1.7.7.7.7.3.pic1.1.1.1.1.1.1" class="ltx_p">what has been upcycled to make lights?</span>
</span></foreignObject></g></g></svg></span>
<span id="S6.F6.1.m1.8.8.8.8.4" class="ltx_td ltx_align_center">
 <svg id="S6.F6.1.m1.8.8.8.8.4.pic1" class="ltx_picture" height="40.71" overflow="visible" version="1.1" width="144"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,40.71) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 34.81 C 0 38.07 2.64 40.71 5.91 40.71 L 138.1 40.71 C 141.36 40.71 144 38.07 144 34.81 L 144 5.91 C 144 2.64 141.36 0 138.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 34.81 C 1.97 36.98 3.73 38.75 5.91 38.75 L 138.1 38.75 C 140.27 38.75 142.03 36.98 142.03 34.81 L 142.03 5.91 C 142.03 3.73 140.27 1.97 138.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 5.91)"><foreignObject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="129.42" color="#000000">
<span id="S6.F6.1.m1.8.8.8.8.4.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:93.5pt;">
<span id="S6.F6.1.m1.8.8.8.8.4.pic1.1.1.1.1.1.1" class="ltx_p">what is the teddy bear sitting on?</span>
</span></foreignObject></g></g></svg></span></span>
<span id="S6.F6.1.m1.12.12.12.12" class="ltx_tr">
<span id="S6.F6.1.m1.9.9.9.9.1" class="ltx_td ltx_align_center"><svg id="S6.F6.1.m1.9.9.9.9.1.pic1" class="ltx_picture" height="40.71" overflow="visible" version="1.1" width="144"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,40.71) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 34.81 C 0 38.07 2.64 40.71 5.91 40.71 L 138.1 40.71 C 141.36 40.71 144 38.07 144 34.81 L 144 5.91 C 144 2.64 141.36 0 138.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 34.81 C 1.97 36.98 3.73 38.75 5.91 38.75 L 138.1 38.75 C 140.27 38.75 142.03 36.98 142.03 34.81 L 142.03 5.91 C 142.03 3.73 140.27 1.97 138.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 5.91)"><foreignObject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="129.42" color="#000000">
<span id="S6.F6.1.m1.9.9.9.9.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:93.5pt;">
<span id="S6.F6.1.m1.9.9.9.9.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p">TextMod <img src="/html/1705.00464/assets/images/7.png" id="S6.F6.1.m1.9.9.9.9.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="11" height="11" alt="Refer to caption">:  2</span>
<span id="S6.F6.1.m1.9.9.9.9.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_p">SpeechMod <img src="/html/1705.00464/assets/images/6.png" id="S6.F6.1.m1.9.9.9.9.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="8" height="8" alt="Refer to caption">:  2</span>
</span></foreignObject></g></g></svg></span>
<span id="S6.F6.1.m1.10.10.10.10.2" class="ltx_td ltx_align_center">
 <svg id="S6.F6.1.m1.10.10.10.10.2.pic1" class="ltx_picture" height="40.71" overflow="visible" version="1.1" width="144"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,40.71) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 34.81 C 0 38.07 2.64 40.71 5.91 40.71 L 138.1 40.71 C 141.36 40.71 144 38.07 144 34.81 L 144 5.91 C 144 2.64 141.36 0 138.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 34.81 C 1.97 36.98 3.73 38.75 5.91 38.75 L 138.1 38.75 C 140.27 38.75 142.03 36.98 142.03 34.81 L 142.03 5.91 C 142.03 3.73 140.27 1.97 138.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 5.91)"><foreignObject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="129.42" color="#000000">
<span id="S6.F6.1.m1.10.10.10.10.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:93.5pt;">
<span id="S6.F6.1.m1.10.10.10.10.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p">TextMod <img src="/html/1705.00464/assets/images/7.png" id="S6.F6.1.m1.10.10.10.10.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="11" height="11" alt="Refer to caption">:  tree</span>
<span id="S6.F6.1.m1.10.10.10.10.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_p">SpeechMod <img src="/html/1705.00464/assets/images/6.png" id="S6.F6.1.m1.10.10.10.10.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="8" height="8" alt="Refer to caption">:  chair</span>
</span></foreignObject></g></g></svg></span>
<span id="S6.F6.1.m1.11.11.11.11.3" class="ltx_td ltx_align_center">
 <svg id="S6.F6.1.m1.11.11.11.11.3.pic1" class="ltx_picture" height="40.71" overflow="visible" version="1.1" width="144"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,40.71) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 34.81 C 0 38.07 2.64 40.71 5.91 40.71 L 138.1 40.71 C 141.36 40.71 144 38.07 144 34.81 L 144 5.91 C 144 2.64 141.36 0 138.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 34.81 C 1.97 36.98 3.73 38.75 5.91 38.75 L 138.1 38.75 C 140.27 38.75 142.03 36.98 142.03 34.81 L 142.03 5.91 C 142.03 3.73 140.27 1.97 138.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 5.91)"><foreignObject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="129.42" color="#000000">
<span id="S6.F6.1.m1.11.11.11.11.3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:93.5pt;">
<span id="S6.F6.1.m1.11.11.11.11.3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p">TextMod <img src="/html/1705.00464/assets/images/7.png" id="S6.F6.1.m1.11.11.11.11.3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="11" height="11" alt="Refer to caption">:  bulb</span>
<span id="S6.F6.1.m1.11.11.11.11.3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_p">SpeechMod <img src="/html/1705.00464/assets/images/6.png" id="S6.F6.1.m1.11.11.11.11.3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="8" height="8" alt="Refer to caption">:  bulb</span>
</span></foreignObject></g></g></svg></span>
<span id="S6.F6.1.m1.12.12.12.12.4" class="ltx_td ltx_align_center">
 <svg id="S6.F6.1.m1.12.12.12.12.4.pic1" class="ltx_picture" height="40.71" overflow="visible" version="1.1" width="144"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,40.71) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 34.81 C 0 38.07 2.64 40.71 5.91 40.71 L 138.1 40.71 C 141.36 40.71 144 38.07 144 34.81 L 144 5.91 C 144 2.64 141.36 0 138.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 34.81 C 1.97 36.98 3.73 38.75 5.91 38.75 L 138.1 38.75 C 140.27 38.75 142.03 36.98 142.03 34.81 L 142.03 5.91 C 142.03 3.73 140.27 1.97 138.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 5.91)"><foreignObject height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="129.42" color="#000000">
<span id="S6.F6.1.m1.12.12.12.12.4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:93.5pt;">
<span id="S6.F6.1.m1.12.12.12.12.4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p">TextMod <img src="/html/1705.00464/assets/images/7.png" id="S6.F6.1.m1.12.12.12.12.4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="11" height="11" alt="Refer to caption">:  chair</span>
<span id="S6.F6.1.m1.12.12.12.12.4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_p">SpeechMod <img src="/html/1705.00464/assets/images/6.png" id="S6.F6.1.m1.12.12.12.12.4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="8" height="8" alt="Refer to caption">:  yes</span>
</span></foreignObject></g></g></svg></span></span>
</span>
</span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Example of speech-based VQA answers. Correct answers in blue and incorrect in red.</figcaption>
</figure>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Future Work</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">As alluded to in previous sections, there are a few research directions that may yield interesting results. One straightforward approach to improving the end-to-end model is by data augmentation. It is widely accepted that effectiveness of neural architectures is data driven, so training with noisy data and different speakers will make the model more robust to inputs during run time. Just as many possibilities exist in improving the architecture. One can add feature extractors, attention mechanisms, GAN training, or any amalgamation of the techniques in the deep learning mainstream. An interesting study would be to enforce the prediction of the question while simultaneously learning to answer the question. Doing so may improve performance, but more importantly allows us to interpret the concepts learned by the neural network.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">Another direction is to restrict the amount of training data available to both approaches to observe their learning efficiency. For example, minor languages may not have a reliable ASR. One can simulate a minor language by training an ASR with only the data available in the training set, and comparing this approach with the end-to-end method trained on the same amount of data.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We have proposed speech-based visual question answering and introduced two approaches that tackle this problem, one of which can be trained end-to-end on audio inputs. Despite its simple architecture, the end-to-end method works well when the test data has audio signatures comparable to its training data. Both methods suffered performance decreases at similar rates when noise is introduced. A pipelined method using an ASR tolerates varied inputs much better because it normalizes the input variance into text before running the VQA module. We release the speech dataset and invite the multimedia research community to explore the intersection of speech and vision.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock"> 
       




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">smi (2015)</span>
<span class="ltx_bibblock">
2015.

</span>
<span class="ltx_bibblock">Smile - Smart Photo Annotation.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://play.google.com/store/apps/details?id=com.neuromorphic.retinet.smile" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://play.google.com/store/apps/details?id=com.neuromorphic.retinet.smile</a>.
(2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amodei
et al<span id="bib.bib3.3.3.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Dario Amodei, Rishita
Anubhai, Eric Battenberg, Carl Case,
Jared Casper, Bryan Catanzaro,
Jingdong Chen, Mike Chrzanowski,
Adam Coates, Greg Diamos, and
others. 2015.

</span>
<span class="ltx_bibblock">Deep speech 2: End-to-end speech recognition in
english and mandarin.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.02595</span>
(2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya
Agrawal, Jiasen Lu, Margaret Mitchell,
Dhruv Batra, C. Lawrence Zitnick, and
Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering. In
<span id="bib.bib4.3.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision
(ICCV)</span>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aytar
et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Yusuf Aytar, Carl
Vondrick, and Antonio Torralba.
2016.

</span>
<span class="ltx_bibblock">SoundNet: Learning Sound Representations from
Unlabeled Video.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1610.09001
(2016).

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1610.09001" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1610.09001</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau
et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau,
Kyunghyun Cho, and Yoshua Bengio.
2014.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to
align and translate.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.0473</span>
(2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bigham
et al<span id="bib.bib7.3.3.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Jeffrey P Bigham,
Chandrika Jayant, Hanjie Ji,
Greg Little, Andrew Miller,
Robert C Miller, Robin Miller,
Aubrey Tatarowicz, Brandyn White,
Samual White, and others.
2010.

</span>
<span class="ltx_bibblock">VizWiz: nearly real-time answers to visual
questions. In <span id="bib.bib7.4.1" class="ltx_text ltx_font_italic">Proceedings of the 23nd annual ACM
symposium on User interface software and technology</span>. ACM,
333–342.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Ming-Ming Cheng, Shuai
Zheng, Wen-Yan Lin, Vibhav Vineet,
Paul Sturgess, Nigel Crook,
Niloy J. Mitra, and Philip Torr.
2014.

</span>
<span class="ltx_bibblock">ImageSpirit: Verbal Guided Image Parsing.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic">ACM Trans. Graph.</span> 34,
1 (2014), 3:1–3:11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernández et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Santiago Fernández,
Alex Graves, and Jürgen
Schmidhuber. 2007.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic">An Application of Recurrent Neural Networks
to Discriminative Keyword Spotting</span>.

</span>
<span class="ltx_bibblock">Springer Berlin Heidelberg,
Berlin, Heidelberg, 220–229.

</span>
<span class="ltx_bibblock">
<span id="bib.bib9.4.1" class="ltx_text ltx_font_typewriter">DOI:<a target="_blank" href="https://doi.org/10.1007/978-3-540-74695-9_23" title="" class="ltx_ref ltx_url">https://doi.org/10.1007/978-3-540-74695-9_23</a></span> 

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fukui et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk
Park, Daylen Yang, Anna Rohrbach,
Trevor Darrell, and Marcus Rohrbach.
2016.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual
question answering and visual grounding.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.01847</span>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
Alex Graves, Santiago
Fernández, Faustino Gomez, and
Jürgen Schmidhuber. 2006.

</span>
<span class="ltx_bibblock">Connectionist temporal classification: labelling
unsegmented sequence data with recurrent neural networks. In
<span id="bib.bib11.3.1" class="ltx_text ltx_font_italic">Proceedings of the 23rd international conference on
Machine learning</span>. ACM, 369–376.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harwath
et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
David Harwath, Antonio
Torralba, and James Glass.
2016.

</span>
<span class="ltx_bibblock">Unsupervised Learning of Spoken Language with
Visual Context. In <span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">Advances in Neural Information
Processing Systems</span>. 1858–1866.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hazen
et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Timothy J. Hazen, Brennan
Sherry, and Mark Adler.
2007.

</span>
<span class="ltx_bibblock">Speech-based annotation and retrieval of digital
photographs. In <span id="bib.bib13.3.1" class="ltx_text ltx_font_italic">INTERSPEECH</span>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalashnikov et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
D.V. Kalashnikov, S.
Mehrotra, Jie Xu, and N.
Venkatasubramanian. 2011.

</span>
<span class="ltx_bibblock">A Semantics-Based Approach for Speech Annotation of
Images.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic">IEEE Trans. Knowl. Data Eng.</span>
23, 9 (2011),
1373–1387.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2014)</span>
<span class="ltx_bibblock">
Diederik Kingma and
Jimmy Ba. 2014.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6980</span>
(2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laput et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Gierad P Laput, Mira
Dontcheva, Gregg Wilensky, Walter Chang,
Aseem Agarwala, Jason Linder, and
Eytan Adar. 2013.

</span>
<span class="ltx_bibblock">PixelTone: a multimodal interface for image
editing. In <span id="bib.bib16.3.1" class="ltx_text ltx_font_italic">CHI</span>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun
et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Yann LeCun, Yoshua
Bengio, and Geoffrey Hinton.
2015.

</span>
<span class="ltx_bibblock">Deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic">Nature</span> 521,
7553 (2015), 436–444.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu
et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang,
Dhruv Batra, and Devi Parikh.
2016.

</span>
<span class="ltx_bibblock">Hierarchical Question-Image Co-Attention for Visual
Question Answering.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and
Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and
Mario Fritz. 2014.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about
real-world scenes based on uncertain input. In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>.
1682–1690.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mnih
et al<span id="bib.bib20.3.3.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Volodymyr Mnih, Nicolas
Heess, Alex Graves, and others.
2014.

</span>
<span class="ltx_bibblock">Recurrent models of visual attention. In
<span id="bib.bib20.4.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>.
2204–2212.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nam et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Hyeonseob Nam, Jung-Woo
Ha, and Jeonghee Kim. 2016.

</span>
<span class="ltx_bibblock">Dual Attention Networks for Multimodal Reasoning
and Matching.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1611.00471
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Povey et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Daniel Povey, Arnab
Ghoshal, Gilles Boulianne, Lukas Burget,
Ondrej Glembek, Nagendra Goel,
Mirko Hannemann, Petr Motlicek,
Yanmin Qian, Petr Schwarz,
Jan Silovsky, Georg Stemmer, and
Karel Vesely. 2011.

</span>
<span class="ltx_bibblock">The Kaldi Speech Recognition Toolkit. In
<span id="bib.bib22.3.1" class="ltx_text ltx_font_italic">IEEE 2011 Workshop on Automatic Speech Recognition
and Understanding</span>. IEEE Signal Processing Society.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">IEEE Catalog No.: CFP11SRW-USB.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan
et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Sherry Ruan, Jacob O
Wobbrock, Kenny Liou, Andrew Ng, and
James Landay. 2016.

</span>
<span class="ltx_bibblock">Speech Is 3x Faster than Typing for English and
Mandarin Text Entry on Mobile Devices.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1608.07323</span>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky
et al<span id="bib.bib24.3.3.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia
Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, and
others. 2015.

</span>
<span class="ltx_bibblock">Imagenet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>
115, 3 (2015),
211–252.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salamon
et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Justin Salamon,
Christopher Jacoby, and Juan Pablo
Bello. 2014.

</span>
<span class="ltx_bibblock">A Dataset and Taxonomy for Urban Sound Research.
In <span id="bib.bib25.3.1" class="ltx_text ltx_font_italic">Proceedings of the ACM International Conference
on Multimedia, MM ’14, Orlando, FL, USA, November 03 - 07, 2014</span>.
1041–1044.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text ltx_font_typewriter">DOI:<a target="_blank" href="https://doi.org/10.1145/2647868.2655045" title="" class="ltx_ref ltx_url">https://doi.org/10.1145/2647868.2655045</a></span> 

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and
Zisserman (2015)</span>
<span class="ltx_bibblock">
K. Simonyan and A.
Zisserman. 2015.

</span>
<span class="ltx_bibblock">Very Deep Convolutional Networks for Large-Scale
Image Recognition. In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">ICLR</span>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srihari and Zhang (2000)</span>
<span class="ltx_bibblock">
R.K. Srihari and
Zhongfei Zhang. 2000.

</span>
<span class="ltx_bibblock">Show&amp;Tell: a semi-automated image annotation
system.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">MultiMedia, IEEE</span> 7,
3 (2000), 61–71.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vinyals
et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Oriol Vinyals, Alexander
Toshev, Samy Bengio, and Dumitru
Erhan. 2015.

</span>
<span class="ltx_bibblock">Show and Tell: A Neural Image Caption Generator.
In <span id="bib.bib28.3.1" class="ltx_text ltx_font_italic">CVPR</span>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong
et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Caiming Xiong, Stephen
Merity, and Richard Socher.
2016.

</span>
<span class="ltx_bibblock">Dynamic memory networks for visual and textual
question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic">arXiv</span> 1603
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Saenko (2016)</span>
<span class="ltx_bibblock">
Huijuan Xu and Kate
Saenko. 2016.

</span>
<span class="ltx_bibblock">Ask, attend and answer: Exploring question-guided
spatial attention for visual question answering. In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>. Springer,
451–466.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He,
Jianfeng Gao, Li Deng, and
Alex Smola. 2016.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question
answering. In <span id="bib.bib31.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition</span>. 21–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Yukun Zhu, Ryan Kiros,
Rich Zemel, Ruslan Salakhutdinov,
Raquel Urtasun, Antonio Torralba, and
Sanja Fidler. 2015.

</span>
<span class="ltx_bibblock">Aligning books and movies: Towards story-like
visual explanations by watching movies and reading books. In
<span id="bib.bib32.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on
Computer Vision</span>. 19–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1705.00463" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1705.00464" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1705.00464">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1705.00464" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1705.00465" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 12:24:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
