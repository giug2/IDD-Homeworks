<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.14272] Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks</title><meta property="og:description" content="The first phase of table recognition is to detect the tabular area in a document. Subsequently, the tabular structures are recognized in the second phase in order to extract information from the respective cells. Table…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.14272">

<!--Generated on Sat Mar  9 02:57:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Deep neural network,  document images,  deep learning,  performance evaluation,  table recognition,  table detection,  table structure recognition,  table analysis.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NewSpotColorSpace</span>
<p id="p1.2" class="ltx_p">PANTONE
<span id="p1.2.1" class="ltx_ERROR undefined">\AddSpotColor</span>PANTONE PANTONE3015C PANTONE<span id="p1.2.2" class="ltx_ERROR undefined">\SpotSpace</span>3015<span id="p1.2.3" class="ltx_ERROR undefined">\SpotSpace</span>C 1 0.3 0 0.2
<span id="p1.2.4" class="ltx_ERROR undefined">\SetPageColorSpace</span>PANTONE


</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\corresp</span>
<p id="p2.2" class="ltx_p">Corresponding author: Khurram Azeem Hashmi (e-mail: khurram_azeem.hashmi@dfki.de).</p>
</div>
<h1 class="ltx_title ltx_title_document">Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">KHURRAM AZEEM HASHMI1,2,3
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> MARCUS LIWICKI4, DIDIER STRICKER1,2, MUHAMMAD ADNAN AFZAL5, MUHAMMAD AHTSHAM AFZAL5 AND MUHAMMAD ZESHAN AFZAL1,2,3
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">German Research Center for Artificial Intelligence, 67663 Kaiserslautern, Germany
</span>
<span class="ltx_contact ltx_role_address">Department of Computer Science, University of Kaiserslautern, 67663 Kaiserslautern, Germany
</span>
<span class="ltx_contact ltx_role_address">Mindgrage, University of Kaiserslautern, 67663 Kaiserslautern, Germany
</span>
<span class="ltx_contact ltx_role_address">Luleå University of Technology, A3570 Luleå, Sweden
</span>
<span class="ltx_contact ltx_role_address">Bilojix Soft Technologies, Bahawalpur. Pakistan
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The first phase of table recognition is to detect the tabular area in a document. Subsequently, the tabular structures are recognized in the second phase in order to extract information from the respective cells. Table detection and structural recognition are pivotal problems in the domain of table understanding. However, table analysis is a perplexing task due to the colossal amount of diversity and asymmetry in tables. Therefore, it is an active area of research in document image analysis. Recent advances in the computing capabilities of graphical processing units have enabled the deep neural networks to outperform traditional state-of-the-art machine learning methods. Table understanding has substantially benefited from the recent breakthroughs in deep neural networks. However, there has not been a consolidated description of the deep learning methods for table detection and table structure recognition. This review paper provides a thorough analysis of the modern methodologies that utilize deep neural networks. This work provided a thorough understanding of the current state-of-the-art and related challenges of table understanding in document images. Furthermore, the leading datasets and their intricacies have been elaborated along with the quantitative results. Moreover, a brief overview is given regarding the promising directions that can serve as a guide to further improve table analysis in document images.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Deep neural network, document images, deep learning, performance evaluation, table recognition, table detection, table structure recognition, table analysis.

</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">\titlepgskip</span>
<p id="p3.2" class="ltx_p">=-15pt</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2104.14272/assets/traditional_approaches.jpg" id="S0.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="406" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Pipeline comparison of traditional and deep learning approaches for table analysis. Feature extraction in traditional approaches is mainly achieved through image processing techniques whereas convolutional networks are employed in deep learning techniques. Unlike traditional approaches, deep learning methods for table understanding are not data dependent and they have better generalization capabilities. </figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Table understanding has gained an immense attraction since the last decade. Tables are the prevalent means of representing and communicating structured data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. With the rise of Deep Neural Networks (DNN), various datasets for table detection, segmentation, and recognition have been published <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. This allows the researchers to employ the DNN to improve state-of-the-art results.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Previously, the problem of table recognition has been treated with traditional approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. One of the earlier works in the area of table analysis has been done by Kieninger <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Along with detecting the tabular area, their system known as T-Recs extracts the structural information of the tables.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Later, machine learning techniques are applied to detect the table. One of the pioneers are Cesarini <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Their proposed system, Tabfinder converts a document into an MXY tree which is a hierarchical representation of the document. It searches for a block region in the horizontal and vertical parallel lines, and then a depth-first search to handle noisy document images leads to a tabular region. Silva <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> adopted rich Hidden-Markov-Models to detect tabular area based on joint probability distributions.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2104.14272/assets/paper_organization.jpg" id="S1.F2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="261" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Organization of explained methodologies in the paper. Concepts written in blue color represent table detection techniques. Methods in red color demonstrate the table segmentation or table structure recognition approaches, whereas the architectures in green color depict the table recognition method, which involves the extraction of cell content in a table. As illustrated, some of the architectures have been exploited in multiple tasks of table understanding.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Support Vector Machines (SVM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> have also been exploited along with some handcrafted features to detect tables <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">Fan et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> tried to detect tables by the fusion of various classifiers trained on linguistic and layout information of documents. Another work carried out by <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">Tran et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> uses a region of interest to detect tables in document images. These regions are further filtered as tables if the text block present in the region of interest satisfies a specific set of rules.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Comprehensive research is conducted by <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">Wang et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> focusing not only on the problem of table detection but table decomposition as well. Their probability optimization-based algorithm is similar to the well-known X-Y cut algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The system published by <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">Shigarov et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> leverages the bounding boxes of words to restore the structure of a table. Since the system is heavily dependent on the metadata, the authors have employed PDF files to execute this experiment.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Figure <a href="#S0.F1" title="Figure 1 ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts the standard pipeline comparison between traditional approaches and deep learning methods for the process of table understanding. Traditional table recognition systems are either not generic enough on different datasets or they require the additional metadata from PDF files. In most of the traditional methods, exhaustive pre and post-processings were also employed to enhance the performance of traditional table recognition systems. However, in deep learning systems, instead of handcrafted features, neural networks mainly convolutional neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> are used to extract features. Subsequently, object detection or segmentation networks attempt to distinguish the tabular part which is further decomposed and recognized in a document image.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The text documents can be classified into two categories. The first category belongs to born-digital documents that contain not only text but the related meta-data such as layout information. One such example is the PDF documents. The second category of documents is acquired using devices such as scanners and cameras. To the best of our knowledge, there is no notable work that has employed deep learning for table recognition in camera-captured images. However, in the literature, one heuristic based approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> exists that works with camera-captured document images. The scope of this survey is to assess the deep learning-based approaches that have performed table recognition on the scanned document images.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">This review paper is organized as follows: Section <a href="#S2" title="II RELATED WORK ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> provides a brief discussion about the reviews and surveys which are already published in the research community; Section <a href="#S3" title="III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> provides an exhaustive discussion about several approaches that have contributed in the area of table understanding by leveraging deep learning concepts. Figure <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> explains the structural flow of mentioned methodologies; Section <a href="#S4" title="IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> investigates all the publicly available datasets that can be exploited for the problems in table analysis; Section <a href="#S5" title="V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> explains the well known evaluation metrics and provides performance analysis of all the discussed approaches in Section <a href="#S3" title="III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>; Section <a href="#S6" title="VI Conclusion ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> concludes the discussion where as Section<a href="#S7" title="VII Future Work ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> highlights various open issues and future directions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.2.1" class="ltx_text ltx_font_smallcaps">RELATED WORK</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The problem of table analysis has been a well-recognized problem for several years. Figure <a href="#S2.F3" title="Figure 3 ‣ II RELATED WORK ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the increasing trend in the number of publications for the last 5 years. Since this is a review paper, we would like to shed some light on the previous surveys and reviews that are already available in the table community. In the chapter <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">Document Recognition</span> in one of his books, Dougherty defines table <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. In the survey on document recognition, Handley <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> elaborated on the task of table recognition along with a precise explanation of previous work done in this domain. Later, <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">Lopresti et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> presented the survey on table understanding in which they discussed the heterogeneity in different kinds of tables. They also pointed out the potential areas where improvement could be made by leveraging many examples. The comprehensive survey was transformed into a tabular form which was later published as a book <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_italic">Zanibbi et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> came up with the exhaustive survey which includes all the recent material and state-of-the-art approaches of that time. They define the problem of table recognition as ”the interaction of models, observations, transformations, and inferences”<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Hurst in his doctoral thesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> defines the interpretation of tables. <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">Silva et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> published another survey in 2006. Along with evaluating the current table processing algorithms, the authors have proposed their own end-to-end table processing method and evaluation metrics to solve the problem of table structure recognition.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_italic">Embley et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> wrote a review illustrating about the table-processing paradigms.In 2014, another review on table recognition and forms is published by <span id="S2.p3.1.2" class="ltx_text ltx_font_italic">Coüasnon et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S2.p3.1.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>. The review covers a brief overview of the recent approaches of that time. In the following year and according to our knowledge, the latest review on the detection and extraction of tables in PDF documents is published by <span id="S2.p3.1.3" class="ltx_text ltx_font_italic">Khusro et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.3.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S2.p3.1.3.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>.</p>
</div>
<figure id="S2.F3" class="ltx_figure ltx_figure_panel"><svg id="S2.1.pic1" class="ltx_picture ltx_figure_panel" height="278.58" overflow="visible" version="1.1" width="344.44"><g transform="translate(0,278.58) matrix(1 0 0 -1 0 0) translate(49.21,0) translate(0,24.32) matrix(1.0 0.0 0.0 1.0 -49.21 -24.32)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(80.34,0) translate(0,50.19)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 -31.77 L 0 -25.86 M 51.89 -31.77 L 51.89 -25.86 M 103.78 -31.77 L 103.78 -25.86 M 155.67 -31.77 L 155.67 -25.86 M 207.56 -31.77 L 207.56 -25.86 M 0 204.2 L 0 198.3 M 51.89 204.2 L 51.89 198.3 M 103.78 204.2 L 103.78 198.3 M 155.67 204.2 L 155.67 198.3 M 207.56 204.2 L 207.56 198.3" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -31.13 20.23 L -25.23 20.23 M -31.13 68.39 L -25.23 68.39 M -31.13 116.56 L -25.23 116.56 M -31.13 164.73 L -25.23 164.73 M 238.69 20.23 L 232.78 20.23 M 238.69 68.39 L 232.78 68.39 M 238.69 116.56 L 232.78 116.56 M 238.69 164.73 L 232.78 164.73" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M -31.13 -25.86 L -31.13 198.3 L 238.69 198.3 L 238.69 -25.86 L -31.13 -25.86 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -13.84 -45.58)" fill="#000000" stroke="#000000"><foreignObject width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.1.pic1.10.10.10.10.10.1.1" class="ltx_text">2015</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 38.05 -45.58)" fill="#000000" stroke="#000000"><foreignObject width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.1.pic1.11.11.11.11.11.1.1" class="ltx_text">2016</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 89.94 -45.58)" fill="#000000" stroke="#000000"><foreignObject width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.1.pic1.12.12.12.12.12.1.1" class="ltx_text">2017</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 141.83 -45.58)" fill="#000000" stroke="#000000"><foreignObject width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.1.pic1.13.13.13.13.13.1.1" class="ltx_text">2018</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 193.72 -45.58)" fill="#000000" stroke="#000000"><foreignObject width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.1.pic1.14.14.14.14.14.1.1" class="ltx_text">2019</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -56.78 15.77)" fill="#000000" stroke="#000000"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">200</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -56.78 63.94)" fill="#000000" stroke="#000000"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="250" display="inline"><semantics id="S2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">250</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -56.78 112.1)" fill="#000000" stroke="#000000"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="300" display="inline"><semantics id="S2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="S2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">300</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -56.78 160.27)" fill="#000000" stroke="#000000"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="350" display="inline"><semantics id="S2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">350</mn><annotation-xml encoding="MathML-Content" id="S2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">350</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">350</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp1"><path d="M -31.13 -25.86 L 238.69 -25.86 L 238.69 198.3 L -31.13 198.3 Z"></path></clipPath><g clip-path="url(#pgfcp1)"><g stroke="#0000FF" fill="#B3B3FF" color="#0000FF"><path d="M -6.92 -25.86 h 13.84 v 65.36 h -13.84 Z M 44.97 -25.86 h 13.84 v 25.86 h -13.84 Z M 96.86 -25.86 h 13.84 v 62.47 h -13.84 Z M 148.75 -25.86 h 13.84 v 159.76 h -13.84 Z M 200.64 -25.86 h 13.84 v 198.3 h -13.84 Z"></path></g><g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 -10.38 44.38)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="220" display="inline"><semantics id="S2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">220</mn><annotation-xml encoding="MathML-Content" id="S2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">220</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">220</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 41.51 4.89)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="179" display="inline"><semantics id="S2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">179</mn><annotation-xml encoding="MathML-Content" id="S2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">179</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">179</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 93.4 41.49)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="217" display="inline"><semantics id="S2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">217</mn><annotation-xml encoding="MathML-Content" id="S2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">217</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">217</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 145.29 138.79)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="318" display="inline"><semantics id="S2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">318</mn><annotation-xml encoding="MathML-Content" id="S2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">318</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">318</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 197.18 177.32)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="358" display="inline"><semantics id="S2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">358</mn><annotation-xml encoding="MathML-Content" id="S2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">358</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">358</annotation></semantics></math></foreignObject></g><g transform="matrix(0.0 1.0 -1.0 0.0 -66.28 67.57)" fill="#000000" stroke="#000000"><foreignObject width="37.28" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.1.pic1.15.15.15.15.15.1.1" class="ltx_text">Count</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -51.93 214.18)" fill="#000000" stroke="#000000"><foreignObject width="308.34" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.1.pic1.16.16.16.16.16.1.1" class="ltx_text ltx_font_bold">Annual Number of Publications in Table Analysis </span></foreignObject></g></g></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Illustration of an increasing trend in the domain of table analysis. This data is collected by checking the yearly publications on table detection and table recognition from year 2015 to the year 2019. </figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">METHODOLOGIES</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">As elaborated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we have also defined the problem of table understanding into three steps:</p>
</div>
<div id="S3.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Table Detection:</span> detecting the tabular boundaries in terms of bounding boxes in document images.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Table Structural Segmentation:</span> defines the structure of table by analyzing information of row and column layouts.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Table Recognition:</span> includes both structural segmentation and parsing information of table cells.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2104.14272/assets/x1.png" id="S3.F4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="146" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Basic flow of table detection along with the methods used in the discussed approaches. In order to locate the tabular boundaries, document image is passed through various deep learning architectures.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Table Detection</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The first part of extracting information from the tables is to identify the tabular boundary in the document images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Figure <a href="#S3.F4" title="Figure 4 ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> explains the fundamental flow of table detection which has been discussed in numerous approaches. Various deep learning concepts have been employed to detect tabular areas from the document images. This section reviews the deep learning techniques which are exploited to perform table detection in document images. For the sake of providing convenience to our readers, we have categorized the approaches into discrete deep learning concepts. Table <a href="#S3.T1" title="TABLE I ‣ Fully Convolutional Networks ‣ III-A2 Semantic Image Segmentation ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> summarizes all the object detection-based table detection approaches,whereas Table <a href="#S3.T2" title="TABLE II ‣ Fully Convolutional Networks ‣ III-A2 Semantic Image Segmentation ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> highlights the advantages and limitations of the methods that have applied other deep learning-based techniques.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Based on our knowledge, the first approach that employed deep learning methods to solve the table detection task is proposed by <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">Hao et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p2.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S3.SS1.p2.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>. Along with the use of a convolutional neural network to extract the image features, authors applied some heuristics by leveraging the PDF metadata. Since this technique is based on PDF documents rather than relying on document images, we decide not to include this research in our performance analysis.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>Object Detection Algorithms</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Object detection is a branch of deep learning which deals with detecting an object in any image or a video frame. Region-based object detection algorithms are mainly divided into two steps: the first one is to generate appropriate proposals also known as <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">region of interest</span>. These regions of interest are classified using convolutional neural networks in the second step.</p>
</div>
<section id="S3.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Transfer Learning</h5>

<div id="S3.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px1.p1.1" class="ltx_p">Transfer learning is the concept of utilizing a pre-trained model on a problem that belongs to a different, but related domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Due to limited number of available labelled datasets, transfer learning has been excessively used in the vision-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. For similar reasons, researchers in the document image analysis community have also powered the capabilities of transfer learning to advance their approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. The capabilities of transfer learning have aided the researchers to reuse the pre-trained networks (trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> or COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>) on the problem of table detection and table structure recognition in document images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. While Section <a href="#S3.SS1.SSS1.Px2" title="Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>, <a href="#S3.SS1.SSS1.Px3" title="Deformable Convolutions ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a> and <a href="#S3.SS1.SSS1.Px6" title="Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a> explains transfer learning-based table detection methods, the techniques that employed transfer learning for the task of table structure recognition are elaborated in Section <a href="#S3.SS2.SSS5" title="III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>5</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Faster R-CNN</h5>

<div id="S3.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px2.p1.1" class="ltx_p">After the improvement of object detection algorithms from Fast R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> to Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, the tables are treated as an object in the document images. Gilani <span id="S3.SS1.SSS1.Px2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> employed deep learning method on the images to detect tables. The technique involves image transformation as a pre-processing step that follows with the table detection. In the image transformation part, a binary image is taken as an input on which Euclidean distance transform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, linear distance transform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, and max distance transform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> are applied on blue, green and red channels of the image respectively. Later, Gilani <span id="S3.SS1.SSS1.Px2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> have used a region-based object detection model called Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. The backbone of their Region Proposal Network (RPN) is based on ZFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. Their approach was able to beat the state-of-the-art results on UNLV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> dataset.</p>
</div>
<div id="S3.SS1.SSS1.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS1.Px2.p2.1" class="ltx_p">One of the works executed on document images by using the capabilities of deep learning has been accomplished by Schreiber <span id="S3.SS1.SSS1.Px2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Their end to end system known as <span id="S3.SS1.SSS1.Px2.p2.1.2" class="ltx_text ltx_font_italic">DeepDeSRT</span> not only detects the tabular region but also distinguishes the structure of the table and both of these tasks are dealt with by applying distinctive deep learning techniques. Table detection has been achieved by using Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. They have experimented with two different architectures as their backbone network: Zeiler and Fergus (ZFNet) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and a deep VGG-16 network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. Models are pre-trained on Pascal VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> dataset. Method for structural segmentation is explained in the Section <a href="#S3.SS2" title="III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a></p>
</div>
<div id="S3.SS1.SSS1.Px2.p3" class="ltx_para">
<p id="S3.SS1.SSS1.Px2.p3.1" class="ltx_p">With an increase in memory of graphical processing units (GPU), a room for bigger public datasets is created to completely leverage the power of GPUs. Minghao <span id="S3.SS1.SSS1.Px2.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> comprehends this need and proposed <span id="S3.SS1.SSS1.Px2.p3.1.2" class="ltx_text ltx_font_italic">TableBank</span>, which contains 417K labeled tables and their respective document images. They have also suggested baseline models by using Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> for the task of table detection. The author proposed a baseline method for structure recognition as well which will be explained later in Section <a href="#S3.SS2" title="III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a></p>
</div>
<div id="S3.SS1.SSS1.Px2.p4" class="ltx_para">
<p id="S3.SS1.SSS1.Px2.p4.1" class="ltx_p">In another research presented in the ICDAR 2019 conference, tables are detected using the combination of Faster R-CNN and further improved using the locating corners method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. The authors define the corners like a square of size <math id="S3.SS1.SSS1.Px2.p4.1.m1.1" class="ltx_Math" alttext="80\times 80" display="inline"><semantics id="S3.SS1.SSS1.Px2.p4.1.m1.1a"><mrow id="S3.SS1.SSS1.Px2.p4.1.m1.1.1" xref="S3.SS1.SSS1.Px2.p4.1.m1.1.1.cmml"><mn id="S3.SS1.SSS1.Px2.p4.1.m1.1.1.2" xref="S3.SS1.SSS1.Px2.p4.1.m1.1.1.2.cmml">80</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.Px2.p4.1.m1.1.1.1" xref="S3.SS1.SSS1.Px2.p4.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS1.Px2.p4.1.m1.1.1.3" xref="S3.SS1.SSS1.Px2.p4.1.m1.1.1.3.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px2.p4.1.m1.1b"><apply id="S3.SS1.SSS1.Px2.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px2.p4.1.m1.1.1"><times id="S3.SS1.SSS1.Px2.p4.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.Px2.p4.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.SSS1.Px2.p4.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.Px2.p4.1.m1.1.1.2">80</cn><cn type="integer" id="S3.SS1.SSS1.Px2.p4.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.Px2.p4.1.m1.1.1.3">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px2.p4.1.m1.1c">80\times 80</annotation></semantics></math> drawn around the vertices of tables. Along with locating the boundary of tables, corners are also detected using the same Faster R-CNN model. These corners are further refined after passing through various heuristics like two consecutive corners are on the same horizontal line. After analyzing the corners, inaccurate corners are filtered and left to form a group. The authors argue that most of the time, inaccuracy in table boundaries is due to inaccurate detection of the left and right side of the boundaries as compared to the top and bottom side of boundaries. Hence, only the right and left sides of a detected table are refined in this experiment. The refinement is carried out by first finding the corresponding corner for a table by calculating the intersection over the union between them. Subsequently, horizontal points of the table are shifted by taking the mean value between the table boundary and the corresponding corner. This article has conducted an experiment on ICDAR 2017 page object detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and reported a 2.8% increase in F-measure as compared to the traditional Faster R-CNN approach.</p>
</div>
</section>
<section id="S3.SS1.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deformable Convolutions</h5>

<div id="S3.SS1.SSS1.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px3.p1.1" class="ltx_p">Another approach is proposed by Siddiquie <span id="S3.SS1.SSS1.Px3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> in 2018 which was a follow-up work of Schreiber <span id="S3.SS1.SSS1.Px3.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. They have performed the table detection tasks by taking advantage of deformable convolutional neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> in the model of Faster R-CNN. The authors claim that deformable convolutions exceeds the performance of traditional convolutions due to having various tabular layouts and scales in the documents. Their model <span id="S3.SS1.SSS1.Px3.p1.1.3" class="ltx_text ltx_font_italic">DeCNT</span> have shown state-of-the-art results on the datasets of ICDAR-2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, ICDAR-2017 POD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, UNLV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and Marmot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.

</p>
</div>
<div id="S3.SS1.SSS1.Px3.p2" class="ltx_para">
<p id="S3.SS1.SSS1.Px3.p2.1" class="ltx_p">Agarwal <span id="S3.SS1.SSS1.Px3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> presented the approach called CDeC-Net (Composite Deformable Cascade Network) to detect tabular boundaries in document images. In this work, the authors empirically established that there is no need to add extra pre/post-processing techniques to obtain state-of-the-art results for table detection. This work is based on a novel cascade Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> along with the composite backbone which is a dual backbone architecture (two ResNeXt-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. In their composite backbone, the authors replace the conventional convolutions with the deformable convolutions to address the problem of detecting tables with arbitrary layouts. With the combination of deformable composite backbone and strong Cascade Mask R-CNN, their proposed system produced comparable results on several publicly available datasets in the table community.</p>
</div>
</section>
<section id="S3.SS1.SSS1.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">YOLO</h5>

<div id="S3.SS1.SSS1.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px4.p1.1" class="ltx_p">YOLO (You Only Look Once) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> which is a famous model for detecting objects in real-world images efficiently has also been employed in the task of table detection by Huang <span id="S3.SS1.SSS1.Px4.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. YOLO is different from region proposal methods because it handles the task of object detection more like a regression instead of a classification problem. YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> is the recent and enhanced version of YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> and is therefore used in this experiment. In order to make the predictions more precise, white-space margins are removed from the predicted tabular area along with the refinement of noisy page objects.</p>
</div>
</section>
<section id="S3.SS1.SSS1.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Mask R-CNN, YOLO, SSD and Retina Net</h5>

<div id="S3.SS1.SSS1.Px5.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px5.p1.1" class="ltx_p">Another research that leverages object detection algorithms is ”The Benefits of Close-Domain Fine-Tuning for Table Detection in Document Images” published by <span id="S3.SS1.SSS1.Px5.p1.1.1" class="ltx_text ltx_font_italic">Casado-García et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. After carrying out an exhaustive evaluation, the authors have demonstrated the improvement in the performance of table detection when fine-tuned from a closer domain. Leveraging the object detection algorithms, the writers have used Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> and Retina Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. To conduct this experiment, two base datasets are selected. The first dataset was PascalVOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> which contains natural scenic images and has no close relation with the datasets present in the table community. The second base dataset was TableBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> which has 417 thousand labeled images further explained in Section <a href="#S4.SS7" title="IV-G TableBank ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-G</span></span></a>. Two separate models were trained on these datasets and tested comprehensively on all ICDAR table competitions datasets along with other datasets like Marmot and UNLV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> which are later explained in Section <a href="#S4" title="IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. An average of 17% in improvement is noted in this article when models are fine-tuned with closer domain datasets as compared to models trained on real-world images.</p>
</div>
</section>
<section id="S3.SS1.SSS1.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Cascade Mask R-CNN</h5>

<div id="S3.SS1.SSS1.Px6.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px6.p1.1" class="ltx_p">Along with the recent improvements in generic spatial feature extraction networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, and object detection networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, we have seen a noticeable improvement in table detection systems. Prasad <span id="S3.SS1.SSS1.Px6.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> published the <span id="S3.SS1.SSS1.Px6.p1.1.2" class="ltx_text ltx_font_italic">CascadeTabNet</span> which is an end-to-end table detection and structure recognition method. In this work, the authors leverage the novel blend of Cascade Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> (which is a multistage Mask R-CNN) with the HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> as a base network. The paper exploited the similar area proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and instead of raw document images, transformed images were fed to the strong Cascade Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. Their proposed system was able to achieve state-of-the-art results on the datasets of ICDAR-2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, ICDAR-2019 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> and TableBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS1.Px6.p2" class="ltx_para">
<p id="S3.SS1.SSS1.Px6.p2.1" class="ltx_p">In one of the very recent works, Zheng <span id="S3.SS1.SSS1.Px6.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> published a framework for both the detection and structure recognition of tables in document images. The authors argue that the proposed system <span id="S3.SS1.SSS1.Px6.p2.1.2" class="ltx_text ltx_font_italic">GTE</span> (Global Table Extractor) is a generic vision-based method in which any object detection algorithm can be employed. The method feeds raw document images to the multiple object detectors that simultaneously detect tables and the individual cells to achieve accurate table detection. The predicted tables by the object detectors are further refined with the help of an additional penalty loss and predicted cellular boundaries. The approach further improves the predicted cellular areas to tackle table structure recognition, and it is explained in Section <a href="#S3.SS2" title="III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>.</p>
</div>
</section>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span>Semantic Image Segmentation</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">In the year 2018, the combination of deep convolutional neural networks, graphical models, and the concepts of saliency features have been applied to detect charts and tables by Kavasidis <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. The authors argued that instead of using the object detection networks, the task of detecting the tables can be posed as a saliency detection. The model is based on a semantic image segmentation technique. It first extracts saliency features and then each pixel is classified whether that pixel belongs to a region of interest or not. To notice long-term dependencies, the model employed dilated convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. In the end, the generated saliency map is propagated to the fully connected Conditional Random Field (CRF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, which further improves the predictions.</p>
</div>
<section id="S3.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fully Convolutional Networks</h5>

<div id="S3.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS2.Px1.p1.1" class="ltx_p"><span id="S3.SS1.SSS2.Px1.p1.1.1" class="ltx_text ltx_font_italic">TableNet</span> powered by deep learning is an end-to-end model for both detecting as well as recognizing the structure of tables in document images presented by Shubham et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>. The proposed method exploits the concepts of fully convolutional networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> with a pre-trained VGG-19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> layer as the base network. The author claims that the problem of identifying the tabular area and structure recognition can be jointly addressed similarly. They further demonstrated how the performance of a new dataset can be enhanced by exploiting the capabilities of transfer learning.</p>
</div>
<figure id="S3.F5" class="ltx_figure ltx_figure_panel"><svg id="S3.SS1.SSS2.Px1.1.pic1" class="ltx_picture ltx_figure_panel" height="279.12" overflow="visible" version="1.1" width="331.14"><g transform="translate(0,279.12) matrix(1 0 0 -1 0 0) translate(42.29,0) translate(0,24.86) matrix(1.0 0.0 0.0 1.0 -42.29 -24.86)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(73.43,0) translate(0,50.72)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 -31.77 L 0 -25.86 M 69.19 -31.77 L 69.19 -25.86 M 138.37 -31.77 L 138.37 -25.86 M 207.56 -31.77 L 207.56 -25.86 M 0 204.19 L 0 198.29 M 69.19 204.19 L 69.19 198.29 M 138.37 204.19 L 138.37 198.29 M 207.56 204.19 L 207.56 198.29" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -31.13 -19.16 L -25.23 -19.16 M -31.13 19.16 L -25.23 19.16 M -31.13 57.48 L -25.23 57.48 M -31.13 95.79 L -25.23 95.79 M -31.13 134.11 L -25.23 134.11 M -31.13 172.43 L -25.23 172.43 M 238.69 -19.16 L 232.78 -19.16 M 238.69 19.16 L 232.78 19.16 M 238.69 57.48 L 232.78 57.48 M 238.69 95.79 L 232.78 95.79 M 238.69 134.11 L 232.78 134.11 M 238.69 172.43 L 232.78 172.43" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M -31.13 -25.86 L -31.13 198.29 L 238.69 198.29 L 238.69 -25.86 L -31.13 -25.86 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -10.67 -46.11)" fill="#000000" stroke="#000000"><foreignObject width="21.33" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS1.SSS2.Px1.1.pic1.11.11.11.11.11.1.1" class="ltx_text">OD</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 53.38 -46.11)" fill="#000000" stroke="#000000"><foreignObject width="31.61" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS1.SSS2.Px1.1.pic1.12.12.12.12.12.1.1" class="ltx_text">GNN</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 122.56 -46.11)" fill="#000000" stroke="#000000"><foreignObject width="31.61" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS1.SSS2.Px1.1.pic1.13.13.13.13.13.1.1" class="ltx_text">GAN</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 197.37 -46.11)" fill="#000000" stroke="#000000"><foreignObject width="20.37" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS1.SSS2.Px1.1.pic1.14.14.14.14.14.1.1" class="ltx_text">SIS</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -42.94 -23.62)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS1.SSS2.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS1.SSS2.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS1.SSS2.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">0</cn></annotation-xml></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -42.94 14.7)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS1.SSS2.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS1.SSS2.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS1.SSS2.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">2</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -42.94 53.02)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS1.SSS2.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS1.SSS2.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS1.SSS2.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">4</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -42.94 91.33)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS1.SSS2.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS1.SSS2.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS1.SSS2.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">6</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -42.94 129.65)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS1.SSS2.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS1.SSS2.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS1.SSS2.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">8</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -49.86 167.97)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS1.SSS2.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS1.SSS2.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS1.SSS2.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">10</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp2"><path d="M -31.13 -25.86 L 238.69 -25.86 L 238.69 198.29 L -31.13 198.29 Z"></path></clipPath><g clip-path="url(#pgfcp2)"><g stroke="#0000FF" fill="#B3B3FF" color="#0000FF"><path d="M -6.92 -19.16 h 13.84 v 191.58 h -13.84 Z M 62.27 -19.16 h 13.84 v 38.32 h -13.84 Z M 131.45 -19.16 h 13.84 v 19.16 h -13.84 Z M 200.64 -19.16 h 13.84 v 38.32 h -13.84 Z"></path></g><g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 -6.92 177.31)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS1.SSS2.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS1.SSS2.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS1.SSS2.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">10</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 65.73 24.05)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS1.SSS2.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS1.SSS2.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS1.SSS2.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">2</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 134.91 4.89)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS1.SSS2.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS1.SSS2.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS1.SSS2.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">1</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 204.1 24.05)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS1.SSS2.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS1.SSS2.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS1.SSS2.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">2</annotation></semantics></math></foreignObject></g><g transform="matrix(0.0 1.0 -1.0 0.0 -59.36 67.57)" fill="#000000" stroke="#000000"><foreignObject width="37.28" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS1.SSS2.Px1.1.pic1.15.15.15.15.15.1.1" class="ltx_text">Count</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -45.55 214.17)" fill="#000000" stroke="#000000"><foreignObject width="298.65" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS1.SSS2.Px1.1.pic1.16.16.16.16.16.1.1" class="ltx_text ltx_font_bold">Deep Learning Methods used in Table Detection </span></foreignObject></g></g></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>OD is Object Detection, SIS means Semantic Image Segmentation, GNN is Graph Neural Networks whereas GAN is used to represent Generative Adversarial Networks. This graph explains what kind of deep learning algorithms are periodically exploited to perform table detection.</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span> A summary of advantages and limitations of various deep learning-based table detection methods that are based on object detection frameworks.</figcaption>
<table id="S3.T1.11" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.1.1.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.1.1.1.1.p1.1" class="ltx_p"><span id="S3.T1.1.1.1.1.p1.1.1" class="ltx_text ltx_font_bold">Literature</span></span>
</span></span></th>
<th id="S3.T1.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.1.1" class="ltx_p"><span id="S3.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Method</span></span>
</span>
</th>
<th id="S3.T1.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.1.1" class="ltx_p"><span id="S3.T1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Highlights</span></span>
</span>
</th>
<th id="S3.T1.1.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S3.T1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.4.1.1" class="ltx_p"><span id="S3.T1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Limitations</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.2" class="ltx_tr">
<td id="S3.T1.2.2.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T1.2.2.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.2.2.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.2.2.1.1.p1.1" class="ltx_p"><span id="S3.T1.2.2.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Gelani et al.<span id="S3.T1.2.2.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.2.2.1.1" class="ltx_p"><span id="S3.T1.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN (Section <a href="#S3.SS1.SSS1.Px2" title="Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>).</span></span>
<span id="S3.T1.2.2.2.1.2" class="ltx_p"><span id="S3.T1.2.2.2.1.2.1" class="ltx_text" style="font-size:80%;">Images are transformed and then fed into the Faster R-CNN.</span></span>
</span>
</td>
<td id="S3.T1.2.2.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.2.3.1.1" class="ltx_p"><span id="S3.T1.2.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T1.2.2.3.1.1.1.1" class="ltx_text ltx_font_medium"> First deep learning based table detection approach on scanned document images, </span>b)<span id="S3.T1.2.2.3.1.1.1.2" class="ltx_text ltx_font_medium"> Transforming RGB pixels to distance metrics facilitates the object detection algorithm.</span></span></span>
</span>
</td>
<td id="S3.T1.2.2.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.2.4.1.1" class="ltx_p"><span id="S3.T1.2.2.4.1.1.1" class="ltx_text" style="font-size:80%;">Extra pre-processing steps involved.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.3.3" class="ltx_tr">
<td id="S3.T1.3.3.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T1.3.3.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.3.3.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.3.3.1.1.p1.1" class="ltx_p"><span id="S3.T1.3.3.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">DeCNT<span id="S3.T1.3.3.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T1.3.3.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.3.3.2.1.1" class="ltx_p"><span id="S3.T1.3.3.2.1.1.1" class="ltx_text" style="font-size:80%;">Deformable convolutions implemented in the Faster R-CNN architecture (Section <a href="#S3.SS1.SSS1.Px3" title="Deformable Convolutions ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.3.3.3.1.1" class="ltx_p"><span id="S3.T1.3.3.3.1.1.1" class="ltx_text" style="font-size:80%;">The dynamic receptive field of deformable convolutional neural networks help in recognizing various tabular boundaries.</span></span>
</span>
</td>
<td id="S3.T1.3.3.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.3.3.4.1.1" class="ltx_p"><span id="S3.T1.3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">Deformable convolutions are computationally intensive as compared to traditional convolutions.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.4.4" class="ltx_tr">
<td id="S3.T1.4.4.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T1.4.4.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.4.4.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.4.4.1.1.p1.1" class="ltx_p"><span id="S3.T1.4.4.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">DeepDeSRT<span id="S3.T1.4.4.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T1.4.4.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.2.1.1" class="ltx_p"><span id="S3.T1.4.4.2.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN with transfer learning techniques (Section <a href="#S3.SS1.SSS1.Px2" title="Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
<td id="S3.T1.4.4.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.3.1.1" class="ltx_p"><span id="S3.T1.4.4.3.1.1.1" class="ltx_text" style="font-size:80%;">Simple and effective end-to-end approach to detect tables and structures of the tables.</span></span>
</span>
</td>
<td id="S3.T1.4.4.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.4.1.1" class="ltx_p"><span id="S3.T1.4.4.4.1.1.1" class="ltx_text" style="font-size:80%;">Not as accurate as compared to other states of the art approaches.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5" class="ltx_tr">
<td id="S3.T1.5.5.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T1.5.5.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.5.5.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.5.5.1.1.p1.1" class="ltx_p"><span id="S3.T1.5.5.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">TableBank<span id="S3.T1.5.5.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T1.5.5.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.2.1.1" class="ltx_p"><span id="S3.T1.5.5.2.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN used as a baseline method for a novel dataset (Section <a href="#S3.SS1.SSS1.Px2" title="Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T1.5.5.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.3.1.1" class="ltx_p"><span id="S3.T1.5.5.3.1.1.1" class="ltx_text" style="font-size:80%;">This approach presents that by leveraging a large dataset such as TableBank, a simple Faster R-CNN can produce impressive results.</span></span>
</span>
</td>
<td id="S3.T1.5.5.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.4.1.1" class="ltx_p"><span id="S3.T1.5.5.4.1.1.1" class="ltx_text" style="font-size:80%;">Just a direct application of Faster R-CNN.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.6" class="ltx_tr">
<td id="S3.T1.6.6.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T1.6.6.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.6.6.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.6.6.1.1.p1.1" class="ltx_p"><span id="S3.T1.6.6.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Sun et al.<span id="S3.T1.6.6.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T1.6.6.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.6.2.1.1" class="ltx_p"><span id="S3.T1.6.6.2.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN with locating corners (Section <a href="#S3.SS1.SSS1.Px2" title="Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T1.6.6.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.6.3.1.1" class="ltx_p"><span id="S3.T1.6.6.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T1.6.6.3.1.1.1.1" class="ltx_text ltx_font_medium"> Faster R-CNN is exploited to detect not only tables but the corners of the tabular boundaries as well, </span>b)<span id="S3.T1.6.6.3.1.1.1.2" class="ltx_text ltx_font_medium"> Novel method produces better results.</span></span></span>
</span>
</td>
<td id="S3.T1.6.6.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.6.4.1.1" class="ltx_p"><span id="S3.T1.6.6.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T1.6.6.4.1.1.1.1" class="ltx_text ltx_font_medium"> Computationally more extensive because of additional detections, </span>b)<span id="S3.T1.6.6.4.1.1.1.2" class="ltx_text ltx_font_medium"> Post-processing steps such as corners’ refinement are required.</span></span></span>
</span>
</td>
</tr>
<tr id="S3.T1.7.7" class="ltx_tr">
<td id="S3.T1.7.7.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T1.7.7.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.7.7.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.7.7.1.1.p1.1" class="ltx_p"><span id="S3.T1.7.7.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Huang et al.<span id="S3.T1.7.7.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T1.7.7.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.7.7.2.1.1" class="ltx_p"><span id="S3.T1.7.7.2.1.1.1" class="ltx_text" style="font-size:80%;">YOLO based table detection method (Section <a href="#S3.SS1.SSS1.Px4" title="YOLO ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T1.7.7.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.7.7.3.1.1" class="ltx_p"><span id="S3.T1.7.7.3.1.1.1" class="ltx_text" style="font-size:80%;">Comparatively, faster and efficient approach.</span></span>
</span>
</td>
<td id="S3.T1.7.7.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.7.7.4.1.1" class="ltx_p"><span id="S3.T1.7.7.4.1.1.1" class="ltx_text" style="font-size:80%;">The proposed method depends on the data driven post-processing techniques.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.8.8" class="ltx_tr">
<td id="S3.T1.8.8.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T1.8.8.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.8.8.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.8.8.1.1.p1.1" class="ltx_p"><span id="S3.T1.8.8.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">García et al.<span id="S3.T1.8.8.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T1.8.8.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.8.8.2.1.1" class="ltx_p"><span id="S3.T1.8.8.2.1.1.1" class="ltx_text" style="font-size:80%;">Employed Mask R-CNN, YOLO, SSD and RetinaNet to compare fine-tuning techniques (Section <a href="#S3.SS1.SSS1.Px5" title="Mask R-CNN, YOLO, SSD and Retina Net ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T1.8.8.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.8.8.3.1.1" class="ltx_p"><span id="S3.T1.8.8.3.1.1.1" class="ltx_text" style="font-size:80%;">Presented the benefits of leveraging a closer domain fine-tuning methods for table detection while employing object detection networks.</span></span>
</span>
</td>
<td id="S3.T1.8.8.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.8.8.4.1.1" class="ltx_p"><span id="S3.T1.8.8.4.1.1.1" class="ltx_text" style="font-size:80%;">Still, closed domain fine-tuning is not enough to reach the state-of-the-art results.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.9.9" class="ltx_tr">
<td id="S3.T1.9.9.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T1.9.9.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.9.9.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.9.9.1.1.p1.1" class="ltx_p"><span id="S3.T1.9.9.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">CascadeTabNet<span id="S3.T1.9.9.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T1.9.9.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.9.9.2.1.1" class="ltx_p"><span id="S3.T1.9.9.2.1.1.1" class="ltx_text" style="font-size:80%;">Employed Cascade Mask R-CNN with an iterative transfer learning approach (Section <a href="#S3.SS1.SSS1.Px6" title="Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T1.9.9.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.9.9.3.1.1" class="ltx_p"><span id="S3.T1.9.9.3.1.1.1" class="ltx_text" style="font-size:80%;">This work presents that transformed images with an iterative transfer learning can reduce the dependency of large-scale datasets.</span></span>
</span>
</td>
<td id="S3.T1.9.9.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.9.9.4.1.1" class="ltx_p"><span id="S3.T1.9.9.4.1.1.1" class="ltx_text" style="font-size:80%;">Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, extra pre-processing steps are involved in this approach.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.10.10" class="ltx_tr">
<td id="S3.T1.10.10.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T1.10.10.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.10.10.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.10.10.1.1.p1.1" class="ltx_p"><span id="S3.T1.10.10.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">CDeC-Net<span id="S3.T1.10.10.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T1.10.10.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.10.10.2.1.1" class="ltx_p"><span id="S3.T1.10.10.2.1.1.1" class="ltx_text" style="font-size:80%;">Cascade Mask R-CNN with a deformable composite backbone (Section <a href="#S3.SS1.SSS1.Px3" title="Deformable Convolutions ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T1.10.10.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T1.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.10.10.3.1.1" class="ltx_p"><span id="S3.T1.10.10.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T1.10.10.3.1.1.1.1" class="ltx_text ltx_font_medium"> Extensive evaluations on publicly available benchmark datasets for table detection. </span>b)<span id="S3.T1.10.10.3.1.1.1.2" class="ltx_text ltx_font_medium"> An end-to-end object detection-based framework leveraging composite backbone to produce state-of-the-art results.</span></span></span>
</span>
</td>
<td id="S3.T1.10.10.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.10.10.4.1.1" class="ltx_p"><span id="S3.T1.10.10.4.1.1.1" class="ltx_text" style="font-size:80%;">Along with the deformable convolutions, a composite backbone is employed which makes the approach computationally intensive.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.11.11" class="ltx_tr">
<td id="S3.T1.11.11.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.11.11.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T1.11.11.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T1.11.11.1.1.p1.1" class="ltx_p"><span id="S3.T1.11.11.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">GTE<span id="S3.T1.11.11.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T1.11.11.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T1.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.11.11.2.1.1" class="ltx_p"><span id="S3.T1.11.11.2.1.1.1" class="ltx_text" style="font-size:80%;">Proposed a generic object detection approach (Section <a href="#S3.SS1.SSS1.Px6" title="Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T1.11.11.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T1.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.11.11.3.1.1" class="ltx_p"><span id="S3.T1.11.11.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a<span id="S3.T1.11.11.3.1.1.1.1" class="ltx_text ltx_font_medium">) An end-to-end technique that can operate on any object detection framework. </span>b<span id="S3.T1.11.11.3.1.1.1.2" class="ltx_text ltx_font_medium">) This work proposed an additional piece-wise constraint loss that benefits the task of table detection.</span></span></span>
</span>
</td>
<td id="S3.T1.11.11.4" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S3.T1.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.11.11.4.1.1" class="ltx_p"><span id="S3.T1.11.11.4.1.1.1" class="ltx_text" style="font-size:80%;">Since the task of table detection is dependent on cell detections, annotations for cellular boundaries are required.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span> A summary of advantages and limitations of various table detection methods that operate on other deep learning-based concepts. The bold horizontal line separates the approaches with different architectures. </figcaption>
<table id="S3.T2.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T2.1.1.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T2.1.1.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T2.1.1.1.1.p1.1" class="ltx_p"><span id="S3.T2.1.1.1.1.p1.1.1" class="ltx_text ltx_font_bold">Literature</span></span>
</span></span></td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.1.1" class="ltx_p"><span id="S3.T2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Method</span></span>
</span>
</td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.3.1.1" class="ltx_p"><span id="S3.T2.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Highlights</span></span>
</span>
</td>
<td id="S3.T2.1.1.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.4.1.1" class="ltx_p"><span id="S3.T2.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Limitations</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T2.2.2.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T2.2.2.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T2.2.2.1.1.p1.1" class="ltx_p"><span id="S3.T2.2.2.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Kavasidis et al.<span id="S3.T2.2.2.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T2.2.2.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.2.1.1" class="ltx_p"><span id="S3.T2.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;">Semantic Image Segmentation with saliency concepts (Section <a href="#S3.SS1.SSS2" title="III-A2 Semantic Image Segmentation ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>2</span></a>).</span></span>
</span>
</td>
<td id="S3.T2.2.2.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.3.1.1" class="ltx_p"><span id="S3.T2.2.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T2.2.2.3.1.1.1.1" class="ltx_text ltx_font_medium"> This method poses the task of table detection as saliency detection, </span>b)<span id="S3.T2.2.2.3.1.1.1.2" class="ltx_text ltx_font_medium"> Dilated convolutions are applied instead of traditional convolutions.</span></span></span>
</span>
</td>
<td id="S3.T2.2.2.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.4.1.1" class="ltx_p"><span id="S3.T2.2.2.4.1.1.1" class="ltx_text" style="font-size:80%;">Multiple processing steps are required to achieve comparable results.</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T2.3.3.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T2.3.3.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T2.3.3.1.1.p1.1" class="ltx_p"><span id="S3.T2.3.3.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">TableNet<span id="S3.T2.3.3.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T2.3.3.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.2.1.1" class="ltx_p"><span id="S3.T2.3.3.2.1.1.1" class="ltx_text" style="font-size:80%;">Fully Convolutional Networks (Section <a href="#S3.SS1.SSS2.Px1" title="Fully Convolutional Networks ‣ III-A2 Semantic Image Segmentation ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>2</span></a>).</span></span>
</span>
</td>
<td id="S3.T2.3.3.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.3.1.1" class="ltx_p"><span id="S3.T2.3.3.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T2.3.3.3.1.1.1.1" class="ltx_text ltx_font_medium"> An end-to-end approach for table detection and structure recognition in document images, </span>b)<span id="S3.T2.3.3.3.1.1.1.2" class="ltx_text ltx_font_medium"> First approach to jointly address the task of table detection and structure recognition with a single method.</span></span></span>
</span>
</td>
<td id="S3.T2.3.3.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.4.1.1" class="ltx_p"><span id="S3.T2.3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">In the case of table structural extraction, this technique only works on column detection.</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.4.4" class="ltx_tr">
<td id="S3.T2.4.4.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T2.4.4.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T2.4.4.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T2.4.4.1.1.p1.1" class="ltx_p"><span id="S3.T2.4.4.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Martin et al.<span id="S3.T2.4.4.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T2.4.4.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.4.2.1.1" class="ltx_p"><span id="S3.T2.4.4.2.1.1.1" class="ltx_text" style="font-size:80%;">Graph Neural Network with the line item detection approach. (Section <a href="#S3.SS1.SSS3" title="III-A3 Graph Neural Networks ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>3</span></a>)</span></span>
</span>
</td>
<td id="S3.T2.4.4.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.4.3.1.1" class="ltx_p"><span id="S3.T2.4.4.3.1.1.1" class="ltx_text" style="font-size:80%;">The method shows promising results on the layout-heavy documents such as invoices.</span></span>
</span>
</td>
<td id="S3.T2.4.4.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.4.4.1.1" class="ltx_p"><span id="S3.T2.4.4.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T2.4.4.4.1.1.1.1" class="ltx_text ltx_font_medium"> Approach is not evaluated on any publicly available table datasets, </span>b)<span id="S3.T2.4.4.4.1.1.1.2" class="ltx_text ltx_font_medium"> Weak baseline method and no comparisons with other state-of-the-art methods.</span></span></span>
</span>
</td>
</tr>
<tr id="S3.T2.5.5" class="ltx_tr">
<td id="S3.T2.5.5.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T2.5.5.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T2.5.5.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T2.5.5.1.1.p1.1" class="ltx_p"><span id="S3.T2.5.5.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Riba et al.<span id="S3.T2.5.5.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T2.5.5.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.5.5.2.1.1" class="ltx_p"><span id="S3.T2.5.5.2.1.1.1" class="ltx_text" style="font-size:80%;">Graph Neural Network by leveraging textual attributes through OCR (Section <a href="#S3.SS1.SSS3" title="III-A3 Graph Neural Networks ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>3</span></a>)</span></span>
</span>
</td>
<td id="S3.T2.5.5.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.5.5.3.1.1" class="ltx_p"><span id="S3.T2.5.5.3.1.1.1" class="ltx_text" style="font-size:80%;">The proposed method leverages more information than just the spatial features.</span></span>
</span>
</td>
<td id="S3.T2.5.5.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T2.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.5.5.4.1.1" class="ltx_p"><span id="S3.T2.5.5.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T2.5.5.4.1.1.1.1" class="ltx_text ltx_font_medium"> This method requires extra annotations apart from the information of tabular area, </span>b)<span id="S3.T2.5.5.4.1.1.1.2" class="ltx_text ltx_font_medium"> No comparisons with other state-of-the-art approaches.</span></span></span>
</span>
</td>
</tr>
<tr id="S3.T2.6.6" class="ltx_tr">
<td id="S3.T2.6.6.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt"><span id="S3.T2.6.6.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T2.6.6.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T2.6.6.1.1.p1.1" class="ltx_p"><span id="S3.T2.6.6.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Li et al.<span id="S3.T2.6.6.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T2.6.6.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt">
<span id="S3.T2.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.6.6.2.1.1" class="ltx_p"><span id="S3.T2.6.6.2.1.1.1" class="ltx_text" style="font-size:80%;">Generative Adversarial Networks and object detection network (Section <a href="#S3.SS1.SSS4" title="III-A4 Generative Adversarial Networks ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>4</span></a>)</span></span>
</span>
</td>
<td id="S3.T2.6.6.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt">
<span id="S3.T2.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.6.6.3.1.1" class="ltx_p"><span id="S3.T2.6.6.3.1.1.1" class="ltx_text" style="font-size:80%;">GAN based approach forces the network to extract similar features for ruling and less-ruled tables.</span></span>
</span>
</td>
<td id="S3.T2.6.6.4" class="ltx_td ltx_align_justify ltx_border_b ltx_border_tt">
<span id="S3.T2.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.6.6.4.1.1" class="ltx_p"><span id="S3.T2.6.6.4.1.1.1" class="ltx_text" style="font-size:80%;">Model with generators is vulnerable in document images having diverse tabular layouts.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS3.5.1.1" class="ltx_text">III-A</span>3 </span>Graph Neural Networks</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Recently, we have seen that the adoption of graph neural networks in the area of table understanding is on the rise. Riba <span id="S3.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> carried out an experiment of detecting tables using graph neural networks in the invoice documents. Due to the limited amount of information available in the images of invoices, the authors argue that graph neural networks are a better fit to detect the tabular area. The paper also publishes the labeled subset of the original RVL-CDIP dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> which is pulbicly available.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">Martin <span id="S3.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> extends the application of graph neural networks by presenting the idea of table understanding using graph convolutions in structured documents like invoices. The proposed research is also conducted on PDF documents however, the authors claim that the model is robust enough to handle other kinds of data sets. In this research, the problem of table detection is solved by combining the task of line item table detection and information extraction. With the line item approach, any word can be easily distinguished whether it is a part of a line item or not. After classifying all words, the tabular area can be efficiently detected since lines in the table separate reasonably well enough as compared to other text-areas in invoices.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2104.14272/assets/x2.png" id="S3.F6.g1" class="ltx_graphics ltx_img_landscape" width="461" height="145" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Basic flow of table structural segmentation along with the methods used in the discussed approaches. Instead of a document image, tabular image is given to the various deep neural architectures in order to recognize the structure of table. </figcaption>
</figure>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS4.5.1.1" class="ltx_text">III-A</span>4 </span>Generative Adversarial Networks</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">Generative Adversarial Networks (GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> have also been exploited to identify tables. The proposed approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> makes sure that the generative network sees no difference between the ruling and less-ruling tables and try to extract identical features in both of the cases. Subsequently, the feature generator is joined with semantic segmentation models like Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> or U-net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>. After combining the GAN-based feature generator with Mask R-CNN, the approach is evaluated on the ICDAR2017 POD dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. Authors claim that this approach will facilitate other object detection and segmentation problems.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Table Structural Segmentation</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Once, the boundary of the table is detected, the next step is to identify the rows and columns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. In this section, we will review the recent approaches that have attempted the problem of table structural segmentation. We have categorized the methodologies according to the architecture of deep neural networks.
Table <a href="#S3.T3" title="TABLE III ‣ Distance Based Weights ‣ III-B2 Graph Neural Networks ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> summarizes these approaches by highlighting their advantages and limitations. Figure <a href="#S3.F6" title="Figure 6 ‣ III-A3 Graph Neural Networks ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the essential flow of table structural segmentation techniques that are discussed in this review paper.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.5.1.1" class="ltx_text">III-B</span>1 </span>Semantic Image Segmentation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Along with table detection, <span id="S3.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">TableNet</span> segments the structure of a table by detecting the columns in respective tables. Shubham et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> used a pre-trained VGG-19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> as a base network that acts as an encoder while a decoder performs the column detection. The author tries to convince the readers that due to the interdependence between the table detection and structural segmentation, both of the problems can be solved efficiently by using a single network.</p>
</div>
<section id="S3.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fully Convolutional Networks</h5>

<div id="S3.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p1.1" class="ltx_p">To recognize the structure in tables, the authors of <span id="S3.SS2.SSS1.Px1.p1.1.1" class="ltx_text ltx_font_italic">DeepDeSRT</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> have exploited the concept of semantic segmentation. They implemented a fully convolutional network proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>. An added pre-processing step of stretching the table vertically for rows and horizontally for columns have provided a valuable advantage in the results. They achieved state-of-the-art results on the ICDAR 2013 table structure recognition dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p2.5" class="ltx_p">Another paper ”Rethinking Semantic Segmentation for Table Structure Recognition in Documents” is proposed by Siddiqui <span id="S3.SS2.SSS1.Px1.p2.5.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>. Just like Schreiber <span id="S3.SS2.SSS1.Px1.p2.5.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, they have formulated the problem of structure recognition as the semantic segmentation problem. The authors have used fully convolutional networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> to segment the rows and columns respectively. Assuming the consistency in a tabular structure, the method of prediction tiling is introduced which reduces the complexity of table structural recognition. The author used the structural models of FCN’s encoder and decoder, and loaded pre-trained models on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>. Given an image, the model produces the features having the same size as the original input image. The tiling process averages the features in rows and columns and combines the features of <math id="S3.SS2.SSS1.Px1.p2.1.m1.1" class="ltx_Math" alttext="H\times W\times C" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.1.m1.1a"><mrow id="S3.SS2.SSS1.Px1.p2.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.2" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.1" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.3" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.1a" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.4" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.4.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.1.m1.1b"><apply id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1"><times id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.1"></times><ci id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.2">𝐻</ci><ci id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.3">𝑊</ci><ci id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.4.cmml" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.4">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.1.m1.1c">H\times W\times C</annotation></semantics></math> (<math id="S3.SS2.SSS1.Px1.p2.2.m2.1" class="ltx_Math" alttext="Height\times Width\times Channel" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.2.m2.1a"><mrow id="S3.SS2.SSS1.Px1.p2.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.cmml"><mrow id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.cmml"><mrow id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.cmml"><mrow id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.cmml"><mrow id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.cmml"><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.2" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.3" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1a" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.4" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1b" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.5" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1c" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.6" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.6.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1d" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.7" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.7.cmml">t</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.1" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.3" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.3.cmml">W</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.1" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.3" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.1a" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.4" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.1b" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.5" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.1c" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.6" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.6.cmml">h</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.1" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.3" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.3.cmml">C</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.3" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1a" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.4" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1b" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.5" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1c" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.6" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1d" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.7" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1e" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.8" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.8.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.2.m2.1b"><apply id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1"><times id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1"></times><apply id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2"><times id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.1"></times><apply id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2"><times id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.1.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.1"></times><apply id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2"><times id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.1.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.1"></times><apply id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2"><times id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.1"></times><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.2">𝐻</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.3">𝑒</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.4.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.4">𝑖</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.5.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.5">𝑔</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.6.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.6">ℎ</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.7.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.2.7">𝑡</ci></apply><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.3.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.2.3">𝑊</ci></apply><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.3.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.3">𝑖</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.4.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.4">𝑑</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.5.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.5">𝑡</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.6.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.2.6">ℎ</ci></apply><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.3">𝐶</ci></apply><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.3">ℎ</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.4.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.4">𝑎</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.5.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.5">𝑛</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.6.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.6">𝑛</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.7.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.7">𝑒</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.8.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.8">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.2.m2.1c">Height\times Width\times Channel</annotation></semantics></math>) into <math id="S3.SS2.SSS1.Px1.p2.3.m3.1" class="ltx_Math" alttext="H\times C" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.3.m3.1a"><mrow id="S3.SS2.SSS1.Px1.p2.3.m3.1.1" xref="S3.SS2.SSS1.Px1.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p2.3.m3.1.1.2" xref="S3.SS2.SSS1.Px1.p2.3.m3.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p2.3.m3.1.1.1" xref="S3.SS2.SSS1.Px1.p2.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p2.3.m3.1.1.3" xref="S3.SS2.SSS1.Px1.p2.3.m3.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.3.m3.1b"><apply id="S3.SS2.SSS1.Px1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.3.m3.1.1"><times id="S3.SS2.SSS1.Px1.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.3.m3.1.1.1"></times><ci id="S3.SS2.SSS1.Px1.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p2.3.m3.1.1.2">𝐻</ci><ci id="S3.SS2.SSS1.Px1.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p2.3.m3.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.3.m3.1c">H\times C</annotation></semantics></math> for rows and <math id="S3.SS2.SSS1.Px1.p2.4.m4.1" class="ltx_Math" alttext="W\times C" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.4.m4.1a"><mrow id="S3.SS2.SSS1.Px1.p2.4.m4.1.1" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.2" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.2.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.1" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.3" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.4.m4.1b"><apply id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1"><times id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.1"></times><ci id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.2">𝑊</ci><ci id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.4.m4.1c">W\times C</annotation></semantics></math> for columns. Features after being convolved are expanded into <math id="S3.SS2.SSS1.Px1.p2.5.m5.1" class="ltx_Math" alttext="H\times W\times C" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.5.m5.1a"><mrow id="S3.SS2.SSS1.Px1.p2.5.m5.1.1" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.2" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.1" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.3" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.1a" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.4" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.4.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.5.m5.1b"><apply id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1"><times id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.1"></times><ci id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.2">𝐻</ci><ci id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.3">𝑊</ci><ci id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.4.cmml" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.4">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.5.m5.1c">H\times W\times C</annotation></semantics></math>. Subsequently, the label of each pixel is obtained through the convolution layer. Finally, post-processing is performed to accomplish the final result. The authors have reported the F1-score of 93.42% with an IOU of 0.5 on the ICDAR 2013 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. Due to the writer’s constraint of consistency, they have to finetune this dataset which is now publicly available to reproduce similar results<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Fine-tuned ICDAR-13 dataset : https://bit.ly/2NhZHCr</span></span></span>.</p>
</div>
<figure id="S3.F7" class="ltx_figure ltx_figure_panel"><svg id="S3.SS2.SSS1.Px1.1.pic1" class="ltx_picture ltx_figure_panel" height="279.12" overflow="visible" version="1.1" width="333.37"><g transform="translate(0,279.12) matrix(1 0 0 -1 0 0) translate(48.44,0) translate(0,24.86) matrix(1.0 0.0 0.0 1.0 -48.44 -24.86)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(79.57,0) translate(0,50.72)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 -31.77 L 0 -25.86 M 51.89 -31.77 L 51.89 -25.86 M 103.78 -31.77 L 103.78 -25.86 M 207.56 -31.77 L 207.56 -25.86 M 155.67 -31.77 L 155.67 -25.86 M 0 204.2 L 0 198.29 M 51.89 204.2 L 51.89 198.29 M 103.78 204.2 L 103.78 198.29 M 207.56 204.2 L 207.56 198.29 M 155.67 204.2 L 155.67 198.29" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -31.13 0 L -25.23 0 M -31.13 43.11 L -25.23 43.11 M -31.13 86.22 L -25.23 86.22 M -31.13 129.32 L -25.23 129.32 M -31.13 172.43 L -25.23 172.43 M 238.69 0 L 232.78 0 M 238.69 43.11 L 232.78 43.11 M 238.69 86.22 L 232.78 86.22 M 238.69 129.32 L 232.78 129.32 M 238.69 172.43 L 232.78 172.43" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M -31.13 -25.86 L -31.13 198.29 L 238.69 198.29 L 238.69 -25.86 L -31.13 -25.86 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -10.67 -46.11)" fill="#000000" stroke="#000000"><foreignObject width="21.33" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS2.SSS1.Px1.1.pic1.11.11.11.11.11.1.1" class="ltx_text">OD</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 41.7 -46.11)" fill="#000000" stroke="#000000"><foreignObject width="20.37" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS2.SSS1.Px1.1.pic1.12.12.12.12.12.1.1" class="ltx_text">SIS</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 88.31 -46.11)" fill="#000000" stroke="#000000"><foreignObject width="30.94" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS2.SSS1.Px1.1.pic1.13.13.13.13.13.1.1" class="ltx_text">RNN</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 191.75 -46.11)" fill="#000000" stroke="#000000"><foreignObject width="31.61" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS2.SSS1.Px1.1.pic1.14.14.14.14.14.1.1" class="ltx_text">GNN</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 140.1 -46.11)" fill="#000000" stroke="#000000"><foreignObject width="31.13" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS2.SSS1.Px1.1.pic1.15.15.15.15.15.1.1" class="ltx_text">DDC</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -42.94 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS2.SSS1.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS2.SSS1.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">2</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -56.01 38.65)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS2.SSS1.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="2.5" display="inline"><semantics id="S3.SS2.SSS1.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">2.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S3.SS2.SSS1.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">2.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">2.5</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -42.94 81.76)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS2.SSS1.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS2.SSS1.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">3</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -56.01 124.86)" fill="#000000" stroke="#000000"><foreignObject width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS2.SSS1.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="3.5" display="inline"><semantics id="S3.SS2.SSS1.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">3.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S3.SS2.SSS1.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">3.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">3.5</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -42.94 167.97)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS2.SSS1.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS2.SSS1.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">4</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp3"><path d="M -31.13 -25.86 L 238.69 -25.86 L 238.69 198.29 L -31.13 198.29 Z"></path></clipPath><g clip-path="url(#pgfcp3)"><g stroke="#0000FF" fill="#B3B3FF" color="#0000FF"><path d="M -6.92 -25.86 h 13.84 v 198.29 h -13.84 Z M 44.97 -25.86 h 13.84 v 112.08 h -13.84 Z M 96.86 -25.86 h 13.84 v 25.86 h -13.84 Z M 200.64 -25.86 h 13.84 v 112.08 h -13.84 Z M 148.75 -25.86 h 13.84 v 25.86 h -13.84 Z"></path></g><g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 177.32)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS2.SSS1.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS2.SSS1.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">4</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 48.43 91.1)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS2.SSS1.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS2.SSS1.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">3</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 100.32 4.89)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS2.SSS1.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS2.SSS1.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">2</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 204.1 91.1)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS2.SSS1.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS2.SSS1.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">3</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 152.21 4.89)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.SS2.SSS1.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS2.SSS1.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">2</annotation></semantics></math></foreignObject></g><g transform="matrix(0.0 1.0 -1.0 0.0 -65.51 67.57)" fill="#000000" stroke="#000000"><foreignObject width="37.28" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS2.SSS1.Px1.1.pic1.16.16.16.16.16.1.1" class="ltx_text">Count</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -41.63 214.18)" fill="#000000" stroke="#000000"><foreignObject width="291.58" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS2.SSS1.Px1.1.pic1.17.17.17.17.17.1.1" class="ltx_text ltx_font_bold">Methods used
in Table Structure Segmentation </span></foreignObject></g></g></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>OD denotes Object Detection, SIS is Semantic Image Segmentation, RNN represents Recurrent Neural Networks, DDC is an abbreviation for Deformable and Dilated Convolutions whereas GNN is Graphical Neural Networks. This graph explains that what kind of deep learning algorithms are periodically exploited to perform table structure segmentation.</figcaption>
</figure>
<div id="S3.SS2.SSS1.Px1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p3.1" class="ltx_p">Zou and Ma <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> proposed another research in which fully convolutional networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> are utilized to develop image-based table structure recognition method. Similar to the idea of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>, the presented work segments the rows, columns, and cells in a table. Connected Component Analysis is used to improve the predicted boundaries of all of the table components <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>. Later, row and column numbers are assigned for each cell based on the position of row and column separators. Moreover, custom heuristics are applied to optimize cellular boundaries.</p>
</div>
</section>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.5.1.1" class="ltx_text">III-B</span>2 </span>Graph Neural Networks</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">So far in most of the mentioned approaches, the problem of segmenting tables in document images is treated with segmentation techniques. In 2019, Qasim <span id="S3.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> exploited the graph neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> to perform table recognition for the first time. The model is constructed with a blend of deep convolutional neural networks to extract image features and graph neural networks to control the relationship among the vertices. They have open-sourced the proposed work to reproduce or improve the claimed results. <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>github.com/shahrukhqasim/TIES-2.0</span></span></span></p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">Another technique powered by graph neural networks to recognize the tabular structure is proposed in the same year by <span id="S3.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_italic">Chi et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>. However, this technique is based on PDF documents instead of images. One contribution from their side worth mentioning is the publication of their large-scale table structure recognition dataset <span id="S3.SS2.SSS2.p2.1.2" class="ltx_text ltx_font_italic">SciTSR</span> which will be discussed in Section <a href="#S4" title="IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<section id="S3.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Distance Based Weights</h5>

<div id="S3.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px1.p1.1" class="ltx_p">Another work to segment tabular structures presented in ICDAR 2019 is about the reconstruction of syntactic structures from the table as known as <span id="S3.SS2.SSS2.Px1.p1.1.1" class="ltx_text ltx_font_italic">ReS<sup id="S3.SS2.SSS2.Px1.p1.1.1.1" class="ltx_sup"><span id="S3.SS2.SSS2.Px1.p1.1.1.1.1" class="ltx_text ltx_font_upright">2</span></sup>TIM</span> published by Xue <span id="S3.SS2.SSS2.Px1.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>. The primary goal of this model is to regress the coordinates for each cell. The novel approach first creates a network that detects neighbors of each cell in a table. Distance-based weight is presented in the paper which will help the network to solve the class imbalance hurdle during training. Experiments were carried out on Chinese medical documents dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> and ICDAR 2013 table competition dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>A summary of advantages and limitations of various deep learning-based methods that have worked on the task of table structure recognition. The bold horizontal line separates the approaches with different architectures.</figcaption>
<table id="S3.T3.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T3.1.1.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.1.1.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.1.1.1.1.p1.1" class="ltx_p"><span id="S3.T3.1.1.1.1.p1.1.1" class="ltx_text ltx_font_bold">Literature</span></span>
</span></span></th>
<th id="S3.T3.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.2.1.1" class="ltx_p"><span id="S3.T3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Method</span></span>
</span>
</th>
<th id="S3.T3.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.3.1.1" class="ltx_p"><span id="S3.T3.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Highlights</span></span>
</span>
</th>
<th id="S3.T3.1.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S3.T3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.4.1.1" class="ltx_p"><span id="S3.T3.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Limitations</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.2.2" class="ltx_tr">
<td id="S3.T3.2.2.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T3.2.2.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.2.2.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.2.2.1.1.p1.1" class="ltx_p"><span id="S3.T3.2.2.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Siddiqui et al.<span id="S3.T3.2.2.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.2.2.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.2.1.1" class="ltx_p"><span id="S3.T3.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;">Deformable Convolution with Faster R-CNN (Section <a href="#S3.SS2.SSS4" title="III-B4 Deformable and Dilated Convolutions ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>4</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.2.2.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.3.1.1" class="ltx_p"><span id="S3.T3.2.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T3.2.2.3.1.1.1.1" class="ltx_text ltx_font_medium"> Published another dataset having structural information of tables. </span>b)<span id="S3.T3.2.2.3.1.1.1.2" class="ltx_text ltx_font_medium"> Deformable convolution allows tackling varied tabular structures.</span></span></span>
</span>
</td>
<td id="S3.T3.2.2.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T3.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.4.1.1" class="ltx_p"><span id="S3.T3.2.2.4.1.1.1" class="ltx_text" style="font-size:80%;">The published work will not work well in case of row/column span in the tables.</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.3.3" class="ltx_tr">
<td id="S3.T3.3.3.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T3.3.3.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.3.3.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.3.3.1.1.p1.1" class="ltx_p"><span id="S3.T3.3.3.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">CascadeTabNet<span id="S3.T3.3.3.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.3.3.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.2.1.1" class="ltx_p"><span id="S3.T3.3.3.2.1.1.1" class="ltx_text" style="font-size:80%;">Cascade Mask R-CNN with HRNet as a backbone network (Section <a href="#S3.SS2.SSS5" title="III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>5</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.3.3.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.3.1.1" class="ltx_p"><span id="S3.T3.3.3.3.1.1.1" class="ltx_text" style="font-size:80%;">An end-to-end approach to directly regress cellular boundaries.</span></span>
</span>
</td>
<td id="S3.T3.3.3.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T3.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.4.1.1" class="ltx_p"><span id="S3.T3.3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">An extra post-processing is required to filter tables (with and without) ruling lines.</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.4" class="ltx_tr">
<td id="S3.T3.4.4.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T3.4.4.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.4.4.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.4.4.1.1.p1.1" class="ltx_p"><span id="S3.T3.4.4.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">GTE<span id="S3.T3.4.4.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.4.4.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.2.1.1" class="ltx_p"><span id="S3.T3.4.4.2.1.1.1" class="ltx_text" style="font-size:80%;">Generic object detection approach (Section <a href="#S3.SS2.SSS5" title="III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>5</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.4.4.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.3.1.1" class="ltx_p"><span id="S3.T3.4.4.3.1.1.1" class="ltx_text" style="font-size:80%;">An hierarchical network with an additional novel cluster-based method to recognize tabular structures.</span></span>
</span>
</td>
<td id="S3.T3.4.4.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T3.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.4.1.1" class="ltx_p"><span id="S3.T3.4.4.4.1.1.1" class="ltx_text" style="font-size:80%;">Final cell structure recognition is conditioned on the precise classification of a table (Graphical ruling lines present or not present).</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.5.5" class="ltx_tr">
<td id="S3.T3.5.5.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T3.5.5.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.5.5.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.5.5.1.1.p1.1" class="ltx_p"><span id="S3.T3.5.5.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Hashmi et al.<span id="S3.T3.5.5.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.5.5.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.5.5.2.1.1" class="ltx_p"><span id="S3.T3.5.5.2.1.1.1" class="ltx_text" style="font-size:80%;">Mask R-CNN with an Anchor optimization method (Section <a href="#S3.SS2.SSS5" title="III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>5</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.5.5.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.5.5.3.1.1" class="ltx_p"><span id="S3.T3.5.5.3.1.1.1" class="ltx_text" style="font-size:80%;">Optimized anchors help region proposal networks to converge faster and better.</span></span>
</span>
</td>
<td id="S3.T3.5.5.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T3.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.5.5.4.1.1" class="ltx_p"><span id="S3.T3.5.5.4.1.1.1" class="ltx_text" style="font-size:80%;">This work depends on the initial pre-processing step of clustering the ground-truth to retrieve suitable anchors.</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.6.6" class="ltx_tr">
<td id="S3.T3.6.6.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T3.6.6.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.6.6.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.6.6.1.1.p1.1" class="ltx_p"><span id="S3.T3.6.6.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Raja et al.<span id="S3.T3.6.6.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.6.6.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.6.6.2.1.1" class="ltx_p"><span id="S3.T3.6.6.2.1.1.1" class="ltx_text" style="font-size:80%;">Mask R-CNN with ResNet-101 as a backbone network (Section <a href="#S3.SS2.SSS5" title="III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>5</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.6.6.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.6.6.3.1.1" class="ltx_p"><span id="S3.T3.6.6.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T3.6.6.3.1.1.1.1" class="ltx_text ltx_font_medium"> A trainable combination of top-down (cell detection) and bottom-up (structure recognition) is presented. </span>b)<span id="S3.T3.6.6.3.1.1.1.2" class="ltx_text ltx_font_medium"> An additional alignment loss is proposed to detect cells accurately.</span></span></span>
</span>
</td>
<td id="S3.T3.6.6.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T3.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.6.6.4.1.1" class="ltx_p"><span id="S3.T3.6.6.4.1.1.1" class="ltx_text" style="font-size:80%;">The approach is vulnerable in the case of empty cells.</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.7.7" class="ltx_tr">
<td id="S3.T3.7.7.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T3.7.7.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.7.7.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.7.7.1.1.p1.1" class="ltx_p"><span id="S3.T3.7.7.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Siddiqui et al.<span id="S3.T3.7.7.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.7.7.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.7.7.2.1.1" class="ltx_p"><span id="S3.T3.7.7.2.1.1.1" class="ltx_text" style="font-size:80%;">Fully Convolutional Networks (Section <a href="#S3.SS2.SSS1.Px1" title="Fully Convolutional Networks ‣ III-B1 Semantic Image Segmentation ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.7.7.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.7.7.3.1.1" class="ltx_p"><span id="S3.T3.7.7.3.1.1.1" class="ltx_text" style="font-size:80%;">The proposed Prediction tiling technique minimizes the complexity of the problem of table structure recognition.</span></span>
</span>
</td>
<td id="S3.T3.7.7.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T3.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.7.7.4.1.1" class="ltx_p"><span id="S3.T3.7.7.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T3.7.7.4.1.1.1.1" class="ltx_text ltx_font_medium"> The method relies on the consistency assumption of tabular structures, </span>b)<span id="S3.T3.7.7.4.1.1.1.2" class="ltx_text ltx_font_medium"> In case of overly-segmented rows/columns, extra post-processing steps are required.</span></span></span>
</span>
</td>
</tr>
<tr id="S3.T3.8.8" class="ltx_tr">
<td id="S3.T3.8.8.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T3.8.8.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.8.8.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.8.8.1.1.p1.1" class="ltx_p"><span id="S3.T3.8.8.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Tensmeyer et al.<span id="S3.T3.8.8.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.8.8.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.8.8.2.1.1" class="ltx_p"><span id="S3.T3.8.8.2.1.1.1" class="ltx_text" style="font-size:80%;">Dilated Convolutions in Fully Convolutional Networks (Section <a href="#S3.SS2.SSS4.Px2" title="Dilated Convolutions ‣ III-B4 Deformable and Dilated Convolutions ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>4</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.8.8.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.8.8.3.1.1" class="ltx_p"><span id="S3.T3.8.8.3.1.1.1" class="ltx_text" style="font-size:80%;">The system works well both on PDF and scanned document images.</span></span>
</span>
</td>
<td id="S3.T3.8.8.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T3.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.8.8.4.1.1" class="ltx_p"><span id="S3.T3.8.8.4.1.1.1" class="ltx_text" style="font-size:80%;">The merging part of the approach is depends on the post-processing heuristics.</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.9.9" class="ltx_tr">
<td id="S3.T3.9.9.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T3.9.9.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.9.9.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.9.9.1.1.p1.1" class="ltx_p"><span id="S3.T3.9.9.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Zou et al.<span id="S3.T3.9.9.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.9.9.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.9.9.2.1.1" class="ltx_p"><span id="S3.T3.9.9.2.1.1.1" class="ltx_text" style="font-size:80%;">Fully Convolutional Networks (Section <a href="#S3.SS2.SSS1.Px1" title="Fully Convolutional Networks ‣ III-B1 Semantic Image Segmentation ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.9.9.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.9.9.3.1.1" class="ltx_p"><span id="S3.T3.9.9.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T3.9.9.3.1.1.1.1" class="ltx_text ltx_font_medium"> Along with segmenting rows and columns, cells are segmented in a table. </span>b)<span id="S3.T3.9.9.3.1.1.1.2" class="ltx_text ltx_font_medium"> Applying Connected component analysis further improves the results.</span></span></span>
</span>
</td>
<td id="S3.T3.9.9.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T3.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.9.9.4.1.1" class="ltx_p"><span id="S3.T3.9.9.4.1.1.1" class="ltx_text" style="font-size:80%;">Handful of post-processing steps involving custom heuristics are required to produce comparative results.</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.10.10" class="ltx_tr">
<td id="S3.T3.10.10.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T3.10.10.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.10.10.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.10.10.1.1.p1.1" class="ltx_p"><span id="S3.T3.10.10.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Qasim et al.<span id="S3.T3.10.10.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.10.10.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.10.10.2.1.1" class="ltx_p"><span id="S3.T3.10.10.2.1.1.1" class="ltx_text" style="font-size:80%;">Graph Neural Networks with Convolutional Neural Networks (Section <a href="#S3.SS2.SSS2" title="III-B2 Graph Neural Networks ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>2</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.10.10.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.10.10.3.1.1" class="ltx_p"><span id="S3.T3.10.10.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T3.10.10.3.1.1.1.1" class="ltx_text ltx_font_medium"> The proposed method exploits both the spatial and textual features, </span>b)<span id="S3.T3.10.10.3.1.1.1.2" class="ltx_text ltx_font_medium"> A novel Monte Carlo based memory efficient training method is also presented in this work.</span></span></span>
</span>
</td>
<td id="S3.T3.10.10.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T3.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.10.10.4.1.1" class="ltx_p"><span id="S3.T3.10.10.4.1.1.1" class="ltx_text" style="font-size:80%;">The system is not evaluated on the publicly available table datasets.</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.11.11" class="ltx_tr">
<td id="S3.T3.11.11.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T3.11.11.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.11.11.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.11.11.1.1.p1.1" class="ltx_p"><span id="S3.T3.11.11.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Xue et al.<span id="S3.T3.11.11.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.11.11.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.11.11.2.1.1" class="ltx_p"><span id="S3.T3.11.11.2.1.1.1" class="ltx_text" style="font-size:80%;">Graph Neural Networks with distance based weights (Section <a href="#S3.SS2.SSS2.Px1" title="Distance Based Weights ‣ III-B2 Graph Neural Networks ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>2</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.11.11.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T3.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.11.11.3.1.1" class="ltx_p"><span id="S3.T3.11.11.3.1.1.1" class="ltx_text" style="font-size:80%;">The distance-based weight technique resolves the class imbalance problem for the cell relationship network.</span></span>
</span>
</td>
<td id="S3.T3.11.11.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T3.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.11.11.4.1.1" class="ltx_p"><span id="S3.T3.11.11.4.1.1.1" class="ltx_text" style="font-size:80%;">The method is vulnerable in the case of sparse tables.</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.12.12" class="ltx_tr">
<td id="S3.T3.12.12.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T3.12.12.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T3.12.12.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T3.12.12.1.1.p1.1" class="ltx_p"><span id="S3.T3.12.12.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Khan et al.<span id="S3.T3.12.12.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T3.12.12.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T3.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.2.1.1" class="ltx_p"><span id="S3.T3.12.12.2.1.1.1" class="ltx_text" style="font-size:80%;">Recurrent Neural Networks (Section <a href="#S3.SS2.SSS3" title="III-B3 Recurrent Neural Networks ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>3</span></a>).</span></span>
</span>
</td>
<td id="S3.T3.12.12.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T3.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.3.1.1" class="ltx_p"><span id="S3.T3.12.12.3.1.1.1" class="ltx_text" style="font-size:80%;">The Bi-directional GRU overcomes the problem of the smaller receptive field of CNNs.</span></span>
</span>
</td>
<td id="S3.T3.12.12.4" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S3.T3.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.4.1.1" class="ltx_p"><span id="S3.T3.12.12.4.1.1.1" class="ltx_text" style="font-size:80%;">A series of pre-processing steps such as binarization, noise removal, and morphological transformation are required.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.5.1.1" class="ltx_text">III-B</span>3 </span>Recurrent Neural Networks</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">So far, we have seen that convolutional neural networks and graph neural networks are employed to perform table structure extraction. Recent research proposed by Khan <span id="S3.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> has experimented with bi-directional recurrent neural networks along with Gated Recurrent Units (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> to extract the structure of the table. The authors argue that the receptive field of the convolutional neural network is not capable enough to capture complete information of row and column in one stride. According to the writers, a pair of bi-directional GRU performs better. One GRU caters to the row identification whereas another detects the column boundary. The author tried two classic recurrent neural network models, Long Short Term Memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> and GRU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>, and found that GRU has more benefits in experimental results. In the end, the authors experimented with the datasets of the table structure recognition sub-task of the UNLV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and ICDAR 2013 table competitions, both surpassing the previous best results. The authors tried to convince that GRU-based sequential models can also be exploited to improve not only the problem of structure recognition but also for the information extraction in the tables.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">Besides the huge dataset, the author of <span id="S3.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_italic">TableBank</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> has published the baseline model for the table structure recognition. Image-to-markup model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> is trained on <span id="S3.SS2.SSS3.p2.1.2" class="ltx_text ltx_font_italic">TableBank</span> dataset. To implement the model, OpenNMT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> is applied which is an open source tool kit for neural machine translation.</p>
</div>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS4.5.1.1" class="ltx_text">III-B</span>4 </span>Deformable and Dilated Convolutions</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.1" class="ltx_p">Along with traditional convolutions, deformable and dilated convolutions have been exploited to recognize tabular structures in document images.</p>
</div>
<section id="S3.SS2.SSS4.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deformable Convolutions</h5>

<div id="S3.SS2.SSS4.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS4.Px1.p1.1" class="ltx_p">Siddiqui <span id="S3.SS2.SSS4.Px1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> advertised another public image-based table recognition dataset known as <span id="S3.SS2.SSS4.Px1.p1.1.2" class="ltx_text ltx_font_italic">TabStructDB</span>. This dataset was curated by using the images from a well known ICDAR 2017 page object detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> which are annotated with structural information. <span id="S3.SS2.SSS4.Px1.p1.1.3" class="ltx_text ltx_font_italic">TabStructDB</span> has been extensively evaluated on the proposed model called <span id="S3.SS2.SSS4.Px1.p1.1.4" class="ltx_text ltx_font_italic">DeepTabStR</span> which can be seen as a follow-up work for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. The author stated that there exists a huge diversity in the tabular layouts and traditional convolutions which operates as a sliding window is not the best choice. Deformable convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> allows the network to adjust the receptive field by considering the current position of an object. Hence, the author leverages the deformable convolution to perform the task of structural recognition of tables. The exercise of table segmentation is operated as an object detection problem in this research. Deformable Faster R-CNN is used in <span id="S3.SS2.SSS4.Px1.p1.1.5" class="ltx_text ltx_font_italic">DeepTabStR</span>, where the traditional ROI-pooling layer is replaced with a deformable ROI-pooling layer. Another important point is highlighted in this research that there still exists room for improvement in the area of structural analysis of tables having inconsistent layouts.</p>
</div>
</section>
<section id="S3.SS2.SSS4.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Dilated Convolutions</h5>

<div id="S3.SS2.SSS4.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS4.Px2.p1.1" class="ltx_p">Another technique employing dilated convolutions <span id="S3.SS2.SSS4.Px2.p1.1.1" class="ltx_text ltx_font_italic">SPLERGE</span> (Split and Merge models) is proposed by Tensmeyer <span id="S3.SS2.SSS4.Px2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>. Their approach consists of two separate deep learning models in which the first model defines the grid-like structure of the table whereas the second model finds out whether cells can be further spanned into multiple rows or columns. The author claims to achieve state-of-the-art performance on the ICDAR 2013 table competition dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS5.5.1.1" class="ltx_text">III-B</span>5 </span>Object Detection Algorithms</h4>

<div id="S3.SS2.SSS5.p1" class="ltx_para">
<p id="S3.SS2.SSS5.p1.1" class="ltx_p">Inspiring from the exceptional results of object detection algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, researchers in the table community have formulated the task of table structure recognition as an object detection problem.</p>
</div>
<div id="S3.SS2.SSS5.p2" class="ltx_para">
<p id="S3.SS2.SSS5.p2.1" class="ltx_p">Hashmi <span id="S3.SS2.SSS5.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> proposed a guided table structure recognition method to detect rows and columns in tables. This paper presents that the localization of rows and columns can be improved by incorporating an anchor optimization method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>. In their proposed work, Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> is employed with optimized anchors to detect the boundaries of rows and columns. The presented work has reported state-of-the-art results on TabStructDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and table structure recognition dataset of ICDAR-2013 (released by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>).</p>
</div>
<div id="S3.SS2.SSS5.p3" class="ltx_para">
<p id="S3.SS2.SSS5.p3.1" class="ltx_p">Until now, we have discussed approaches that detect tabular rows and columns to retrieve the final structure of a table. Contrary to the previous approaches, Raja et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> introduced a table structure recognition method that directly regresses the cellular boundaries. The authors employed Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> with a ResNet-101 backbone pre-trained on MS-COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. In their object detection framework, dilated convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> are implemented in the region proposal network. Furthermore, the authors introduced alignment loss that also contributes to the overall loss function. Later, graph convolutional networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> are applied to obtain the row and column relationship between the predicted cells. The whole process is trained in an end-to-end fashion. The paper presents extensive evaluations on several publicly available datasets for the task of table structure recognition.</p>
</div>
<div id="S3.SS2.SSS5.p4" class="ltx_para">
<p id="S3.SS2.SSS5.p4.1" class="ltx_p">Another approach that directly localize the cellular boundaries in tables is presented in <span id="S3.SS2.SSS5.p4.1.1" class="ltx_text ltx_font_italic">CascadeTabNet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. In this approach, tabular images are given to the Cascade Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> that predicts the cellular mask along with the classification of the table as bordered or borderless. Subsequently, individual post-processing is applied to bordered and borderless tables to retrieve the final cellular boundaries.</p>
</div>
<div id="S3.SS2.SSS5.p5" class="ltx_para">
<p id="S3.SS2.SSS5.p5.1" class="ltx_p">The system GTE proposed by Zheng <span id="S3.SS2.SSS5.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> is an end-to-end framework that not only detects the tables but recognizes the structures of tables in document images. Analogous to the approach of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, the authors have suggested two different cell detection networks i.e: 1) For graphical ruling lines present in a table. 2) No graphical ruling lines in a table. Instead of a tabular image, a complete document image with a table mask is propagated to the classification network. Based on the predicted class, the image is passed to the appropriate cell network to retrieve the final cell boundaries.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>A summary of advantages and limitations of deep learning-based methods that have solely worked on the task of table recognition on scanned document images.</figcaption>
<table id="S3.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T4.1.1.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T4.1.1.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T4.1.1.1.1.p1.1" class="ltx_p"><span id="S3.T4.1.1.1.1.p1.1.1" class="ltx_text ltx_font_bold">Literature</span></span>
</span></span></th>
<th id="S3.T4.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.2.1.1" class="ltx_p"><span id="S3.T4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Method</span></span>
</span>
</th>
<th id="S3.T4.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.3.1.1" class="ltx_p"><span id="S3.T4.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Highlights</span></span>
</span>
</th>
<th id="S3.T4.1.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S3.T4.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.4.1.1" class="ltx_p"><span id="S3.T4.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Limitations</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.2.2" class="ltx_tr">
<td id="S3.T4.2.2.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span id="S3.T4.2.2.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T4.2.2.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T4.2.2.1.1.p1.1" class="ltx_p"><span id="S3.T4.2.2.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Zhong et al.<span id="S3.T4.2.2.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T4.2.2.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T4.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.2.2.2.1.1" class="ltx_p"><span id="S3.T4.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;">Attention based encoder dual decoder (Section <a href="#S3.SS3.SSS1" title="III-C1 Encoder-Dual-Decoder ‣ III-C Table Recognition ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span>1</span></a>).</span></span>
</span>
</td>
<td id="S3.T4.2.2.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T4.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.2.2.3.1.1" class="ltx_p"><span id="S3.T4.2.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T4.2.2.3.1.1.1.1" class="ltx_text ltx_font_medium"> Published a large-scale table dataset, </span>b)<span id="S3.T4.2.2.3.1.1.1.2" class="ltx_text ltx_font_medium"> The approach presents a novel evaluation metrics <span id="S3.T4.2.2.3.1.1.1.2.1" class="ltx_text ltx_font_italic">TEDS</span> to evaluate table recognition methods.</span></span></span>
</span>
</td>
<td id="S3.T4.2.2.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T4.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.2.2.4.1.1" class="ltx_p"><span id="S3.T4.2.2.4.1.1.1" class="ltx_text" style="font-size:80%;">The approach is not directly comparable with other state-of-the-art methods.</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.3.3" class="ltx_tr">
<td id="S3.T4.3.3.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T4.3.3.1.1" class="ltx_inline-logical-block ltx_align_top">
<span id="S3.T4.3.3.1.1.p1" class="ltx_para ltx_noindent">
<span id="S3.T4.3.3.1.1.p1.1" class="ltx_p"><span id="S3.T4.3.3.1.1.p1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Deng et al.<span id="S3.T4.3.3.1.1.p1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite></span></span></span>
</span></span></td>
<td id="S3.T4.3.3.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T4.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.2.1.1" class="ltx_p"><span id="S3.T4.3.3.2.1.1.1" class="ltx_text" style="font-size:80%;">Encoder decoder network presented as the baseline model (Section <a href="#S3.SS3.SSS2" title="III-C2 Encoder Decoder Network ‣ III-C Table Recognition ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span>2</span></a>).</span></span>
</span>
</td>
<td id="S3.T4.3.3.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T4.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.3.1.1" class="ltx_p"><span id="S3.T4.3.3.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">a)<span id="S3.T4.3.3.3.1.1.1.1" class="ltx_text ltx_font_medium"> Contributed with another large-scale dataset in the field of table understanding, </span>b)<span id="S3.T4.3.3.3.1.1.1.2" class="ltx_text ltx_font_medium"> Challenges in end-to-end table recognition are discussed in the presented work.</span></span></span>
</span>
</td>
<td id="S3.T4.3.3.4" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S3.T4.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.4.1.1" class="ltx_p"><span id="S3.T4.3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">The proposed baseline method is not evaluated on the other publicly available table recognition datasets.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Table Recognition</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">As explained in Section <a href="#S3" title="III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, the task of table recognition covers the job of table structure extraction along with extracting the text from the table cells. Relatively, less progress has been accomplished in this specific domain.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In this section, we will cover the recent experiments that have attempted the problem of table recognition. Table <a href="#S3.T4" title="TABLE IV ‣ III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> summarizes these approaches by highlighting their advantages and limitations.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS1.5.1.1" class="ltx_text">III-C</span>1 </span>Encoder-Dual-Decoder</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Recently, research on image-based table recognition proposed by Zhong <span id="S3.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> is published. In this research, the authors proposed a new dataset known as <span id="S3.SS3.SSS1.p1.1.2" class="ltx_text ltx_font_italic">PubTabNet</span> which is explained in Section <a href="#S4.SS13" title="IV-M PubTabNet ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-M</span></span></a>. The authors have attempted to resolve the problem of inferring both the structure recognition of tables and the information present in their respective cells. The writers of the paper have treated the task of structure recognition and table recognition separately. They proposed the attention-based Encoder-Dual-Decoder (EDD) architecture. The encoder extracts the essential spatial features, then the first decoder segments the table into rows and columns whereas another decoder attempts to identify the content of a table cell. In this research, a new Tree-Edit-Distance-based Similarity (TEDS) metrics is presented to evaluate the quality of cell content identification.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS2.5.1.1" class="ltx_text">III-C</span>2 </span>Encoder Decoder Network</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Another dataset <span id="S3.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_italic">TABLE2LATEX-450K<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_upright">3</span></span><span id="footnote3.5" class="ltx_text ltx_font_upright">https://github.com/bloomberg/TABLE2LATEX.</span></span></span></span></span> has been published recently in the ICDAR conference comprises of arXiv articles. Along with the dataset, Deng <span id="S3.SS3.SSS2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> discussed the current challenges in the end-to-end table recognition and highlights the worth of a bigger dataset in this field. The creators of this dataset have also conferred the baseline models ( <span id="S3.SS3.SSS2.p1.1.3" class="ltx_text ltx_font_italic">IM2TEX</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> on the mentioned dataset by using an encoder-decoder architecture with an attention mechanism. IM2TEX model is implemented on OpenNMT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>. With the probable increase in hardware capabilities of the GPUs in the future, the authors claim that this dataset will be proved as a promising contribution.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">It is important to mention that apart from these two approaches, other methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> have extracted the contents of cells in order to recognize either the tabular boundaries or tabular structures.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Datasets</span>
</h2>

<figure id="S4.F8" class="ltx_figure"><img src="/html/2104.14272/assets/x3.png" id="S4.F8.g1" class="ltx_graphics ltx_img_landscape" width="461" height="318" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Sample document images taken from the datasets of ICDAR-2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, ICDAR-2017-POD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, UNLV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and UW3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>. The red boundaries represent the tabular region. The diversity between samples in a dataset is quite evident. </figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The performance of deep neural networks has a direct relation with the size of the dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. In this section, we will discuss all of the well-known datasets that are publicly available to deal with the problem of table detection and table structural recognition in document images.
Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> contains a comprehensive explanation of all the mentioned datasets which are employed to perform and compare detection, structural segmentation and recognition of tables in document images. Figure <a href="#S4.F8" title="Figure 8 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> demonstrates samples from some of the distinguished datasets in table community.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">ICDAR-2013</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">International Conference on Document Analysis and Recognition (ICDAR) 2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> is the most renowned dataset among the researchers in the table community. This dataset is published for the table competition organized by the ICDAR conference in 2013. This dataset has the annotations for both table detection and table recognition. The dataset consists of PDF files which are often converted into images to be utilized in the various approaches. The dataset contains structured tables, graphs, charts, and text as information. There are a total of 238 images in the dataset, out of which 128 incorporates tables. This dataset has been extensively used to compare state-of-the-art approaches. As mentioned in the Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, this dataset has annotations for all of the three tasks of table understanding which are discussed in the paper. A couple of samples from this dataset are illustrated in Figure <a href="#S4.F8" title="Figure 8 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (a).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">ICDAR-2017-POD</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">This dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> is also proposed for the competition of Page Object Detection (POD) in ICDAR 2017. This dataset is widely used to evaluate approaches for table detection. This dataset is fairly bigger than the ICDAR 2013 table dataset. It comprises of total 2417 images including tables, formulas, and figures. In many instances, this dataset is divided into 1600 images (731 tabular regions) which are used for training while the rest of 817 images (350 tabular regions) are employed for the testing purpose. A pair of instances of this dataset are demonstrated in Figure <a href="#S4.F8" title="Figure 8 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (b). This dataset has only information for the tabular boundaries as explained in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">UNLV</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The UNLV dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is a recognized dataset in the field of document image analysis. This dataset composed of scanned document images from various sources like financial reports, magazines, and research papers having diverse tabular layouts. Although the dataset contains approximately 10,000 images, only 427 images contain tabular regions. Frequently, these 427 images have been used to conduct various experiments in the research community. This dataset has been used for all the three tasks of table analysis which are discussed in the paper. Figure <a href="#S4.F8" title="Figure 8 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (c) illustrates a couple of samples from this dataset.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">UW3</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">UW3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> is another popular dataset for researchers working in the area of document image analysis. This dataset contains scanned documents from books and magazines. There are approximately 1600 scanned document images out of which only 165 images have table regions. Annotated table coordinates are present in the XML format. Two samples from this dataset are demonstrated in Figure <a href="#S4.F8" title="Figure 8 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (d). Although this dataset has limited number of tabular regions, it has annotations for all the three problems of table understanding that are discussed in the paper.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">ICDAR-2019</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Recently, Competition on Table Detection and Recognition (cTDaR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> is carried out in ICDAR 2019. In the competition, two new datasets are proposed: modern and historical datasets. The modern dataset contains samples from scientific papers, forms, and financial documents. Whereas the archival dataset includes images from hand-written accounting ledgers, schedules of train, simple tabular prints from old books, and many more. The prescribed train-test split for detecting tables in the modern dataset is 600 images for training while 240 images for the test. Similarly, for the historical dataset 600 images for the training and 199 images for the testing part are the recommended data distribution. As summarized in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, the dataset has information for tabular boundaries and annotations for the cell area as well. This novel dataset is challenging in nature because it contains both modern and historical (archived) document images. This dataset will be used to evaluate the robustness of table analysis methods. In order to understand the diversity, a couple of samples from both the historical and modern datasets are depicted in Figure <a href="#S4.F9" title="Figure 9 ‣ IV-E ICDAR-2019 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2104.14272/assets/icdar_19.jpg" id="S4.F9.g1" class="ltx_graphics ltx_img_portrait" width="598" height="795" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Examples of archival and modern document images taken from the ICDAR-2019 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> which is explained in Section <a href="#S4.SS5" title="IV-E ICDAR-2019 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span></span></a>. The red boundaries represent the tabular region.</figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.5.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.6.2" class="ltx_text ltx_font_italic">Marmot</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">Not long ago, Marmot <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>http://www.icst.pku.edu.cn/cpdp/sjzy/index.htm</span></span></span> is one of the largest publicly available datasets and extensively used by the researchers in the area of table understanding. This dataset has been proposed by the Institute of Computer Science and Technology (Peking University) and later explained by Fang <span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. There are 2000 images in the dataset composed of English and Chinese conference papers from 1970 to 2011. The dataset is highly useful for training the networks due to having diverse and very complex page layouts. There is a roughly 1:1 ratio between positive to negative images in the dataset. Some occasions of incorrect ground-truth annotations have been reported in the past which are later cleaned by Schreiber <span id="S4.SS6.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. As mentioned in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, this dataset has annotations for the tabular boundaries and it is widely exploited to train deep neural networks for table detection.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS7.5.1.1" class="ltx_text">IV-G</span> </span><span id="S4.SS7.6.2" class="ltx_text ltx_font_italic">TableBank</span>
</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">In early 2019, Minghao <span id="S4.SS7.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> realized the need for large datasets in the table community and published TableBank, a dataset comprising of 417 thousand labeled images having tabular information. This dataset has been collected by crawling over documents available online in <span id="S4.SS7.p1.1.2" class="ltx_text ltx_font_italic">.docx</span> format. Another source of data for this dataset is LaTeX documents which were collected from the database of arXiv <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://arxiv.org</span></span></span>. The publishers of this dataset argue that this contribution will facilitate the researchers to leverage the power of deep learning and fine-tuning methods. The authors claim that this dataset can be used for both table detection and structural recognition tasks. However, we are unable to find annotations for structural recognition in the dataset <span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://github.com/doc-analysis/TableBank</span></span></span>. Important information for the dataset is summarized in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
</section>
<section id="S4.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS8.5.1.1" class="ltx_text">IV-H</span> </span><span id="S4.SS8.6.2" class="ltx_text ltx_font_italic">TabStructDB</span>
</h3>

<div id="S4.SS8.p1" class="ltx_para">
<p id="S4.SS8.p1.1" class="ltx_p">In the ICDAR conference 2019, along with the table competition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, other researchers have also published new datasets in the field of table analysis. One of the dataset known as TabStructDB<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://bit.ly/2XonOEx</span></span></span> is published by Siddiqui <span id="S4.SS8.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. Since the ICDAR-2017-POD dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> has only information for the tabular boundaries, the author leverages this dataset and annotated them with structural information comprising of boundaries of respective rows and columns in the table. To maintain consistency, the authors have also kept the same dataset split as mentioned in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>. Significant information regarding the dataset is summarized in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. Since this dataset provides information regarding the boundaries of rows and columns, it facilitates the researchers to treat the task of table structure recognition as object detection or semantic segmentation problem.</p>
</div>
</section>
<section id="S4.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS9.5.1.1" class="ltx_text">IV-I</span> </span><span id="S4.SS9.6.2" class="ltx_text ltx_font_italic">TABLE2LATEX-450K</span>
</h3>

<div id="S4.SS9.p1" class="ltx_para">
<p id="S4.SS9.p1.1" class="ltx_p">Another large dataset that is published in the recent ICDAR conference is TABLE2LATEX-450K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>. The dataset contains 450 thousand annotated tables along with their corresponding images. This huge dataset is constructed by crawling over the arXiv articles from the year 1991 to 2016 and all the LaTeX source documents were downloaded. After the extraction of source code and subsequent refinement, the high-quality labeled dataset is obtained. As mentioned in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, the dataset contains annotations for the structural segmentation of tables and the content of table cells. Along with the dataset, publishers have made all the pre-processing scripts publicly available<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://github.com/bloomberg/TABLE2LATEX.</span></span></span>. This dataset is an important contribution to tackle the problem of table structural segmentation and table recognition in document images because it enables the researchers to train the massive deep learning architectures from scratch which can be further fine-tuned on relatively smaller datasets.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Table Datasets. TD denotes Table Detection, TSR is Table Structure Recognition wheras TR is Table Recognition.</figcaption>
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.3.4.1" class="ltx_tr">
<td id="S4.T5.3.4.1.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.1.1.1.1" class="ltx_p"><span id="S4.T5.3.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</td>
<td id="S4.T5.3.4.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.1.2.1.1" class="ltx_p"><span id="S4.T5.3.4.1.2.1.1.1" class="ltx_text ltx_font_bold">TD</span></span>
</span>
</td>
<td id="S4.T5.3.4.1.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.1.3.1.1" class="ltx_p"><span id="S4.T5.3.4.1.3.1.1.1" class="ltx_text ltx_font_bold">TSR</span></span>
</span>
</td>
<td id="S4.T5.3.4.1.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.4.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.1.4.1.1" class="ltx_p"><span id="S4.T5.3.4.1.4.1.1.1" class="ltx_text ltx_font_bold">TR</span></span>
</span>
</td>
<td id="S4.T5.3.4.1.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.4.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.1.5.1.1" class="ltx_p"><span id="S4.T5.3.4.1.5.1.1.1" class="ltx_text ltx_font_bold"># Samples</span></span>
</span>
</td>
<td id="S4.T5.3.4.1.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.4.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.1.6.1.1" class="ltx_p"><span id="S4.T5.3.4.1.6.1.1.1" class="ltx_text ltx_font_bold">Image Type</span></span>
</span>
</td>
<td id="S4.T5.3.4.1.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.4.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.1.7.1.1" class="ltx_p"><span id="S4.T5.3.4.1.7.1.1.1" class="ltx_text ltx_font_bold">Location</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.5.2" class="ltx_tr">
<td id="S4.T5.3.5.2.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.2.1.1.1" class="ltx_p"><span id="S4.T5.3.5.2.1.1.1.1" class="ltx_text" style="font-size:80%;">ICDAR-2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> (Section <a href="#S4.SS1" title="IV-A ICDAR-2013 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.5.2.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.2.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.5.2.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.2.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.5.2.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.5.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.2.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.5.2.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.5.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.2.5.1.1" class="ltx_p"><span id="S4.T5.3.5.2.5.1.1.1" class="ltx_text" style="font-size:90%;">238</span></span>
</span>
</td>
<td id="S4.T5.3.5.2.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.5.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.2.6.1.1" class="ltx_p"><span id="S4.T5.3.5.2.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.5.2.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.5.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.2.7.1.1" class="ltx_p"><a target="_blank" href="http://www.tamirhassan.com/html/dataset.html" title="" class="ltx_ref ltx_href" style="font-size:80%;">http://www.tamirhassan.com/html/dataset.html</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.6.3" class="ltx_tr">
<td id="S4.T5.3.6.3.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.3.1.1.1" class="ltx_p"><span id="S4.T5.3.6.3.1.1.1.1" class="ltx_text" style="font-size:80%;">ICDAR-2017-POD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> (Section <a href="#S4.SS2" title="IV-B ICDAR-2017-POD ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.6.3.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.3.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.6.3.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.3.3.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.6.3.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.6.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.3.4.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.6.3.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.6.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.3.5.1.1" class="ltx_p"><span id="S4.T5.3.6.3.5.1.1.1" class="ltx_text" style="font-size:90%;">2.4K</span></span>
</span>
</td>
<td id="S4.T5.3.6.3.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.6.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.3.6.1.1" class="ltx_p"><span id="S4.T5.3.6.3.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.6.3.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.6.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.3.7.1.1" class="ltx_p"><a target="_blank" href="http://www.icst.pku.edu.cn/cpdp" title="" class="ltx_ref ltx_href" style="font-size:80%;">http://www.icst.pku.edu.cn/cpdp</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.7.4" class="ltx_tr">
<td id="S4.T5.3.7.4.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.7.4.1.1.1" class="ltx_p"><span id="S4.T5.3.7.4.1.1.1.1" class="ltx_text" style="font-size:80%;">ICDAR-2019<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> (Section <a href="#S4.SS5" title="IV-E ICDAR-2019 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.7.4.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.7.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.7.4.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.7.4.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.7.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.7.4.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.7.4.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.7.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.7.4.4.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.7.4.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.7.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.7.4.5.1.1" class="ltx_p"><span id="S4.T5.3.7.4.5.1.1.1" class="ltx_text" style="font-size:90%;">3.6K</span></span>
</span>
</td>
<td id="S4.T5.3.7.4.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.7.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.7.4.6.1.1" class="ltx_p"><span id="S4.T5.3.7.4.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.7.4.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.7.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.7.4.7.1.1" class="ltx_p"><a target="_blank" href="https://zenodo.org/record/2649217" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://zenodo.org/record/2649217</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.8.5" class="ltx_tr">
<td id="S4.T5.3.8.5.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.8.5.1.1.1" class="ltx_p"><span id="S4.T5.3.8.5.1.1.1.1" class="ltx_text" style="font-size:80%;">UNLV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> (Section <a href="#S4.SS3" title="IV-C UNLV ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.8.5.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.8.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.8.5.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.8.5.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.8.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.8.5.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.8.5.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.8.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.8.5.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.8.5.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.8.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.8.5.5.1.1" class="ltx_p"><span id="S4.T5.3.8.5.5.1.1.1" class="ltx_text" style="font-size:90%;">427</span></span>
</span>
</td>
<td id="S4.T5.3.8.5.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.8.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.8.5.6.1.1" class="ltx_p"><span id="S4.T5.3.8.5.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.8.5.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.8.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.8.5.7.1.1" class="ltx_p"><a target="_blank" href="https://drive.google.com/file/d/1ETq5hhoIgCzzom6yivkokhQ8DoOm6nDs" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://drive.google.com/file/d/</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.9.6" class="ltx_tr">
<td id="S4.T5.3.9.6.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.9.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.9.6.1.1.1" class="ltx_p"><span id="S4.T5.3.9.6.1.1.1.1" class="ltx_text" style="font-size:80%;">Marmot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> (Section <a href="#S4.SS6" title="IV-F Marmot ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-F</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.9.6.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.9.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.9.6.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.9.6.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.9.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.9.6.3.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.9.6.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.9.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.9.6.4.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.9.6.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.9.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.9.6.5.1.1" class="ltx_p"><span id="S4.T5.3.9.6.5.1.1.1" class="ltx_text" style="font-size:90%;">958</span></span>
</span>
</td>
<td id="S4.T5.3.9.6.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.9.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.9.6.6.1.1" class="ltx_p"><span id="S4.T5.3.9.6.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.9.6.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.9.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.9.6.7.1.1" class="ltx_p"><a target="_blank" href="http://www.icst.pku.edu.cn/cpdp/sjzy/" title="" class="ltx_ref ltx_href" style="font-size:80%;">http://www.icst.pku.edu.cn/cpdp/sjzy/</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.10.7" class="ltx_tr">
<td id="S4.T5.3.10.7.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.10.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.10.7.1.1.1" class="ltx_p"><span id="S4.T5.3.10.7.1.1.1.1" class="ltx_text" style="font-size:80%;">UW3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> (Section <a href="#S4.SS4" title="IV-D UW3 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.10.7.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.10.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.10.7.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.10.7.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.10.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.10.7.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.10.7.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.10.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.10.7.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.10.7.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.10.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.10.7.5.1.1" class="ltx_p"><span id="S4.T5.3.10.7.5.1.1.1" class="ltx_text" style="font-size:90%;">165</span></span>
</span>
</td>
<td id="S4.T5.3.10.7.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.10.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.10.7.6.1.1" class="ltx_p"><span id="S4.T5.3.10.7.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.10.7.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.10.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.10.7.7.1.1" class="ltx_p"><a target="_blank" href="http://tc11.cvc.uab.es/datasets/DFKITGT2010_1/" title="" class="ltx_ref ltx_href" style="font-size:80%;">http://tc11.cvc.uab.es/datasets/DFKITGT2010_1/</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.3" class="ltx_tr">
<td id="S4.T5.3.3.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.4.1.1" class="ltx_p"><span id="S4.T5.3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">TableBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> (Section <a href="#S4.SS7" title="IV-G TableBank ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-G</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.3.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.5.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.3.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.6.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.3.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.7.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.3.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.3.3.3" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.3.3.3" class="ltx_p"><math id="S4.T5.1.1.1.1.1.m1.1" class="ltx_Math" alttext="417" display="inline"><semantics id="S4.T5.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T5.1.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.1.m1.1.1.cmml">417</mn><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.T5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1">417</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.1.m1.1c">417</annotation></semantics></math><span id="S4.T5.3.3.3.3.3.2" class="ltx_text" style="font-size:80%;">K<math id="S4.T5.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="(TD)" display="inline"><semantics id="S4.T5.2.2.2.2.2.1.m1.1a"><mrow id="S4.T5.2.2.2.2.2.1.m1.1.1.1" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S4.T5.2.2.2.2.2.1.m1.1.1.1.2" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.cmml">(</mo><mrow id="S4.T5.2.2.2.2.2.1.m1.1.1.1.1" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.cmml"><mi id="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.2" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.1" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.3" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.3.cmml">D</mi></mrow><mo stretchy="false" id="S4.T5.2.2.2.2.2.1.m1.1.1.1.3" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.2.1.m1.1b"><apply id="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.cmml" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1"><times id="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.1.cmml" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.1"></times><ci id="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.2.cmml" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.2">𝑇</ci><ci id="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.3.cmml" xref="S4.T5.2.2.2.2.2.1.m1.1.1.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.2.1.m1.1c">(TD)</annotation></semantics></math>, <math id="S4.T5.3.3.3.3.3.2.m2.1" class="ltx_Math" alttext="145" display="inline"><semantics id="S4.T5.3.3.3.3.3.2.m2.1a"><mn id="S4.T5.3.3.3.3.3.2.m2.1.1" xref="S4.T5.3.3.3.3.3.2.m2.1.1.cmml">145</mn><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.3.3.2.m2.1b"><cn type="integer" id="S4.T5.3.3.3.3.3.2.m2.1.1.cmml" xref="S4.T5.3.3.3.3.3.2.m2.1.1">145</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.3.3.2.m2.1c">145</annotation></semantics></math>K (TSR)</span></span>
</span>
</td>
<td id="S4.T5.3.3.8" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.8.1.1" class="ltx_p"><span id="S4.T5.3.3.8.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.3.9" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.9.1.1" class="ltx_p"><a target="_blank" href="https://github.com/doc-analysis/TableBank/" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/doc-analysis/TableBank</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.11.8" class="ltx_tr">
<td id="S4.T5.3.11.8.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.11.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.11.8.1.1.1" class="ltx_p"><span id="S4.T5.3.11.8.1.1.1.1" class="ltx_text" style="font-size:80%;">TabStructDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> (Section <a href="#S4.SS8" title="IV-H TabStructDB ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-H</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.11.8.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.11.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.11.8.2.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.11.8.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.11.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.11.8.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.11.8.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.11.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.11.8.4.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.11.8.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.11.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.11.8.5.1.1" class="ltx_p"><span id="S4.T5.3.11.8.5.1.1.1" class="ltx_text" style="font-size:90%;">2.4K</span></span>
</span>
</td>
<td id="S4.T5.3.11.8.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.11.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.11.8.6.1.1" class="ltx_p"><span id="S4.T5.3.11.8.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.11.8.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.11.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.11.8.7.1.1" class="ltx_p"><a target="_blank" href="https://bit.ly/2XonOEx" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://bit.ly/2XonOEx</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.12.9" class="ltx_tr">
<td id="S4.T5.3.12.9.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.12.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.12.9.1.1.1" class="ltx_p"><span id="S4.T5.3.12.9.1.1.1.1" class="ltx_text" style="font-size:80%;">TABLE2LATEX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> (Section <a href="#S4.SS9" title="IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-I</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.12.9.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.12.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.12.9.2.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.12.9.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.12.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.12.9.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.12.9.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.12.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.12.9.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.12.9.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.12.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.12.9.5.1.1" class="ltx_p"><span id="S4.T5.3.12.9.5.1.1.1" class="ltx_text" style="font-size:90%;">450K</span></span>
</span>
</td>
<td id="S4.T5.3.12.9.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.12.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.12.9.6.1.1" class="ltx_p"><span id="S4.T5.3.12.9.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.12.9.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.12.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.12.9.7.1.1" class="ltx_p"><a target="_blank" href="https://github.com/bloomberg/TABLE2LATEX/" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/bloomberg/TABLE2LATEX</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.13.10" class="ltx_tr">
<td id="S4.T5.3.13.10.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.13.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.13.10.1.1.1" class="ltx_p"><span id="S4.T5.3.13.10.1.1.1.1" class="ltx_text" style="font-size:80%;">SciTSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> (Section <a href="#S4.SS10" title="IV-J SciTSR ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-J</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.13.10.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.13.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.13.10.2.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.13.10.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.13.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.13.10.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.13.10.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.13.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.13.10.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.13.10.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.13.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.13.10.5.1.1" class="ltx_p"><span id="S4.T5.3.13.10.5.1.1.1" class="ltx_text" style="font-size:90%;">15K</span></span>
</span>
</td>
<td id="S4.T5.3.13.10.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.13.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.13.10.6.1.1" class="ltx_p"><span id="S4.T5.3.13.10.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.13.10.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.13.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.13.10.7.1.1" class="ltx_p"><a target="_blank" href="https://github.com/Academic-Hammer/SciTSR" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/Academic-Hammer/SciTSR</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.14.11" class="ltx_tr">
<td id="S4.T5.3.14.11.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.14.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.14.11.1.1.1" class="ltx_p"><span id="S4.T5.3.14.11.1.1.1.1" class="ltx_text" style="font-size:80%;">DeepFigures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (Section <a href="#S4.SS11" title="IV-K DeepFigures ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-K</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.14.11.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.14.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.14.11.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.14.11.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.14.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.14.11.3.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.14.11.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.14.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.14.11.4.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.14.11.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.14.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.14.11.5.1.1" class="ltx_p"><span id="S4.T5.3.14.11.5.1.1.1" class="ltx_text" style="font-size:90%;">1.4M</span></span>
</span>
</td>
<td id="S4.T5.3.14.11.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.14.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.14.11.6.1.1" class="ltx_p"><span id="S4.T5.3.14.11.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.14.11.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.14.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.14.11.7.1.1" class="ltx_p"><a target="_blank" href="https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/deepfigures/jcdl-deepfigures-labels.tar.gz" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.15.12" class="ltx_tr">
<td id="S4.T5.3.15.12.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.15.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.15.12.1.1.1" class="ltx_p"><span id="S4.T5.3.15.12.1.1.1.1" class="ltx_text" style="font-size:80%;">RVL-CDIP (Subset) (Section <a href="#S4.SS12" title="IV-L RVL-CDIP (Subset) ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-L</span></span></a>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite></span></span>
</span>
</td>
<td id="S4.T5.3.15.12.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.15.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.15.12.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.15.12.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.15.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.15.12.3.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.15.12.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.15.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.15.12.4.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.15.12.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.15.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.15.12.5.1.1" class="ltx_p"><span id="S4.T5.3.15.12.5.1.1.1" class="ltx_text" style="font-size:90%;">518</span></span>
</span>
</td>
<td id="S4.T5.3.15.12.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.15.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.15.12.6.1.1" class="ltx_p"><span id="S4.T5.3.15.12.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.15.12.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.15.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.15.12.7.1.1" class="ltx_p"><a target="_blank" href="https://zenodo.org/record/3257319" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://zenodo.org/record/3257319</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.16.13" class="ltx_tr">
<td id="S4.T5.3.16.13.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.16.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.16.13.1.1.1" class="ltx_p"><span id="S4.T5.3.16.13.1.1.1.1" class="ltx_text" style="font-size:80%;">PubTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> (Section <a href="#S4.SS13" title="IV-M PubTabNet ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-M</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.16.13.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.16.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.16.13.2.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.16.13.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.16.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.16.13.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.16.13.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.16.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.16.13.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.16.13.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.16.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.16.13.5.1.1" class="ltx_p"><span id="S4.T5.3.16.13.5.1.1.1" class="ltx_text" style="font-size:90%;">568K</span></span>
</span>
</td>
<td id="S4.T5.3.16.13.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.16.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.16.13.6.1.1" class="ltx_p"><span id="S4.T5.3.16.13.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.16.13.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.16.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.16.13.7.1.1" class="ltx_p"><a target="_blank" href="https://github.com/ibm-aur-nlp/PubTabNet" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/ibm-aur-nlp/PubTabNet</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.17.14" class="ltx_tr">
<td id="S4.T5.3.17.14.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.17.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.17.14.1.1.1" class="ltx_p"><span id="S4.T5.3.17.14.1.1.1.1" class="ltx_text" style="font-size:80%;">IIT-AR-13k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> (Section <a href="#S4.SS14" title="IV-N IIIT-AR-13K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-N</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.17.14.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.17.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.17.14.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.17.14.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.17.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.17.14.3.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.17.14.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.17.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.17.14.4.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.17.14.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.17.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.17.14.5.1.1" class="ltx_p"><span id="S4.T5.3.17.14.5.1.1.1" class="ltx_text" style="font-size:90%;">13K</span></span>
</span>
</td>
<td id="S4.T5.3.17.14.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T5.3.17.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.17.14.6.1.1" class="ltx_p"><span id="S4.T5.3.17.14.6.1.1.1" class="ltx_text" style="font-size:80%;">Scanned</span></span>
</span>
</td>
<td id="S4.T5.3.17.14.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T5.3.17.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.17.14.7.1.1" class="ltx_p"><a target="_blank" href="http://cvit.iiit.ac.in/usodi/iiitar13k.php" title="" class="ltx_ref ltx_href" style="font-size:80%;">http://cvit.iiit.ac.in/usodi/iiitar13k.php</a></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.18.15" class="ltx_tr">
<td id="S4.T5.3.18.15.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.3.18.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.18.15.1.1.1" class="ltx_p"><span id="S4.T5.3.18.15.1.1.1.1" class="ltx_text" style="font-size:80%;">CamCap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> (Section <a href="#S4.SS15" title="IV-O CamCap ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-O</span></span></a>)</span></span>
</span>
</td>
<td id="S4.T5.3.18.15.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.3.18.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.18.15.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.18.15.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.3.18.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.18.15.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T5.3.18.15.4" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.3.18.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.18.15.4.1.1" class="ltx_p">✗</span>
</span>
</td>
<td id="S4.T5.3.18.15.5" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.3.18.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.18.15.5.1.1" class="ltx_p"><span id="S4.T5.3.18.15.5.1.1.1" class="ltx_text" style="font-size:90%;">75</span></span>
</span>
</td>
<td id="S4.T5.3.18.15.6" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.3.18.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.18.15.6.1.1" class="ltx_p"><span id="S4.T5.3.18.15.6.1.1.1" class="ltx_text" style="font-size:80%;">Camera-captured</span></span>
</span>
</td>
<td id="S4.T5.3.18.15.7" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S4.T5.3.18.15.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.18.15.7.1.1" class="ltx_p"><a target="_blank" href="http://ispl.snu.ac.kr/~cusisi/dataset.zip" title="" class="ltx_ref ltx_href" style="font-size:80%;">http://ispl.snu.ac.kr/ cusisi/dataset.zip</a></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS10" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS10.5.1.1" class="ltx_text">IV-J</span> </span><span id="S4.SS10.6.2" class="ltx_text ltx_font_italic">SciTSR</span>
</h3>

<div id="S4.SS10.p1" class="ltx_para">
<p id="S4.SS10.p1.1" class="ltx_p">SciTSR is another dataset released in 2019 by Zewen, <span id="S4.SS10.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>. According to the authors, this is one of the largest publicly available dataset for the task of table structure recognition <span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://github.com/Academic-Hammer/SciTSR</span></span></span>. The dataset consists of 15 thousands tables in PDF format along with its annotations. The dataset is constructed by crawling LaTeX source files from the arXiv. Roughly 25% of the dataset consists of complicated tables that span into multiple rows or columns. This dataset has annotations for table structural segmentation and table recognition as summarized in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. Because of having complex tabular structures, this dataset can be exploited to improve state-of-the-art systems dealing with structural segmentation and recognition of tables having complicated layouts.</p>
</div>
</section>
<section id="S4.SS11" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS11.5.1.1" class="ltx_text">IV-K</span> </span><span id="S4.SS11.6.2" class="ltx_text ltx_font_italic">DeepFigures</span>
</h3>

<div id="S4.SS11.p1" class="ltx_para">
<p id="S4.SS11.p1.1" class="ltx_p">Based on our knowledge, DeepFigures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is the biggest dataset publicly available to perform the task of table detection. The dataset contains over 1.4 million documents along with their corresponding bounding boxes of tables and figures. The authors leverage the scientific articles available online on the arXiv and PubMed databases to develop the dataset. The ground truth of the dataset <span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/deepfigures/jcdl-deepfigures-labels.tar.gz</span></span></span> is available in XML format. As highlighted in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, this dataset only contains bounding boxes for the tables. In order to completely exploit deep neural networks for the problem of table detection, this large-scale dataset can be treated as a base dataset to implement closer domain fine-tuning techniques.</p>
</div>
</section>
<section id="S4.SS12" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS12.5.1.1" class="ltx_text">IV-L</span> </span><span id="S4.SS12.6.2" class="ltx_text ltx_font_italic">RVL-CDIP (Subset)</span>
</h3>

<div id="S4.SS12.p1" class="ltx_para">
<p id="S4.SS12.p1.1" class="ltx_p">The RVL-CDIP (Ryerson Vision Lab Complex Document Information Processing) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> is a renowned dataset in the document analysis community. It contains 400 thousand images equally distributed into 16 classes. <span id="S4.SS12.p1.1.1" class="ltx_text ltx_font_italic">Pau et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> leverages the RVL-CDIP dataset by annotating its 518 invoices. The dataset <span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://zenodo.org/record/3257319</span></span></span> has been made publicly available for the task of table detection. The dataset has only annotations for the tabular boundaries as mentioned in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. This subset of the actual RVL-CDIP dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> is an important contribution for evaluating table detection systems specifically designed for invoice document images.</p>
</div>
</section>
<section id="S4.SS13" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS13.5.1.1" class="ltx_text">IV-M</span> </span><span id="S4.SS13.6.2" class="ltx_text ltx_font_italic">PubTabNet</span>
</h3>

<div id="S4.SS13.p1" class="ltx_para">
<p id="S4.SS13.p1.1" class="ltx_p">PubTabNet is another dataset published in December 2019 by Zhong <span id="S4.SS13.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. PubTabNet<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://github.com/ibm-aur-nlp/PubTabNet</span></span></span> is currently the largest publicly available dataset that contains over 568 thousand images with their corresponding structural information of tables and content present in each cell. This dataset is created by collecting scientific articles from PubMed Central<sup id="S4.SS13.p1.1.2" class="ltx_sup"><span id="S4.SS13.p1.1.2.1" class="ltx_text ltx_font_italic">TM</span></sup> Open Access Subset (PMCOA). The ground truth format of this dataset is in HTML which can be useful for web applications. The authors are confident that this dataset will boost the performance of information extraction systems in the table and they are also planning to publish ground truth for the respective table cells in the future. The important information for the dataset is summarized in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. Along with the TABLE2LATEX-450K dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>, PubTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> allows researchers the independence of training complete parameters of the deep neural networks on the task of table structure extraction or table recognition.</p>
</div>
</section>
<section id="S4.SS14" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS14.5.1.1" class="ltx_text">IV-N</span> </span><span id="S4.SS14.6.2" class="ltx_text ltx_font_italic">IIIT-AR-13K</span>
</h3>

<div id="S4.SS14.p1" class="ltx_para">
<p id="S4.SS14.p1.1" class="ltx_p">Recently, Mondal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> contributed to the community of graphical page object detection by introducing a novel dataset known as <span id="S4.SS14.p1.1.1" class="ltx_text ltx_font_italic">IIT-AR-13K</span>. The authors generated this dataset by collecting publicly available annual reports written in English and other languages. The authors claim that this is the largest manually annotated dataset published for solving the problem of graphical page object detection. Apart from the tables, the dataset includes annotations for figures, natural images, logos, and signatures. The publishers of this dataset have provided the train, validation, and test splits for various tasks of page object detection. For table detection, 11000 samples are used for training, whereas 2000 and 3000 samples are assigned for validation and testing purposes, respectively.</p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2104.14272/assets/cam_cap.jpg" id="S4.F10.g1" class="ltx_graphics ltx_img_landscape" width="598" height="384" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Examples of real camera-captured images taken from the CamCap dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> which is explained in Section <a href="#S4.SS15" title="IV-O CamCap ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-O</span></span></a>. The red boundaries represent the tabular region.</figcaption>
</figure>
</section>
<section id="S4.SS15" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS15.5.1.1" class="ltx_text">IV-O</span> </span><span id="S4.SS15.6.2" class="ltx_text ltx_font_italic">CamCap</span>
</h3>

<div id="S4.SS15.p1" class="ltx_para">
<p id="S4.SS15.p1.1" class="ltx_p">CamCap is the last dataset which we have included in this survey consists of the camera-captured images. This dataset is proposed by Seo <span id="S4.SS15.p1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS15.p1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S4.SS15.p1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>. It contains only 85 images (38 tables on curved surfaces having 1295 cells and 47 tables on the planar surfaces consisting of 1162 cells). Figure <a href="#S4.F10" title="Figure 10 ‣ IV-N IIIT-AR-13K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> contains few samples from this dataset illustrating the challenges. The proposed dataset is publicly available and can be utilized for the task of table detection and table structure recognition as summarized in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. In order to assess the robustness of table detection methods on camera-captured document images, this dataset is an important contribution.</p>
</div>
<div id="S4.SS15.p2" class="ltx_para">
<p id="S4.SS15.p2.1" class="ltx_p">It is important to mention that Qasim <span id="S4.SS15.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> published a method to synthetically create camera captured images from the UNLV dataset. An instance of a synthetically created camera-captured image is depicted in Figure <a href="#S4.F11" title="Figure 11 ‣ IV-O CamCap ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2104.14272/assets/cc_img.jpg" id="S4.F11.g1" class="ltx_graphics ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Example of a synthetically created camera captured image by linear perspective transform method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Evaluation</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we will cover the well known evaluation metrics along with the exhaustive evaluation comparisons of all the quoted methodologies from Section <a href="#S3" title="III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Evaluation Metrics</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Before throwing some light on the performance evaluation, it is appropriate to talk about the evaluation metrics first which are adopted to assess the performances of discussed approaches.</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS1.5.1.1" class="ltx_text">V-A</span>1 </span>Precision</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">Precision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> is defined as the percentage of a predicted region that belongs to the ground truth. An illustration of different types of precision is explained in Figure <a href="#S5.F12" title="Figure 12 ‣ V-A1 Precision ‣ V-A Evaluation Metrics ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. The formula for precision is mentioned below :</p>
<table id="S5.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E1.m1.1" class="ltx_Math" alttext="\frac{\text{Predicted area in ground truth}}{\text{Total area of predicted region}}=\frac{\text{TP}}{\text{TP $+$ FP}}" display="block"><semantics id="S5.E1.m1.1a"><mrow id="S5.E1.m1.1.2" xref="S5.E1.m1.1.2.cmml"><mfrac id="S5.E1.m1.1.2.2" xref="S5.E1.m1.1.2.2.cmml"><mtext id="S5.E1.m1.1.2.2.2" xref="S5.E1.m1.1.2.2.2a.cmml">Predicted area in ground truth</mtext><mtext id="S5.E1.m1.1.2.2.3" xref="S5.E1.m1.1.2.2.3a.cmml">Total area of predicted region</mtext></mfrac><mo id="S5.E1.m1.1.2.1" xref="S5.E1.m1.1.2.1.cmml">=</mo><mfrac id="S5.E1.m1.1.1" xref="S5.E1.m1.1.1.cmml"><mtext id="S5.E1.m1.1.1.3" xref="S5.E1.m1.1.1.3a.cmml">TP</mtext><mrow id="S5.E1.m1.1.1.1" xref="S5.E1.m1.1.1.1c.cmml"><mtext id="S5.E1.m1.1.1.1a" xref="S5.E1.m1.1.1.1c.cmml">TP </mtext><mo id="S5.E1.m1.1.1.1.1.1.m1.1.1" xref="S5.E1.m1.1.1.1.1.1.m1.1.1.cmml">+</mo><mtext id="S5.E1.m1.1.1.1b" xref="S5.E1.m1.1.1.1c.cmml"> FP</mtext></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.E1.m1.1b"><apply id="S5.E1.m1.1.2.cmml" xref="S5.E1.m1.1.2"><eq id="S5.E1.m1.1.2.1.cmml" xref="S5.E1.m1.1.2.1"></eq><apply id="S5.E1.m1.1.2.2.cmml" xref="S5.E1.m1.1.2.2"><divide id="S5.E1.m1.1.2.2.1.cmml" xref="S5.E1.m1.1.2.2"></divide><ci id="S5.E1.m1.1.2.2.2a.cmml" xref="S5.E1.m1.1.2.2.2"><mtext id="S5.E1.m1.1.2.2.2.cmml" xref="S5.E1.m1.1.2.2.2">Predicted area in ground truth</mtext></ci><ci id="S5.E1.m1.1.2.2.3a.cmml" xref="S5.E1.m1.1.2.2.3"><mtext id="S5.E1.m1.1.2.2.3.cmml" xref="S5.E1.m1.1.2.2.3">Total area of predicted region</mtext></ci></apply><apply id="S5.E1.m1.1.1.cmml" xref="S5.E1.m1.1.1"><divide id="S5.E1.m1.1.1.2.cmml" xref="S5.E1.m1.1.1"></divide><ci id="S5.E1.m1.1.1.3a.cmml" xref="S5.E1.m1.1.1.3"><mtext id="S5.E1.m1.1.1.3.cmml" xref="S5.E1.m1.1.1.3">TP</mtext></ci><ci id="S5.E1.m1.1.1.1c.cmml" xref="S5.E1.m1.1.1.1"><mrow id="S5.E1.m1.1.1.1.cmml" xref="S5.E1.m1.1.1.1"><mtext id="S5.E1.m1.1.1.1a.cmml" xref="S5.E1.m1.1.1.1">TP </mtext><mo id="S5.E1.m1.1.1.1.1.1.m1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.m1.1.1">+</mo><mtext id="S5.E1.m1.1.1.1b.cmml" xref="S5.E1.m1.1.1.1"> FP</mtext></mrow></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.1c">\frac{\text{Predicted area in ground truth}}{\text{Total area of predicted region}}=\frac{\text{TP}}{\text{TP $+$ FP}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S5.F12" class="ltx_figure"><img src="/html/2104.14272/assets/precision.jpg" id="S5.F12.g1" class="ltx_graphics ltx_img_landscape" width="598" height="290" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Example of precision in object detection problems where the IOU threshold is set to 0.5. The leftmost case will not be counted as precise whereas the other two predictions are precise because their IOU value is greater than 0.5. Green color represents the ground truth and red color depicts the predicted bounding boxes.</figcaption>
</figure>
<figure id="S5.F13" class="ltx_figure"><img src="/html/2104.14272/assets/tbl_det_precise.jpg" id="S5.F13.g1" class="ltx_graphics ltx_img_landscape" width="598" height="427" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Example of precision in reference to the task of table detection. The green color represents the ground truth whereas the red color depicts the predicted tabular area. In the first case, the prediction is not a precise one because IOU between the predicted bounding box and the ground truth is less than 0.5. The table prediction on the right side is precise because it covers an almost complete tabular area.</figcaption>
</figure>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS2.5.1.1" class="ltx_text">V-A</span>2 </span>Recall</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">Recall <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> is calculated as the percentage of ground truth region that is present in the predicted region. The formula for recall is explained as follows :</p>
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1.1" class="ltx_Math" alttext="\frac{\text{Ground truth area in predicted region}}{\text{Total area of ground truth region}}=\frac{\text{TP}}{\text{TP $+$ FN}}" display="block"><semantics id="S5.E2.m1.1a"><mrow id="S5.E2.m1.1.2" xref="S5.E2.m1.1.2.cmml"><mfrac id="S5.E2.m1.1.2.2" xref="S5.E2.m1.1.2.2.cmml"><mtext id="S5.E2.m1.1.2.2.2" xref="S5.E2.m1.1.2.2.2a.cmml">Ground truth area in predicted region</mtext><mtext id="S5.E2.m1.1.2.2.3" xref="S5.E2.m1.1.2.2.3a.cmml">Total area of ground truth region</mtext></mfrac><mo id="S5.E2.m1.1.2.1" xref="S5.E2.m1.1.2.1.cmml">=</mo><mfrac id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml"><mtext id="S5.E2.m1.1.1.3" xref="S5.E2.m1.1.1.3a.cmml">TP</mtext><mrow id="S5.E2.m1.1.1.1" xref="S5.E2.m1.1.1.1c.cmml"><mtext id="S5.E2.m1.1.1.1a" xref="S5.E2.m1.1.1.1c.cmml">TP </mtext><mo id="S5.E2.m1.1.1.1.1.1.m1.1.1" xref="S5.E2.m1.1.1.1.1.1.m1.1.1.cmml">+</mo><mtext id="S5.E2.m1.1.1.1b" xref="S5.E2.m1.1.1.1c.cmml"> FN</mtext></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.1b"><apply id="S5.E2.m1.1.2.cmml" xref="S5.E2.m1.1.2"><eq id="S5.E2.m1.1.2.1.cmml" xref="S5.E2.m1.1.2.1"></eq><apply id="S5.E2.m1.1.2.2.cmml" xref="S5.E2.m1.1.2.2"><divide id="S5.E2.m1.1.2.2.1.cmml" xref="S5.E2.m1.1.2.2"></divide><ci id="S5.E2.m1.1.2.2.2a.cmml" xref="S5.E2.m1.1.2.2.2"><mtext id="S5.E2.m1.1.2.2.2.cmml" xref="S5.E2.m1.1.2.2.2">Ground truth area in predicted region</mtext></ci><ci id="S5.E2.m1.1.2.2.3a.cmml" xref="S5.E2.m1.1.2.2.3"><mtext id="S5.E2.m1.1.2.2.3.cmml" xref="S5.E2.m1.1.2.2.3">Total area of ground truth region</mtext></ci></apply><apply id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1"><divide id="S5.E2.m1.1.1.2.cmml" xref="S5.E2.m1.1.1"></divide><ci id="S5.E2.m1.1.1.3a.cmml" xref="S5.E2.m1.1.1.3"><mtext id="S5.E2.m1.1.1.3.cmml" xref="S5.E2.m1.1.1.3">TP</mtext></ci><ci id="S5.E2.m1.1.1.1c.cmml" xref="S5.E2.m1.1.1.1"><mrow id="S5.E2.m1.1.1.1.cmml" xref="S5.E2.m1.1.1.1"><mtext id="S5.E2.m1.1.1.1a.cmml" xref="S5.E2.m1.1.1.1">TP </mtext><mo id="S5.E2.m1.1.1.1.1.1.m1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.1.m1.1.1">+</mo><mtext id="S5.E2.m1.1.1.1b.cmml" xref="S5.E2.m1.1.1.1"> FN</mtext></mrow></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.1c">\frac{\text{Ground truth area in predicted region}}{\text{Total area of ground truth region}}=\frac{\text{TP}}{\text{TP $+$ FN}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS3.5.1.1" class="ltx_text">V-A</span>3 </span>F-Measure</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para">
<p id="S5.SS1.SSS3.p1.1" class="ltx_p">F-measure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> is calculated by taking the harmonic mean of precision and Recall . The formula for F-measure is :</p>
<table id="S5.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E3.m1.3" class="ltx_Math" alttext="\frac{\text{$2\times$ Precision $\times$ Recall}}{\text{Precision $+$ Recall}}" display="block"><semantics id="S5.E3.m1.3a"><mfrac id="S5.E3.m1.3.3" xref="S5.E3.m1.3.3.cmml"><mrow id="S5.E3.m1.2.2.2" xref="S5.E3.m1.2.2.2c.cmml"><mrow id="S5.E3.m1.1.1.1.1.1.m1.1" xref="S5.E3.m1.1.1.1.1.1.m1.1.cmml"><mn id="S5.E3.m1.1.1.1.1.1.m1.1.1" xref="S5.E3.m1.1.1.1.1.1.m1.1.1.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S5.E3.m1.1.1.1.1.1.m1.1.2" xref="S5.E3.m1.1.1.1.1.1.m1.1.2.cmml">×</mo></mrow><mtext id="S5.E3.m1.2.2.2a" xref="S5.E3.m1.2.2.2c.cmml"> Precision </mtext><mo lspace="0.222em" rspace="0.222em" id="S5.E3.m1.2.2.2.2.2.m2.1.1" xref="S5.E3.m1.2.2.2.2.2.m2.1.1.cmml">×</mo><mtext id="S5.E3.m1.2.2.2b" xref="S5.E3.m1.2.2.2c.cmml"> Recall</mtext></mrow><mrow id="S5.E3.m1.3.3.3" xref="S5.E3.m1.3.3.3c.cmml"><mtext id="S5.E3.m1.3.3.3a" xref="S5.E3.m1.3.3.3c.cmml">Precision </mtext><mo id="S5.E3.m1.3.3.3.1.1.m1.1.1" xref="S5.E3.m1.3.3.3.1.1.m1.1.1.cmml">+</mo><mtext id="S5.E3.m1.3.3.3b" xref="S5.E3.m1.3.3.3c.cmml"> Recall</mtext></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S5.E3.m1.3b"><apply id="S5.E3.m1.3.3.cmml" xref="S5.E3.m1.3.3"><divide id="S5.E3.m1.3.3.4.cmml" xref="S5.E3.m1.3.3"></divide><ci id="S5.E3.m1.2.2.2c.cmml" xref="S5.E3.m1.2.2.2"><mrow id="S5.E3.m1.2.2.2.cmml" xref="S5.E3.m1.2.2.2"><mrow id="S5.E3.m1.1.1.1.1.1.m1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.m1.1"><mn id="S5.E3.m1.1.1.1.1.1.m1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.m1.1.1">2</mn><mo id="S5.E3.m1.1.1.1.1.1.m1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.m1.1.2">×</mo></mrow><mtext id="S5.E3.m1.2.2.2a.cmml" xref="S5.E3.m1.2.2.2"> Precision </mtext><mo id="S5.E3.m1.2.2.2.2.2.m2.1.1.cmml" xref="S5.E3.m1.2.2.2.2.2.m2.1.1">×</mo><mtext id="S5.E3.m1.2.2.2b.cmml" xref="S5.E3.m1.2.2.2"> Recall</mtext></mrow></ci><ci id="S5.E3.m1.3.3.3c.cmml" xref="S5.E3.m1.3.3.3"><mrow id="S5.E3.m1.3.3.3.cmml" xref="S5.E3.m1.3.3.3"><mtext id="S5.E3.m1.3.3.3a.cmml" xref="S5.E3.m1.3.3.3">Precision </mtext><mo id="S5.E3.m1.3.3.3.1.1.m1.1.1.cmml" xref="S5.E3.m1.3.3.3.1.1.m1.1.1">+</mo><mtext id="S5.E3.m1.3.3.3b.cmml" xref="S5.E3.m1.3.3.3"> Recall</mtext></mrow></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.3c">\frac{\text{$2\times$ Precision $\times$ Recall}}{\text{Precision $+$ Recall}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S5.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS4.5.1.1" class="ltx_text">V-A</span>4 </span>Intersection Over Union (IOU)</h4>

<div id="S5.SS1.SSS4.p1" class="ltx_para">
<p id="S5.SS1.SSS4.p1.1" class="ltx_p">Intersection over union <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite> is an important evaluation metric which is regularly employed to determine the performance of object detection algorithms. It is the measure of how much the predicted region is overlapping with the actual ground truth region. It is defined as follows :</p>
<table id="S5.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E4.m1.1" class="ltx_Math" alttext="\frac{\text{Area of Overlap region}}{\text{Area of Union region}}" display="block"><semantics id="S5.E4.m1.1a"><mfrac id="S5.E4.m1.1.1" xref="S5.E4.m1.1.1.cmml"><mtext id="S5.E4.m1.1.1.2" xref="S5.E4.m1.1.1.2a.cmml">Area of Overlap region</mtext><mtext id="S5.E4.m1.1.1.3" xref="S5.E4.m1.1.1.3a.cmml">Area of Union region</mtext></mfrac><annotation-xml encoding="MathML-Content" id="S5.E4.m1.1b"><apply id="S5.E4.m1.1.1.cmml" xref="S5.E4.m1.1.1"><divide id="S5.E4.m1.1.1.1.cmml" xref="S5.E4.m1.1.1"></divide><ci id="S5.E4.m1.1.1.2a.cmml" xref="S5.E4.m1.1.1.2"><mtext id="S5.E4.m1.1.1.2.cmml" xref="S5.E4.m1.1.1.2">Area of Overlap region</mtext></ci><ci id="S5.E4.m1.1.1.3a.cmml" xref="S5.E4.m1.1.1.3"><mtext id="S5.E4.m1.1.1.3.cmml" xref="S5.E4.m1.1.1.3">Area of Union region</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E4.m1.1c">\frac{\text{Area of Overlap region}}{\text{Area of Union region}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S5.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS1.SSS5.5.1.1" class="ltx_text">V-A</span>5 </span>BLEU score</h4>

<div id="S5.SS1.SSS5.p1" class="ltx_para">
<p id="S5.SS1.SSS5.p1.1" class="ltx_p">BLEU (Bilingual Evaluation Understudy) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> is an evaluation method utilized to compare in various machine translation problems. After comparing the predicted text with the actual ground truth, a score is calculated. The BLEU metric scores the prediction from 0 to 1 where 1 is the optimal score for the predicted text.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Evaluations for Table Detection</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The problem of table detection is to distinguish the tabular area in the document image and regress the coordinates of a bounding box that is classified as a tabular region. Table <a href="#S5.T6" title="TABLE VI ‣ V-B Evaluations for Table Detection ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> explains the performance comparison of various table detection methods that have been discussed in detail in Section <a href="#S3.SS1" title="III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>. In most of the cases, the performance of the table detection methods is evaluated on ICDAR-2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, ICDAR-2017-POD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and UNLV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> datasets.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The threshold of Intersection Over Union (IOU) for calculating precision and recall is also defined in Table <a href="#S5.T6" title="TABLE VI ‣ V-B Evaluations for Table Detection ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>. Figure <a href="#S5.F13" title="Figure 13 ‣ V-A1 Precision ‣ V-A Evaluation Metrics ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> explains the definition of a precise and imprecise prediction in reference to the task of table detections. Results having the highest accuracies in all respective datasets are highlighted. It is crucial to mention that some of the approaches have not quoted the threshold value for IOU; however, they have compared their results with other methods where the threshold value is defined. Hence, we have considered the same threshold value for those procedures.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">We could not incorporate the results of the literature presented by <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_italic">Martin et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> because they have not adopted any standard dataset for the comparison, and compared their novel method with logistic regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>. The results demonstrate that their model has surpassed the logistic regression method.




</p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Table Detection Performance Comparison. The double horizontal line partitions the results obtained on various datasets. Outstanding results in all the respective datasets are highlighted. For the ICDAR-2019 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, all of the three approaches are not directly comparable to each other because they report F-Measure on different IOU thresholds. Hence, results on ICDAR-2019 dataset are not highlighted. </figcaption>
<table id="S5.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<td id="S5.T6.1.1.1.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.1.1.1.1.1" class="ltx_p"><span id="S5.T6.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Literature</span></span>
</span>
</td>
<td id="S5.T6.1.1.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.1.1.2.1.1" class="ltx_p"><span id="S5.T6.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Year</span></span>
</span>
</td>
<td id="S5.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S5.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">IOU</span></td>
<td id="S5.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.5.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S5.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.6.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S5.T6.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.7.1" class="ltx_text ltx_font_bold">F-Measure</span></td>
<td id="S5.T6.1.1.1.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.1.1.8.1.1" class="ltx_p"><span id="S5.T6.1.1.1.8.1.1.1" class="ltx_text ltx_font_bold">Method</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.2.2" class="ltx_tr">
<td id="S5.T6.1.2.2.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.2.2.1.1.1" class="ltx_p"><span id="S5.T6.1.2.2.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Gelani et al.<span id="S5.T6.1.2.2.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.2.2.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.2.2.2.1.1" class="ltx_p"><span id="S5.T6.1.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">2017</span></span>
</span>
</td>
<td id="S5.T6.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.2.2.3.1" class="ltx_text" style="font-size:80%;">UNLV</span></td>
<td id="S5.T6.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.2.2.4.1" class="ltx_text" style="font-size:90%;">0.9</span></td>
<td id="S5.T6.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">82.3</span></td>
<td id="S5.T6.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.2.2.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">90.6</span></td>
<td id="S5.T6.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.2.2.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">86.3</span></td>
<td id="S5.T6.1.2.2.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.2.2.8.1.1" class="ltx_p"><span id="S5.T6.1.2.2.8.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN (Section <a href="#S3.SS1.SSS1.Px2" title="Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.3.3" class="ltx_tr">
<td id="S5.T6.1.3.3.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.3.3.1.1.1" class="ltx_p"><span id="S5.T6.1.3.3.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">García et al.<span id="S5.T6.1.3.3.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.3.3.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.3.3.2.1.1" class="ltx_p"><span id="S5.T6.1.3.3.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T6.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.3.3.3.1" class="ltx_text" style="font-size:80%;">UNLV</span></td>
<td id="S5.T6.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.3.3.4.1" class="ltx_text" style="font-size:90%;">0.9</span></td>
<td id="S5.T6.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.3.3.5.1" class="ltx_text" style="font-size:90%;">48.0</span></td>
<td id="S5.T6.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.3.3.6.1" class="ltx_text" style="font-size:90%;">49.0</span></td>
<td id="S5.T6.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.3.3.7.1" class="ltx_text" style="font-size:90%;">49.0</span></td>
<td id="S5.T6.1.3.3.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.3.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.3.3.8.1.1" class="ltx_p"><span id="S5.T6.1.3.3.8.1.1.1" class="ltx_text" style="font-size:80%;">YOLO (Section <a href="#S3.SS1.SSS1.Px4" title="YOLO ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.4.4" class="ltx_tr">
<td id="S5.T6.1.4.4.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.4.4.1.1.1" class="ltx_p"><span id="S5.T6.1.4.4.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">DeCNT<span id="S5.T6.1.4.4.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.4.4.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.4.4.2.1.1" class="ltx_p"><span id="S5.T6.1.4.4.2.1.1.1" class="ltx_text" style="font-size:90%;">2018</span></span>
</span>
</td>
<td id="S5.T6.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.4.4.3.1" class="ltx_text" style="font-size:80%;">UNLV</span></td>
<td id="S5.T6.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.4.4.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.4.4.5.1" class="ltx_text" style="font-size:90%;">78.6</span></td>
<td id="S5.T6.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.4.4.6.1" class="ltx_text" style="font-size:90%;">74.9</span></td>
<td id="S5.T6.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.4.4.7.1" class="ltx_text" style="font-size:90%;">76.7</span></td>
<td id="S5.T6.1.4.4.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.4.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.4.4.8.1.1" class="ltx_p"><span id="S5.T6.1.4.4.8.1.1.1" class="ltx_text" style="font-size:80%;">Deformable Convolutions (Section <a href="#S3.SS1.SSS1.Px3" title="Deformable Convolutions ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.5.5" class="ltx_tr">
<td id="S5.T6.1.5.5.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span id="S5.T6.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.5.5.1.1.1" class="ltx_p"><span id="S5.T6.1.5.5.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">DeepDeSRT<span id="S5.T6.1.5.5.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.5.5.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span id="S5.T6.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.5.5.2.1.1" class="ltx_p"><span id="S5.T6.1.5.5.2.1.1.1" class="ltx_text" style="font-size:90%;">2017</span></span>
</span>
</td>
<td id="S5.T6.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.5.5.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2013</span></td>
<td id="S5.T6.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.5.5.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.5.5.5.1" class="ltx_text" style="font-size:90%;">97.4</span></td>
<td id="S5.T6.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.5.5.6.1" class="ltx_text" style="font-size:90%;">96.1</span></td>
<td id="S5.T6.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.5.5.7.1" class="ltx_text" style="font-size:90%;">96.7</span></td>
<td id="S5.T6.1.5.5.8" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T6.1.5.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.5.5.8.1.1" class="ltx_p"><span id="S5.T6.1.5.5.8.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN (Section <a href="#S3.SS1.SSS1.Px2" title="Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.6.6" class="ltx_tr">
<td id="S5.T6.1.6.6.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.6.6.1.1.1" class="ltx_p"><span id="S5.T6.1.6.6.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Kavasidis et al.<span id="S5.T6.1.6.6.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.6.6.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.6.6.2.1.1" class="ltx_p"><span id="S5.T6.1.6.6.2.1.1.1" class="ltx_text" style="font-size:90%;">2018</span></span>
</span>
</td>
<td id="S5.T6.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.6.6.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2013</span></td>
<td id="S5.T6.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.6.6.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.6.6.5.1" class="ltx_text" style="font-size:90%;">97.5</span></td>
<td id="S5.T6.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.6.6.6.1" class="ltx_text" style="font-size:90%;">98.1</span></td>
<td id="S5.T6.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.6.6.7.1" class="ltx_text" style="font-size:90%;">97.8</span></td>
<td id="S5.T6.1.6.6.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.6.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.6.6.8.1.1" class="ltx_p"><span id="S5.T6.1.6.6.8.1.1.1" class="ltx_text" style="font-size:80%;">Semantic Image Segmentation ( Section <a href="#S3.SS1.SSS2" title="III-A2 Semantic Image Segmentation ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>2</span></a> )</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.7.7" class="ltx_tr">
<td id="S5.T6.1.7.7.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.7.7.1.1.1" class="ltx_p"><span id="S5.T6.1.7.7.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">DeCNT<span id="S5.T6.1.7.7.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.7.7.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.7.7.2.1.1" class="ltx_p"><span id="S5.T6.1.7.7.2.1.1.1" class="ltx_text" style="font-size:90%;">2018</span></span>
</span>
</td>
<td id="S5.T6.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.7.7.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2013</span></td>
<td id="S5.T6.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.7.7.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.7.7.5.1" class="ltx_text" style="font-size:90%;">99.6</span></td>
<td id="S5.T6.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.7.7.6.1" class="ltx_text" style="font-size:90%;">99.6</span></td>
<td id="S5.T6.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.7.7.7.1" class="ltx_text" style="font-size:90%;">99.6</span></td>
<td id="S5.T6.1.7.7.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.7.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.7.7.8.1.1" class="ltx_p"><span id="S5.T6.1.7.7.8.1.1.1" class="ltx_text" style="font-size:80%;">Deformable Convolutions (Section <a href="#S3.SS1.SSS1.Px3" title="Deformable Convolutions ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.8.8" class="ltx_tr">
<td id="S5.T6.1.8.8.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.8.8.1.1.1" class="ltx_p"><span id="S5.T6.1.8.8.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Huang et al.<span id="S5.T6.1.8.8.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.8.8.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.8.8.2.1.1" class="ltx_p"><span id="S5.T6.1.8.8.2.1.1.1" class="ltx_text" style="font-size:90%;">2015</span></span>
</span>
</td>
<td id="S5.T6.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.8.8.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2013</span></td>
<td id="S5.T6.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.8.8.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.8.8.5.1" class="ltx_text" style="font-size:90%;">100</span></td>
<td id="S5.T6.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.8.8.6.1" class="ltx_text" style="font-size:90%;">94.9</span></td>
<td id="S5.T6.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.8.8.7.1" class="ltx_text" style="font-size:90%;">97.3</span></td>
<td id="S5.T6.1.8.8.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.8.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.8.8.8.1.1" class="ltx_p"><span id="S5.T6.1.8.8.8.1.1.1" class="ltx_text" style="font-size:80%;">YOLO (Section <a href="#S3.SS1.SSS1.Px4" title="YOLO ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.9.9" class="ltx_tr">
<td id="S5.T6.1.9.9.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.9.9.1.1.1" class="ltx_p"><span id="S5.T6.1.9.9.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TableBank<span id="S5.T6.1.9.9.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.9.9.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.9.9.2.1.1" class="ltx_p"><span id="S5.T6.1.9.9.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T6.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.9.9.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2013</span></td>
<td id="S5.T6.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.9.9.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.9.9.5.1" class="ltx_text" style="font-size:90%;">96.2</span></td>
<td id="S5.T6.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.9.9.6.1" class="ltx_text" style="font-size:90%;">96.2</span></td>
<td id="S5.T6.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.9.9.7.1" class="ltx_text" style="font-size:90%;">96.2</span></td>
<td id="S5.T6.1.9.9.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.9.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.9.9.8.1.1" class="ltx_p"><span id="S5.T6.1.9.9.8.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN (Section <a href="#S3.SS1.SSS1.Px2" title="Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.10.10" class="ltx_tr">
<td id="S5.T6.1.10.10.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.10.10.1.1.1" class="ltx_p"><span id="S5.T6.1.10.10.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TableNet<span id="S5.T6.1.10.10.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.10.10.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.10.10.2.1.1" class="ltx_p"><span id="S5.T6.1.10.10.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T6.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.10.10.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2013</span></td>
<td id="S5.T6.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.10.10.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.10.10.5.1" class="ltx_text" style="font-size:90%;">96.3</span></td>
<td id="S5.T6.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.10.10.6.1" class="ltx_text" style="font-size:90%;">96.9</span></td>
<td id="S5.T6.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.10.10.7.1" class="ltx_text" style="font-size:90%;">96.6</span></td>
<td id="S5.T6.1.10.10.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.10.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.10.10.8.1.1" class="ltx_p"><span id="S5.T6.1.10.10.8.1.1.1" class="ltx_text" style="font-size:80%;">Fully Convolutional Networks (Section <a href="#S3.SS1.SSS2.Px1" title="Fully Convolutional Networks ‣ III-A2 Semantic Image Segmentation ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>2</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.11.11" class="ltx_tr">
<td id="S5.T6.1.11.11.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.11.11.1.1.1" class="ltx_p"><span id="S5.T6.1.11.11.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CascadeTabNet<span id="S5.T6.1.11.11.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.11.11.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.11.11.2.1.1" class="ltx_p"><span id="S5.T6.1.11.11.2.1.1.1" class="ltx_text" style="font-size:90%;">2020</span></span>
</span>
</td>
<td id="S5.T6.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.11.11.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2013</span></td>
<td id="S5.T6.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.11.11.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.11.11.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">100</span></td>
<td id="S5.T6.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.11.11.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">100</span></td>
<td id="S5.T6.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.11.11.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">100</span></td>
<td id="S5.T6.1.11.11.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.11.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.11.11.8.1.1" class="ltx_p"><span id="S5.T6.1.11.11.8.1.1.1" class="ltx_text" style="font-size:80%;">Cascade Mask R-CNN 
<br class="ltx_break">(Section <a href="#S3.SS1.SSS1.Px6" title="Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.12.12" class="ltx_tr">
<td id="S5.T6.1.12.12.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.12.12.1.1.1" class="ltx_p"><span id="S5.T6.1.12.12.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GTE<span id="S5.T6.1.12.12.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.12.12.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.12.12.2.1.1" class="ltx_p"><span id="S5.T6.1.12.12.2.1.1.1" class="ltx_text" style="font-size:90%;">2021</span></span>
</span>
</td>
<td id="S5.T6.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.12.12.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2013</span></td>
<td id="S5.T6.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.12.12.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.12.12.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T6.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.12.12.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T6.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.12.12.7.1" class="ltx_text" style="font-size:90%;">95.7</span></td>
<td id="S5.T6.1.12.12.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.12.12.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.12.12.8.1.1" class="ltx_p"><span id="S5.T6.1.12.12.8.1.1.1" class="ltx_text" style="font-size:80%;">Object Detection (Section <a href="#S3.SS1.SSS1.Px6" title="Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.13.13" class="ltx_tr">
<td id="S5.T6.1.13.13.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.13.13.1.1.1" class="ltx_p"><span id="S5.T6.1.13.13.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CDeC-Net<span id="S5.T6.1.13.13.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.13.13.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.13.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.13.13.2.1.1" class="ltx_p"><span id="S5.T6.1.13.13.2.1.1.1" class="ltx_text" style="font-size:90%;">2020</span></span>
</span>
</td>
<td id="S5.T6.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.13.13.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2013</span></td>
<td id="S5.T6.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.13.13.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.13.13.5.1" class="ltx_text" style="font-size:90%;">94.2</span></td>
<td id="S5.T6.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.13.13.6.1" class="ltx_text" style="font-size:90%;">99.3</span></td>
<td id="S5.T6.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.13.13.7.1" class="ltx_text" style="font-size:90%;">96.8</span></td>
<td id="S5.T6.1.13.13.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.13.13.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.13.13.8.1.1" class="ltx_p"><span id="S5.T6.1.13.13.8.1.1.1" class="ltx_text" style="font-size:80%;">Cascade Mask R-CNN 
<br class="ltx_break">(Section <a href="#S3.SS1.SSS1.Px6" title="Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.14.14" class="ltx_tr">
<td id="S5.T6.1.14.14.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.14.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.14.14.1.1.1" class="ltx_p"><span id="S5.T6.1.14.14.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">García et al.<span id="S5.T6.1.14.14.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.14.14.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.14.14.2.1.1" class="ltx_p"><span id="S5.T6.1.14.14.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T6.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.14.14.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2013</span></td>
<td id="S5.T6.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.14.14.4.1" class="ltx_text" style="font-size:90%;">0.6</span></td>
<td id="S5.T6.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.14.14.5.1" class="ltx_text" style="font-size:90%;">70.0</span></td>
<td id="S5.T6.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.14.14.6.1" class="ltx_text" style="font-size:90%;">97.0</span></td>
<td id="S5.T6.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.14.14.7.1" class="ltx_text" style="font-size:90%;">81.0</span></td>
<td id="S5.T6.1.14.14.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.14.14.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.14.14.8.1.1" class="ltx_p"><span id="S5.T6.1.14.14.8.1.1.1" class="ltx_text" style="font-size:80%;">Mask R-CNN (Section <a href="#S3.SS1.SSS1.Px5" title="Mask R-CNN, YOLO, SSD and Retina Net ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.15.15" class="ltx_tr">
<td id="S5.T6.1.15.15.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span id="S5.T6.1.15.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.15.15.1.1.1" class="ltx_p"><span id="S5.T6.1.15.15.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Li et al.<span id="S5.T6.1.15.15.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.15.15.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span id="S5.T6.1.15.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.15.15.2.1.1" class="ltx_p"><span id="S5.T6.1.15.15.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T6.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.15.15.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2017</span></td>
<td id="S5.T6.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.15.15.4.1" class="ltx_text" style="font-size:90%;">0.6</span></td>
<td id="S5.T6.1.15.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.15.15.5.1" class="ltx_text" style="font-size:90%;">94.4</span></td>
<td id="S5.T6.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.15.15.6.1" class="ltx_text" style="font-size:90%;">94.4</span></td>
<td id="S5.T6.1.15.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.15.15.7.1" class="ltx_text" style="font-size:90%;">94.4</span></td>
<td id="S5.T6.1.15.15.8" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T6.1.15.15.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.15.15.8.1.1" class="ltx_p"><span id="S5.T6.1.15.15.8.1.1.1" class="ltx_text" style="font-size:80%;">GANs (Section <a href="#S3.SS1.SSS4" title="III-A4 Generative Adversarial Networks ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>4</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.16.16" class="ltx_tr">
<td id="S5.T6.1.16.16.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.16.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.16.16.1.1.1" class="ltx_p"><span id="S5.T6.1.16.16.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">DeCNT<span id="S5.T6.1.16.16.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.16.16.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.16.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.16.16.2.1.1" class="ltx_p"><span id="S5.T6.1.16.16.2.1.1.1" class="ltx_text" style="font-size:90%;">2018</span></span>
</span>
</td>
<td id="S5.T6.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.16.16.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2017</span></td>
<td id="S5.T6.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.16.16.4.1" class="ltx_text" style="font-size:90%;">0.6</span></td>
<td id="S5.T6.1.16.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.16.16.5.1" class="ltx_text" style="font-size:90%;">97.1</span></td>
<td id="S5.T6.1.16.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.16.16.6.1" class="ltx_text" style="font-size:90%;">96.5</span></td>
<td id="S5.T6.1.16.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.16.16.7.1" class="ltx_text" style="font-size:90%;">96.8</span></td>
<td id="S5.T6.1.16.16.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.16.16.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.16.16.8.1.1" class="ltx_p"><span id="S5.T6.1.16.16.8.1.1.1" class="ltx_text" style="font-size:80%;">Deformable Convolutions (Section <a href="#S3.SS1.SSS1.Px3" title="Deformable Convolutions ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.17.17" class="ltx_tr">
<td id="S5.T6.1.17.17.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.17.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.17.17.1.1.1" class="ltx_p"><span id="S5.T6.1.17.17.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Huang et al.<span id="S5.T6.1.17.17.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.17.17.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.17.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.17.17.2.1.1" class="ltx_p"><span id="S5.T6.1.17.17.2.1.1.1" class="ltx_text" style="font-size:90%;">2015</span></span>
</span>
</td>
<td id="S5.T6.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.17.17.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2017</span></td>
<td id="S5.T6.1.17.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.17.17.4.1" class="ltx_text" style="font-size:90%;">0.6</span></td>
<td id="S5.T6.1.17.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.17.17.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">97.8</span></td>
<td id="S5.T6.1.17.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.17.17.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">97.2</span></td>
<td id="S5.T6.1.17.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.17.17.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">97.5</span></td>
<td id="S5.T6.1.17.17.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.17.17.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.17.17.8.1.1" class="ltx_p"><span id="S5.T6.1.17.17.8.1.1.1" class="ltx_text" style="font-size:80%;">YOLO (Section <a href="#S3.SS1.SSS1.Px4" title="YOLO ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.18.18" class="ltx_tr">
<td id="S5.T6.1.18.18.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.18.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.18.18.1.1.1" class="ltx_p"><span id="S5.T6.1.18.18.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sun et al.<span id="S5.T6.1.18.18.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.18.18.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.18.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.18.18.2.1.1" class="ltx_p"><span id="S5.T6.1.18.18.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T6.1.18.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.18.18.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2017</span></td>
<td id="S5.T6.1.18.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.18.18.4.1" class="ltx_text" style="font-size:90%;">0.6</span></td>
<td id="S5.T6.1.18.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.18.18.5.1" class="ltx_text" style="font-size:90%;">94.3</span></td>
<td id="S5.T6.1.18.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.18.18.6.1" class="ltx_text" style="font-size:90%;">95.6</span></td>
<td id="S5.T6.1.18.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.18.18.7.1" class="ltx_text" style="font-size:90%;">94.5</span></td>
<td id="S5.T6.1.18.18.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.18.18.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.18.18.8.1.1" class="ltx_p"><span id="S5.T6.1.18.18.8.1.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN (Section <a href="#S3.SS1.SSS1.Px2" title="Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.19.19" class="ltx_tr">
<td id="S5.T6.1.19.19.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.19.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.19.19.1.1.1" class="ltx_p"><span id="S5.T6.1.19.19.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">García et al.<span id="S5.T6.1.19.19.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.19.19.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.19.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.19.19.2.1.1" class="ltx_p"><span id="S5.T6.1.19.19.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T6.1.19.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.19.19.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2017</span></td>
<td id="S5.T6.1.19.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.19.19.4.1" class="ltx_text" style="font-size:90%;">0.6</span></td>
<td id="S5.T6.1.19.19.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.19.19.5.1" class="ltx_text" style="font-size:90%;">92.0</span></td>
<td id="S5.T6.1.19.19.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.19.19.6.1" class="ltx_text" style="font-size:90%;">87.0</span></td>
<td id="S5.T6.1.19.19.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.19.19.7.1" class="ltx_text" style="font-size:90%;">89.0</span></td>
<td id="S5.T6.1.19.19.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.19.19.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.19.19.8.1.1" class="ltx_p"><span id="S5.T6.1.19.19.8.1.1.1" class="ltx_text" style="font-size:80%;">Retina Net (Section <a href="#S3.SS1.SSS1.Px5" title="Mask R-CNN, YOLO, SSD and Retina Net ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.20.20" class="ltx_tr">
<td id="S5.T6.1.20.20.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.20.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.20.20.1.1.1" class="ltx_p"><span id="S5.T6.1.20.20.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CDeC-Net<span id="S5.T6.1.20.20.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.20.20.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.20.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.20.20.2.1.1" class="ltx_p"><span id="S5.T6.1.20.20.2.1.1.1" class="ltx_text" style="font-size:90%;">2020</span></span>
</span>
</td>
<td id="S5.T6.1.20.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.20.20.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2017</span></td>
<td id="S5.T6.1.20.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.20.20.4.1" class="ltx_text" style="font-size:90%;">0.6</span></td>
<td id="S5.T6.1.20.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.20.20.5.1" class="ltx_text" style="font-size:90%;">89.9</span></td>
<td id="S5.T6.1.20.20.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.20.20.6.1" class="ltx_text" style="font-size:90%;">96.9</span></td>
<td id="S5.T6.1.20.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.20.20.7.1" class="ltx_text" style="font-size:90%;">93.4</span></td>
<td id="S5.T6.1.20.20.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.20.20.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.20.20.8.1.1" class="ltx_p"><span id="S5.T6.1.20.20.8.1.1.1" class="ltx_text" style="font-size:80%;">Cascade Mask R-CNN 
<br class="ltx_break">(Section <a href="#S3.SS1.SSS1.Px6" title="Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.21.21" class="ltx_tr">
<td id="S5.T6.1.21.21.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span id="S5.T6.1.21.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.21.21.1.1.1" class="ltx_p"><span id="S5.T6.1.21.21.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CascadeTabNet<span id="S5.T6.1.21.21.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.21.21.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span id="S5.T6.1.21.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.21.21.2.1.1" class="ltx_p"><span id="S5.T6.1.21.21.2.1.1.1" class="ltx_text" style="font-size:90%;">2020</span></span>
</span>
</td>
<td id="S5.T6.1.21.21.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.21.21.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2019</span></td>
<td id="S5.T6.1.21.21.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.21.21.4.1" class="ltx_text" style="font-size:90%;">0.6</span></td>
<td id="S5.T6.1.21.21.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.21.21.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T6.1.21.21.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.21.21.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T6.1.21.21.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T6.1.21.21.7.1" class="ltx_text" style="font-size:90%;">94.3</span></td>
<td id="S5.T6.1.21.21.8" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T6.1.21.21.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.21.21.8.1.1" class="ltx_p"><span id="S5.T6.1.21.21.8.1.1.1" class="ltx_text" style="font-size:80%;">Cascade Mask R-CNN 
<br class="ltx_break">(Section <a href="#S3.SS1.SSS1.Px6" title="Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.22.22" class="ltx_tr">
<td id="S5.T6.1.22.22.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.22.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.22.22.1.1.1" class="ltx_p"><span id="S5.T6.1.22.22.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CDeC-Net<span id="S5.T6.1.22.22.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.22.22.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.22.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.22.22.2.1.1" class="ltx_p"><span id="S5.T6.1.22.22.2.1.1.1" class="ltx_text" style="font-size:90%;">2020</span></span>
</span>
</td>
<td id="S5.T6.1.22.22.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.22.22.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2019</span></td>
<td id="S5.T6.1.22.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.22.22.4.1" class="ltx_text" style="font-size:90%;">0.6</span></td>
<td id="S5.T6.1.22.22.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.22.22.5.1" class="ltx_text" style="font-size:90%;">93.9</span></td>
<td id="S5.T6.1.22.22.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.22.22.6.1" class="ltx_text" style="font-size:90%;">98.0</span></td>
<td id="S5.T6.1.22.22.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.22.22.7.1" class="ltx_text" style="font-size:90%;">95.9</span></td>
<td id="S5.T6.1.22.22.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.22.22.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.22.22.8.1.1" class="ltx_p"><span id="S5.T6.1.22.22.8.1.1.1" class="ltx_text" style="font-size:80%;">Cascade Mask R-CNN 
<br class="ltx_break">(Section <a href="#S3.SS1.SSS1.Px6" title="Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.23.23" class="ltx_tr">
<td id="S5.T6.1.23.23.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.23.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.23.23.1.1.1" class="ltx_p"><span id="S5.T6.1.23.23.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GTE<span id="S5.T6.1.23.23.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.23.23.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T6.1.23.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.23.23.2.1.1" class="ltx_p"><span id="S5.T6.1.23.23.2.1.1.1" class="ltx_text" style="font-size:90%;">2021</span></span>
</span>
</td>
<td id="S5.T6.1.23.23.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.23.23.3.1" class="ltx_text" style="font-size:80%;">ICDAR-2019</span></td>
<td id="S5.T6.1.23.23.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.23.23.4.1" class="ltx_text" style="font-size:90%;">0.8</span></td>
<td id="S5.T6.1.23.23.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.23.23.5.1" class="ltx_text" style="font-size:90%;">96.0</span></td>
<td id="S5.T6.1.23.23.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.23.23.6.1" class="ltx_text" style="font-size:90%;">95.0</span></td>
<td id="S5.T6.1.23.23.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.23.23.7.1" class="ltx_text" style="font-size:90%;">95.5</span></td>
<td id="S5.T6.1.23.23.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T6.1.23.23.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.23.23.8.1.1" class="ltx_p"><span id="S5.T6.1.23.23.8.1.1.1" class="ltx_text" style="font-size:80%;">Object Detection (Section <a href="#S3.SS1.SSS1.Px6" title="Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T6.1.24.24" class="ltx_tr">
<td id="S5.T6.1.24.24.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt">
<span id="S5.T6.1.24.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.24.24.1.1.1" class="ltx_p"><span id="S5.T6.1.24.24.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Riba et al.<span id="S5.T6.1.24.24.1.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite></span></span></span>
</span>
</td>
<td id="S5.T6.1.24.24.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt">
<span id="S5.T6.1.24.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.24.24.2.1.1" class="ltx_p"><span id="S5.T6.1.24.24.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T6.1.24.24.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S5.T6.1.24.24.3.1" class="ltx_text" style="font-size:80%;">RVL-CDIP</span></td>
<td id="S5.T6.1.24.24.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S5.T6.1.24.24.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T6.1.24.24.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S5.T6.1.24.24.5.1" class="ltx_text" style="font-size:90%;">15.2</span></td>
<td id="S5.T6.1.24.24.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S5.T6.1.24.24.6.1" class="ltx_text" style="font-size:90%;">36.5</span></td>
<td id="S5.T6.1.24.24.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S5.T6.1.24.24.7.1" class="ltx_text" style="font-size:90%;">21.5</span></td>
<td id="S5.T6.1.24.24.8" class="ltx_td ltx_align_justify ltx_border_b ltx_border_tt">
<span id="S5.T6.1.24.24.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.24.24.8.1.1" class="ltx_p"><span id="S5.T6.1.24.24.8.1.1.1" class="ltx_text" style="font-size:80%;">Graph Neural Network (Section <a href="#S3.SS1.SSS3" title="III-A3 Graph Neural Networks ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>3</span></a>)</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F14" class="ltx_figure"><img src="/html/2104.14272/assets/tbl_str_precise.jpg" id="S5.F14.g1" class="ltx_graphics ltx_img_landscape" width="598" height="344" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Example of precision in reference to the task of table structural segmentation. Green color represents the ground truth whereas the red color depicts the predicted bounding boxes. For simplicity, precision for detection of rows and columns are shown separately. The IOU threshold in the shown examples is considered as 0.5.</figcaption>
</figure>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>Table Structural Segmentation Performance. Outstanding results are highlighted. Results in the last two rows are not directly comparable with other methods because PDF files are employed instead of document images. </figcaption>
<table id="S5.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T7.1.2.1" class="ltx_tr">
<td id="S5.T7.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T7.1.2.1.1.1" class="ltx_text ltx_font_bold">Literature</span></td>
<td id="S5.T7.1.2.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" rowspan="2">
<span id="S5.T7.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.2.1.2.1.1" class="ltx_p"><span id="S5.T7.1.2.1.2.1.1.1" class="ltx_text ltx_font_bold">Year</span></span>
</span>
</td>
<td id="S5.T7.1.2.1.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" rowspan="2">
<span id="S5.T7.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.2.1.3.1.1" class="ltx_p"><span id="S5.T7.1.2.1.3.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</td>
<td id="S5.T7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T7.1.2.1.4.1" class="ltx_text ltx_font_bold">IOU</span></td>
<td id="S5.T7.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S5.T7.1.2.1.5.1" class="ltx_text ltx_font_bold">Average Row/Column</span></td>
<td id="S5.T7.1.2.1.6" class="ltx_td ltx_align_justify ltx_border_t" rowspan="2">
<span id="S5.T7.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.2.1.6.1.1" class="ltx_p"><span id="S5.T7.1.2.1.6.1.1.1" class="ltx_text ltx_font_bold">Method</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.1.3.2" class="ltx_tr">
<td id="S5.T7.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.3.2.1.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S5.T7.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.3.2.2.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S5.T7.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.3.2.3.1" class="ltx_text ltx_font_bold">F-Measure</span></td>
</tr>
<tr id="S5.T7.1.4.3" class="ltx_tr">
<td id="S5.T7.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T7.1.4.3.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">DeepDeSRT<span id="S5.T7.1.4.3.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite></span></span></td>
<td id="S5.T7.1.4.3.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.4.3.2.1.1" class="ltx_p"><span id="S5.T7.1.4.3.2.1.1.1" class="ltx_text" style="font-size:90%;">2017</span></span>
</span>
</td>
<td id="S5.T7.1.4.3.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.4.3.3.1.1" class="ltx_p"><span id="S5.T7.1.4.3.3.1.1.1" class="ltx_text" style="font-size:90%;">ICDAR-2013</span></span>
</span>
</td>
<td id="S5.T7.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.4.3.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T7.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.4.3.5.1" class="ltx_text" style="font-size:90%;">95.9</span></td>
<td id="S5.T7.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.4.3.6.1" class="ltx_text" style="font-size:90%;">87.3</span></td>
<td id="S5.T7.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.4.3.7.1" class="ltx_text" style="font-size:90%;">91.4</span></td>
<td id="S5.T7.1.4.3.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T7.1.4.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.4.3.8.1.1" class="ltx_p"><span id="S5.T7.1.4.3.8.1.1.1" class="ltx_text" style="font-size:80%;">Fully Convolutional Networks (Section <a href="#S3.SS2.SSS1.Px1" title="Fully Convolutional Networks ‣ III-B1 Semantic Image Segmentation ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.1.5.4" class="ltx_tr">
<td id="S5.T7.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T7.1.5.4.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">DeepTabStR<span id="S5.T7.1.5.4.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite></span></span></td>
<td id="S5.T7.1.5.4.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.5.4.2.1.1" class="ltx_p"><span id="S5.T7.1.5.4.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T7.1.5.4.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.5.4.3.1.1" class="ltx_p"><span id="S5.T7.1.5.4.3.1.1.1" class="ltx_text" style="font-size:90%;">ICDAR-2013</span></span>
</span>
</td>
<td id="S5.T7.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.5.4.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T7.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.5.4.5.1" class="ltx_text" style="font-size:90%;">93.1</span></td>
<td id="S5.T7.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.5.4.6.1" class="ltx_text" style="font-size:90%;">93.0</span></td>
<td id="S5.T7.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.5.4.7.1" class="ltx_text" style="font-size:90%;">92.9</span></td>
<td id="S5.T7.1.5.4.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T7.1.5.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.5.4.8.1.1" class="ltx_p"><span id="S5.T7.1.5.4.8.1.1.1" class="ltx_text" style="font-size:80%;">Deformable Convolutions (Section <a href="#S3.SS2.SSS4" title="III-B4 Deformable and Dilated Convolutions ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>4</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.1.1" class="ltx_tr">
<td id="S5.T7.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T7.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ReS<sup id="S5.T7.1.1.1.1.1" class="ltx_sup"><span id="S5.T7.1.1.1.1.1.1" class="ltx_text ltx_font_upright">2</span></sup>TIM<span id="S5.T7.1.1.1.1.2" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite></span></span></td>
<td id="S5.T7.1.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.1.2.1.1" class="ltx_p"><span id="S5.T7.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T7.1.1.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.1.3.1.1" class="ltx_p"><span id="S5.T7.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">ICDAR-2013</span></span>
</span>
</td>
<td id="S5.T7.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.1.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T7.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.1.5.1" class="ltx_text" style="font-size:90%;">92.6</span></td>
<td id="S5.T7.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.1.6.1" class="ltx_text" style="font-size:90%;">44.7</span></td>
<td id="S5.T7.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.1.7.1" class="ltx_text" style="font-size:90%;">60.3</span></td>
<td id="S5.T7.1.1.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T7.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.1.8.1.1" class="ltx_p"><span id="S5.T7.1.1.8.1.1.1" class="ltx_text" style="font-size:80%;">Distance based weight technique (Section <a href="#S3.SS2.SSS2.Px1" title="Distance Based Weights ‣ III-B2 Graph Neural Networks ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>2</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.1.6.5" class="ltx_tr">
<td id="S5.T7.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T7.1.6.5.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Siddiqui et al.<span id="S5.T7.1.6.5.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite></span></span></td>
<td id="S5.T7.1.6.5.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.6.5.2.1.1" class="ltx_p"><span id="S5.T7.1.6.5.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T7.1.6.5.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.6.5.3.1.1" class="ltx_p"><span id="S5.T7.1.6.5.3.1.1.1" class="ltx_text" style="font-size:90%;">ICDAR-2013</span></span>
</span>
</td>
<td id="S5.T7.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.6.5.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T7.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.6.5.5.1" class="ltx_text" style="font-size:90%;">92.5</span></td>
<td id="S5.T7.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.6.5.6.1" class="ltx_text" style="font-size:90%;">92.7</span></td>
<td id="S5.T7.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.6.5.7.1" class="ltx_text" style="font-size:90%;">92.3</span></td>
<td id="S5.T7.1.6.5.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T7.1.6.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.6.5.8.1.1" class="ltx_p"><span id="S5.T7.1.6.5.8.1.1.1" class="ltx_text" style="font-size:80%;">Fully Convolutional Networks (Section <a href="#S3.SS2.SSS1.Px1" title="Fully Convolutional Networks ‣ III-B1 Semantic Image Segmentation ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.1.7.6" class="ltx_tr">
<td id="S5.T7.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T7.1.7.6.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Khan et al.<span id="S5.T7.1.7.6.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite></span></span></td>
<td id="S5.T7.1.7.6.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.7.6.2.1.1" class="ltx_p"><span id="S5.T7.1.7.6.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T7.1.7.6.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.7.6.3.1.1" class="ltx_p"><span id="S5.T7.1.7.6.3.1.1.1" class="ltx_text" style="font-size:90%;">ICDAR-2013</span></span>
</span>
</td>
<td id="S5.T7.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.7.6.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T7.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.7.6.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">96.9</span></td>
<td id="S5.T7.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.7.6.6.1" class="ltx_text" style="font-size:90%;">90.1</span></td>
<td id="S5.T7.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.7.6.7.1" class="ltx_text" style="font-size:90%;">93.3</span></td>
<td id="S5.T7.1.7.6.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T7.1.7.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.7.6.8.1.1" class="ltx_p"><span id="S5.T7.1.7.6.8.1.1.1" class="ltx_text" style="font-size:80%;">Bi-directional Recurrent Neural Networks (Section <a href="#S3.SS2.SSS3" title="III-B3 Recurrent Neural Networks ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>3</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.1.8.7" class="ltx_tr">
<td id="S5.T7.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T7.1.8.7.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TableNet<span id="S5.T7.1.8.7.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite></span></span></td>
<td id="S5.T7.1.8.7.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.8.7.2.1.1" class="ltx_p"><span id="S5.T7.1.8.7.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T7.1.8.7.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.8.7.3.1.1" class="ltx_p"><span id="S5.T7.1.8.7.3.1.1.1" class="ltx_text" style="font-size:90%;">ICDAR-2013</span></span>
</span>
</td>
<td id="S5.T7.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.8.7.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T7.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.8.7.5.1" class="ltx_text" style="font-size:90%;">92.1</span></td>
<td id="S5.T7.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.8.7.6.1" class="ltx_text" style="font-size:90%;">89.9</span></td>
<td id="S5.T7.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.8.7.7.1" class="ltx_text" style="font-size:90%;">90.1</span></td>
<td id="S5.T7.1.8.7.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T7.1.8.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.8.7.8.1.1" class="ltx_p"><span id="S5.T7.1.8.7.8.1.1.1" class="ltx_text" style="font-size:80%;">Fully Convolutional Networks (Section <a href="#S3.SS2.SSS1.Px1" title="Fully Convolutional Networks ‣ III-B1 Semantic Image Segmentation ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>1</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.1.9.8" class="ltx_tr">
<td id="S5.T7.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T7.1.9.8.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Hashmi et al.<span id="S5.T7.1.9.8.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite></span></span></td>
<td id="S5.T7.1.9.8.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.9.8.2.1.1" class="ltx_p"><span id="S5.T7.1.9.8.2.1.1.1" class="ltx_text" style="font-size:90%;">2021</span></span>
</span>
</td>
<td id="S5.T7.1.9.8.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.9.8.3.1.1" class="ltx_p"><span id="S5.T7.1.9.8.3.1.1.1" class="ltx_text" style="font-size:90%;">ICDAR-2013</span></span>
</span>
</td>
<td id="S5.T7.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.9.8.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T7.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.9.8.5.1" class="ltx_text" style="font-size:90%;">95.4</span></td>
<td id="S5.T7.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.9.8.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">95.6</span></td>
<td id="S5.T7.1.9.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.9.8.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">95.5</span></td>
<td id="S5.T7.1.9.8.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T7.1.9.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.9.8.8.1.1" class="ltx_p"><span id="S5.T7.1.9.8.8.1.1.1" class="ltx_text" style="font-size:80%;">Object Detection (Section <a href="#S3.SS2.SSS5" title="III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>5</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.1.10.9" class="ltx_tr">
<td id="S5.T7.1.10.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T7.1.10.9.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Raja et al.<span id="S5.T7.1.10.9.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite></span></span></td>
<td id="S5.T7.1.10.9.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.10.9.2.1.1" class="ltx_p"><span id="S5.T7.1.10.9.2.1.1.1" class="ltx_text" style="font-size:90%;">2020</span></span>
</span>
</td>
<td id="S5.T7.1.10.9.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T7.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.10.9.3.1.1" class="ltx_p"><span id="S5.T7.1.10.9.3.1.1.1" class="ltx_text" style="font-size:90%;">ICDAR-2013</span></span>
</span>
</td>
<td id="S5.T7.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.10.9.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T7.1.10.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.10.9.5.1" class="ltx_text" style="font-size:90%;">92.7</span></td>
<td id="S5.T7.1.10.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.10.9.6.1" class="ltx_text" style="font-size:90%;">91.1</span></td>
<td id="S5.T7.1.10.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.10.9.7.1" class="ltx_text" style="font-size:90%;">91.9</span></td>
<td id="S5.T7.1.10.9.8" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T7.1.10.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.10.9.8.1.1" class="ltx_p"><span id="S5.T7.1.10.9.8.1.1.1" class="ltx_text" style="font-size:80%;">Object Detection (Section <a href="#S3.SS2.SSS5" title="III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>5</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.1.11.10" class="ltx_tr">
<td id="S5.T7.1.11.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S5.T7.1.11.10.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Tensmeyer et al. <span id="S5.T7.1.11.10.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite></span></span></td>
<td id="S5.T7.1.11.10.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span id="S5.T7.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.11.10.2.1.1" class="ltx_p"><span id="S5.T7.1.11.10.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T7.1.11.10.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span id="S5.T7.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.11.10.3.1.1" class="ltx_p"><span id="S5.T7.1.11.10.3.1.1.1" class="ltx_text" style="font-size:90%;">ICDAR-2013</span></span>
</span>
</td>
<td id="S5.T7.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T7.1.11.10.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T7.1.11.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T7.1.11.10.5.1" class="ltx_text" style="font-size:90%;">95.8</span></td>
<td id="S5.T7.1.11.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T7.1.11.10.6.1" class="ltx_text" style="font-size:90%;">94.6</span></td>
<td id="S5.T7.1.11.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T7.1.11.10.7.1" class="ltx_text" style="font-size:90%;">95.2</span></td>
<td id="S5.T7.1.11.10.8" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T7.1.11.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.11.10.8.1.1" class="ltx_p"><span id="S5.T7.1.11.10.8.1.1.1" class="ltx_text" style="font-size:80%;">Dilated Convolutions (Section <a href="#S3.SS2.SSS4" title="III-B4 Deformable and Dilated Convolutions ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>4</span></a>)</span></span>
</span>
</td>
</tr>
<tr id="S5.T7.1.12.11" class="ltx_tr">
<td id="S5.T7.1.12.11.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T7.1.12.11.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GraphTSR<span id="S5.T7.1.12.11.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite></span></span></td>
<td id="S5.T7.1.12.11.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T7.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.12.11.2.1.1" class="ltx_p"><span id="S5.T7.1.12.11.2.1.1.1" class="ltx_text" style="font-size:90%;">2019</span></span>
</span>
</td>
<td id="S5.T7.1.12.11.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T7.1.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.12.11.3.1.1" class="ltx_p"><span id="S5.T7.1.12.11.3.1.1.1" class="ltx_text" style="font-size:90%;">ICDAR-2013</span></span>
</span>
</td>
<td id="S5.T7.1.12.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T7.1.12.11.4.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="S5.T7.1.12.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T7.1.12.11.5.1" class="ltx_text" style="font-size:90%;">88.5</span></td>
<td id="S5.T7.1.12.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T7.1.12.11.6.1" class="ltx_text" style="font-size:90%;">86.0</span></td>
<td id="S5.T7.1.12.11.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T7.1.12.11.7.1" class="ltx_text" style="font-size:90%;">87.2</span></td>
<td id="S5.T7.1.12.11.8" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S5.T7.1.12.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T7.1.12.11.8.1.1" class="ltx_p"><span id="S5.T7.1.12.11.8.1.1.1" class="ltx_text" style="font-size:80%;">Fully Convolutional Networks (Section <a href="#S3.SS2.SSS1.Px1" title="Fully Convolutional Networks ‣ III-B1 Semantic Image Segmentation ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>1</span></a>)</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VIII: </span>Table Structural Segmentation Performance on the dataset of ICDAR-2019 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>. For brevity and clarity, these results are separately presented in this table.</figcaption>
<table id="S5.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T8.1.1.1" class="ltx_tr">
<th id="S5.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T8.1.1.1.1.1" class="ltx_text ltx_font_bold">Literature</span>
</th>
<th id="S5.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T8.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S5.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S5.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T8.1.1.1.4.1" class="ltx_inline-block">
<span id="S5.T8.1.1.1.4.1.1" class="ltx_p"><span id="S5.T8.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold"></span></span>
<span id="S5.T8.1.1.1.4.1.2" class="ltx_p"><span id="S5.T8.1.1.1.4.1.2.1" class="ltx_text ltx_font_bold">F-Measure</span></span>
<span id="S5.T8.1.1.1.4.1.3" class="ltx_p"><span id="S5.T8.1.1.1.4.1.3.1" class="ltx_text ltx_font_bold">(0.6)</span></span>
</span>
</th>
<th id="S5.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T8.1.1.1.5.1" class="ltx_inline-block">
<span id="S5.T8.1.1.1.5.1.1" class="ltx_p"><span id="S5.T8.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold"></span></span>
<span id="S5.T8.1.1.1.5.1.2" class="ltx_p"><span id="S5.T8.1.1.1.5.1.2.1" class="ltx_text ltx_font_bold">F-Measure</span></span>
<span id="S5.T8.1.1.1.5.1.3" class="ltx_p"><span id="S5.T8.1.1.1.5.1.3.1" class="ltx_text ltx_font_bold">(0.8)</span></span>
</span>
</th>
<th id="S5.T8.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S5.T8.1.1.1.6.1" class="ltx_text ltx_font_bold">Method</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T8.1.2.1" class="ltx_tr">
<td id="S5.T8.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.2.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CascadeTabNet<span id="S5.T8.1.2.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></span></span></td>
<td id="S5.T8.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.2.1.2.1" class="ltx_text" style="font-size:90%;">2020</span></td>
<td id="S5.T8.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.2.1.3.1" class="ltx_text" style="font-size:90%;">ICDAR-2019</span></td>
<td id="S5.T8.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">43.8</span></td>
<td id="S5.T8.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">19.0</span></td>
<td id="S5.T8.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T8.1.2.1.6.1" class="ltx_text" style="font-size:80%;">Object detection (Section <a href="#S3.SS2.SSS5" title="III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>5</span></a>)</span></td>
</tr>
<tr id="S5.T8.1.3.2" class="ltx_tr">
<td id="S5.T8.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.3.2.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GTE<span id="S5.T8.1.3.2.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></span></span></td>
<td id="S5.T8.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.3.2.2.1" class="ltx_text" style="font-size:90%;">2021</span></td>
<td id="S5.T8.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.3.2.3.1" class="ltx_text" style="font-size:90%;">ICDAR-2019</span></td>
<td id="S5.T8.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.3.2.4.1" class="ltx_text" style="font-size:90%;">38.5</span></td>
<td id="S5.T8.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.3.2.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T8.1.3.2.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T8.1.3.2.6.1" class="ltx_text" style="font-size:80%;">Object detection (Section <a href="#S3.SS2.SSS5" title="III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>5</span></a>)</span></td>
</tr>
<tr id="S5.T8.1.4.3" class="ltx_tr">
<td id="S5.T8.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T8.1.4.3.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Zou et al.<span id="S5.T8.1.4.3.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite></span></span></td>
<td id="S5.T8.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T8.1.4.3.2.1" class="ltx_text" style="font-size:90%;">2020</span></td>
<td id="S5.T8.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T8.1.4.3.3.1" class="ltx_text" style="font-size:90%;">ICDAR-2019</span></td>
<td id="S5.T8.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T8.1.4.3.4.1" class="ltx_text" style="font-size:90%;">13.1</span></td>
<td id="S5.T8.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.1</td>
<td id="S5.T8.1.4.3.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_t"><span id="S5.T8.1.4.3.6.1" class="ltx_text" style="font-size:80%;">Fully Convolutional Networks (Section <a href="#S3.SS2.SSS1.Px1" title="Fully Convolutional Networks ‣ III-B1 Semantic Image Segmentation ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>1</span></a>)</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Another method by Qasim <span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> which is explained in Section <a href="#S3.SS1.SSS3" title="III-A3 Graph Neural Networks ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>3</span></a> did not use any well known dataset to evaluate their approach. However, they have tested their approach on the synthetic dataset by using two types of graph neural networks which are <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>. Along with the graph neural networks, a fully convolutional neural network was used to conduct a fair comparison. After an exhaustive evaluation, the fusion of graph neural network and the convolutional neural network has surpassed all the other methods with a perfect matching accuracy of 96.9. The approach which uses only graph neural networks has delivered perfect matching accuracy of 65.6, which still exceeds the accuracy of the method using only fully convolutional neural networks.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Evaluations for Table Structural Segmentation</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The task of table structural segmentation is evaluated based on how accurate the rows or columns of the tables are separated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Figure <a href="#S5.F14" title="Figure 14 ‣ V-B Evaluations for Table Detection ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> illustrates the meaning of an imprecise and precise prediction for both of the tasks of the row and column detections. Table <a href="#S5.T7" title="TABLE VII ‣ V-B Evaluations for Table Detection ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> summarizes the performance comparison of numerous approaches which have executed the task of table structural segmentation on the ICDAR 2013 table competition dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. The results with the highest accuracies are highlighted in the table. It is important to mention that apart from the methods mentioned in Table <a href="#S5.T7" title="TABLE VII ‣ V-B Evaluations for Table Detection ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>, there are two other approaches which are discussed in section <a href="#S3.SS2" title="III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>. We could not incorporate their results in Table <a href="#S5.T7" title="TABLE VII ‣ V-B Evaluations for Table Detection ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> because the approaches are neither evaluated on any standard dataset nor utilized the standard evaluation metrics. However, their results are explained in the following paragraph.</p>
</div>
<figure id="S5.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IX: </span>Table Recognition Performance. Results mentioned in this table are not directly comparable with each other because different datasets and evaluation metrics have been used.</figcaption>
<table id="S5.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T9.1.1.1" class="ltx_tr">
<th id="S5.T9.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T9.1.1.1.1.1" class="ltx_text ltx_font_bold">Literature</span>
</th>
<th id="S5.T9.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T9.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S5.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T9.1.1.1.3.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S5.T9.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T9.1.1.1.4.1" class="ltx_text ltx_font_bold">Evaluation Metric</span></th>
<th id="S5.T9.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T9.1.1.1.5.1" class="ltx_text ltx_font_bold">Score</span></th>
<th id="S5.T9.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S5.T9.1.1.1.6.1" class="ltx_text ltx_font_bold">Method</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T9.1.2.1" class="ltx_tr">
<td id="S5.T9.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.1.2.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Deng et al.<span id="S5.T9.1.2.1.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite></span></span></td>
<td id="S5.T9.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.1.2.1.2.1" class="ltx_text" style="font-size:90%;">2019</span></td>
<td id="S5.T9.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.1.2.1.3.1" class="ltx_text" style="font-size:90%;">TABLE2LATEX-450K</span></td>
<td id="S5.T9.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.1.2.1.4.1" class="ltx_text" style="font-size:90%;">BLEU</span></td>
<td id="S5.T9.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.1.2.1.5.1" class="ltx_text" style="font-size:90%;">40.3</span></td>
<td id="S5.T9.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T9.1.2.1.6.1" class="ltx_text" style="font-size:80%;">Encoder Decoder Network (Section <a href="#S3.SS3.SSS2" title="III-C2 Encoder Decoder Network ‣ III-C Table Recognition ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span>2</span></a>)</span></td>
</tr>
<tr id="S5.T9.1.3.2" class="ltx_tr">
<td id="S5.T9.1.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T9.1.3.2.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Zhong et al.<span id="S5.T9.1.3.2.1.1.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></span></span></td>
<td id="S5.T9.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T9.1.3.2.2.1" class="ltx_text" style="font-size:90%;">2020</span></td>
<td id="S5.T9.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T9.1.3.2.3.1" class="ltx_text" style="font-size:90%;">PubTabNet</span></td>
<td id="S5.T9.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T9.1.3.2.4.1" class="ltx_text" style="font-size:90%;">TEDS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></span></td>
<td id="S5.T9.1.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T9.1.3.2.5.1" class="ltx_text" style="font-size:90%;">88.3</span></td>
<td id="S5.T9.1.3.2.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_t"><span id="S5.T9.1.3.2.6.1" class="ltx_text" style="font-size:80%;">Encoder Dual Decoder Model (Section <a href="#S3.SS3.SSS1" title="III-C1 Encoder-Dual-Decoder ‣ III-C Table Recognition ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span>1</span></a>)</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The creators of the <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_italic">TableBank</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> have proposed baseline model for table structure segmentation along with table detection. To examine the performance of their baseline model for table structure recognition on <span id="S5.SS3.p2.1.2" class="ltx_text ltx_font_italic">TableBank</span> dataset, they have employed the 4-gram BLEU score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> as the evaluation metric. The result shows that when their Image-to-Text model is trained on the Word+Latex dataset, it gives the BLEU score of 0.7382 and also generalizes better in all the cases.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.5.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.6.2" class="ltx_text ltx_font_italic">Evaluations for Table Recognition</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Table recognition consists of both segmenting the structure of tables and extracting the information from the cells. In this section, we will present the evaluations of the couple of approaches that are discussed above in Section <a href="#S3.SS3" title="III-C Table Recognition ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">In the study of challenges in end-to-end neural scientific table recognition, the author Deng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> have tested their image-to-text model on the TABLE2LATEX-450K dataset. The model obtained <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="32.40\%" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mrow id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml"><mn id="S5.SS4.p2.1.m1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.2.cmml">32.40</mn><mo id="S5.SS4.p2.1.m1.1.1.1" xref="S5.SS4.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><apply id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS4.p2.1.m1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS4.p2.1.m1.1.1.2.cmml" xref="S5.SS4.p2.1.m1.1.1.2">32.40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">32.40\%</annotation></semantics></math> exact match accuracy with a BLEU score of 40.33. The authors have also examined the model that how well it identifies the structure of the table. It has been concluded that the model encounters problems in case of complex structures having multi-column (rows).</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Another research by <span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_italic">Zhong et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> has also carried out experiments on the task of table recognition. To evaluate the observations, they have come up with their own evaluation metric called TEDS in which the similarity is calculated using the same tree edit distance proposed by <span id="S5.SS4.p3.1.2" class="ltx_text ltx_font_italic">Pawlik et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>. Their Encoder-Dual-Decoder (EDD) model has beaten all the other baseline models with the TEDS score of <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="88.3\%" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mrow id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml"><mn id="S5.SS4.p3.1.m1.1.1.2" xref="S5.SS4.p3.1.m1.1.1.2.cmml">88.3</mn><mo id="S5.SS4.p3.1.m1.1.1.1" xref="S5.SS4.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><apply id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS4.p3.1.m1.1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS4.p3.1.m1.1.1.2.cmml" xref="S5.SS4.p3.1.m1.1.1.2">88.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">88.3\%</annotation></semantics></math> on the PubTabNet dataset.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">The results of both of the discussed methods are summarized in Table <a href="#S5.T9" title="TABLE IX ‣ V-C Evaluations for Table Structural Segmentation ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IX</span></a>. It is important to mention that the presented approaches are not directly comparable to each other because of the disparate datasets and evaluation metrics utilized in these techniques.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Table analysis is a crucial and well-studied problem in the area of document analysis community. The exploitation of deep learning concepts have remarkably revolutionized the problem of table understanding and has set new standards. In this review paper, we have discussed some recent contemporary procedures that have applied the notions of deep learning to progress the task of information extraction from tables in document images. In Section <a href="#S3" title="III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we have explained the approaches that have exploited deep learning to perform table detection, structure segmentation, and recognition. Figure <a href="#S3.F5" title="Figure 5 ‣ Fully Convolutional Networks ‣ III-A2 Semantic Image Segmentation ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Figure <a href="#S3.F7" title="Figure 7 ‣ Fully Convolutional Networks ‣ III-B1 Semantic Image Segmentation ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrate the most and least famous adopted methods for table detection and structure segmentation respectively. We have summarized all the publicly available datasets along with their access information in Table <a href="#S4.T5" title="TABLE V ‣ IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In Tables <a href="#S5.T6" title="TABLE VI ‣ V-B Evaluations for Table Detection ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, <a href="#S5.T7" title="TABLE VII ‣ V-B Evaluations for Table Detection ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> and <a href="#S5.T9" title="TABLE IX ‣ V-C Evaluations for Table Structural Segmentation ‣ V Evaluation ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IX</span></a>, we have provided an exhaustive performance comparison of the discussed approaches on various datasets. We have discussed that state-of-the-art methods for table detection on well known publicly available datasets have achieved near perfection results. Once the tabular area is detected, there comes a task of structural segmentation of tables and table recognition subsequently. After examining several recent approaches, we believe that there is still room left for improvement in both of these areas.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Future Work</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">While analyzing and comparing miscellaneous methodologies, we have noticed some aspects that should be highlighted so they can be taken care of in future works. For table detection, one of the most exploited evaluation metrics is IOU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. The majority of approaches that are discussed in this paper have compared their methods with the previous state-of-the-art methods on the basis of precision, recall, and F-measure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>. These three metrics are calculated on a specific IOU threshold established by the authors. We strongly believe that the threshold value for IOU needs to be standardized in order to have an impartial comparison. Another important factor that we have seen missing in several research papers is about mentioning the performance time while comparing different methods. In a few cases, semantic segmentation is proven to outperform other methods for table structure segmentation in terms of accuracy. However, the description about execution time is not evident.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">So far, traditional approaches have been exploited to detect tables from the camera-captured document images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. The power of deep learning methods could be leveraged to improve the state-of-the-art table analysis systems in this domain. Deep learning leverages huge datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Recently, large publicly available datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> have been published that provide annotations not only for the table structure extraction but also for table detection. We expect that these contemporary datasets will be tested. The results of table segmentation and recognition methods can be further enhanced by exploiting the blend of various deep learning concepts with recently published datasets. To the best of our knowledge, reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>, <a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> has not been investigated in the domain of table analysis but some work exists for information extraction from document images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>. Nonetheless, it is an exciting and promising future direction for table detection and recognition as well.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Sarawagi, <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Information extraction</em>.   Now Publishers Inc, 2008.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Shahab, F. Shafait, T. Kieninger, and A. Dengel, “An open approach towards
the benchmarking of table structure recognition systems,” in
<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 9th IAPR International Workshop on Document Analysis
Systems</em>, 2010, pp. 113–120.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Fang, X. Tao, Z. Tang, R. Qiu, and Y. Liu, “Dataset, ground-truth and
performance metrics for table detection evaluation,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2012 10th IAPR
International Workshop on Document Analysis Systems</em>.   IEEE, 2012, pp. 445–449.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y.-S. Kim and K.-H. Lee, “Extracting logical structures from html tables,”
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computer Standards &amp; Interfaces</em>, vol. 30, no. 5, pp. 296–308, 2008.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H.-H. Chen, S.-C. Tsai, and J.-H. Tsai, “Mining tables from large scale html
texts,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">COLING 2000 Volume 1: The 18th International Conference on
Computational Linguistics</em>, 2000.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
H. Masuda, S. Tsukamoto, S. Yasutomi, and H. Nakagawa, “Recognition of html
table structure,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">The First International Joint Conference on
Natural Language Processing (IJCNLP-04)</em>, 2004, pp. 183–188.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C.-Y. Tyan, H. K. Huang, and T. Niki, “Generator for document with html tagged
table having data elements which preserve layout relationships of information
in bitmap image of original document,” Apr. 6 1999, uS Patent 5,893,127.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
T. Kieninger and A. Dengel, “A paper-to-html table converting system,” in
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of document analysis systems (DAS)</em>, vol. 98, 1998, pp.
356–365.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T. G. Kieninger, “Table structure recognition based on robust block
segmentation,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Document Recognition V</em>, vol. 3305.   International Society for Optics and Photonics, 1998,
pp. 22–32.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T. Kieninger and A. Dengel, “Applying the t-recs table recognition system to
the business letter domain,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of Sixth International
Conference on Document Analysis and Recognition</em>.   IEEE, 2001, pp. 518–522.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
F. Cesarini, S. Marinai, L. Sarti, and G. Soda, “Trainable table location in
document images,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Object recognition supported by user interaction
for service robots</em>, vol. 3.   IEEE,
2002, pp. 236–240.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. C. e Silva, “Learning rich hidden markov models in document analysis: Table
location,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2009 10th International Conference on Document Analysis
and Recognition</em>.   IEEE, 2009, pp.
843–847.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. Cortes and V. Vapnik, “Support-vector networks,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Machine learning</em>,
vol. 20, no. 3, pp. 273–297, 1995.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. Kasar, P. Barlas, S. Adam, C. Chatelain, and T. Paquet, “Learning to detect
tables in scanned document images using line information,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2013
12th International Conference on Document Analysis and Recognition</em>.   IEEE, 2013, pp. 1185–1189.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Fan and D. S. Kim, “Detecting table region in pdf documents using distant
supervision,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1506.08891</em>, 2015.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D. N. Tran, T. A. Tran, A. Oh, S. H. Kim, and I. S. Na, “Table detection from
document image using vertical arrangement of text blocks,”
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International Journal of Contents</em>, vol. 11, no. 4, pp. 77–85, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y. Wang, I. T. Phillips, and R. M. Haralick, “Table structure understanding
and its performance evaluation,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Pattern recognition</em>, vol. 37, no. 7,
pp. 1479–1497, 2004.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
G. NAGY, “Hierarchical representation of optically scanned documents,” in
<em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proc. 7th Int. Conf. Pattern Recognition</em>, 1984, pp. 347–349.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A. Shigarov, A. Mikhailov, and A. Altaev, “Configurable table structure
recognition in untagged pdf documents,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 ACM
Symposium on Document Engineering</em>, 2016, pp. 119–122.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with
deep convolutional neural networks,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, vol. 25, pp. 1097–1105, 2012.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
W. Seo, H. I. Koo, and N. I. Cho, “Junction-based table detection in
camera-captured document images,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">International Journal on Document
Analysis and Recognition (IJDAR)</em>, vol. 18, no. 1, pp. 47–57, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
E. R. Dougherty, <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Electronic imaging technology</em>.   SPIE Press, 1999, vol. 60.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. C. Handley, “Table analysis for multiline cell identification,” in
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Document Recognition and Retrieval VIII</em>, vol. 4307.   International Society for Optics and Photonics, 2000,
pp. 34–43.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
D. Lopresti and G. Nagy, “Automated table processing: An (opinionated)
survey,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third IAPR Workshop on Graphics
Recognition</em>, 1999, pp. 109–134.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
——, “A tabular survey of automated table processing,” in
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Workshop on Graphics Recognition</em>.   Springer, 1999, pp. 93–120.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
R. Zanibbi, D. Blostein, and J. R. Cordy, “A survey of table recognition,”
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Document Analysis and Recognition</em>, vol. 7, no. 1, pp. 1–16, 2004.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
D. W. Embley, M. Hurst, D. Lopresti, and G. Nagy, “Table-processing paradigms:
a research survey,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International Journal of Document Analysis and
Recognition (IJDAR)</em>, vol. 8, no. 2-3, pp. 66–86, 2006.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M. F. Hurst, “The interpretation of tables in texts,” Ph.D. dissertation,
University of Edinburgh, 2000.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. C. e Silva, A. M. Jorge, and L. Torgo, “Design of an end-to-end method to
extract information from tables,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International Journal of Document
Analysis and Recognition (IJDAR)</em>, vol. 8, no. 2-3, pp. 144–171, 2006.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
B. Coüasnon and A. Lemaitre, “Recognition of tables and forms,” 2014.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Khusro, A. Latif, and I. Ullah, “On methods and tools of table detection,
extraction and annotation in pdf documents,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Journal of Information
Science</em>, vol. 41, no. 1, pp. 41–57, 2015.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
X. Zhong, E. ShafieiBavani, and A. J. Yepes, “Image-based table recognition:
data, model, and evaluation,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.10683</em>, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. Hu, R. S. Kashi, D. Lopresti, and G. T. Wilfong, “Evaluating the
performance of table processing algorithms,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">International Journal on
Document Analysis and Recognition</em>, vol. 4, no. 3, pp. 140–153, 2002.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
L. Hao, L. Gao, X. Yi, and Z. Tang, “A table detection method for pdf
documents based on convolutional neural networks,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">2016 12th IAPR
Workshop on Document Analysis Systems (DAS)</em>.   IEEE, 2016, pp. 287–292.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
L. Torrey and J. Shavlik, “Transfer learning,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Handbook of research
on machine learning applications and trends: algorithms, methods, and
techniques</em>.   IGI global, 2010, pp.
242–264.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Y. Zhu, Y. Chen, Z. Lu, S. Pan, G.-R. Xue, Y. Yu, and Q. Yang, “Heterogeneous
transfer learning for image classification,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
AAAI Conference on Artificial Intelligence</em>, vol. 25, no. 1, 2011.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
B. Kulis, K. Saenko, and T. Darrell, “What you saw is not what you get: Domain
adaptation using asymmetric kernel transforms,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">CVPR 2011</em>.   IEEE, 2011, pp. 1785–1792.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
C. Wang and S. Mahadevan, “Heterogeneous domain adaptation using manifold
alignment,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">IJCAI proceedings-international joint conference on
artificial intelligence</em>, vol. 22, no. 1, 2011, p. 1541.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
W. Li, L. Duan, D. Xu, and I. W. Tsang, “Learning with augmented features for
supervised and semi-supervised heterogeneous domain adaptation,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE
transactions on pattern analysis and machine intelligence</em>, vol. 36, no. 6,
pp. 1134–1148, 2013.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
M. Loey, F. Smarandache, and N. E. M Khalifa, “Within the lack of chest
covid-19 x-ray dataset: a novel detection model based on gan and deep
transfer learning,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Symmetry</em>, vol. 12, no. 4, p. 651, 2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. Z. Afzal, S. Capobianco, M. I. Malik, S. Marinai, T. M. Breuel, A. Dengel,
and M. Liwicki, “Deepdocclassifier: Document classification with deep
convolutional neural network,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">2015 13th international conference
on document analysis and recognition (ICDAR)</em>.   IEEE, 2015, pp. 1111–1115.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A. Das, S. Roy, U. Bhattacharya, and S. K. Parui, “Document image
classification with intra-domain transfer learning and stacked generalization
of deep convolutional neural networks,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">2018 24th International
Conference on Pattern Recognition (ICPR)</em>.   IEEE, 2018, pp. 3180–3185.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
A. Gilani, S. R. Qasim, I. Malik, and F. Shafait, “Table detection using deep
learning,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">2017 14th IAPR international conference on document
analysis and recognition (ICDAR)</em>, vol. 1.   IEEE, 2017, pp. 771–776.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
S. Schreiber, S. Agne, I. Wolf, A. Dengel, and S. Ahmed, “Deepdesrt: Deep
learning for detection and structure recognition of tables in document
images,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">2017 14th IAPR international conference on document
analysis and recognition (ICDAR)</em>, vol. 1.   IEEE, 2017, pp. 1162–1167.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
S. A. Siddiqui, M. I. Malik, S. Agne, A. Dengel, and S. Ahmed, “Decnt: Deep
deformable cnn for table detection,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 6, pp.
74 151–74 161, 2018.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Y. Huang, Q. Yan, Y. Li, Y. Chen, X. Wang, L. Gao, and Z. Tang, “A yolo-based
table detection method,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document
Analysis and Recognition (ICDAR)</em>.   IEEE, 2019, pp. 813–818.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
D. Prasad, A. Gadpal, K. Kapadni, M. Visave, and K. Sultanpure,
“Cascadetabnet: An approach for end to end table detection and structure
recognition from image-based documents,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em>,
2020, pp. 572–573.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
M. Agarwal, A. Mondal, and C. Jawahar, “Cdec-net: Composite deformable cascade
network for table detection in document images,” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2008.10831</em>, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
S. A. Siddiqui, I. A. Fateh, S. T. R. Rizvi, A. Dengel, and S. Ahmed,
“Deeptabstr: Deep learning based table structure recognition,” in
<em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document Analysis and Recognition
(ICDAR)</em>.   IEEE, 2019, pp. 1403–1409.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
K. A. Hashmi, D. Stricker, M. Liwicki, M. N. Afzal, and M. Z. Afzal, “Guided
table structure recognition through anchor optimization,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2104.10538</em>, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
X. Zheng, D. Burdick, L. Popa, X. Zhong, and N. X. R. Wang, “Global table
extractor (gte): A framework for joint table identification and cell
structure recognition using visual context,” in <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2021, pp.
697–706.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
S. Raja, A. Mondal, and C. Jawahar, “Table structure recognition using
top-down and bottom-up cues,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer
Vision</em>.   Springer, 2020, pp. 70–86.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
R. Girshick, “Fast r-cnn,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</em>, 2015, pp. 1440–1448.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1506.01497</em>, 2015.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
H. Breu, J. Gil, D. Kirkpatrick, and M. Werman, “Linear time euclidean
distance transform algorithms,” <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis
and Machine Intelligence</em>, vol. 17, no. 5, pp. 529–533, 1995.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
R. Fabbri, L. D. F. Costa, J. C. Torelli, and O. M. Bruno, “2d euclidean
distance transform algorithms: A comparative survey,” <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">ACM Computing
Surveys (CSUR)</em>, vol. 40, no. 1, pp. 1–44, 2008.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
I. Ragnemalm, “The euclidean distance transform in arbitrary dimensions,”
<em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition Letters</em>, vol. 14, no. 11, pp. 883–888, 1993.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional
networks,” in <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>.   Springer, 2014, pp. 818–833.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.1556</em>, 2014.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, “The
pascal visual object classes (voc) challenge,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">International journal
of computer vision</em>, vol. 88, no. 2, pp. 303–338, 2010.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
M. Li, L. Cui, S. Huang, F. Wei, M. Zhou, and Z. Li, “Tablebank: Table
benchmark for image-based table detection and recognition,” in
<em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 12th Language Resources and Evaluation Conference</em>,
2020, pp. 1918–1925.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
N. Sun, Y. Zhu, and X. Hu, “Faster r-cnn based table detection combining
corner locating,” in <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document
Analysis and Recognition (ICDAR)</em>.   IEEE, 2019, pp. 1314–1319.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
L. Gao, X. Yi, Z. Jiang, L. Hao, and Z. Tang, “Icdar2017 competition on page
object detection,” in <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">2017 14th IAPR International Conference on
Document Analysis and Recognition (ICDAR)</em>, vol. 1.   IEEE, 2017, pp. 1417–1422.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable
convolutional networks,” in <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</em>, 2017, pp. 764–773.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
M. Göbel, T. Hassan, E. Oro, and G. Orsi, “Icdar 2013 table competition,”
in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">2013 12th International Conference on Document Analysis and
Recognition</em>.   IEEE, 2013, pp.
1449–1453.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Z. Cai and N. Vasconcelos, “Cascade r-cnn: Delving into high quality object
detection,” in <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision
and pattern recognition</em>, 2018, pp. 6154–6162.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” in <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2017, pp. 1492–1500.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Y. Liu, Y. Wang, S. Wang, T. Liang, Q. Zhao, Z. Tang, and H. Ling, “Cbnet: A
novel composite backbone network architecture for object detection,” in
<em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>,
vol. 34, no. 07, 2020, pp. 11 653–11 660.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2016, pp. 779–788.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1804.02767</em>, 2018.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Á. Casado-García, C. Domínguez, J. Heras, E. Mata, and
V. Pascual, “The benefits of close-domain fine-tuning for table detection in
document images,” in <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">International Workshop on Document Analysis
Systems</em>.   Springer, 2020, pp.
199–215.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in
<em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>,
2017, pp. 2961–2969.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Y. Deng, A. Kanervisto, and A. M. Rush, “What you get is what you see: A
visual markup decompiler,” <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.04938</em>, vol. 10,
pp. 32–37, 2016.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">European conference on
computer vision</em>.   Springer, 2016, pp.
21–37.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
dense object detection,” in <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</em>, 2017, pp. 2980–2988.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan,
X. Wang <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Deep high-resolution representation learning for
visual recognition,” <em id="bib.bib77.2.2" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, 2020.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
S. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and P. H. Torr,
“Res2net: A new multi-scale backbone architecture,” <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">IEEE transactions
on pattern analysis and machine intelligence</em>, 2019.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi,
W. Ouyang <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Hybrid task cascade for instance segmentation,”
in <em id="bib.bib79.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</em>, 2019, pp. 4974–4983.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
L. Gao, Y. Huang, H. Déjean, J.-L. Meunier, Q. Yan, Y. Fang, F. Kleber, and
E. Lang, “Icdar 2019 competition on table detection and recognition
(ctdar),” in <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document Analysis and
Recognition (ICDAR)</em>.   IEEE, 2019, pp.
1510–1515.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
I. Kavasidis, S. Palazzo, C. Spampinato, C. Pino, D. Giordano, D. Giuffrida,
and P. Messina, “A saliency-based convolutional neural network for table and
chart detection in digitized documents,” <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1804.06236</em>, 2018.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
F. Yu and V. Koltun, “Multi-scale context aggregation by dilated convolutions
international conference on learning representations (iclr) 2016,” 2016.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
P. Krähenbühl and V. Koltun, “Efficient inference in fully connected
crfs with gaussian edge potentials,” <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, vol. 24, pp. 109–117, 2011.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
S. S. Paliwal, D. Vishwanath, R. Rahul, M. Sharma, and L. Vig, “Tablenet: Deep
learning model for end-to-end table detection and tabular data extraction
from scanned document images,” in <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on
Document Analysis and Recognition (ICDAR)</em>.   IEEE, 2019, pp. 128–133.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
semantic segmentation,” in <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on
computer vision and pattern recognition</em>, 2015, pp. 3431–3440.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
M. Holeček, A. Hoskovec, P. Baudiš, and P. Klinger, “Table
understanding in structured documents,” in <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">2019 International
Conference on Document Analysis and Recognition Workshops (ICDARW)</em>,
vol. 5.   IEEE, 2019, pp. 158–164.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
P. Riba, A. Dutta, L. Goldmann, A. Fornés, O. Ramos, and J. Lladós,
“Table detection in invoice documents by graph neural networks,” in
<em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document Analysis and Recognition
(ICDAR)</em>.   IEEE, 2019, pp. 122–127.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Y. Li, L. Gao, Z. Tang, Q. Yan, and Y. Huang, “A gan-based feature generator
for table detection,” in <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document
Analysis and Recognition (ICDAR)</em>.   IEEE, 2019, pp. 763–768.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
A. W. Harley, A. Ufkes, and K. G. Derpanis, “Evaluation of deep convolutional
nets for document image classification and retrieval,” in <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">2015 13th
International Conference on Document Analysis and Recognition (ICDAR)</em>.   IEEE, 2015, pp. 991–995.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, “Generative adversarial networks,” <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1406.2661</em>, 2014.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for
biomedical image segmentation,” in <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">International Conference on Medical
image computing and computer-assisted intervention</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
S. A. Siddiqui, P. I. Khan, A. Dengel, and S. Ahmed, “Rethinking semantic
segmentation for table structure recognition in documents,” in <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">2019
International Conference on Document Analysis and Recognition (ICDAR)</em>.   IEEE, 2019, pp. 1397–1402.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Imagenet large scale
visual recognition challenge,” <em id="bib.bib93.2.2" class="ltx_emph ltx_font_italic">International journal of computer
vision</em>, vol. 115, no. 3, pp. 211–252, 2015.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Y. Zou and J. Ma, “A deep semantic segmentation model for image-based table
structure recognition,” in <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">2020 15th IEEE International Conference on
Signal Processing (ICSP)</em>, vol. 1.   IEEE, 2020, pp. 274–280.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
M. B. Dillencourt, H. Samet, and M. Tamminen, “A general approach to
connected-component labeling for arbitrary image representations,”
<em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Journal of the ACM (JACM)</em>, vol. 39, no. 2, pp. 253–280, 1992.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
S. R. Qasim, H. Mahmood, and F. Shafait, “Rethinking table recognition using
graph neural networks,” in <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document
Analysis and Recognition (ICDAR)</em>.   IEEE, 2019, pp. 142–147.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, “The
graph neural network model,” <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks</em>,
vol. 20, no. 1, pp. 61–80, 2008.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Z. Chi, H. Huang, H.-D. Xu, H. Yu, W. Yin, and X.-L. Mao, “Complicated table
structure recognition,” <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.04729</em>, 2019.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
W. Xue, Q. Li, and D. Tao, “Res2tim: reconstruct syntactic structures from
table images,” in <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document Analysis
and Recognition (ICDAR)</em>.   IEEE, 2019,
pp. 749–755.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
W. Xue, Q. Li, Z. Zhang, Y. Zhao, and H. Wang, “Table analysis and information
extraction for medical laboratory reports,” in <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">2018 IEEE 16th Intl
Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on
Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence
and Computing and Cyber Science and Technology Congress
(DASC/PiCom/DataCom/CyberSciTech)</em>.   IEEE, 2018, pp. 193–199.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
C. Tensmeyer, V. I. Morariu, B. Price, S. Cohen, and T. Martinez, “Deep
splitting and merging for table structure decomposition,” in <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">2019
International Conference on Document Analysis and Recognition (ICDAR)</em>.   IEEE, 2019, pp. 114–121.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
S. A. Khan, S. M. D. Khalid, M. A. Shahzad, and F. Shafait, “Table structure
extraction with bi-directional gated recurrent unit networks,” in <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">2019
International Conference on Document Analysis and Recognition (ICDAR)</em>.   IEEE, 2019, pp. 1366–1371.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of gated
recurrent neural networks on sequence modeling,” <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1412.3555</em>, 2014.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Neural
computation</em>, vol. 9, no. 8, pp. 1735–1780, 1997.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush, “Opennmt: Open-source
toolkit for neural machine translation,” <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1701.02810</em>, 2017.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
J. Wang, K. Chen, S. Yang, C. C. Loy, and D. Lin, “Region proposal by guided
anchoring,” in <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</em>, 2019, pp. 2965–2974.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
F. Yu and V. Koltun, “Multi-scale context aggregation by dilated
convolutions,” <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1511.07122</em>, 2015.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
T. N. Kipf and M. Welling, “Semi-supervised classification with graph
convolutional networks,” <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.02907</em>, 2016.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Y. Deng, D. Rosenberg, and G. Mann, “Challenges in end-to-end neural
scientific table recognition,” in <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on
Document Analysis and Recognition (ICDAR)</em>.   IEEE, 2019, pp. 894–901.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Y. Deng, A. Kanervisto, J. Ling, and A. M. Rush, “Image-to-markup generation
with coarse-to-fine attention,” in <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>.   PMLR, 2017, pp. 980–989.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
S. F. Rashid, A. Akmal, M. Adnan, A. A. Aslam, and A. Dengel, “Table
recognition in heterogeneous documents using machine learning,” in
<em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">2017 14th IAPR International Conference on Document Analysis and
Recognition (ICDAR)</em>, vol. 1.   IEEE,
2017, pp. 777–782.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
I. Phillips, “User’s reference manual for the uw english/technical document
image database iii,” <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">UW-III English/Technical Document Image Database
Manual</em>, 1996.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
A. Mondal, P. Lipps, and C. Jawahar, “Iiit-ar-13k: a new dataset for graphical
object detection in documents,” in <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">International Workshop on Document
Analysis Systems</em>.   Springer, 2020, pp.
216–230.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
D. M. Powers, “Evaluation: from precision, recall and f-measure to roc,
informedness, markedness and correlation,” <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2010.16061</em>, 2020.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
M. B. Blaschko and C. H. Lampert, “Learning to localize objects with
structured output regression,” in <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">European conference on computer
vision</em>.   Springer, 2008, pp. 2–15.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic
evaluation of machine translation,” in <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual
meeting of the Association for Computational Linguistics</em>, 2002, pp.
311–318.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
D. G. Kleinbaum, K. Dietz, M. Gail, M. Klein, and M. Klein, <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Logistic
regression</em>.   Springer, 2002.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon,
“Dynamic graph cnn for learning on point clouds,” <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">Acm Transactions On
Graphics (tog)</em>, vol. 38, no. 5, pp. 1–12, 2019.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
S. R. Qasim, J. Kieseler, Y. Iiyama, and M. Pierini, “Learning representations
of irregular particle-detector geometry with distance-weighted graph
networks,” <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">The European Physical Journal C</em>, vol. 79, no. 7, pp.
1–11, 2019.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
M. Pawlik and N. Augsten, “Tree edit distance: Robust and memory-efficient,”
<em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">Information Systems</em>, vol. 56, pp. 157–173, 2016.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Human-level control through deep reinforcement learning,” <em id="bib.bib121.2.2" class="ltx_emph ltx_font_italic">nature</em>,
vol. 518, no. 7540, pp. 529–533, 2015.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
A. G. Barto and R. S. Sutton, “Reinforcement learning,” <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">Handbook of
brain theory and neural networks</em>, pp. 804–809, 1995.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
J. Park, E. Lee, Y. Kim, I. Kang, H. I. Koo, and N. I. Cho, “Multi-lingual
optical character recognition system using the reinforcement learning of
character segmenter,” <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp. 174 437–174 448,
2020.

</span>
</li>
</ul>
</section>
<div id="p4" class="ltx_para">
<span id="p4.1" class="ltx_ERROR undefined">\EOD</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.14271" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.14272" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.14272">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.14272" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.14273" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 02:57:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
