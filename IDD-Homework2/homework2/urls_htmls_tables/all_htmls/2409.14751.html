<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.14751] UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection</title><meta property="og:description" content="4D millimeter-wave (MMW) radar, which provides both height information and dense point cloud data over 3D MMW radar, has become increasingly popular in 3D object detection. In recent years, radar-vision fusion models h…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.14751">

<!--Generated on Sun Oct  6 01:50:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id6.6.6" class="ltx_text" style="font-size:90%;">Haocheng Zhao,<sup id="id6.6.6.1" class="ltx_sup"><span id="id6.6.6.1.1" class="ltx_text ltx_font_italic">1234†</span></sup>,
Runwei Guan<sup id="id6.6.6.2" class="ltx_sup"><span id="id6.6.6.2.1" class="ltx_text ltx_font_italic">1234†</span></sup>,
Taoyu Wu<sup id="id6.6.6.3" class="ltx_sup"><span id="id6.6.6.3.1" class="ltx_text ltx_font_italic">134</span></sup>,Ka Lok Man<sup id="id6.6.6.4" class="ltx_sup"><span id="id6.6.6.4.1" class="ltx_text ltx_font_italic">3</span></sup>,
Limin Yu<sup id="id6.6.6.5" class="ltx_sup"><span id="id6.6.6.5.1" class="ltx_text ltx_font_italic">3⋆</span></sup>,
Yutao Yue<sup id="id6.6.6.6" class="ltx_sup"><span id="id6.6.6.6.1" class="ltx_text ltx_font_italic">514⋆</span></sup>






</span>
</span><span class="ltx_author_notes"><sup id="id13.13.id1" class="ltx_sup"><span id="id13.13.id1.1" class="ltx_text" style="font-size:90%;">⋆</span></sup><span id="id14.14.id2" class="ltx_text" style="font-size:90%;">Corresponding author.</span><sup id="id15.15.id1" class="ltx_sup"><span id="id15.15.id1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">1</span></sup><span id="id16.16.id2" class="ltx_text" style="font-size:90%;"> Institute of Deep Perception Technology, JITRI, Wuxi, China</span><sup id="id17.17.id1" class="ltx_sup"><span id="id17.17.id1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2</span></sup><span id="id18.18.id2" class="ltx_text" style="font-size:90%;"> Department of Electrical Engineering and Electronics, University of Liverpool, Liverpool, UK</span><sup id="id19.19.id1" class="ltx_sup"><span id="id19.19.id1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">3</span></sup><span id="id20.20.id2" class="ltx_text" style="font-size:90%;"> Department of School of Advanced Technology, Xi’an Jiaotong-Liverpool University, Suzhou, China</span><sup id="id21.21.id1" class="ltx_sup"><span id="id21.21.id1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">4</span></sup><span id="id22.22.id2" class="ltx_text" style="font-size:90%;"> XJTLU-JITRI Academy of Technology, Xi’an Jiaotong-Liverpool University, Suzhou, China</span><sup id="id23.23.id1" class="ltx_sup"><span id="id23.23.id1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">5</span></sup><span id="id24.24.id2" class="ltx_text" style="font-size:90%;"> The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id25.id1" class="ltx_p">4D millimeter-wave (MMW) radar, which provides both height information and dense point cloud data over 3D MMW radar, has become increasingly popular in 3D object detection. In recent years, radar-vision fusion models have demonstrated performance close to that of LiDAR-based models, offering advantages in terms of lower hardware costs and better resilience in extreme conditions. However, many radar-vision fusion models treat radar as a sparse LiDAR, underutilizing radar-specific information. Additionally, these multi-modal networks are often sensitive to the failure of a single modality, particularly vision. To address these challenges, we propose the Radar Depth Lift-Splat-Shoot (RDL) module, which integrates radar-specific data into the depth prediction process, enhancing the quality of visual Bird’s-Eye View (BEV) features. We further introduce a Unified Feature Fusion (UFF) approach that extracts BEV features across different modalities using shared module. To assess the robustness of multi-modal models, we develop a novel Failure Test (FT) ablation experiment, which simulates vision modality failure by injecting Gaussian noise. We conduct extensive experiments on the View-of-Delft (VoD) and TJ4D datasets. The results demonstrate that our proposed Unified BEVFusion (UniBEVFusion) network significantly outperforms state-of-the-art models on the TJ4D dataset, with improvements of 1.44 in 3D and 1.72 in BEV object detection accuracy.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Millimeter-wave (MMW) radar is widely used in roadside and vehicle-mounted transportation applications due to its reliable distance and velocity detection capabilities, even under extreme weather conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. However, the sparse nature of radar point cloud data and the lack of height information have posed challenges for accurate 3D object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. With recent advancements in 4D MMW radar technology, there is growing interest in utilizing this radar for 3D object detection, either as a standalone radar modality or fused with cameras <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Radar-vision fusion has been shown to reduce hardware costs, enhance performance in extreme conditions, and maintain reasonable 3D object detection accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In vision-based 3D object detection, a widely adopted approach is to project 2D image features into a Bird’s-Eye View (BEV) using intrinsic and extrinsic camera parameters along with accurate depth prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, a well-known LiDAR-Vision fusion model, provides an efficient architecture for fusing multi-modal data, improving upon methods like Lift-Splat-Shoot (LSS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and pooling through optimizations and parallelization. Additionally, BEVFusion uses point cloud coordinates to assist with depth prediction, which is crucial for maintaining stability and accuracy in the model. Our reproduction shows competitive results in the radar-vision datasets, and our proposed UniBEVFusion network further improves the design.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, in recent researches, radar has often been treated as a sparse LiDAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and its specific characteristics are underutilized. A recent reproduction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> of BEVFusion in radar-vision performs even similar to the results of pure radar detection. We argue that radar data should be fully leveraged in fusion models, and radar-specific information should be integrated into the depth prediction process to improve overall model performance. To address this, we propose Radar Depth LSS (RDL), which incorporates additional radar data, such as Radar Cross-Section (RCS), into the depth prediction process to enhance detection accuracy.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Moreover, multi-modal networks are particularly vulnerable to the failure of a single modality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, especially visual data. These networks often rely heavily on existence of both radar and image inputs, and their performance can degrade significantly when one modality is damaged or in adverse environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. To evaluate the robustness of multi-modal models in such cases, we propose a novel ablation experiment called the Failure Test (FT), in which substantial noise is added to the visual input to simulate visual failure. As shown in our experiments, applying FT to BEVFusion results in a dramatic drop in performance, even below that of single-modal networks. To address this issue, we developed a novel multi-modal fusion module, Unified Feature Fusion (UFF), which unifies feature extraction and enhances features across different modalities to mitigate the impact of failure.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.14751/assets/overview.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="221" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Overview of the proposed UniBEVFusion network. The network consists of four main stages: Image, Radar, Fusion, and BEV. The Image and Radar stages are responsible for extracting BEV features from the image and radar, respectively. The Fusion stage is responsible for the fusion of the BEV features from the Image and Radar stages. The BEV stage is responsible for the final BEV feature extraction and 3D object detection head.</span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The contribution points of this paper are summarized as:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose the Radar Depth LSS (RDL) module, which integrates radar-specific information into the depth prediction process to improve the vision BEV feature transformation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose the novel fusion module Unified Feature Fusion (UFF) to extract features from different modalities and fuse them together.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We propose the novel Failure Test (FT) ablation experiment for multi-modal fusion in the case of near-failure of vision modality.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">LiDAR Point Cloud 3D Object Detection</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Point cloud-based 3D object detection has evolved significantly with point-based, projection-based, and voxel-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and PointNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> capture global spatial information from raw point clouds but are computationally intensive due to their two-stage structure. Projection-based methods reduce computation cost by projecting point clouds into three 2D feature map <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Voxel-based methods convert irregular point clouds into a regular voxel grid, reducing computational costs without sacrificing spatial feature resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Building on voxel grids, PointPillars further optimize the computation by using pillars-based instead of voxels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Point clouds provide accurate depth information, while images offer rich semantic information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Aligning these modalities is fundamental to fusion networks. Camera data can be projected into the 3D coordinate system using intrinsic and extrinsic parameters, facilitating fusion with LiDAR point clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. To balance speed and performance, a common approach is to project both image and point cloud features into the bird’s-eye view (BEV) coordinate system. BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> optimized the Lift-Splat-Shoot (LSS) pipeline and added point cloud projections to the camera coordinate system to aid in depth prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Our proposed UniBEVFusion builds upon BEVFusion, optimizing radar feature integration for radar-vision fusion.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Radar Point Cloud 3D Object Detection</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">With the development of 4D millimeter-wave radar and the availability of open datasets, more researchers have explored radar-based object detection. Early work treated radar point clouds as a sparse LiDAR-like data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, applying LiDAR object detection model such as PP-Radar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, reproduced BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Although promising, a significant gap remains between radar and LiDAR performance. Utilizing radar velocity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, radar coordination <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, novel network modules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, LiDAR distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, adding gate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and semantic alignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> can improve the performance of radar-based object detection.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In this paper, we focus on radar-vision feature fusion, which has shown promising results in recent studies. RADIANT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> proposed a multi-stage fusion, including feature and detection head. FUTR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> propose a modality-agnostic feature sampler to fuse radar, lidar, and camera. RCBEVDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> proposed an multi-head query-based method and a RCS-aware encoder that aligns BEV features using radar-specific information. RCFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> generates pseudo-images from radar data and improves model performance with orthogonal feature transformations. LXL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> enhances depth feature fusion by integrating radar and visual voxel features, achieving State-of-The-Art (SOTA) results on multiple radar-vision datasets. Our proposed UniBEVFusion will comparison the performance with these SOTA networks on the VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and TJ4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> datasets.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Overview</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Fig.<a href="#S1.F1" title="Figure 1 ‣ I INTRODUCTION ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overall architecture of our proposed UniBEVFusion network, which contains four main parts: Image, Radar, Fusion, and BEV. Image and radar stream are responsible for extracting BEV features from the image and radar, respectively. The fusion stage handle the fusion of the BEV feature from the image and radar stream. The BEV stage is responsible for the final BEV feature extraction and 3D object detection head.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Besides, the image encoder in image stream is a pre-trained swinTransformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which is used to extract features from the image. The radar encoder in radar stream and BEV stream is basically similar to PointPillar from the baseline of View-of-Delft (VoD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which use PillarFeatureNet, SECOND, and SECONDFPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. The 3D object detection head is a common 3D object detection head, which is used to predict the 3D bounding box and classification results.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Radar Depth Lift-Splat-Shoot (RDL)</span>
</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.14751/assets/radar.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Radar Depth Lift-Splat-Shoot (RDL) module.</span></figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">LSS is an important milestone in visual-based 3D object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, but relies on correct depth prediction and computation is inefficient. BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> provides a better optimzied LSS module, which gives projected point cloud as initial value of depth. Therefore, we inherit the View Transform module of BEVFusion and our RDL is based on this design.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">As shown in the Fig.<a href="#S3.F2" title="Figure 2 ‣ III-B Radar Depth Lift-Splat-Shoot (RDL) ‣ III Methodology ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we first extract the coordination and RCS information, and then concat them to the depth prediction module. In fact, at this stage, we performes a early fusion of radar data and visual features. The extra information of point cloud data on VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, TJ4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and common LiDAR are shown in Table <a href="#S3.T1" title="TABLE I ‣ III-B Radar Depth Lift-Splat-Shoot (RDL) ‣ III Methodology ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.3.4.1" class="ltx_tr" style="background-color:#EFEFEF;">
<th id="S3.T1.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S3.T1.3.4.1.1.1" class="ltx_text" style="background-color:#EFEFEF;">Sensor</span></th>
<th id="S3.T1.3.4.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.3.4.1.2.1" class="ltx_text" style="background-color:#EFEFEF;">Extra information</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Radar in VoD</th>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_left"><math id="S3.T1.1.1.1.m1.7" class="ltx_Math" alttext="x,\ y,\ z,\ RCS,\ V_{r},\ V_{r}^{\prime},\ t" display="inline"><semantics id="S3.T1.1.1.1.m1.7a"><mrow id="S3.T1.1.1.1.m1.7.7.3" xref="S3.T1.1.1.1.m1.7.7.4.cmml"><mi id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">x</mi><mo rspace="0.667em" id="S3.T1.1.1.1.m1.7.7.3.4" xref="S3.T1.1.1.1.m1.7.7.4.cmml">,</mo><mi id="S3.T1.1.1.1.m1.2.2" xref="S3.T1.1.1.1.m1.2.2.cmml">y</mi><mo rspace="0.667em" id="S3.T1.1.1.1.m1.7.7.3.5" xref="S3.T1.1.1.1.m1.7.7.4.cmml">,</mo><mi id="S3.T1.1.1.1.m1.3.3" xref="S3.T1.1.1.1.m1.3.3.cmml">z</mi><mo rspace="0.667em" id="S3.T1.1.1.1.m1.7.7.3.6" xref="S3.T1.1.1.1.m1.7.7.4.cmml">,</mo><mrow id="S3.T1.1.1.1.m1.5.5.1.1" xref="S3.T1.1.1.1.m1.5.5.1.1.cmml"><mi id="S3.T1.1.1.1.m1.5.5.1.1.2" xref="S3.T1.1.1.1.m1.5.5.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.T1.1.1.1.m1.5.5.1.1.1" xref="S3.T1.1.1.1.m1.5.5.1.1.1.cmml">​</mo><mi id="S3.T1.1.1.1.m1.5.5.1.1.3" xref="S3.T1.1.1.1.m1.5.5.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.T1.1.1.1.m1.5.5.1.1.1a" xref="S3.T1.1.1.1.m1.5.5.1.1.1.cmml">​</mo><mi id="S3.T1.1.1.1.m1.5.5.1.1.4" xref="S3.T1.1.1.1.m1.5.5.1.1.4.cmml">S</mi></mrow><mo rspace="0.667em" id="S3.T1.1.1.1.m1.7.7.3.7" xref="S3.T1.1.1.1.m1.7.7.4.cmml">,</mo><msub id="S3.T1.1.1.1.m1.6.6.2.2" xref="S3.T1.1.1.1.m1.6.6.2.2.cmml"><mi id="S3.T1.1.1.1.m1.6.6.2.2.2" xref="S3.T1.1.1.1.m1.6.6.2.2.2.cmml">V</mi><mi id="S3.T1.1.1.1.m1.6.6.2.2.3" xref="S3.T1.1.1.1.m1.6.6.2.2.3.cmml">r</mi></msub><mo rspace="0.667em" id="S3.T1.1.1.1.m1.7.7.3.8" xref="S3.T1.1.1.1.m1.7.7.4.cmml">,</mo><msubsup id="S3.T1.1.1.1.m1.7.7.3.3" xref="S3.T1.1.1.1.m1.7.7.3.3.cmml"><mi id="S3.T1.1.1.1.m1.7.7.3.3.2.2" xref="S3.T1.1.1.1.m1.7.7.3.3.2.2.cmml">V</mi><mi id="S3.T1.1.1.1.m1.7.7.3.3.2.3" xref="S3.T1.1.1.1.m1.7.7.3.3.2.3.cmml">r</mi><mo id="S3.T1.1.1.1.m1.7.7.3.3.3" xref="S3.T1.1.1.1.m1.7.7.3.3.3.cmml">′</mo></msubsup><mo rspace="0.667em" id="S3.T1.1.1.1.m1.7.7.3.9" xref="S3.T1.1.1.1.m1.7.7.4.cmml">,</mo><mi id="S3.T1.1.1.1.m1.4.4" xref="S3.T1.1.1.1.m1.4.4.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.7b"><list id="S3.T1.1.1.1.m1.7.7.4.cmml" xref="S3.T1.1.1.1.m1.7.7.3"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">𝑥</ci><ci id="S3.T1.1.1.1.m1.2.2.cmml" xref="S3.T1.1.1.1.m1.2.2">𝑦</ci><ci id="S3.T1.1.1.1.m1.3.3.cmml" xref="S3.T1.1.1.1.m1.3.3">𝑧</ci><apply id="S3.T1.1.1.1.m1.5.5.1.1.cmml" xref="S3.T1.1.1.1.m1.5.5.1.1"><times id="S3.T1.1.1.1.m1.5.5.1.1.1.cmml" xref="S3.T1.1.1.1.m1.5.5.1.1.1"></times><ci id="S3.T1.1.1.1.m1.5.5.1.1.2.cmml" xref="S3.T1.1.1.1.m1.5.5.1.1.2">𝑅</ci><ci id="S3.T1.1.1.1.m1.5.5.1.1.3.cmml" xref="S3.T1.1.1.1.m1.5.5.1.1.3">𝐶</ci><ci id="S3.T1.1.1.1.m1.5.5.1.1.4.cmml" xref="S3.T1.1.1.1.m1.5.5.1.1.4">𝑆</ci></apply><apply id="S3.T1.1.1.1.m1.6.6.2.2.cmml" xref="S3.T1.1.1.1.m1.6.6.2.2"><csymbol cd="ambiguous" id="S3.T1.1.1.1.m1.6.6.2.2.1.cmml" xref="S3.T1.1.1.1.m1.6.6.2.2">subscript</csymbol><ci id="S3.T1.1.1.1.m1.6.6.2.2.2.cmml" xref="S3.T1.1.1.1.m1.6.6.2.2.2">𝑉</ci><ci id="S3.T1.1.1.1.m1.6.6.2.2.3.cmml" xref="S3.T1.1.1.1.m1.6.6.2.2.3">𝑟</ci></apply><apply id="S3.T1.1.1.1.m1.7.7.3.3.cmml" xref="S3.T1.1.1.1.m1.7.7.3.3"><csymbol cd="ambiguous" id="S3.T1.1.1.1.m1.7.7.3.3.1.cmml" xref="S3.T1.1.1.1.m1.7.7.3.3">superscript</csymbol><apply id="S3.T1.1.1.1.m1.7.7.3.3.2.cmml" xref="S3.T1.1.1.1.m1.7.7.3.3"><csymbol cd="ambiguous" id="S3.T1.1.1.1.m1.7.7.3.3.2.1.cmml" xref="S3.T1.1.1.1.m1.7.7.3.3">subscript</csymbol><ci id="S3.T1.1.1.1.m1.7.7.3.3.2.2.cmml" xref="S3.T1.1.1.1.m1.7.7.3.3.2.2">𝑉</ci><ci id="S3.T1.1.1.1.m1.7.7.3.3.2.3.cmml" xref="S3.T1.1.1.1.m1.7.7.3.3.2.3">𝑟</ci></apply><ci id="S3.T1.1.1.1.m1.7.7.3.3.3.cmml" xref="S3.T1.1.1.1.m1.7.7.3.3.3">′</ci></apply><ci id="S3.T1.1.1.1.m1.4.4.cmml" xref="S3.T1.1.1.1.m1.4.4">𝑡</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.7c">x,\ y,\ z,\ RCS,\ V_{r},\ V_{r}^{\prime},\ t</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.2.2" class="ltx_tr">
<th id="S3.T1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Radar in TJ4D</th>
<td id="S3.T1.2.2.1" class="ltx_td ltx_align_left"><math id="S3.T1.2.2.1.m1.7" class="ltx_Math" alttext="x,\ y,\ z,\ R,\ RCS,\ \alpha,\ \beta" display="inline"><semantics id="S3.T1.2.2.1.m1.7a"><mrow id="S3.T1.2.2.1.m1.7.7.1" xref="S3.T1.2.2.1.m1.7.7.2.cmml"><mi id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml">x</mi><mo rspace="0.667em" id="S3.T1.2.2.1.m1.7.7.1.2" xref="S3.T1.2.2.1.m1.7.7.2.cmml">,</mo><mi id="S3.T1.2.2.1.m1.2.2" xref="S3.T1.2.2.1.m1.2.2.cmml">y</mi><mo rspace="0.667em" id="S3.T1.2.2.1.m1.7.7.1.3" xref="S3.T1.2.2.1.m1.7.7.2.cmml">,</mo><mi id="S3.T1.2.2.1.m1.3.3" xref="S3.T1.2.2.1.m1.3.3.cmml">z</mi><mo rspace="0.667em" id="S3.T1.2.2.1.m1.7.7.1.4" xref="S3.T1.2.2.1.m1.7.7.2.cmml">,</mo><mi id="S3.T1.2.2.1.m1.4.4" xref="S3.T1.2.2.1.m1.4.4.cmml">R</mi><mo rspace="0.667em" id="S3.T1.2.2.1.m1.7.7.1.5" xref="S3.T1.2.2.1.m1.7.7.2.cmml">,</mo><mrow id="S3.T1.2.2.1.m1.7.7.1.1" xref="S3.T1.2.2.1.m1.7.7.1.1.cmml"><mi id="S3.T1.2.2.1.m1.7.7.1.1.2" xref="S3.T1.2.2.1.m1.7.7.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.T1.2.2.1.m1.7.7.1.1.1" xref="S3.T1.2.2.1.m1.7.7.1.1.1.cmml">​</mo><mi id="S3.T1.2.2.1.m1.7.7.1.1.3" xref="S3.T1.2.2.1.m1.7.7.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.T1.2.2.1.m1.7.7.1.1.1a" xref="S3.T1.2.2.1.m1.7.7.1.1.1.cmml">​</mo><mi id="S3.T1.2.2.1.m1.7.7.1.1.4" xref="S3.T1.2.2.1.m1.7.7.1.1.4.cmml">S</mi></mrow><mo rspace="0.667em" id="S3.T1.2.2.1.m1.7.7.1.6" xref="S3.T1.2.2.1.m1.7.7.2.cmml">,</mo><mi id="S3.T1.2.2.1.m1.5.5" xref="S3.T1.2.2.1.m1.5.5.cmml">α</mi><mo rspace="0.667em" id="S3.T1.2.2.1.m1.7.7.1.7" xref="S3.T1.2.2.1.m1.7.7.2.cmml">,</mo><mi id="S3.T1.2.2.1.m1.6.6" xref="S3.T1.2.2.1.m1.6.6.cmml">β</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.7b"><list id="S3.T1.2.2.1.m1.7.7.2.cmml" xref="S3.T1.2.2.1.m1.7.7.1"><ci id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1">𝑥</ci><ci id="S3.T1.2.2.1.m1.2.2.cmml" xref="S3.T1.2.2.1.m1.2.2">𝑦</ci><ci id="S3.T1.2.2.1.m1.3.3.cmml" xref="S3.T1.2.2.1.m1.3.3">𝑧</ci><ci id="S3.T1.2.2.1.m1.4.4.cmml" xref="S3.T1.2.2.1.m1.4.4">𝑅</ci><apply id="S3.T1.2.2.1.m1.7.7.1.1.cmml" xref="S3.T1.2.2.1.m1.7.7.1.1"><times id="S3.T1.2.2.1.m1.7.7.1.1.1.cmml" xref="S3.T1.2.2.1.m1.7.7.1.1.1"></times><ci id="S3.T1.2.2.1.m1.7.7.1.1.2.cmml" xref="S3.T1.2.2.1.m1.7.7.1.1.2">𝑅</ci><ci id="S3.T1.2.2.1.m1.7.7.1.1.3.cmml" xref="S3.T1.2.2.1.m1.7.7.1.1.3">𝐶</ci><ci id="S3.T1.2.2.1.m1.7.7.1.1.4.cmml" xref="S3.T1.2.2.1.m1.7.7.1.1.4">𝑆</ci></apply><ci id="S3.T1.2.2.1.m1.5.5.cmml" xref="S3.T1.2.2.1.m1.5.5">𝛼</ci><ci id="S3.T1.2.2.1.m1.6.6.cmml" xref="S3.T1.2.2.1.m1.6.6">𝛽</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.7c">x,\ y,\ z,\ R,\ RCS,\ \alpha,\ \beta</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.3.3" class="ltx_tr">
<th id="S3.T1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">LiDAR</th>
<td id="S3.T1.3.3.1" class="ltx_td ltx_align_left ltx_border_b"><math id="S3.T1.3.3.1.m1.4" class="ltx_Math" alttext="x,\ y,\ z,\ \text{intensity}" display="inline"><semantics id="S3.T1.3.3.1.m1.4a"><mrow id="S3.T1.3.3.1.m1.4.5.2" xref="S3.T1.3.3.1.m1.4.5.1.cmml"><mi id="S3.T1.3.3.1.m1.1.1" xref="S3.T1.3.3.1.m1.1.1.cmml">x</mi><mo rspace="0.667em" id="S3.T1.3.3.1.m1.4.5.2.1" xref="S3.T1.3.3.1.m1.4.5.1.cmml">,</mo><mi id="S3.T1.3.3.1.m1.2.2" xref="S3.T1.3.3.1.m1.2.2.cmml">y</mi><mo rspace="0.667em" id="S3.T1.3.3.1.m1.4.5.2.2" xref="S3.T1.3.3.1.m1.4.5.1.cmml">,</mo><mi id="S3.T1.3.3.1.m1.3.3" xref="S3.T1.3.3.1.m1.3.3.cmml">z</mi><mo rspace="0.667em" id="S3.T1.3.3.1.m1.4.5.2.3" xref="S3.T1.3.3.1.m1.4.5.1.cmml">,</mo><mtext id="S3.T1.3.3.1.m1.4.4" xref="S3.T1.3.3.1.m1.4.4a.cmml">intensity</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.4b"><list id="S3.T1.3.3.1.m1.4.5.1.cmml" xref="S3.T1.3.3.1.m1.4.5.2"><ci id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1">𝑥</ci><ci id="S3.T1.3.3.1.m1.2.2.cmml" xref="S3.T1.3.3.1.m1.2.2">𝑦</ci><ci id="S3.T1.3.3.1.m1.3.3.cmml" xref="S3.T1.3.3.1.m1.3.3">𝑧</ci><ci id="S3.T1.3.3.1.m1.4.4a.cmml" xref="S3.T1.3.3.1.m1.4.4"><mtext id="S3.T1.3.3.1.m1.4.4.cmml" xref="S3.T1.3.3.1.m1.4.4">intensity</mtext></ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.4c">x,\ y,\ z,\ \text{intensity}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.19.8.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S3.T1.17.7" class="ltx_text" style="font-size:90%;">Extra information of different sensors. <math id="S3.T1.11.1.m1.3" class="ltx_Math" alttext="x,y,z" display="inline"><semantics id="S3.T1.11.1.m1.3b"><mrow id="S3.T1.11.1.m1.3.4.2" xref="S3.T1.11.1.m1.3.4.1.cmml"><mi id="S3.T1.11.1.m1.1.1" xref="S3.T1.11.1.m1.1.1.cmml">x</mi><mo id="S3.T1.11.1.m1.3.4.2.1" xref="S3.T1.11.1.m1.3.4.1.cmml">,</mo><mi id="S3.T1.11.1.m1.2.2" xref="S3.T1.11.1.m1.2.2.cmml">y</mi><mo id="S3.T1.11.1.m1.3.4.2.2" xref="S3.T1.11.1.m1.3.4.1.cmml">,</mo><mi id="S3.T1.11.1.m1.3.3" xref="S3.T1.11.1.m1.3.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.11.1.m1.3c"><list id="S3.T1.11.1.m1.3.4.1.cmml" xref="S3.T1.11.1.m1.3.4.2"><ci id="S3.T1.11.1.m1.1.1.cmml" xref="S3.T1.11.1.m1.1.1">𝑥</ci><ci id="S3.T1.11.1.m1.2.2.cmml" xref="S3.T1.11.1.m1.2.2">𝑦</ci><ci id="S3.T1.11.1.m1.3.3.cmml" xref="S3.T1.11.1.m1.3.3">𝑧</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.1.m1.3d">x,y,z</annotation></semantics></math> are the coordinate information, <math id="S3.T1.12.2.m2.1" class="ltx_Math" alttext="RCS" display="inline"><semantics id="S3.T1.12.2.m2.1b"><mrow id="S3.T1.12.2.m2.1.1" xref="S3.T1.12.2.m2.1.1.cmml"><mi id="S3.T1.12.2.m2.1.1.2" xref="S3.T1.12.2.m2.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.T1.12.2.m2.1.1.1" xref="S3.T1.12.2.m2.1.1.1.cmml">​</mo><mi id="S3.T1.12.2.m2.1.1.3" xref="S3.T1.12.2.m2.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.T1.12.2.m2.1.1.1b" xref="S3.T1.12.2.m2.1.1.1.cmml">​</mo><mi id="S3.T1.12.2.m2.1.1.4" xref="S3.T1.12.2.m2.1.1.4.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.12.2.m2.1c"><apply id="S3.T1.12.2.m2.1.1.cmml" xref="S3.T1.12.2.m2.1.1"><times id="S3.T1.12.2.m2.1.1.1.cmml" xref="S3.T1.12.2.m2.1.1.1"></times><ci id="S3.T1.12.2.m2.1.1.2.cmml" xref="S3.T1.12.2.m2.1.1.2">𝑅</ci><ci id="S3.T1.12.2.m2.1.1.3.cmml" xref="S3.T1.12.2.m2.1.1.3">𝐶</ci><ci id="S3.T1.12.2.m2.1.1.4.cmml" xref="S3.T1.12.2.m2.1.1.4">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.2.m2.1d">RCS</annotation></semantics></math> is the radar signal strength, <math id="S3.T1.13.3.m3.1" class="ltx_Math" alttext="V_{r}" display="inline"><semantics id="S3.T1.13.3.m3.1b"><msub id="S3.T1.13.3.m3.1.1" xref="S3.T1.13.3.m3.1.1.cmml"><mi id="S3.T1.13.3.m3.1.1.2" xref="S3.T1.13.3.m3.1.1.2.cmml">V</mi><mi id="S3.T1.13.3.m3.1.1.3" xref="S3.T1.13.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T1.13.3.m3.1c"><apply id="S3.T1.13.3.m3.1.1.cmml" xref="S3.T1.13.3.m3.1.1"><csymbol cd="ambiguous" id="S3.T1.13.3.m3.1.1.1.cmml" xref="S3.T1.13.3.m3.1.1">subscript</csymbol><ci id="S3.T1.13.3.m3.1.1.2.cmml" xref="S3.T1.13.3.m3.1.1.2">𝑉</ci><ci id="S3.T1.13.3.m3.1.1.3.cmml" xref="S3.T1.13.3.m3.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.3.m3.1d">V_{r}</annotation></semantics></math> and <math id="S3.T1.14.4.m4.1" class="ltx_Math" alttext="V_{r}^{\prime}" display="inline"><semantics id="S3.T1.14.4.m4.1b"><msubsup id="S3.T1.14.4.m4.1.1" xref="S3.T1.14.4.m4.1.1.cmml"><mi id="S3.T1.14.4.m4.1.1.2.2" xref="S3.T1.14.4.m4.1.1.2.2.cmml">V</mi><mi id="S3.T1.14.4.m4.1.1.2.3" xref="S3.T1.14.4.m4.1.1.2.3.cmml">r</mi><mo id="S3.T1.14.4.m4.1.1.3" xref="S3.T1.14.4.m4.1.1.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.T1.14.4.m4.1c"><apply id="S3.T1.14.4.m4.1.1.cmml" xref="S3.T1.14.4.m4.1.1"><csymbol cd="ambiguous" id="S3.T1.14.4.m4.1.1.1.cmml" xref="S3.T1.14.4.m4.1.1">superscript</csymbol><apply id="S3.T1.14.4.m4.1.1.2.cmml" xref="S3.T1.14.4.m4.1.1"><csymbol cd="ambiguous" id="S3.T1.14.4.m4.1.1.2.1.cmml" xref="S3.T1.14.4.m4.1.1">subscript</csymbol><ci id="S3.T1.14.4.m4.1.1.2.2.cmml" xref="S3.T1.14.4.m4.1.1.2.2">𝑉</ci><ci id="S3.T1.14.4.m4.1.1.2.3.cmml" xref="S3.T1.14.4.m4.1.1.2.3">𝑟</ci></apply><ci id="S3.T1.14.4.m4.1.1.3.cmml" xref="S3.T1.14.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.4.m4.1d">V_{r}^{\prime}</annotation></semantics></math> are the relative and absolute velocity, <math id="S3.T1.15.5.m5.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.T1.15.5.m5.1b"><mi id="S3.T1.15.5.m5.1.1" xref="S3.T1.15.5.m5.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.T1.15.5.m5.1c"><ci id="S3.T1.15.5.m5.1.1.cmml" xref="S3.T1.15.5.m5.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.5.m5.1d">R</annotation></semantics></math> is the distance, and <math id="S3.T1.16.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.T1.16.6.m6.1b"><mi id="S3.T1.16.6.m6.1.1" xref="S3.T1.16.6.m6.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.T1.16.6.m6.1c"><ci id="S3.T1.16.6.m6.1.1.cmml" xref="S3.T1.16.6.m6.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.6.m6.1d">\alpha</annotation></semantics></math> and <math id="S3.T1.17.7.m7.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.T1.17.7.m7.1b"><mi id="S3.T1.17.7.m7.1.1" xref="S3.T1.17.7.m7.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.T1.17.7.m7.1c"><ci id="S3.T1.17.7.m7.1.1.cmml" xref="S3.T1.17.7.m7.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.17.7.m7.1d">\beta</annotation></semantics></math> are the horizontal and vertical angles.</span></figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.4" class="ltx_p">RCS is a key feature of radar data, which is related to the size, shape, and material of the object <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. RDL reflects the physical characteristics of the objects in the depth prediction and retains this information in the later BEV features. The transform module is used to transform the radar depth features input channel number from <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="N+1" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">N</mi><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">+</mo><mn id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><plus id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></plus><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">N+1</annotation></semantics></math> to <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mn id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><cn type="integer" id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">64</annotation></semantics></math>, where <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">N</annotation></semantics></math> and <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mn id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><cn type="integer" id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">1</annotation></semantics></math> are the number of extra information channels (e.g., RCS, velocity) and depth information, respectively.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Unified Feature Fusion (UFF)</span>
</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.14751/assets/fusion.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Unified Feature Fusion (UFF).</span></figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The UFF module, shown in Fig.<a href="#S3.F3" title="Figure 3 ‣ III-C Unified Feature Fusion (UFF) ‣ III Methodology ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, is specifically designed to improve the reliability of multi-modal fusion by addressing the inherent differences between different sensor modalities. It consists of several key components: the Channel Unifier, the Shared Feature Encoder, the Softmax Concatenation Fusion, and the Fused Feature Encoder. The Channel Unifier aligns the feature dimensions of different modalities using 1x1 convolutions, ensuring a consistent channel representation across modalities. This not only simplifies the fusion process, but also enables more effective extraction of cross-modal features.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The Shared Feature Encoder plays a critical role in the normalization of feature representations from different modalities, mitigating discrepancies that may be due to modality-specific characteristics. Thus, it helps reduce performance degradation when a modality fails or provides suboptimal data. The softmax concatenation fusion integrates these processed features, while the use of softmax weighting allows the network to emphasize the most salient information across modalities, improving the overall quality of the feature fusion.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Both the Shared Feature Encoder and the Fused Feature Encoder are implemented as residual blocks, which facilitates deeper feature learning and promotes gradient flow during training. In addition to increasing the robustness of the fusion process, this architecture ensures that the fused features preserve essential information from each modality.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Failure Test (FT)</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.2" class="ltx_p">In order to rigorously evaluate the robustness of the model under conditions where the vision modality fails, we propose a vision failure test. In contrast to the multi-view approach used in the CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, where robustness is evaluated for multiple views, we introduce Gaussian noise directly into the single-view image data sets to simulate the degradation of the visual input. This allows us to observe how detection performance changes with increasing noise level. The noisy image <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="I^{\prime}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msup id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">I</mi><mo id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝐼</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">I^{\prime}</annotation></semantics></math> in FT<sub id="S3.SS4.p1.2.1" class="ltx_sub"><span id="S3.SS4.p1.2.1.1" class="ltx_text ltx_font_italic">ρ</span></sub> is defined as</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1X.2.1.1.m1.2" class="ltx_Math" alttext="\displaystyle I^{\prime}=I+\rho\cdot\mathbb{N}(0,\sigma^{2})," display="inline"><semantics id="S3.E1X.2.1.1.m1.2a"><mrow id="S3.E1X.2.1.1.m1.2.2.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.cmml"><mrow id="S3.E1X.2.1.1.m1.2.2.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.cmml"><msup id="S3.E1X.2.1.1.m1.2.2.1.1.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.cmml"><mi id="S3.E1X.2.1.1.m1.2.2.1.1.3.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.2.cmml">I</mi><mo id="S3.E1X.2.1.1.m1.2.2.1.1.3.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.3.cmml">′</mo></msup><mo id="S3.E1X.2.1.1.m1.2.2.1.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.2.2.1.1.1.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.3.cmml">I</mi><mo id="S3.E1X.2.1.1.m1.2.2.1.1.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.2.cmml">+</mo><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.cmml"><mi id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.2.cmml">ρ</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.1.cmml">⋅</mo><mi mathvariant="normal" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.3.cmml">ℕ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.2.cmml">(</mo><mn id="S3.E1X.2.1.1.m1.1.1" xref="S3.E1X.2.1.1.m1.1.1.cmml">0</mn><mo id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.2.cmml">,</mo><msup id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml">σ</mi><mn id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.4" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1X.2.1.1.m1.2.2.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.2b"><apply id="S3.E1X.2.1.1.m1.2.2.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1"><eq id="S3.E1X.2.1.1.m1.2.2.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.2"></eq><apply id="S3.E1X.2.1.1.m1.2.2.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.2.2.1.1.3.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.3">superscript</csymbol><ci id="S3.E1X.2.1.1.m1.2.2.1.1.3.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.2">𝐼</ci><ci id="S3.E1X.2.1.1.m1.2.2.1.1.3.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.3">′</ci></apply><apply id="S3.E1X.2.1.1.m1.2.2.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1"><plus id="S3.E1X.2.1.1.m1.2.2.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.2"></plus><ci id="S3.E1X.2.1.1.m1.2.2.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.3">𝐼</ci><apply id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1"><times id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.2"></times><apply id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3"><ci id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.1">⋅</ci><ci id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.2">𝜌</ci><ci id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.3.3">ℕ</ci></apply><interval closure="open" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1"><cn type="integer" id="S3.E1X.2.1.1.m1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1">0</cn><apply id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.2">𝜎</ci><cn type="integer" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.3">2</cn></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.2c">\displaystyle I^{\prime}=I+\rho\cdot\mathbb{N}(0,\sigma^{2}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
</tbody>
</table>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<th id="S3.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" rowspan="2"><span id="S3.T2.1.2.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r" colspan="4"><span id="S3.T2.1.2.1.2.1" class="ltx_text ltx_font_bold">Entrire Annotated Area</span></td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_center" colspan="4"><span id="S3.T2.1.2.1.3.1" class="ltx_text ltx_font_bold">Driving Corridor Area</span></td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.3.2.1.1" class="ltx_text ltx_font_bold">Car</span></td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.3.2.2.1" class="ltx_text ltx_font_bold">Ped</span></td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.3.2.3.1" class="ltx_text ltx_font_bold">Cyc</span></td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.1.3.2.4.1" class="ltx_text ltx_font_bold">mAP</span></td>
<td id="S3.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.3.2.5.1" class="ltx_text ltx_font_bold">Car</span></td>
<td id="S3.T2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.3.2.6.1" class="ltx_text ltx_font_bold">Ped</span></td>
<td id="S3.T2.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.3.2.7.1" class="ltx_text ltx_font_bold">Cyc</span></td>
<td id="S3.T2.1.3.2.8" class="ltx_td ltx_align_center ltx_border_l ltx_border_t"><span id="S3.T2.1.3.2.8.1" class="ltx_text ltx_font_bold">mAP</span></td>
</tr>
<tr id="S3.T2.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.1.1.1.1" class="ltx_text ltx_font_bold">BEVFusion<sup id="S3.T2.1.1.1.1.1" class="ltx_sup"><span id="S3.T2.1.1.1.1.1.1" class="ltx_text ltx_font_medium">∗</span></sup></span></th>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_t">42.02</td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_t">38.98</td>
<td id="S3.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.54</td>
<td id="S3.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.51</td>
<td id="S3.T2.1.1.6" class="ltx_td ltx_align_center ltx_border_t">72.23</td>
<td id="S3.T2.1.1.7" class="ltx_td ltx_align_center ltx_border_t">48.67</td>
<td id="S3.T2.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.57</td>
<td id="S3.T2.1.1.9" class="ltx_td ltx_align_center ltx_border_t">69.02</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<th id="S3.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.4.3.1.1" class="ltx_text ltx_font_bold">RCFusion</span></th>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_center">41.70</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_center">38.95</td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">68.31</td>
<td id="S3.T2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">49.65</td>
<td id="S3.T2.1.4.3.6" class="ltx_td ltx_align_center">71.87</td>
<td id="S3.T2.1.4.3.7" class="ltx_td ltx_align_center">47.50</td>
<td id="S3.T2.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r">88.33</td>
<td id="S3.T2.1.4.3.9" class="ltx_td ltx_align_center">69.23</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<th id="S3.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.5.4.1.1" class="ltx_text ltx_font_bold">FUTR3D</span></th>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_center">46.01</td>
<td id="S3.T2.1.5.4.3" class="ltx_td ltx_align_center">35.11</td>
<td id="S3.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">65.98</td>
<td id="S3.T2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">49.03</td>
<td id="S3.T2.1.5.4.6" class="ltx_td ltx_align_center">78.66</td>
<td id="S3.T2.1.5.4.7" class="ltx_td ltx_align_center">43.10</td>
<td id="S3.T2.1.5.4.8" class="ltx_td ltx_align_center ltx_border_r">86.19</td>
<td id="S3.T2.1.5.4.9" class="ltx_td ltx_align_center">69.32</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<th id="S3.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.6.5.1.1" class="ltx_text ltx_font_bold">GRC-Net</span></th>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_align_center">27.90</td>
<td id="S3.T2.1.6.5.3" class="ltx_td ltx_align_center">31.00</td>
<td id="S3.T2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">64.60</td>
<td id="S3.T2.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r">41.10</td>
<td id="S3.T2.1.6.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.6.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.6.5.8" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.1.6.5.9" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.1.7.6" class="ltx_tr">
<th id="S3.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.7.6.1.1" class="ltx_text ltx_font_bold">RCBEVDet</span></th>
<td id="S3.T2.1.7.6.2" class="ltx_td ltx_align_center">40.60</td>
<td id="S3.T2.1.7.6.3" class="ltx_td ltx_align_center">38.80</td>
<td id="S3.T2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">70.40</td>
<td id="S3.T2.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r">49.90</td>
<td id="S3.T2.1.7.6.6" class="ltx_td ltx_align_center">72.40</td>
<td id="S3.T2.1.7.6.7" class="ltx_td ltx_align_center">49.80</td>
<td id="S3.T2.1.7.6.8" class="ltx_td ltx_align_center ltx_border_r">87.00</td>
<td id="S3.T2.1.7.6.9" class="ltx_td ltx_align_center">69.80</td>
</tr>
<tr id="S3.T2.1.8.7" class="ltx_tr">
<th id="S3.T2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.1.8.7.1.1" class="ltx_text ltx_font_bold">LXL</span></th>
<td id="S3.T2.1.8.7.2" class="ltx_td ltx_align_center">42.33</td>
<td id="S3.T2.1.8.7.3" class="ltx_td ltx_align_center">49.48</td>
<td id="S3.T2.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">77.12</td>
<td id="S3.T2.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r">56.31</td>
<td id="S3.T2.1.8.7.6" class="ltx_td ltx_align_center">72.18</td>
<td id="S3.T2.1.8.7.7" class="ltx_td ltx_align_center">58.30</td>
<td id="S3.T2.1.8.7.8" class="ltx_td ltx_align_center ltx_border_r">88.31</td>
<td id="S3.T2.1.8.7.9" class="ltx_td ltx_align_center">72.93</td>
</tr>
<tr id="S3.T2.1.9.8" class="ltx_tr">
<th id="S3.T2.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.1.9.8.1.1" class="ltx_text ltx_font_bold">BEVFusion</span></th>
<td id="S3.T2.1.9.8.2" class="ltx_td ltx_align_center ltx_border_t">40.85</td>
<td id="S3.T2.1.9.8.3" class="ltx_td ltx_align_center ltx_border_t">47.60</td>
<td id="S3.T2.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.92</td>
<td id="S3.T2.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.79</td>
<td id="S3.T2.1.9.8.6" class="ltx_td ltx_align_center ltx_border_t">71.93</td>
<td id="S3.T2.1.9.8.7" class="ltx_td ltx_align_center ltx_border_t">57.10</td>
<td id="S3.T2.1.9.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.23</td>
<td id="S3.T2.1.9.8.9" class="ltx_td ltx_align_center ltx_border_t">72.42</td>
</tr>
<tr id="S3.T2.1.10.9" class="ltx_tr">
<th id="S3.T2.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S3.T2.1.10.9.1.1" class="ltx_text ltx_font_bold">UniBEVFusion</span></th>
<td id="S3.T2.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb">42.22</td>
<td id="S3.T2.1.10.9.3" class="ltx_td ltx_align_center ltx_border_bb">47.11</td>
<td id="S3.T2.1.10.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">72.94</td>
<td id="S3.T2.1.10.9.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">54.09</td>
<td id="S3.T2.1.10.9.6" class="ltx_td ltx_align_center ltx_border_bb">72.10</td>
<td id="S3.T2.1.10.9.7" class="ltx_td ltx_align_center ltx_border_bb">57.71</td>
<td id="S3.T2.1.10.9.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">93.29</td>
<td id="S3.T2.1.10.9.9" class="ltx_td ltx_align_center ltx_border_bb">74.37</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.6.2.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S3.T2.3.1" class="ltx_text" style="font-size:90%;">Results on VoD. BEVFusion<sup id="S3.T2.3.1.1" class="ltx_sup">∗</sup> is the reproduction results from LXL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</span></figcaption>
</figure>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.6" class="ltx_p">where <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\rho</annotation></semantics></math> is the noise level, <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">I</annotation></semantics></math> is the original clean image, <math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="I^{\prime}" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><msup id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml"><mi id="S3.SS4.p3.3.m3.1.1.2" xref="S3.SS4.p3.3.m3.1.1.2.cmml">I</mi><mo id="S3.SS4.p3.3.m3.1.1.3" xref="S3.SS4.p3.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><apply id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">superscript</csymbol><ci id="S3.SS4.p3.3.m3.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.2">𝐼</ci><ci id="S3.SS4.p3.3.m3.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">I^{\prime}</annotation></semantics></math> is the noise corrupted image, and <math id="S3.SS4.p3.4.m4.2" class="ltx_Math" alttext="\mathbb{N}(0,\sigma^{2})" display="inline"><semantics id="S3.SS4.p3.4.m4.2a"><mrow id="S3.SS4.p3.4.m4.2.2" xref="S3.SS4.p3.4.m4.2.2.cmml"><mi mathvariant="normal" id="S3.SS4.p3.4.m4.2.2.3" xref="S3.SS4.p3.4.m4.2.2.3.cmml">ℕ</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.4.m4.2.2.2" xref="S3.SS4.p3.4.m4.2.2.2.cmml">​</mo><mrow id="S3.SS4.p3.4.m4.2.2.1.1" xref="S3.SS4.p3.4.m4.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS4.p3.4.m4.2.2.1.1.2" xref="S3.SS4.p3.4.m4.2.2.1.2.cmml">(</mo><mn id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">0</mn><mo id="S3.SS4.p3.4.m4.2.2.1.1.3" xref="S3.SS4.p3.4.m4.2.2.1.2.cmml">,</mo><msup id="S3.SS4.p3.4.m4.2.2.1.1.1" xref="S3.SS4.p3.4.m4.2.2.1.1.1.cmml"><mi id="S3.SS4.p3.4.m4.2.2.1.1.1.2" xref="S3.SS4.p3.4.m4.2.2.1.1.1.2.cmml">σ</mi><mn id="S3.SS4.p3.4.m4.2.2.1.1.1.3" xref="S3.SS4.p3.4.m4.2.2.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S3.SS4.p3.4.m4.2.2.1.1.4" xref="S3.SS4.p3.4.m4.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.2b"><apply id="S3.SS4.p3.4.m4.2.2.cmml" xref="S3.SS4.p3.4.m4.2.2"><times id="S3.SS4.p3.4.m4.2.2.2.cmml" xref="S3.SS4.p3.4.m4.2.2.2"></times><ci id="S3.SS4.p3.4.m4.2.2.3.cmml" xref="S3.SS4.p3.4.m4.2.2.3">ℕ</ci><interval closure="open" id="S3.SS4.p3.4.m4.2.2.1.2.cmml" xref="S3.SS4.p3.4.m4.2.2.1.1"><cn type="integer" id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">0</cn><apply id="S3.SS4.p3.4.m4.2.2.1.1.1.cmml" xref="S3.SS4.p3.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS4.p3.4.m4.2.2.1.1.1">superscript</csymbol><ci id="S3.SS4.p3.4.m4.2.2.1.1.1.2.cmml" xref="S3.SS4.p3.4.m4.2.2.1.1.1.2">𝜎</ci><cn type="integer" id="S3.SS4.p3.4.m4.2.2.1.1.1.3.cmml" xref="S3.SS4.p3.4.m4.2.2.1.1.1.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.2c">\mathbb{N}(0,\sigma^{2})</annotation></semantics></math> is Gaussian noise with mean 0 and variance <math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><msup id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml"><mi id="S3.SS4.p3.5.m5.1.1.2" xref="S3.SS4.p3.5.m5.1.1.2.cmml">σ</mi><mn id="S3.SS4.p3.5.m5.1.1.3" xref="S3.SS4.p3.5.m5.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><apply id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.5.m5.1.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">superscript</csymbol><ci id="S3.SS4.p3.5.m5.1.1.2.cmml" xref="S3.SS4.p3.5.m5.1.1.2">𝜎</ci><cn type="integer" id="S3.SS4.p3.5.m5.1.1.3.cmml" xref="S3.SS4.p3.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">\sigma^{2}</annotation></semantics></math>. By systematically varying <math id="S3.SS4.p3.6.m6.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S3.SS4.p3.6.m6.1a"><mi id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><ci id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">\rho</annotation></semantics></math>, we evaluate the performance of the model under different noise intensities. As presented in section <a href="#S4.SS4" title="IV-D Failure Test ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>, both BEVFusion and our proposed UniBEVFusion show sensitivity to noise in the visual modality, highlighting the impact of modality-specific degradation on overall model performance. This analysis underscores the importance of robust multi-modal fusion in maintaining detection accuracy even under adverse conditions.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We first give the brief introduction to the datasets used in the experiments in Section <a href="#S4.SS1" title="IV-A Datasets ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>. Then we compare the performance of different models on VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and TJ4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> in Section <a href="#S4.SS2" title="IV-B Results on VoD ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a> and Section <a href="#S4.SS3" title="IV-C Results on TJ4D ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>, respectively. The results of FT and the ablation study is shown in Section <a href="#S4.SS4" title="IV-D Failure Test ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>. Lastly, we test the performance of different image resolutions in Section <a href="#S4.SS5" title="IV-E Image Resolution ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span></span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.9" class="ltx_p">The datasets used in this paper, VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and TJ4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, both provide 4D MMW radar data. Radar point clouds in VoD includes [<math id="S4.SS1.p1.1.m1.7" class="ltx_Math" alttext="x,\ y,\ z,\ RCS,\ V_{r},\ V_{r}^{\prime},\ t" display="inline"><semantics id="S4.SS1.p1.1.m1.7a"><mrow id="S4.SS1.p1.1.m1.7.7.3" xref="S4.SS1.p1.1.m1.7.7.4.cmml"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">x</mi><mo rspace="0.667em" id="S4.SS1.p1.1.m1.7.7.3.4" xref="S4.SS1.p1.1.m1.7.7.4.cmml">,</mo><mi id="S4.SS1.p1.1.m1.2.2" xref="S4.SS1.p1.1.m1.2.2.cmml">y</mi><mo rspace="0.667em" id="S4.SS1.p1.1.m1.7.7.3.5" xref="S4.SS1.p1.1.m1.7.7.4.cmml">,</mo><mi id="S4.SS1.p1.1.m1.3.3" xref="S4.SS1.p1.1.m1.3.3.cmml">z</mi><mo rspace="0.667em" id="S4.SS1.p1.1.m1.7.7.3.6" xref="S4.SS1.p1.1.m1.7.7.4.cmml">,</mo><mrow id="S4.SS1.p1.1.m1.5.5.1.1" xref="S4.SS1.p1.1.m1.5.5.1.1.cmml"><mi id="S4.SS1.p1.1.m1.5.5.1.1.2" xref="S4.SS1.p1.1.m1.5.5.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.5.5.1.1.1" xref="S4.SS1.p1.1.m1.5.5.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.1.m1.5.5.1.1.3" xref="S4.SS1.p1.1.m1.5.5.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.5.5.1.1.1a" xref="S4.SS1.p1.1.m1.5.5.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.1.m1.5.5.1.1.4" xref="S4.SS1.p1.1.m1.5.5.1.1.4.cmml">S</mi></mrow><mo rspace="0.667em" id="S4.SS1.p1.1.m1.7.7.3.7" xref="S4.SS1.p1.1.m1.7.7.4.cmml">,</mo><msub id="S4.SS1.p1.1.m1.6.6.2.2" xref="S4.SS1.p1.1.m1.6.6.2.2.cmml"><mi id="S4.SS1.p1.1.m1.6.6.2.2.2" xref="S4.SS1.p1.1.m1.6.6.2.2.2.cmml">V</mi><mi id="S4.SS1.p1.1.m1.6.6.2.2.3" xref="S4.SS1.p1.1.m1.6.6.2.2.3.cmml">r</mi></msub><mo rspace="0.667em" id="S4.SS1.p1.1.m1.7.7.3.8" xref="S4.SS1.p1.1.m1.7.7.4.cmml">,</mo><msubsup id="S4.SS1.p1.1.m1.7.7.3.3" xref="S4.SS1.p1.1.m1.7.7.3.3.cmml"><mi id="S4.SS1.p1.1.m1.7.7.3.3.2.2" xref="S4.SS1.p1.1.m1.7.7.3.3.2.2.cmml">V</mi><mi id="S4.SS1.p1.1.m1.7.7.3.3.2.3" xref="S4.SS1.p1.1.m1.7.7.3.3.2.3.cmml">r</mi><mo id="S4.SS1.p1.1.m1.7.7.3.3.3" xref="S4.SS1.p1.1.m1.7.7.3.3.3.cmml">′</mo></msubsup><mo rspace="0.667em" id="S4.SS1.p1.1.m1.7.7.3.9" xref="S4.SS1.p1.1.m1.7.7.4.cmml">,</mo><mi id="S4.SS1.p1.1.m1.4.4" xref="S4.SS1.p1.1.m1.4.4.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.7b"><list id="S4.SS1.p1.1.m1.7.7.4.cmml" xref="S4.SS1.p1.1.m1.7.7.3"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝑥</ci><ci id="S4.SS1.p1.1.m1.2.2.cmml" xref="S4.SS1.p1.1.m1.2.2">𝑦</ci><ci id="S4.SS1.p1.1.m1.3.3.cmml" xref="S4.SS1.p1.1.m1.3.3">𝑧</ci><apply id="S4.SS1.p1.1.m1.5.5.1.1.cmml" xref="S4.SS1.p1.1.m1.5.5.1.1"><times id="S4.SS1.p1.1.m1.5.5.1.1.1.cmml" xref="S4.SS1.p1.1.m1.5.5.1.1.1"></times><ci id="S4.SS1.p1.1.m1.5.5.1.1.2.cmml" xref="S4.SS1.p1.1.m1.5.5.1.1.2">𝑅</ci><ci id="S4.SS1.p1.1.m1.5.5.1.1.3.cmml" xref="S4.SS1.p1.1.m1.5.5.1.1.3">𝐶</ci><ci id="S4.SS1.p1.1.m1.5.5.1.1.4.cmml" xref="S4.SS1.p1.1.m1.5.5.1.1.4">𝑆</ci></apply><apply id="S4.SS1.p1.1.m1.6.6.2.2.cmml" xref="S4.SS1.p1.1.m1.6.6.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.6.6.2.2.1.cmml" xref="S4.SS1.p1.1.m1.6.6.2.2">subscript</csymbol><ci id="S4.SS1.p1.1.m1.6.6.2.2.2.cmml" xref="S4.SS1.p1.1.m1.6.6.2.2.2">𝑉</ci><ci id="S4.SS1.p1.1.m1.6.6.2.2.3.cmml" xref="S4.SS1.p1.1.m1.6.6.2.2.3">𝑟</ci></apply><apply id="S4.SS1.p1.1.m1.7.7.3.3.cmml" xref="S4.SS1.p1.1.m1.7.7.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.7.7.3.3.1.cmml" xref="S4.SS1.p1.1.m1.7.7.3.3">superscript</csymbol><apply id="S4.SS1.p1.1.m1.7.7.3.3.2.cmml" xref="S4.SS1.p1.1.m1.7.7.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.7.7.3.3.2.1.cmml" xref="S4.SS1.p1.1.m1.7.7.3.3">subscript</csymbol><ci id="S4.SS1.p1.1.m1.7.7.3.3.2.2.cmml" xref="S4.SS1.p1.1.m1.7.7.3.3.2.2">𝑉</ci><ci id="S4.SS1.p1.1.m1.7.7.3.3.2.3.cmml" xref="S4.SS1.p1.1.m1.7.7.3.3.2.3">𝑟</ci></apply><ci id="S4.SS1.p1.1.m1.7.7.3.3.3.cmml" xref="S4.SS1.p1.1.m1.7.7.3.3.3">′</ci></apply><ci id="S4.SS1.p1.1.m1.4.4.cmml" xref="S4.SS1.p1.1.m1.4.4">𝑡</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.7c">x,\ y,\ z,\ RCS,\ V_{r},\ V_{r}^{\prime},\ t</annotation></semantics></math>], while TJ4D includes [<math id="S4.SS1.p1.2.m2.7" class="ltx_Math" alttext="x,\ y,\ z,\ R,\ RCS,\ \alpha,\ \beta" display="inline"><semantics id="S4.SS1.p1.2.m2.7a"><mrow id="S4.SS1.p1.2.m2.7.7.1" xref="S4.SS1.p1.2.m2.7.7.2.cmml"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">x</mi><mo rspace="0.667em" id="S4.SS1.p1.2.m2.7.7.1.2" xref="S4.SS1.p1.2.m2.7.7.2.cmml">,</mo><mi id="S4.SS1.p1.2.m2.2.2" xref="S4.SS1.p1.2.m2.2.2.cmml">y</mi><mo rspace="0.667em" id="S4.SS1.p1.2.m2.7.7.1.3" xref="S4.SS1.p1.2.m2.7.7.2.cmml">,</mo><mi id="S4.SS1.p1.2.m2.3.3" xref="S4.SS1.p1.2.m2.3.3.cmml">z</mi><mo rspace="0.667em" id="S4.SS1.p1.2.m2.7.7.1.4" xref="S4.SS1.p1.2.m2.7.7.2.cmml">,</mo><mi id="S4.SS1.p1.2.m2.4.4" xref="S4.SS1.p1.2.m2.4.4.cmml">R</mi><mo rspace="0.667em" id="S4.SS1.p1.2.m2.7.7.1.5" xref="S4.SS1.p1.2.m2.7.7.2.cmml">,</mo><mrow id="S4.SS1.p1.2.m2.7.7.1.1" xref="S4.SS1.p1.2.m2.7.7.1.1.cmml"><mi id="S4.SS1.p1.2.m2.7.7.1.1.2" xref="S4.SS1.p1.2.m2.7.7.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.7.7.1.1.1" xref="S4.SS1.p1.2.m2.7.7.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.2.m2.7.7.1.1.3" xref="S4.SS1.p1.2.m2.7.7.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.7.7.1.1.1a" xref="S4.SS1.p1.2.m2.7.7.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.2.m2.7.7.1.1.4" xref="S4.SS1.p1.2.m2.7.7.1.1.4.cmml">S</mi></mrow><mo rspace="0.667em" id="S4.SS1.p1.2.m2.7.7.1.6" xref="S4.SS1.p1.2.m2.7.7.2.cmml">,</mo><mi id="S4.SS1.p1.2.m2.5.5" xref="S4.SS1.p1.2.m2.5.5.cmml">α</mi><mo rspace="0.667em" id="S4.SS1.p1.2.m2.7.7.1.7" xref="S4.SS1.p1.2.m2.7.7.2.cmml">,</mo><mi id="S4.SS1.p1.2.m2.6.6" xref="S4.SS1.p1.2.m2.6.6.cmml">β</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.7b"><list id="S4.SS1.p1.2.m2.7.7.2.cmml" xref="S4.SS1.p1.2.m2.7.7.1"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑥</ci><ci id="S4.SS1.p1.2.m2.2.2.cmml" xref="S4.SS1.p1.2.m2.2.2">𝑦</ci><ci id="S4.SS1.p1.2.m2.3.3.cmml" xref="S4.SS1.p1.2.m2.3.3">𝑧</ci><ci id="S4.SS1.p1.2.m2.4.4.cmml" xref="S4.SS1.p1.2.m2.4.4">𝑅</ci><apply id="S4.SS1.p1.2.m2.7.7.1.1.cmml" xref="S4.SS1.p1.2.m2.7.7.1.1"><times id="S4.SS1.p1.2.m2.7.7.1.1.1.cmml" xref="S4.SS1.p1.2.m2.7.7.1.1.1"></times><ci id="S4.SS1.p1.2.m2.7.7.1.1.2.cmml" xref="S4.SS1.p1.2.m2.7.7.1.1.2">𝑅</ci><ci id="S4.SS1.p1.2.m2.7.7.1.1.3.cmml" xref="S4.SS1.p1.2.m2.7.7.1.1.3">𝐶</ci><ci id="S4.SS1.p1.2.m2.7.7.1.1.4.cmml" xref="S4.SS1.p1.2.m2.7.7.1.1.4">𝑆</ci></apply><ci id="S4.SS1.p1.2.m2.5.5.cmml" xref="S4.SS1.p1.2.m2.5.5">𝛼</ci><ci id="S4.SS1.p1.2.m2.6.6.cmml" xref="S4.SS1.p1.2.m2.6.6">𝛽</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.7c">x,\ y,\ z,\ R,\ RCS,\ \alpha,\ \beta</annotation></semantics></math>], where <math id="S4.SS1.p1.3.m3.3" class="ltx_Math" alttext="x,y,z" display="inline"><semantics id="S4.SS1.p1.3.m3.3a"><mrow id="S4.SS1.p1.3.m3.3.4.2" xref="S4.SS1.p1.3.m3.3.4.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">x</mi><mo id="S4.SS1.p1.3.m3.3.4.2.1" xref="S4.SS1.p1.3.m3.3.4.1.cmml">,</mo><mi id="S4.SS1.p1.3.m3.2.2" xref="S4.SS1.p1.3.m3.2.2.cmml">y</mi><mo id="S4.SS1.p1.3.m3.3.4.2.2" xref="S4.SS1.p1.3.m3.3.4.1.cmml">,</mo><mi id="S4.SS1.p1.3.m3.3.3" xref="S4.SS1.p1.3.m3.3.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.3b"><list id="S4.SS1.p1.3.m3.3.4.1.cmml" xref="S4.SS1.p1.3.m3.3.4.2"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">𝑥</ci><ci id="S4.SS1.p1.3.m3.2.2.cmml" xref="S4.SS1.p1.3.m3.2.2">𝑦</ci><ci id="S4.SS1.p1.3.m3.3.3.cmml" xref="S4.SS1.p1.3.m3.3.3">𝑧</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.3c">x,y,z</annotation></semantics></math> represent coordinates, <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="RCS" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.1a" xref="S4.SS1.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.4.m4.1.1.4" xref="S4.SS1.p1.4.m4.1.1.4.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><times id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"></times><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">𝑅</ci><ci id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3">𝐶</ci><ci id="S4.SS1.p1.4.m4.1.1.4.cmml" xref="S4.SS1.p1.4.m4.1.1.4">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">RCS</annotation></semantics></math> is radar signal strength, <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="V_{r}" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><msub id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml"><mi id="S4.SS1.p1.5.m5.1.1.2" xref="S4.SS1.p1.5.m5.1.1.2.cmml">V</mi><mi id="S4.SS1.p1.5.m5.1.1.3" xref="S4.SS1.p1.5.m5.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><apply id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.p1.5.m5.1.1.2">𝑉</ci><ci id="S4.SS1.p1.5.m5.1.1.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">V_{r}</annotation></semantics></math> and <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="V_{r}^{\prime}" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><msubsup id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mi id="S4.SS1.p1.6.m6.1.1.2.2" xref="S4.SS1.p1.6.m6.1.1.2.2.cmml">V</mi><mi id="S4.SS1.p1.6.m6.1.1.2.3" xref="S4.SS1.p1.6.m6.1.1.2.3.cmml">r</mi><mo id="S4.SS1.p1.6.m6.1.1.3" xref="S4.SS1.p1.6.m6.1.1.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">superscript</csymbol><apply id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m6.1.1.2.1.cmml" xref="S4.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.p1.6.m6.1.1.2.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2.2">𝑉</ci><ci id="S4.SS1.p1.6.m6.1.1.2.3.cmml" xref="S4.SS1.p1.6.m6.1.1.2.3">𝑟</ci></apply><ci id="S4.SS1.p1.6.m6.1.1.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">V_{r}^{\prime}</annotation></semantics></math> are relative and absolute velocities, <math id="S4.SS1.p1.7.m7.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.SS1.p1.7.m7.1a"><mi id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><ci id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">R</annotation></semantics></math> is distance, and <math id="S4.SS1.p1.8.m8.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p1.8.m8.1a"><mi id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.1b"><ci id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.1c">\alpha</annotation></semantics></math>, <math id="S4.SS1.p1.9.m9.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.SS1.p1.9.m9.1a"><mi id="S4.SS1.p1.9.m9.1.1" xref="S4.SS1.p1.9.m9.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.9.m9.1b"><ci id="S4.SS1.p1.9.m9.1.1.cmml" xref="S4.SS1.p1.9.m9.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.9.m9.1c">\beta</annotation></semantics></math> are angles.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The VoD dataset includes categories for car, pedestrian, and cyclist, while TJ4D adds trucks. We followed the official method, segmenting VoD’s 6435 frames into 5139 for training and 1296 for validation, and TJ4D’s 7757 frames into 5717 for training and 2040 for validation. In this paper, our experimental camera resolutions are resized to [608, 968] for VoD and [480, 640] for TJ4D.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">For evaluation, we used Mean Average Precision (mAP) with IoU thresholds of 0.5 for cars/trucks and 0.25 for pedestrians/bicycles. VoD’s official evaluation includes RoI 3D detection within [<math id="S4.SS1.p3.1.m1.2" class="ltx_Math" alttext="-4\leq x\leq 4m,z\leq 25m" display="inline"><semantics id="S4.SS1.p3.1.m1.2a"><mrow id="S4.SS1.p3.1.m1.2.2.2" xref="S4.SS1.p3.1.m1.2.2.3.cmml"><mrow id="S4.SS1.p3.1.m1.1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.1.cmml"><mrow id="S4.SS1.p3.1.m1.1.1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.1.1.2.cmml"><mo id="S4.SS1.p3.1.m1.1.1.1.1.2a" xref="S4.SS1.p3.1.m1.1.1.1.1.2.cmml">−</mo><mn id="S4.SS1.p3.1.m1.1.1.1.1.2.2" xref="S4.SS1.p3.1.m1.1.1.1.1.2.2.cmml">4</mn></mrow><mo id="S4.SS1.p3.1.m1.1.1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.1.1.3.cmml">≤</mo><mi id="S4.SS1.p3.1.m1.1.1.1.1.4" xref="S4.SS1.p3.1.m1.1.1.1.1.4.cmml">x</mi><mo id="S4.SS1.p3.1.m1.1.1.1.1.5" xref="S4.SS1.p3.1.m1.1.1.1.1.5.cmml">≤</mo><mrow id="S4.SS1.p3.1.m1.1.1.1.1.6" xref="S4.SS1.p3.1.m1.1.1.1.1.6.cmml"><mn id="S4.SS1.p3.1.m1.1.1.1.1.6.2" xref="S4.SS1.p3.1.m1.1.1.1.1.6.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p3.1.m1.1.1.1.1.6.1" xref="S4.SS1.p3.1.m1.1.1.1.1.6.1.cmml">​</mo><mi id="S4.SS1.p3.1.m1.1.1.1.1.6.3" xref="S4.SS1.p3.1.m1.1.1.1.1.6.3.cmml">m</mi></mrow></mrow><mo id="S4.SS1.p3.1.m1.2.2.2.3" xref="S4.SS1.p3.1.m1.2.2.3a.cmml">,</mo><mrow id="S4.SS1.p3.1.m1.2.2.2.2" xref="S4.SS1.p3.1.m1.2.2.2.2.cmml"><mi id="S4.SS1.p3.1.m1.2.2.2.2.2" xref="S4.SS1.p3.1.m1.2.2.2.2.2.cmml">z</mi><mo id="S4.SS1.p3.1.m1.2.2.2.2.1" xref="S4.SS1.p3.1.m1.2.2.2.2.1.cmml">≤</mo><mrow id="S4.SS1.p3.1.m1.2.2.2.2.3" xref="S4.SS1.p3.1.m1.2.2.2.2.3.cmml"><mn id="S4.SS1.p3.1.m1.2.2.2.2.3.2" xref="S4.SS1.p3.1.m1.2.2.2.2.3.2.cmml">25</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p3.1.m1.2.2.2.2.3.1" xref="S4.SS1.p3.1.m1.2.2.2.2.3.1.cmml">​</mo><mi id="S4.SS1.p3.1.m1.2.2.2.2.3.3" xref="S4.SS1.p3.1.m1.2.2.2.2.3.3.cmml">m</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.2b"><apply id="S4.SS1.p3.1.m1.2.2.3.cmml" xref="S4.SS1.p3.1.m1.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.2.2.3a.cmml" xref="S4.SS1.p3.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S4.SS1.p3.1.m1.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1"><and id="S4.SS1.p3.1.m1.1.1.1.1a.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1"></and><apply id="S4.SS1.p3.1.m1.1.1.1.1b.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1"><leq id="S4.SS1.p3.1.m1.1.1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.3"></leq><apply id="S4.SS1.p3.1.m1.1.1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.2"><minus id="S4.SS1.p3.1.m1.1.1.1.1.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.2"></minus><cn type="integer" id="S4.SS1.p3.1.m1.1.1.1.1.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.2.2">4</cn></apply><ci id="S4.SS1.p3.1.m1.1.1.1.1.4.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.4">𝑥</ci></apply><apply id="S4.SS1.p3.1.m1.1.1.1.1c.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1"><leq id="S4.SS1.p3.1.m1.1.1.1.1.5.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.5"></leq><share href="#S4.SS1.p3.1.m1.1.1.1.1.4.cmml" id="S4.SS1.p3.1.m1.1.1.1.1d.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1"></share><apply id="S4.SS1.p3.1.m1.1.1.1.1.6.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.6"><times id="S4.SS1.p3.1.m1.1.1.1.1.6.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.6.1"></times><cn type="integer" id="S4.SS1.p3.1.m1.1.1.1.1.6.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.6.2">4</cn><ci id="S4.SS1.p3.1.m1.1.1.1.1.6.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.6.3">𝑚</ci></apply></apply></apply><apply id="S4.SS1.p3.1.m1.2.2.2.2.cmml" xref="S4.SS1.p3.1.m1.2.2.2.2"><leq id="S4.SS1.p3.1.m1.2.2.2.2.1.cmml" xref="S4.SS1.p3.1.m1.2.2.2.2.1"></leq><ci id="S4.SS1.p3.1.m1.2.2.2.2.2.cmml" xref="S4.SS1.p3.1.m1.2.2.2.2.2">𝑧</ci><apply id="S4.SS1.p3.1.m1.2.2.2.2.3.cmml" xref="S4.SS1.p3.1.m1.2.2.2.2.3"><times id="S4.SS1.p3.1.m1.2.2.2.2.3.1.cmml" xref="S4.SS1.p3.1.m1.2.2.2.2.3.1"></times><cn type="integer" id="S4.SS1.p3.1.m1.2.2.2.2.3.2.cmml" xref="S4.SS1.p3.1.m1.2.2.2.2.3.2">25</cn><ci id="S4.SS1.p3.1.m1.2.2.2.2.3.3.cmml" xref="S4.SS1.p3.1.m1.2.2.2.2.3.3">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.2c">-4\leq x\leq 4m,z\leq 25m</annotation></semantics></math>], while TJ4D evaluates 3D and BEV detection across all ranges.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Results on VoD</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Table <a href="#S3.T2" title="TABLE II ‣ III-D Failure Test (FT) ‣ III Methodology ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows the performance of our model on the validation set of VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, where the <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="mAP" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1a" xref="S4.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.4" xref="S4.SS2.p1.1.m1.1.1.4.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑚</ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝐴</ci><ci id="S4.SS2.p1.1.m1.1.1.4.cmml" xref="S4.SS2.p1.1.m1.1.1.4">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">mAP</annotation></semantics></math> is slightly lower than that of the LXL fusion network in the Entire Annotation Area (EAA). LXL achieves State-of-the-Art (SOTA) performance across the multi-modal radar datasets. However, in the more critical Driving Corridor Area (DCA), which is constrained by distance, UniBEVFusion outperforms LXL. While our model performs slightly worse than LXL in the detection of cars and pedestrians, it significantly outperforms LXL in the detection of cyclists. Overall, UniBEVFusion shows superior performance in the DCA, which is crucial for autonomous driving tasks, and maintains competitive results in the EAA, where it outperforms the other algorithms.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Furthermore, it is noteworthy that our reproduced BEVFusion outperforms previously reported results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. By modifying the detection head and radar PillarFeatureNet to align with UniBEVFusion, we have achieved a higher level of performance. This improved BEVFusion serves as a robust baseline for evaluating the effectiveness of our proposed UniBEVFusion network.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2409.14751/assets/results.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="462" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.6.3.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;">Comparison of detection results between UniBEVFusion and BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. 2D GT and 3D GT are the ground truth of 2D and 3D detection, respectively. The BEV and BEV<math id="S4.F4.3.1.m1.1" class="ltx_Math" alttext="{}_{\text{Feat}}" display="inline"><semantics id="S4.F4.3.1.m1.1b"><msub id="S4.F4.3.1.m1.1.1" xref="S4.F4.3.1.m1.1.1.cmml"><mi id="S4.F4.3.1.m1.1.1b" xref="S4.F4.3.1.m1.1.1.cmml"></mi><mtext id="S4.F4.3.1.m1.1.1.1" xref="S4.F4.3.1.m1.1.1.1a.cmml">Feat</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.F4.3.1.m1.1c"><apply id="S4.F4.3.1.m1.1.1.cmml" xref="S4.F4.3.1.m1.1.1"><ci id="S4.F4.3.1.m1.1.1.1a.cmml" xref="S4.F4.3.1.m1.1.1.1"><mtext mathsize="70%" id="S4.F4.3.1.m1.1.1.1.cmml" xref="S4.F4.3.1.m1.1.1.1">Feat</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.3.1.m1.1d">{}_{\text{Feat}}</annotation></semantics></math> are the detection results and fused BEV feature of BEVFusion, respectively. The UniBEV and UniBEV<math id="S4.F4.4.2.m2.1" class="ltx_Math" alttext="{}_{\text{Feat}}" display="inline"><semantics id="S4.F4.4.2.m2.1b"><msub id="S4.F4.4.2.m2.1.1" xref="S4.F4.4.2.m2.1.1.cmml"><mi id="S4.F4.4.2.m2.1.1b" xref="S4.F4.4.2.m2.1.1.cmml"></mi><mtext id="S4.F4.4.2.m2.1.1.1" xref="S4.F4.4.2.m2.1.1.1a.cmml">Feat</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.F4.4.2.m2.1c"><apply id="S4.F4.4.2.m2.1.1.cmml" xref="S4.F4.4.2.m2.1.1"><ci id="S4.F4.4.2.m2.1.1.1a.cmml" xref="S4.F4.4.2.m2.1.1.1"><mtext mathsize="70%" id="S4.F4.4.2.m2.1.1.1.cmml" xref="S4.F4.4.2.m2.1.1.1">Feat</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.4.2.m2.1d">{}_{\text{Feat}}</annotation></semantics></math> are the detection results and fused BEV feature of UniBEVFusion, respectively. Red, green, and blue boxes represent cars, pedestrians, and cyclists, respectively.</span></figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Results in Fig.<a href="#S4.F4" title="Figure 4 ‣ IV-B Results on VoD ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> validate the performance of UniBEVFusion compared to Ground Truth (GT) and BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The right section of the figure shows the fused BEV features, where UniBEVFusion covers a larger area than BEVFusion, though with lower overall feature magnitudes due to the Softmax layer in the UFF module. Despite this, the features in key regions remain strong, and the UFF module effectively extracts features from different modalities, providing broader context and more stable fused features for object detection.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">UniBEVFusion demonstrates superior performance in handling occlusions (Fig.<a href="#S4.F4" title="Figure 4 ‣ IV-B Results on VoD ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> A, C, E, F), where its larger feature field allows it to detect occluded objects more reliably, reducing the likelihood of dismissing them as noise. In shadowed and partially occluded scenarios (Fig.<a href="#S4.F4" title="Figure 4 ‣ IV-B Results on VoD ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> B, C), where vision alone struggles, UniBEVFusion accurately identifies the target using radar-specific information from the RDL module. Additionally, in close-range detection (Fig.<a href="#S4.F4" title="Figure 4 ‣ IV-B Results on VoD ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> D), UniBEVFusion succeeds where BEVFusion fails, likely due to the latter’s lack of sufficient contextual information in the fused BEV feature. Overall, UniBEVFusion performs better in occlusion, shadow, and both short- and long-range detection, with the UFF and RDL modules enhancing performance in various scenarios.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Results on TJ4D</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Compared to the VoD dataset’s point cloud range [[0, 51.2], [-25.6, 25.6], [-3, 2]] <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, the TJ4D dataset covers a significantly larger range [[0, 69.12], [-39.68, 39.68], [-4, 2]] <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, which introduces additional complexity for 3D object detection. Despite this increased difficulty, the performance of UniBEVFusion on TJ4D, as shown in Table <a href="#S4.T3" title="TABLE III ‣ IV-C Results on TJ4D ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, is consistent with its results on VoD, and it even surpasses the validation outcomes of the LXL algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">UniBEVFusion achieves improvements of 1.44 and 1.72 over LXL in 3D object detection and BEV accuracy, respectively. Notably, in the Car detection task, it outperforms RCFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> by 5.54 in 3D detection and by 9.37 in BEV detection. These results highlight the effectiveness of the RDL and UFF modules, which significantly enhance the model’s performance and robustness, making UniBEVFusion particularly well-suited for 3D object detection in more challenging and expansive environments.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" rowspan="2"><span id="S4.T3.2.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S4.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r" colspan="5"><span id="S4.T3.2.1.1.2.1" class="ltx_text ltx_font_bold">3D</span></td>
<td id="S4.T3.2.1.1.3" class="ltx_td ltx_align_center" colspan="5"><span id="S4.T3.2.1.1.3.1" class="ltx_text ltx_font_bold">BEV</span></td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.2.1.1" class="ltx_text ltx_font_bold">Car</span></td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.2.2.1" class="ltx_text ltx_font_bold">Ped</span></td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.2.3.1" class="ltx_text ltx_font_bold">Cyc</span></td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.2.4.1" class="ltx_text ltx_font_bold">Tru</span></td>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.2.2.2.5.1" class="ltx_text ltx_font_bold">mAP</span></td>
<td id="S4.T3.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.2.6.1" class="ltx_text ltx_font_bold">Car</span></td>
<td id="S4.T3.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.2.7.1" class="ltx_text ltx_font_bold">Ped</span></td>
<td id="S4.T3.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.2.8.1" class="ltx_text ltx_font_bold">Cyc</span></td>
<td id="S4.T3.2.2.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.2.9.1" class="ltx_text ltx_font_bold">Tru</span></td>
<td id="S4.T3.2.2.2.10" class="ltx_td ltx_align_center ltx_border_l ltx_border_t"><span id="S4.T3.2.2.2.10.1" class="ltx_text ltx_font_bold">mAP</span></td>
</tr>
<tr id="S4.T3.2.3.3" class="ltx_tr">
<th id="S4.T3.2.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T3.2.3.3.1.1" class="ltx_text ltx_font_bold">MVX-Net</span></th>
<td id="S4.T3.2.3.3.2" class="ltx_td ltx_align_center ltx_border_t">22.28</td>
<td id="S4.T3.2.3.3.3" class="ltx_td ltx_align_center ltx_border_t">19.57</td>
<td id="S4.T3.2.3.3.4" class="ltx_td ltx_align_center ltx_border_t">50.70</td>
<td id="S4.T3.2.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.21</td>
<td id="S4.T3.2.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25.94</td>
<td id="S4.T3.2.3.3.7" class="ltx_td ltx_align_center ltx_border_t">37.46</td>
<td id="S4.T3.2.3.3.8" class="ltx_td ltx_align_center ltx_border_t">22.70</td>
<td id="S4.T3.2.3.3.9" class="ltx_td ltx_align_center ltx_border_t">54.69</td>
<td id="S4.T3.2.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.07</td>
<td id="S4.T3.2.3.3.11" class="ltx_td ltx_align_center ltx_border_t">33.23</td>
</tr>
<tr id="S4.T3.2.4.4" class="ltx_tr">
<th id="S4.T3.2.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.2.4.4.1.1" class="ltx_text ltx_font_bold">FUTR3D</span></th>
<td id="S4.T3.2.4.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.4.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.4.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.4.4.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.2.4.4.6" class="ltx_td ltx_align_center ltx_border_r">32.42</td>
<td id="S4.T3.2.4.4.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.4.4.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.4.4.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.4.4.10" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.2.4.4.11" class="ltx_td ltx_align_center">37.51</td>
</tr>
<tr id="S4.T3.2.5.5" class="ltx_tr">
<th id="S4.T3.2.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.2.5.5.1.1" class="ltx_text ltx_font_bold">RCFusion</span></th>
<td id="S4.T3.2.5.5.2" class="ltx_td ltx_align_center">29.72</td>
<td id="S4.T3.2.5.5.3" class="ltx_td ltx_align_center">27.17</td>
<td id="S4.T3.2.5.5.4" class="ltx_td ltx_align_center">54.93</td>
<td id="S4.T3.2.5.5.5" class="ltx_td ltx_align_center ltx_border_r">23.56</td>
<td id="S4.T3.2.5.5.6" class="ltx_td ltx_align_center ltx_border_r">33.85</td>
<td id="S4.T3.2.5.5.7" class="ltx_td ltx_align_center">40.89</td>
<td id="S4.T3.2.5.5.8" class="ltx_td ltx_align_center">30.95</td>
<td id="S4.T3.2.5.5.9" class="ltx_td ltx_align_center">58.30</td>
<td id="S4.T3.2.5.5.10" class="ltx_td ltx_align_center ltx_border_r">28.92</td>
<td id="S4.T3.2.5.5.11" class="ltx_td ltx_align_center">39.76</td>
</tr>
<tr id="S4.T3.2.6.6" class="ltx_tr">
<th id="S4.T3.2.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.2.6.6.1.1" class="ltx_text ltx_font_bold">LXL</span></th>
<td id="S4.T3.2.6.6.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.6.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.6.6.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.2.6.6.6" class="ltx_td ltx_align_center ltx_border_r">36.32</td>
<td id="S4.T3.2.6.6.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.6.6.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.6.6.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.6.6.10" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.2.6.6.11" class="ltx_td ltx_align_center">41.20</td>
</tr>
<tr id="S4.T3.2.7.7" class="ltx_tr">
<th id="S4.T3.2.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T3.2.7.7.1.1" class="ltx_text ltx_font_bold">BEVFusion</span></th>
<td id="S4.T3.2.7.7.2" class="ltx_td ltx_align_center ltx_border_t">38.09</td>
<td id="S4.T3.2.7.7.3" class="ltx_td ltx_align_center ltx_border_t">29.45</td>
<td id="S4.T3.2.7.7.4" class="ltx_td ltx_align_center ltx_border_t">51.26</td>
<td id="S4.T3.2.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.73</td>
<td id="S4.T3.2.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.63</td>
<td id="S4.T3.2.7.7.7" class="ltx_td ltx_align_center ltx_border_t">48.53</td>
<td id="S4.T3.2.7.7.8" class="ltx_td ltx_align_center ltx_border_t">32.04</td>
<td id="S4.T3.2.7.7.9" class="ltx_td ltx_align_center ltx_border_t">55.40</td>
<td id="S4.T3.2.7.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28.96</td>
<td id="S4.T3.2.7.7.11" class="ltx_td ltx_align_center ltx_border_t">41.23</td>
</tr>
<tr id="S4.T3.2.8.8" class="ltx_tr">
<th id="S4.T3.2.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T3.2.8.8.1.1" class="ltx_text ltx_font_bold">UniBEVFusion</span></th>
<td id="S4.T3.2.8.8.2" class="ltx_td ltx_align_center ltx_border_bb">44.26</td>
<td id="S4.T3.2.8.8.3" class="ltx_td ltx_align_center ltx_border_bb">27.92</td>
<td id="S4.T3.2.8.8.4" class="ltx_td ltx_align_center ltx_border_bb">51.11</td>
<td id="S4.T3.2.8.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">27.75</td>
<td id="S4.T3.2.8.8.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">37.76</td>
<td id="S4.T3.2.8.8.7" class="ltx_td ltx_align_center ltx_border_bb">50.43</td>
<td id="S4.T3.2.8.8.8" class="ltx_td ltx_align_center ltx_border_bb">29.57</td>
<td id="S4.T3.2.8.8.9" class="ltx_td ltx_align_center ltx_border_bb">56.48</td>
<td id="S4.T3.2.8.8.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">35.22</td>
<td id="S4.T3.2.8.8.11" class="ltx_td ltx_align_center ltx_border_bb">42.92</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">Comparison of the results on TJ4D.</span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Failure Test</span>
</h3>

<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.16" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.16.17.1" class="ltx_tr">
<th id="S4.T4.16.17.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" rowspan="2"><span id="S4.T4.16.17.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T4.16.17.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" rowspan="2"><span id="S4.T4.16.17.1.2.1" class="ltx_text ltx_font_bold">RDL</span></th>
<th id="S4.T4.16.17.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" rowspan="2"><span id="S4.T4.16.17.1.3.1" class="ltx_text ltx_font_bold">UFF</span></th>
<th id="S4.T4.16.17.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4"><span id="S4.T4.16.17.1.4.1" class="ltx_text ltx_font_bold">TJ4D 3D mAP</span></th>
<th id="S4.T4.16.17.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l" colspan="4"><span id="S4.T4.16.17.1.5.1" class="ltx_text ltx_font_bold">TJ4D BEV mAP</span></th>
</tr>
<tr id="S4.T4.8.8" class="ltx_tr">
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.1.1.1.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.1.1.1.1.1" class="ltx_sub"><span id="S4.T4.1.1.1.1.1.1" class="ltx_text ltx_font_medium">0</span></sub></span></th>
<th id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.2.2.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.2.2.2.1.1" class="ltx_sub"><span id="S4.T4.2.2.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.5</span></sub></span></th>
<th id="S4.T4.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.3.3.3.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.3.3.3.1.1" class="ltx_sub"><span id="S4.T4.3.3.3.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.7</span></sub></span></th>
<th id="S4.T4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.4.4.4.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.4.4.4.1.1" class="ltx_sub"><span id="S4.T4.4.4.4.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.9</span></sub></span></th>
<th id="S4.T4.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.5.5.5.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.5.5.5.1.1" class="ltx_sub"><span id="S4.T4.5.5.5.1.1.1" class="ltx_text ltx_font_medium">0</span></sub></span></th>
<th id="S4.T4.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.6.6.6.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.6.6.6.1.1" class="ltx_sub"><span id="S4.T4.6.6.6.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.5</span></sub></span></th>
<th id="S4.T4.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.7.7.7.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.7.7.7.1.1" class="ltx_sub"><span id="S4.T4.7.7.7.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.7</span></sub></span></th>
<th id="S4.T4.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.8.8.8.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.8.8.8.1.1" class="ltx_sub"><span id="S4.T4.8.8.8.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.9</span></sub></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.16.18.1" class="ltx_tr">
<td id="S4.T4.16.18.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.16.18.1.1.1" class="ltx_text ltx_font_bold">BEVFusion</span></td>
<td id="S4.T4.16.18.1.2" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S4.T4.16.18.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S4.T4.16.18.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.63</td>
<td id="S4.T4.16.18.1.5" class="ltx_td ltx_align_center ltx_border_t">21.03</td>
<td id="S4.T4.16.18.1.6" class="ltx_td ltx_align_center ltx_border_t">17.17</td>
<td id="S4.T4.16.18.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.43</td>
<td id="S4.T4.16.18.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.23</td>
<td id="S4.T4.16.18.1.9" class="ltx_td ltx_align_center ltx_border_t">26.49</td>
<td id="S4.T4.16.18.1.10" class="ltx_td ltx_align_center ltx_border_t">21.06</td>
<td id="S4.T4.16.18.1.11" class="ltx_td ltx_align_center ltx_border_t">14.03</td>
</tr>
<tr id="S4.T4.16.19.2" class="ltx_tr">
<td id="S4.T4.16.19.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.16.19.2.1.1" class="ltx_text ltx_font_bold">BEVFusion</span></td>
<td id="S4.T4.16.19.2.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T4.16.19.2.3" class="ltx_td ltx_align_center ltx_border_r">✗</td>
<td id="S4.T4.16.19.2.4" class="ltx_td ltx_align_center ltx_border_r">36.23</td>
<td id="S4.T4.16.19.2.5" class="ltx_td ltx_align_center">21.23</td>
<td id="S4.T4.16.19.2.6" class="ltx_td ltx_align_center">17.26</td>
<td id="S4.T4.16.19.2.7" class="ltx_td ltx_align_center ltx_border_r">11.84</td>
<td id="S4.T4.16.19.2.8" class="ltx_td ltx_align_center ltx_border_r">41.98</td>
<td id="S4.T4.16.19.2.9" class="ltx_td ltx_align_center">26.80</td>
<td id="S4.T4.16.19.2.10" class="ltx_td ltx_align_center">21.71</td>
<td id="S4.T4.16.19.2.11" class="ltx_td ltx_align_center">14.53</td>
</tr>
<tr id="S4.T4.16.20.3" class="ltx_tr">
<td id="S4.T4.16.20.3.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.16.20.3.1.1" class="ltx_text ltx_font_bold">UniBEVFusion</span></td>
<td id="S4.T4.16.20.3.2" class="ltx_td ltx_align_center">✗</td>
<td id="S4.T4.16.20.3.3" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T4.16.20.3.4" class="ltx_td ltx_align_center ltx_border_r">36.84</td>
<td id="S4.T4.16.20.3.5" class="ltx_td ltx_align_center">22.54</td>
<td id="S4.T4.16.20.3.6" class="ltx_td ltx_align_center">17.61</td>
<td id="S4.T4.16.20.3.7" class="ltx_td ltx_align_center ltx_border_r">12.01</td>
<td id="S4.T4.16.20.3.8" class="ltx_td ltx_align_center ltx_border_r">42.49</td>
<td id="S4.T4.16.20.3.9" class="ltx_td ltx_align_center">27.19</td>
<td id="S4.T4.16.20.3.10" class="ltx_td ltx_align_center">21.81</td>
<td id="S4.T4.16.20.3.11" class="ltx_td ltx_align_center">15.55</td>
</tr>
<tr id="S4.T4.16.21.4" class="ltx_tr">
<td id="S4.T4.16.21.4.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.16.21.4.1.1" class="ltx_text ltx_font_bold">UniBEVFusion</span></td>
<td id="S4.T4.16.21.4.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T4.16.21.4.3" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T4.16.21.4.4" class="ltx_td ltx_align_center ltx_border_r">37.76</td>
<td id="S4.T4.16.21.4.5" class="ltx_td ltx_align_center">22.79</td>
<td id="S4.T4.16.21.4.6" class="ltx_td ltx_align_center">17.44</td>
<td id="S4.T4.16.21.4.7" class="ltx_td ltx_align_center ltx_border_r">12.47</td>
<td id="S4.T4.16.21.4.8" class="ltx_td ltx_align_center ltx_border_r">42.92</td>
<td id="S4.T4.16.21.4.9" class="ltx_td ltx_align_center">27.70</td>
<td id="S4.T4.16.21.4.10" class="ltx_td ltx_align_center">22.11</td>
<td id="S4.T4.16.21.4.11" class="ltx_td ltx_align_center">16.17</td>
</tr>
<tr id="S4.T4.16.22.5" class="ltx_tr">
<th id="S4.T4.16.22.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.16.22.5.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T4.16.22.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S4.T4.16.22.5.2.1" class="ltx_text ltx_font_bold">RDL</span></th>
<th id="S4.T4.16.22.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.16.22.5.3.1" class="ltx_text ltx_font_bold">UFF</span></th>
<th id="S4.T4.16.22.5.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4"><span id="S4.T4.16.22.5.4.1" class="ltx_text ltx_font_bold">VoD ALL</span></th>
<th id="S4.T4.16.22.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t" colspan="4"><span id="S4.T4.16.22.5.5.1" class="ltx_text ltx_font_bold">VoD RoI</span></th>
</tr>
<tr id="S4.T4.16.16" class="ltx_tr">
<th id="S4.T4.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.9.9.1.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.9.9.1.1.1" class="ltx_sub"><span id="S4.T4.9.9.1.1.1.1" class="ltx_text ltx_font_medium">0</span></sub></span></th>
<th id="S4.T4.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.10.10.2.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.10.10.2.1.1" class="ltx_sub"><span id="S4.T4.10.10.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.5</span></sub></span></th>
<th id="S4.T4.11.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.11.11.3.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.11.11.3.1.1" class="ltx_sub"><span id="S4.T4.11.11.3.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.7</span></sub></span></th>
<th id="S4.T4.12.12.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.12.12.4.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.12.12.4.1.1" class="ltx_sub"><span id="S4.T4.12.12.4.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.9</span></sub></span></th>
<th id="S4.T4.13.13.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.13.13.5.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.13.13.5.1.1" class="ltx_sub"><span id="S4.T4.13.13.5.1.1.1" class="ltx_text ltx_font_medium">0</span></sub></span></th>
<th id="S4.T4.14.14.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.14.14.6.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.14.14.6.1.1" class="ltx_sub"><span id="S4.T4.14.14.6.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.5</span></sub></span></th>
<th id="S4.T4.15.15.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.15.15.7.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.15.15.7.1.1" class="ltx_sub"><span id="S4.T4.15.15.7.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.7</span></sub></span></th>
<th id="S4.T4.16.16.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.16.16.8.1" class="ltx_text ltx_font_bold">FT<sub id="S4.T4.16.16.8.1.1" class="ltx_sub"><span id="S4.T4.16.16.8.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">0.9</span></sub></span></th>
</tr>
<tr id="S4.T4.16.23.6" class="ltx_tr">
<td id="S4.T4.16.23.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.16.23.6.1.1" class="ltx_text ltx_font_bold">BEVFusion</span></td>
<td id="S4.T4.16.23.6.2" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S4.T4.16.23.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S4.T4.16.23.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.79</td>
<td id="S4.T4.16.23.6.5" class="ltx_td ltx_align_center ltx_border_t">41.04</td>
<td id="S4.T4.16.23.6.6" class="ltx_td ltx_align_center ltx_border_t">36.78</td>
<td id="S4.T4.16.23.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.37</td>
<td id="S4.T4.16.23.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.42</td>
<td id="S4.T4.16.23.6.9" class="ltx_td ltx_align_center ltx_border_t">56.13</td>
<td id="S4.T4.16.23.6.10" class="ltx_td ltx_align_center ltx_border_t">50.01</td>
<td id="S4.T4.16.23.6.11" class="ltx_td ltx_align_center ltx_border_t">44.32</td>
</tr>
<tr id="S4.T4.16.24.7" class="ltx_tr">
<td id="S4.T4.16.24.7.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.16.24.7.1.1" class="ltx_text ltx_font_bold">BEVFusion</span></td>
<td id="S4.T4.16.24.7.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T4.16.24.7.3" class="ltx_td ltx_align_center ltx_border_r">✗</td>
<td id="S4.T4.16.24.7.4" class="ltx_td ltx_align_center ltx_border_r">53.77</td>
<td id="S4.T4.16.24.7.5" class="ltx_td ltx_align_center">41.15</td>
<td id="S4.T4.16.24.7.6" class="ltx_td ltx_align_center">36.80</td>
<td id="S4.T4.16.24.7.7" class="ltx_td ltx_align_center ltx_border_r">30.35</td>
<td id="S4.T4.16.24.7.8" class="ltx_td ltx_align_center ltx_border_r">74.02</td>
<td id="S4.T4.16.24.7.9" class="ltx_td ltx_align_center">56.24</td>
<td id="S4.T4.16.24.7.10" class="ltx_td ltx_align_center">50.69</td>
<td id="S4.T4.16.24.7.11" class="ltx_td ltx_align_center">44.53</td>
</tr>
<tr id="S4.T4.16.25.8" class="ltx_tr">
<td id="S4.T4.16.25.8.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.16.25.8.1.1" class="ltx_text ltx_font_bold">UniBEVFusion</span></td>
<td id="S4.T4.16.25.8.2" class="ltx_td ltx_align_center">✗</td>
<td id="S4.T4.16.25.8.3" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T4.16.25.8.4" class="ltx_td ltx_align_center ltx_border_r">53.70</td>
<td id="S4.T4.16.25.8.5" class="ltx_td ltx_align_center">41.42</td>
<td id="S4.T4.16.25.8.6" class="ltx_td ltx_align_center">37.69</td>
<td id="S4.T4.16.25.8.7" class="ltx_td ltx_align_center ltx_border_r">31.70</td>
<td id="S4.T4.16.25.8.8" class="ltx_td ltx_align_center ltx_border_r">72.50</td>
<td id="S4.T4.16.25.8.9" class="ltx_td ltx_align_center">58.00</td>
<td id="S4.T4.16.25.8.10" class="ltx_td ltx_align_center">51.04</td>
<td id="S4.T4.16.25.8.11" class="ltx_td ltx_align_center">44.27</td>
</tr>
<tr id="S4.T4.16.26.9" class="ltx_tr">
<td id="S4.T4.16.26.9.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.16.26.9.1.1" class="ltx_text ltx_font_bold">UniBEVFusion</span></td>
<td id="S4.T4.16.26.9.2" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S4.T4.16.26.9.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S4.T4.16.26.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">54.09</td>
<td id="S4.T4.16.26.9.5" class="ltx_td ltx_align_center ltx_border_bb">41.69</td>
<td id="S4.T4.16.26.9.6" class="ltx_td ltx_align_center ltx_border_bb">37.26</td>
<td id="S4.T4.16.26.9.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">33.03</td>
<td id="S4.T4.16.26.9.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">74.37</td>
<td id="S4.T4.16.26.9.9" class="ltx_td ltx_align_center ltx_border_bb">58.87</td>
<td id="S4.T4.16.26.9.10" class="ltx_td ltx_align_center ltx_border_bb">51.74</td>
<td id="S4.T4.16.26.9.11" class="ltx_td ltx_align_center ltx_border_bb">45.33</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.18.1.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S4.T4.19.2" class="ltx_text" style="font-size:90%;">Comparison between BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and UniBEVFusion in Failure Test (FT).</span></figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.6" class="ltx_p">Based on the previous introduction, we evaluate BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and our proposed UniBEVFusion model using <math id="S4.SS4.p1.1.m1.3" class="ltx_Math" alttext="\rho=[0.5,0.7,0.9]" display="inline"><semantics id="S4.SS4.p1.1.m1.3a"><mrow id="S4.SS4.p1.1.m1.3.4" xref="S4.SS4.p1.1.m1.3.4.cmml"><mi id="S4.SS4.p1.1.m1.3.4.2" xref="S4.SS4.p1.1.m1.3.4.2.cmml">ρ</mi><mo id="S4.SS4.p1.1.m1.3.4.1" xref="S4.SS4.p1.1.m1.3.4.1.cmml">=</mo><mrow id="S4.SS4.p1.1.m1.3.4.3.2" xref="S4.SS4.p1.1.m1.3.4.3.1.cmml"><mo stretchy="false" id="S4.SS4.p1.1.m1.3.4.3.2.1" xref="S4.SS4.p1.1.m1.3.4.3.1.cmml">[</mo><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">0.5</mn><mo id="S4.SS4.p1.1.m1.3.4.3.2.2" xref="S4.SS4.p1.1.m1.3.4.3.1.cmml">,</mo><mn id="S4.SS4.p1.1.m1.2.2" xref="S4.SS4.p1.1.m1.2.2.cmml">0.7</mn><mo id="S4.SS4.p1.1.m1.3.4.3.2.3" xref="S4.SS4.p1.1.m1.3.4.3.1.cmml">,</mo><mn id="S4.SS4.p1.1.m1.3.3" xref="S4.SS4.p1.1.m1.3.3.cmml">0.9</mn><mo stretchy="false" id="S4.SS4.p1.1.m1.3.4.3.2.4" xref="S4.SS4.p1.1.m1.3.4.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.3b"><apply id="S4.SS4.p1.1.m1.3.4.cmml" xref="S4.SS4.p1.1.m1.3.4"><eq id="S4.SS4.p1.1.m1.3.4.1.cmml" xref="S4.SS4.p1.1.m1.3.4.1"></eq><ci id="S4.SS4.p1.1.m1.3.4.2.cmml" xref="S4.SS4.p1.1.m1.3.4.2">𝜌</ci><list id="S4.SS4.p1.1.m1.3.4.3.1.cmml" xref="S4.SS4.p1.1.m1.3.4.3.2"><cn type="float" id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">0.5</cn><cn type="float" id="S4.SS4.p1.1.m1.2.2.cmml" xref="S4.SS4.p1.1.m1.2.2">0.7</cn><cn type="float" id="S4.SS4.p1.1.m1.3.3.cmml" xref="S4.SS4.p1.1.m1.3.3">0.9</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.3c">\rho=[0.5,0.7,0.9]</annotation></semantics></math>. Since the design of the noise is related to the random numbers, the average of 10 operations was taken for all the test results. Results in Table <a href="#S4.T4" title="TABLE IV ‣ IV-D Failure Test ‣ IV Experiments ‣ UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows the performance of baseline FT<sub id="S4.SS4.p1.6.1" class="ltx_sub">0</sub>, and evaluations FT<sub id="S4.SS4.p1.6.2" class="ltx_sub"><span id="S4.SS4.p1.6.2.1" class="ltx_text ltx_font_italic">0.5</span></sub>, FT<sub id="S4.SS4.p1.6.3" class="ltx_sub"><span id="S4.SS4.p1.6.3.1" class="ltx_text ltx_font_italic">0.7</span></sub>, and FT<sub id="S4.SS4.p1.6.4" class="ltx_sub"><span id="S4.SS4.p1.6.4.1" class="ltx_text ltx_font_italic">0.9</span></sub>. As the baseline FT<sub id="S4.SS4.p1.6.5" class="ltx_sub">0</sub> is the normal evaluation results of these model, thus, we will also discuss the effectiveness of the RDL and UFF in this section.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">For BEVFusion model in TJ4D FT evaluation, adding RDL improves much in baseline performance, but the FT results are close. RDL is designed for accurate depth prediction in image stream, and it can not guarantee the robust results when image is failure. Adding UFF improves both the baseline and FT performance, which indicates that the UFF is effective in improving the robustness of the model. As for the UniBEVFusion model, the conclusion is similar to the BEVFusion model, and the overall FT results are better than BEVFusion.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">In VoD FT evaluation, conclusion are different in two different evaluation range. For entire annotation area, the results are close, and we can not tell the effectiveness of the RDL on this dataset. The UFF bring much leading than the BEVFusion model, which indicates that the UFF is effective in improving the robustness of the model. However, for the driving corridor area, the basic conclusion are similar to the TJ4D. Moreover, with the noise level <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mi id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><ci id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\rho</annotation></semantics></math> increasing, the performance gap between the two models is getting smaller for our UniBEVFusion in all evaluation.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">On top of the results, we can conclude that the UFF and RDL are effective in improving the performance of multi-modal model. Besides, UFF provides a better robustness in the case of vision failure.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.4.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.5.2" class="ltx_text ltx_font_italic">Image Resolution</span>
</h3>

<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.2" class="ltx_tr">
<th id="S4.T5.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T5.2.2.3.1" class="ltx_text ltx_font_bold">Scale</span></th>
<th id="S4.T5.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T5.2.2.4.1" class="ltx_text ltx_font_bold">Image Size</span></th>
<th id="S4.T5.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T5.2.2.5.1" class="ltx_text ltx_font_bold">RDL</span></th>
<th id="S4.T5.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.2.2.6.1" class="ltx_text ltx_font_bold">3D</span></th>
<th id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<math id="S4.T5.1.1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\Delta</annotation></semantics></math><span id="S4.T5.1.1.1.1" class="ltx_text ltx_font_bold"> (%)</span>
</th>
<th id="S4.T5.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l"><span id="S4.T5.2.2.7.1" class="ltx_text ltx_font_bold">BEV</span></th>
<th id="S4.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<math id="S4.T5.2.2.2.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S4.T5.2.2.2.m1.1a"><mi mathvariant="normal" id="S4.T5.2.2.2.m1.1.1" xref="S4.T5.2.2.2.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.m1.1b"><ci id="S4.T5.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.m1.1c">\Delta</annotation></semantics></math><span id="S4.T5.2.2.2.1" class="ltx_text ltx_font_bold"> (%)</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.2.3.1" class="ltx_tr">
<td id="S4.T5.2.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.3.1.1.1" class="ltx_text ltx_font_bold">1.00</span></td>
<td id="S4.T5.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.3.1.2.1" class="ltx_text ltx_font_bold">[960, 1280]</span></td>
<td id="S4.T5.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S4.T5.2.3.1.4" class="ltx_td ltx_align_right ltx_border_t">12.02</td>
<td id="S4.T5.2.3.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.0%</td>
<td id="S4.T5.2.3.1.6" class="ltx_td ltx_align_right ltx_border_t">14.74</td>
<td id="S4.T5.2.3.1.7" class="ltx_td ltx_align_right ltx_border_t">0.0%</td>
</tr>
<tr id="S4.T5.2.4.2" class="ltx_tr">
<td id="S4.T5.2.4.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.2.4.2.1.1" class="ltx_text ltx_font_bold">1.00</span></td>
<td id="S4.T5.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.2.4.2.2.1" class="ltx_text ltx_font_bold">[960, 1280]</span></td>
<td id="S4.T5.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T5.2.4.2.4" class="ltx_td ltx_align_right">13.19</td>
<td id="S4.T5.2.4.2.5" class="ltx_td ltx_align_right ltx_border_r">9.8%</td>
<td id="S4.T5.2.4.2.6" class="ltx_td ltx_align_right">26.73</td>
<td id="S4.T5.2.4.2.7" class="ltx_td ltx_align_right">5.9%</td>
</tr>
<tr id="S4.T5.2.5.3" class="ltx_tr">
<td id="S4.T5.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.5.3.1.1" class="ltx_text ltx_font_bold">0.75</span></td>
<td id="S4.T5.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.5.3.2.1" class="ltx_text ltx_font_bold">[720, 960]</span></td>
<td id="S4.T5.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S4.T5.2.5.3.4" class="ltx_td ltx_align_right ltx_border_t">14.81</td>
<td id="S4.T5.2.5.3.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.0%</td>
<td id="S4.T5.2.5.3.6" class="ltx_td ltx_align_right ltx_border_t">17.85</td>
<td id="S4.T5.2.5.3.7" class="ltx_td ltx_align_right ltx_border_t">0.0%</td>
</tr>
<tr id="S4.T5.2.6.4" class="ltx_tr">
<td id="S4.T5.2.6.4.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.2.6.4.1.1" class="ltx_text ltx_font_bold">0.75</span></td>
<td id="S4.T5.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.2.6.4.2.1" class="ltx_text ltx_font_bold">[720, 960]</span></td>
<td id="S4.T5.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T5.2.6.4.4" class="ltx_td ltx_align_right">16.91</td>
<td id="S4.T5.2.6.4.5" class="ltx_td ltx_align_right ltx_border_r">14.2%</td>
<td id="S4.T5.2.6.4.6" class="ltx_td ltx_align_right">30.39</td>
<td id="S4.T5.2.6.4.7" class="ltx_td ltx_align_right">3.9%</td>
</tr>
<tr id="S4.T5.2.7.5" class="ltx_tr">
<td id="S4.T5.2.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.7.5.1.1" class="ltx_text ltx_font_bold">0.50</span></td>
<td id="S4.T5.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.7.5.2.1" class="ltx_text ltx_font_bold">[480, 640]</span></td>
<td id="S4.T5.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S4.T5.2.7.5.4" class="ltx_td ltx_align_right ltx_border_t">13.66</td>
<td id="S4.T5.2.7.5.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.0%</td>
<td id="S4.T5.2.7.5.6" class="ltx_td ltx_align_right ltx_border_t">16.72</td>
<td id="S4.T5.2.7.5.7" class="ltx_td ltx_align_right ltx_border_t">0.0%</td>
</tr>
<tr id="S4.T5.2.8.6" class="ltx_tr">
<td id="S4.T5.2.8.6.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.2.8.6.1.1" class="ltx_text ltx_font_bold">0.50</span></td>
<td id="S4.T5.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.2.8.6.2.1" class="ltx_text ltx_font_bold">[480, 640]</span></td>
<td id="S4.T5.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T5.2.8.6.4" class="ltx_td ltx_align_right">14.46</td>
<td id="S4.T5.2.8.6.5" class="ltx_td ltx_align_right ltx_border_r">5.8%</td>
<td id="S4.T5.2.8.6.6" class="ltx_td ltx_align_right">29.68</td>
<td id="S4.T5.2.8.6.7" class="ltx_td ltx_align_right">4.3%</td>
</tr>
<tr id="S4.T5.2.9.7" class="ltx_tr">
<td id="S4.T5.2.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.9.7.1.1" class="ltx_text ltx_font_bold">0.25</span></td>
<td id="S4.T5.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.9.7.2.1" class="ltx_text ltx_font_bold">[240, 320]</span></td>
<td id="S4.T5.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S4.T5.2.9.7.4" class="ltx_td ltx_align_right ltx_border_t">7.54</td>
<td id="S4.T5.2.9.7.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.0%</td>
<td id="S4.T5.2.9.7.6" class="ltx_td ltx_align_right ltx_border_t">7.63</td>
<td id="S4.T5.2.9.7.7" class="ltx_td ltx_align_right ltx_border_t">0.0%</td>
</tr>
<tr id="S4.T5.2.10.8" class="ltx_tr">
<td id="S4.T5.2.10.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.2.10.8.1.1" class="ltx_text ltx_font_bold">0.25</span></td>
<td id="S4.T5.2.10.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.2.10.8.2.1" class="ltx_text ltx_font_bold">[240, 320]</span></td>
<td id="S4.T5.2.10.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S4.T5.2.10.8.4" class="ltx_td ltx_align_right ltx_border_bb">6.44</td>
<td id="S4.T5.2.10.8.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">-14.6%</td>
<td id="S4.T5.2.10.8.6" class="ltx_td ltx_align_right ltx_border_bb">10.02</td>
<td id="S4.T5.2.10.8.7" class="ltx_td ltx_align_right ltx_border_bb">-2.1%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.4.1.1" class="ltx_text" style="font-size:90%;">TABLE V</span>: </span><span id="S4.T5.5.2" class="ltx_text" style="font-size:90%;">Comparison of different image resolutions on TJ4D.</span></figcaption>
</figure>
<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">In RCFusion, they shows that larger image sizes have a positive impact on the fusion model results, but also increase the arithmetic consumption and decrease the FPS. Immediately following the discussion on image sizes, we test the performance of the pure camera modality on the TJ4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> dataset for different image scaling as well as validate our proposed RDL. It is worth noting that although we are testing the performance of pure image data, BEVFusion still uses coordination information from the point cloud to assist depth prediction.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Evaluating scaling from 0.25 to 0.75 shows a consistent trend with RCFusion, where smaller scales result in missing information and reduced performance. Interestingly, full-size images performed worse than 0.5 and 0.75 due to the model being tuned for 0.5 and overfitting on detailed images. The 0.25 scale yielded the worst results due to excessive detail loss and sparse features after BEV transformation. Despite the slightly better performance at 0.75, we opted for 0.50 scaling for operational speed.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">Moreover, comparing the effectiveness of our proposed RDL, the results of 0.50 1 outperform the original BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> by at least 5.84% and 3.88% in 3D and BEV, respectively. However, the performance at 0.25 is reduced by 14.6% and 2.1%, respectively. In the absence of image information, image features and coordinate information are misaligned. RCS information representing the shape, material, and size of the object is also incorrectly added to features, resulting in learning wrong features and worse performance.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we demonstrated that the UniBEVFusion network achieves state-of-the-art performance on the TJ4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and driving corridor of the View-of-Delft (VoD) datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The results indicate that UniBEVFusion significantly improves detection performance, particularly in challenging conditions such as shadows, occlusions, short-range, and long-range scenarios. Our proposed Radar Depth Lift-Splat-Shoot (RDL) module and Unified Feature Fusion (UFF) framework are effective in enhancing the model’s performance. Specifically, RDL integrates radar depth and RCS information into the depth prediction process, boosting the accuracy of vision-based 3D object detection. UFF mitigates the model’s reliance on the simultaneous availability of multiple modalities, improving its robustness against single-modality failures. Although Gaussian noise was the only simulation solution used in the Failure Test (FT), it still provided valuable insights into the model’s robustness. In future work, we plan to further optimize UFF and RDL to improve the performance of multi-modal models in scenarios where one modality fails. In addition, we will incorporate more diverse failure modes into the FT and develop more precise evaluation metrics to better assess robustness.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Li Wang, Xinyu Zhang, Ziying Song, Jiangfeng Bi, Guoxin Zhang, Haiyue Wei, Liyao Tang, Lei Yang, Jun Li, Caiyan Jia, et al.,

</span>
<span class="ltx_bibblock">“Multi-modal 3d object detection in autonomous driving: A survey and taxonomy,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, vol. 8, no. 7, pp. 3781–3798, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Yingjie Wang, Qiuyu Mao, Hanqi Zhu, Jiajun Deng, Yu Zhang, Jianmin Ji, Houqiang Li, and Yanyong Zhang,

</span>
<span class="ltx_bibblock">“Multi-modal 3d object detection in autonomous driving: a survey,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, vol. 131, no. 8, pp. 2122–2152, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Huijuan Wang, Xinyue Chen, Quanbo Yuan, and Peng Liu,

</span>
<span class="ltx_bibblock">“A review of 3d object detection based on autonomous driving,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">The Visual Computer</span>, pp. 1–19, 2024.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Shanliang Yao, Runwei Guan, Xiaoyu Huang, Zhuoxiao Li, Xiangyu Sha, Yong Yue, Eng Gee Lim, Hyungjoon Seo, Ka Lok Man, Xiaohui Zhu, et al.,

</span>
<span class="ltx_bibblock">“Radar-camera fusion for object detection and semantic segmentation in autonomous driving: A comprehensive review,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Lianqing Zheng, Sen Li, Bin Tan, Long Yang, Sihan Chen, Libo Huang, Jie Bai, Xichan Zhu, and Zhixiong Ma,

</span>
<span class="ltx_bibblock">“Rcfusion: Fusing 4-d radar and camera with bird’s-eye view features for 3-d object detection,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Instrumentation and Measurement</span>, vol. 72, pp. 1–14, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Alexander Musiat, Laurenz Reichardt, Michael Schulze, and Oliver Wasenmüller,

</span>
<span class="ltx_bibblock">“Radarpillars: Efficient object detection from 4d radar point clouds,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2408.05020</span>, 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Weiyi Xiong, Jianan Liu, Tao Huang, Qing-Long Han, Yuxuan Xia, and Bing Zhu,

</span>
<span class="ltx_bibblock">“Lxl: Lidar excluded lean 3d object detection with 4d imaging radar and camera fusion,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Jonah Philion and Sanja Fidler,

</span>
<span class="ltx_bibblock">“Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16</span>. Springer, 2020, pp. 194–210.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, and Song Han,

</span>
<span class="ltx_bibblock">“Bevfusion: Multi-task multi-sensor fusion with unified bird’s-eye view representation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">2023 IEEE international conference on robotics and automation (ICRA)</span>. IEEE, 2023, pp. 2774–2781.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Can Cui, Yunsheng Ma, Juanwu Lu, and Ziran Wang,

</span>
<span class="ltx_bibblock">“Redformer: Radar enlightens the darkness of camera perception with transformers,”

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yuwei Cheng, Hu Xu, and Yimin Liu,

</span>
<span class="ltx_bibblock">“Robust small object detection on the water surface through fusion of camera and millimeter wave radar,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</span>, 2021, pp. 15263–15272.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Kaikai Deng, Dong Zhao, Qiaoyue Han, Zihan Zhang, Shuyue Wang, and Huadong Ma,

</span>
<span class="ltx_bibblock">“Global-local feature enhancement network for robust object detection using mmwave radar and camera,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>. IEEE, 2022, pp. 4708–4712.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kshitiz Bansal, Keshav Rungta, and Dinesh Bharadia,

</span>
<span class="ltx_bibblock">“Radsegnet: A reliable approach to radar camera fusion,”

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2208.03849</span>, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas,

</span>
<span class="ltx_bibblock">“Pointnet: Deep learning on point sets for 3d classification and segmentation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, 2017, pp. 652–660.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Bo Li, Tianlei Zhang, and Tian Xia,

</span>
<span class="ltx_bibblock">“Vehicle detection from 3d lidar using fully convolutional network,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1608.07916</span>, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Bin Yang, Wenjie Luo, and Raquel Urtasun,

</span>
<span class="ltx_bibblock">“Pixor: Real-time 3d object detection from point clouds,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</span>, 2018, pp. 7652–7660.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yin Zhou and Oncel Tuzel,

</span>
<span class="ltx_bibblock">“Voxelnet: End-to-end learning for point cloud based 3d object detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, 2018, pp. 4490–4499.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom,

</span>
<span class="ltx_bibblock">“Pointpillars: Fast encoders for object detection from point clouds,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, 2019, pp. 12697–12705.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Marcelo Contreras, Aayush Jain, Neel P Bhatt, Arunava Banerjee, and Ehsan Hashemi,

</span>
<span class="ltx_bibblock">“A survey on 3d object detection in real time for autonomous driving,”

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Frontiers in Robotics and AI</span>, vol. 11, pp. 1212070, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Andras Palffy, Ewoud Pool, Srimannarayana Baratam, Julian F. P. Kooij, and Dariu M. Gavrila,

</span>
<span class="ltx_bibblock">“Multi-class road user detection with 3+1d radar in the view-of-delft dataset,”

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and Automation Letters</span>, vol. 7, no. 2, pp. 4961–4968, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Youngseok Kim, Juyeb Shin, Sanmin Kim, In-Jae Lee, Jun Won Choi, and Dongsuk Kum,

</span>
<span class="ltx_bibblock">“Crn: Camera radar net for accurate, robust, efficient 3d perception,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, 2023, pp. 17615–17626.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Tim Broedermann, Christos Sakaridis, Dengxin Dai, and Luc Van Gool,

</span>
<span class="ltx_bibblock">“Hrfuser: A multi-resolution sensor fusion architecture for 2d object detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)</span>. IEEE, 2023, pp. 4159–4166.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Andras Palffy, Julian FP Kooij, and Dariu M Gavrila,

</span>
<span class="ltx_bibblock">“Detecting darting out pedestrians with occlusion aware sensor fusion of radar and stereo camera,”

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, vol. 8, no. 2, pp. 1459–1472, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Zedong Yu, Weibing Wan, Maiyu Ren, Xiuyuan Zheng, and Zhijun Fang,

</span>
<span class="ltx_bibblock">“Sparsefusion3d: Sparse sensor fusion for 3d object detection by radar and camera in environmental perception,”

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Xiangyuan Peng, Miao Tang, Huawei Sun, Kay Bierzynski, Lorenzo Servadei, and Robert Wille,

</span>
<span class="ltx_bibblock">“Mufasa: Multi-view fusion and adaptation network with spatial awareness for radar object detection,”

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2408.00565</span>, 2024.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Lingjun Zhao, Jingyu Song, and Katherine A Skinner,

</span>
<span class="ltx_bibblock">“Crkd: Enhanced camera-radar object detection with cross-modality knowledge distillation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, 2024, pp. 15470–15480.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Lili Fan, Changxian Zeng, Yunjie Li, Xu Wang, and Dongpu Cao,

</span>
<span class="ltx_bibblock">“Grc-net: Fusing gat-based 4d radar and camera for 3d object detection,”

</span>
<span class="ltx_bibblock">Tech. Rep., SAE Technical Paper, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Zizhang Wu, Guilian Chen, Yuanzhu Gan, Lei Wang, and Jian Pu,

</span>
<span class="ltx_bibblock">“Mvfusion: Multi-view 3d object detection with semantic-aligned radar and camera fusion,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">2023 IEEE International Conference on Robotics and Automation (ICRA)</span>. IEEE, 2023, pp. 2766–2773.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Yunfei Long, Abhinav Kumar, Daniel Morris, Xiaoming Liu, Marcos Castro, and Punarjay Chakravarty,

</span>
<span class="ltx_bibblock">“Radiant: Radar-image association network for 3d object detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>, 2023, vol. 37, pp. 1808–1816.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Xuanyao Chen, Tianyuan Zhang, Yue Wang, Yilun Wang, and Hang Zhao,

</span>
<span class="ltx_bibblock">“Futr3d: A unified sensor fusion framework for 3d detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, 2023, pp. 172–181.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Zhiwei Lin, Zhe Liu, Zhongyu Xia, Xinhao Wang, Yongtao Wang, Shengxiang Qi, Yang Dong, Nan Dong, Le Zhang, and Ce Zhu,

</span>
<span class="ltx_bibblock">“Rcbevdet: Radar-camera fusion in bird’s eye view for 3d object detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, 2024, pp. 14928–14937.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Lianqing Zheng, Zhixiong Ma, Xichan Zhu, Bin Tan, Sen Li, Kai Long, Weiqi Sun, Sihan Chen, Lu Zhang, Mengyue Wan, Libo Huang, and Jie Bai,

</span>
<span class="ltx_bibblock">“Tj4dradset: A 4d radar dataset for autonomous driving,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)</span>, 2022, pp. 493–498.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo,

</span>
<span class="ltx_bibblock">“Swin transformer: Hierarchical vision transformer using shifted windows,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</span>, 2021, pp. 10012–10022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Yan Yan, Yuxing Mao, and Bo Li,

</span>
<span class="ltx_bibblock">“Second: Sparsely embedded convolutional detection,”

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 18, no. 10, pp. 3337, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.14750" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.14751" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.14751">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.14751" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.14752" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:50:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
