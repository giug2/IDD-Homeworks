<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.05701] Synthetic Data for Robust Identification of Typical and Atypical Serotonergic Neurons using Convolutional Neural Networks</title><meta property="og:description" content="Serotonergic neurons in the raphe nuclei exhibit diverse electrophysiological
properties and functional roles, yet conventional identification methods
rely on restrictive criteria that likely overlook atypical serotone…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data for Robust Identification of Typical and Atypical Serotonergic Neurons using Convolutional Neural Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data for Robust Identification of Typical and Atypical Serotonergic Neurons using Convolutional Neural Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.05701">

<!--Generated on Mon Aug  5 17:39:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Deep Learning Models Serotonergic Neurons Convolutional Neural Networks Synthetic Data Spike Recognition">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Grupo de Fisica Matematica (IST), Av. Rovisco Pais, Lisboa, 1049-001, Portugal </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Departamento de Matematica, Universidade do Algarve, Campus de Gambelas, Faro, 8005-139,Portugal
</span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Department of Neuroscience, Psychology, Drug Research and Child Health (NEUROFARBA), University of Florence,Viale G. Pieraccini 6,Firenze,50139,Toscana,Italy</span></span></span>
<h1 class="ltx_title ltx_title_document">Synthetic Data for Robust Identification of Typical and Atypical Serotonergic Neurons using Convolutional Neural Networks </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniele Corradetti 
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alessandro Bernardi
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Renato Corradetti
</span><span class="ltx_author_notes">33</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Serotonergic neurons in the raphe nuclei exhibit diverse electrophysiological
properties and functional roles, yet conventional identification methods
rely on restrictive criteria that likely overlook atypical serotonergic
cells. The use of convolutional neural network (CNN) for comprehensive
classification of both typical and atypical serotonergic neurons is
an interesting one, but the key challenge is often given by the limited
experimental data available for training. This study presents a procedure for synthetic data generation that combines smoothed spike waveforms with heterogeneous noise masks from real recordings. This approach expanded the training set while
mitigating overfitting of background noise signatures. CNN models
trained on the augmented dataset achieved high accuracy (96.2% true
positive rate, 88.8% true negative rate) on non-homogeneous test
data collected under different experimental conditions than the training,
validation and testing data.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Deep Learning Models Serotonergic Neurons Convolutional Neural Networks Synthetic Data Spike Recognition
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Serotonergic neurons of the raphe nuclei play an important role in
regulating diverse brain functions and behaviors, including mood,
cognition, sleep, appetite, and pain modulation (Okaty et al., 2019). However,
the serotonergic system is highly heterogeneous, comprised of subpopulations
of neurons with distinct anatomical projections, neurochemistry, physiology,
and functional roles. Elucidating the diversity of serotonergic neurons
is essential to understand how modulatory control of brain states
emerges from serotonergic network dynamics. Traditionally, serotonergic
neurons have been identified in electrophysiology studies based on
“typical” extracellular spiking
characteristics, namely, slow regular firing and long spike duration
(Vandermaelen and Aghajanian, 1983). While useful, these criteria likely
overlook atypical serotonergic neurons, introducing a selection bias
that limits insights into the full diversity of the serotonergic system
(Otaky et al., 2019; Calizo et al., 2011). Deep learning methods like convolutional neural
networks (CNNs) offer a powerful alternative for serotonergic neuron
identification. By learning distinctive spike waveform features, CNNs
can accurately discriminate both typical and atypical serotonergic neurons from non-serotonergic cells
(Corradetti et al., 2024). However, developing robust CNN models for recognizing serotonergic cells from recordings poses practical challenges since large datasets from identified neurons are required for model training, while experimental recordings from serotonergic neurons, identified by methods independent of the signal recordings, are typically limited to hundreds of cells. A first way of overcoming the problem
would be through the use of a naive data augmentation, extracting
short segments as surrogate samples from each recording, thus increasing
by one or two orders of magnitudes the number of labeled samples.
Unfortunately, directly augmenting a limited number of recordings
risks is a huge source of overfitting. A key role in this phenomenon
is represented by the specific noise signature of the experiment which
characterize all the experiments and that is therefore learned by
the model in order rather than meaningful action potential features.
This study addresses the challenges in developing a deep learning model for atypical serotonergic cell recognition using synthetic data generation.
Indeed, this work shows that carefully designed synthetic data augmentation
from limited electrophysiology data can be of use in the development
of deep learning models with high accuracy recognition (Corradetti
et al. 2024).
The paper is structured as follows. Section 2 discusses the limitations of traditional visual identification methods for serotonergic neurons, while Section 3 examines the problems in applying straightforward deep learning approaches mainly due to the issue of limited experimental data. Section 4 presents a methodology for generating synthetic data to augment the training set. Section 5 describes a practical case study implementing the approach. Finally, Section 6 concludes the paper and discusses future research directions.
</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.05701/assets/FigDLSpike.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="230" height="189" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Appearance of extracellularly recorded action potentials according to the recording
arrangement. A. Spikes from a serotonergic neuron recorded in voltage recording configuration,
typically used in vivo or in vitro with sharp microelectrodes. Left trace shows two spikes on a slow
recording timebase. Note the predominance of background noise over the spikes in the recorded
segment. Right trace shows one of the spikes at faster timebase. Note the rapid positive upstroke
followed by a negative phase comprising two subsequent downstrokes. B. Appearance of the
same signals as obtained with the loose-seal patch-clamp current recording configuration used in
the present work.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Limitations of Visual Identification Methods </h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Serotonergic neurons have historically been identified in electrophysiology
studies using subjective visual criteria centered on “typical”
spiking characteristics. The most commonly applied criteria include
spike shape, spike duration, and regularity of firing (Vandermaelen
and Aghajanian 1983). Idealized serotonergic neurons exhibit polyphasic spikes with an initial fast deflection followed by one or two slower deflections in the opposite direction (see e.g. Fig. 1). Spike duration, measured from initial rise to second downstroke,
is relatively long, generally &gt;1.2,ms. Firing is regular at slow
rates around 0.5-2.5 Hz. Neurons meeting all these criteria can be classified
as serotonergic with high confidence. However, sole reliance on these
restrictive “typical characteristics”
overlooks the diversity of firing patterns and spike morphologies
across serotonergic subpopulations. Recordings in brain slices from
genetically-identified serotonergic neurons revealed a distribution
of spike durations spanning 0.5 ms to &gt;3 ms, with many neurons exhibiting
spikes narrower than the 1.2 ms criterion (Mlinar et al., 2016; Corradetti
et al., 2024). While the overall distribution skews towards longer durations,
nearly a third of genetically-confirmed serotonergic neurons have
spike widths resembling conventional non-serotonergic neurons. Firing
regularity also varies significantly. Most serotonergic neurons display
regular, slow firing. But burst firing, rhythmic oscillations, and
irregular patterns are observed in certain subpopulations (Hajós et al., 1995; Calizo et al., 2011; Mlinar et al., 2016). Diversity in firing likely reflects differences in inputs
and intrinsic membrane properties between anatomical groups. Again,
many serotonergic neurons exhibit firing indistinguishable from conventional
non-serotonergic cells. Reliance on narrow spike criteria also overlooks
spike shape variations in serotonergic neurons. While triphasic spikes
are quite typical, spikes range from biphasic to polyphasic (Mlinar
et al., 2016). Non-serotonergic neurons likewise display considerable
variability in spike shape, including broad spikes resembling serotonergic
morphologies.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">These data demonstrate the insufficiency of restrictive visual criteria
for comprehensively identifying serotonergic neurons in electrophysiology
recordings. Sole reliance on “typical characteristics”
such as measuring the upstroke/downstroke interval (UDI) may result
not indicative enough for immediate serotonergic neuron identification
and introduces a strong selection bias that likely overlooks much
of the diversity of serotonergic neuron physiology and function. Researchers
adhering strictly to conventional spike criteria may discard neurons
that are genuinely serotonergic but exhibit narrower spike width,
irregular firing, or atypical shape. This precludes studying how functional
differences between serotonergic subpopulations emerge from heterogeneity
in spiking characteristics and intrinsic properties. Conversely, non-serotonergic
neurons with spike width and shape similar to conventional serotonergic
criteria may be erroneously classified without additional verification.
Finally, it would be highly beneficial for research groups to have a model capable of recognising serotonergic cells with high accuracy and in a fraction of a second from a few spike events while the experiment is still ongoing. Such script can be easily
done once a specific deep-learning model is developed from a reasonable
amount of collected data and tailored to the experimental set-up of
the research group. Indeed, the inference time needed for a model
similar to that presented here is of a few millisecond and with an
accuracy definitely higher than that of visual discrimination, thus
suitable for real-life experiments.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Problems with the Deep Learning Methods </h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">While deep learning methods like CNNs offer immense potential for
serotonergic neuron identification, developing robust models from
electrophysiology data poses a few practical challenges. A fundamental
issue is the limited number of experimental recordings available.
Despite access to an extensive serotonergic cell recording database (Mlinar et al., 2016), the total number of recorded cells was only in the order of a few hundred. The reason for this scarcity
relies on the advanced procedures needed for this type of experiment.
Indeed, since the recognition has to be independent of the recording,
then serotonergic and non-serotonergic neurons must be identified
on the basis of serotonergic system-specific fluorescent protein expression
(serotonergic) or lack of expression (non-serotonergic). The
procedures needed to obtain the three transgenic mouse lines with
serotonergic system-specific fluorescent protein expression used in
the present work: i) Tph2::SCFP (TSC transgenic mouse line); ii) Pet1-Cre::Rosa26.YFP
(PRY transgenic mouse line); iii) Pet1-Cre::CAG.eGFP (PCG transgenic
mouse line) are explained in detail in (Mlinar et al., 2016, Montalbano
et al., 2015).</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Given the scarcity of data, a common tactic would be to expand the
limited data through aggressive augmentation, such as extracting short
segments from longer recordings. A first naive approach in this direction
would be to select segments of a few seconds, in order to obtain multiple
samples of the spike signal along with enough time to assess firing
regularity. However, this naive approach proved to be problematic. Indeed,
the core issue is that each cell recording possesses an intrinsic
background noise signature. This signature becomes a distinct marker
that models learn for discriminating between serotonergic and non-serotonergic
cells. Although segments appear distinct, models can exploit minor
noise correlations rather than encoding robust spike features. One
thus may have excellent accuracy on validation and test data that
are derived from the same experimental recordings as the training
set, but dramatically declined performances on data from different
experiments not used in training. Even more problematic is the fact
that the sources of the noise signature include neighboring cell activity
and probe positioning, both of which change in different cells, but
also environmental factors that remain constant throughout the whole
experimental day.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Our studies (Corradetti et al., 2024) yielded to a few important conclusions
and suggestions: First, each deep learning model must be rigorously
evaluated on <em id="S3.p3.1.1" class="ltx_emph ltx_font_italic">non-homogeneous data</em> that are not only external
to the training, validation and test dataset, but also collected on
different experimental days than those used in the training and thus
with different noise profiles. Therefore, while one can proceed with
data augmentation extracting small segments from the recording, then
blob all of them before randomizing the splitting in training, validation
and test data; one must also preserve a relevant part of the data,
e.g. &gt;15-20%, for non-homogenous testing being sure of not having
the data with noise signature similar to that used in the training.
Second, the authors found the model overfitting was highly sensitive to the
length of the extracted segments. Although firing frequency is highly
important for visual discrimination, our deep learning models benefited
immensely from limiting clips to just the central 4 ms region surrounding
spikes (Fig. 2). This showed to improve the focus on the spike alone and reduces
the overfitting given by the noise signature. Finally, a very efficient
solution for expanding the training data is given by the generation
of a synthetic data set for which the authors develop a very specific procedure
(see the next section) that combine smoothed spikes signals along
with real noise masks.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2407.05701/assets/A131031_170.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="1025" height="890" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example on how single events were isolated and selected. The image
depicts the recording of the serotonergic cell A131031#170 and the
4 ms event of triggered at point 1068617, i.e. at 26.715 sec.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2407.05701/assets/EPIASynthFormationSD.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="1422" height="1764" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of 4 synthetic spikes generated
by the event triggered at 1068617, i.e. 26.715 sec, of the serotonergic
cell A131031#170. Top trace: the original recording of the event.
The panels report four spike obtained by processing the original trace
with different noise masks (see methods).</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Generation of Synthetic Data </h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The use of synthetic data for deep learning models is increasingly
important nowadays, more so in this case due to the lack of a large
amount of collected data and the difficulty of experiments. In the
generation of synthetic data for emulating spike recordings of serotonergic
cells, some elements are important: firstly, it is paramount to maintain
a spike waveform resulting from a depolarization and repolarization
of the cell consistent with the biological ones; secondly, it is necessary
to avoid imposing a spike morphology that is too uniform and which
does not take into account the biological variability of individual
cells; finally, it is desirable for the signal to contain a plausible
but original and variable noise signature in order to have less sensitive
trained models. The following procedure for generating synthetic data
is developed in order to meet all the previous requirements. Note
that not all requirements point toward the same operational direction.
Indeed, one could think of eliminate from all spikes the background
noise averaging all events (as it is often used for visual discrimination
of serotonergic cells) thus obtaining a theoretical and ideal spike
waveform that one would eventually combine with an <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">ad hoc</em> background
noise. That would be an interesting solution that would, nevertheless
compress the biological variability of a cell to a single action potential
waveform. On the other hand, one might want to change arbitrarly the
noise background in order to have very different signatures, but at
the same time the unsupervised application of an arbitrary noise mask
would yield in altering too substantially the spike waveform leading
to unrealistic action potentials.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.6" class="ltx_p">In our synthetic data generation procedure (Fig. 3), each original training
data sample, i.e., each single event, is smoothed through a simple
moving average (SMA) of range 3. The reason for using a SMA of range
3 is due to the need to combine two requirements: the need to smooth
the original signal from the specific noise of the recording (for
which SMA are a common technique), and the need to maintain the structure
of the signal as mentioned above. Indeed, the rapid depolarization
of the cell is such that the most relevant data of the spike recording
are often condensed in about a dozen of recording points. This means
that considering a SMA with range <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="n&gt;3" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">n</mi><mo id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><gt id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></gt><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">𝑛</ci><cn type="integer" id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">n&gt;3</annotation></semantics></math> could undermine the fundamental
information inside the signal, while <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="n=2" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">n</mi><mo id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><eq id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"></eq><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">𝑛</ci><cn type="integer" id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">n=2</annotation></semantics></math> might not be sufficient
to remove the background noise. A visual inspection of the averaged
signal in <a href="#S3.F3" title="Figure 3 ‣ 3 Problems with the Deep Learning Methods ‣ Synthetic Data for Robust Identification of Typical and Atypical Serotonergic Neurons using Convolutional Neural Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that the bottom of the event
is not altered by applying an SMA of range 3, while a higher ranged
SMA could create a smooth bottom instead of a spike. Concretely, supposing
a <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.p2.3.m3.1a"><mn id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><cn type="integer" id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">4</annotation></semantics></math> ms sample recorded at 40 kHz, the values of the smoothed sample
<math id="S4.p2.4.m4.1" class="ltx_Math" alttext="\left\{y^{\prime}_{m}\right\}" display="inline"><semantics id="S4.p2.4.m4.1a"><mrow id="S4.p2.4.m4.1.1.1" xref="S4.p2.4.m4.1.1.2.cmml"><mo id="S4.p2.4.m4.1.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">{</mo><msubsup id="S4.p2.4.m4.1.1.1.1" xref="S4.p2.4.m4.1.1.1.1.cmml"><mi id="S4.p2.4.m4.1.1.1.1.2.2" xref="S4.p2.4.m4.1.1.1.1.2.2.cmml">y</mi><mi id="S4.p2.4.m4.1.1.1.1.3" xref="S4.p2.4.m4.1.1.1.1.3.cmml">m</mi><mo id="S4.p2.4.m4.1.1.1.1.2.3" xref="S4.p2.4.m4.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S4.p2.4.m4.1.1.1.3" xref="S4.p2.4.m4.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><set id="S4.p2.4.m4.1.1.2.cmml" xref="S4.p2.4.m4.1.1.1"><apply id="S4.p2.4.m4.1.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S4.p2.4.m4.1.1.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1.1">subscript</csymbol><apply id="S4.p2.4.m4.1.1.1.1.2.cmml" xref="S4.p2.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S4.p2.4.m4.1.1.1.1.2.1.cmml" xref="S4.p2.4.m4.1.1.1.1">superscript</csymbol><ci id="S4.p2.4.m4.1.1.1.1.2.2.cmml" xref="S4.p2.4.m4.1.1.1.1.2.2">𝑦</ci><ci id="S4.p2.4.m4.1.1.1.1.2.3.cmml" xref="S4.p2.4.m4.1.1.1.1.2.3">′</ci></apply><ci id="S4.p2.4.m4.1.1.1.1.3.cmml" xref="S4.p2.4.m4.1.1.1.1.3">𝑚</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">\left\{y^{\prime}_{m}\right\}</annotation></semantics></math> with <math id="S4.p2.5.m5.3" class="ltx_Math" alttext="m\in\left\{1,...,160\right\}" display="inline"><semantics id="S4.p2.5.m5.3a"><mrow id="S4.p2.5.m5.3.4" xref="S4.p2.5.m5.3.4.cmml"><mi id="S4.p2.5.m5.3.4.2" xref="S4.p2.5.m5.3.4.2.cmml">m</mi><mo id="S4.p2.5.m5.3.4.1" xref="S4.p2.5.m5.3.4.1.cmml">∈</mo><mrow id="S4.p2.5.m5.3.4.3.2" xref="S4.p2.5.m5.3.4.3.1.cmml"><mo id="S4.p2.5.m5.3.4.3.2.1" xref="S4.p2.5.m5.3.4.3.1.cmml">{</mo><mn id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml">1</mn><mo id="S4.p2.5.m5.3.4.3.2.2" xref="S4.p2.5.m5.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S4.p2.5.m5.2.2" xref="S4.p2.5.m5.2.2.cmml">…</mi><mo id="S4.p2.5.m5.3.4.3.2.3" xref="S4.p2.5.m5.3.4.3.1.cmml">,</mo><mn id="S4.p2.5.m5.3.3" xref="S4.p2.5.m5.3.3.cmml">160</mn><mo id="S4.p2.5.m5.3.4.3.2.4" xref="S4.p2.5.m5.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.3b"><apply id="S4.p2.5.m5.3.4.cmml" xref="S4.p2.5.m5.3.4"><in id="S4.p2.5.m5.3.4.1.cmml" xref="S4.p2.5.m5.3.4.1"></in><ci id="S4.p2.5.m5.3.4.2.cmml" xref="S4.p2.5.m5.3.4.2">𝑚</ci><set id="S4.p2.5.m5.3.4.3.1.cmml" xref="S4.p2.5.m5.3.4.3.2"><cn type="integer" id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1">1</cn><ci id="S4.p2.5.m5.2.2.cmml" xref="S4.p2.5.m5.2.2">…</ci><cn type="integer" id="S4.p2.5.m5.3.3.cmml" xref="S4.p2.5.m5.3.3">160</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.3c">m\in\left\{1,...,160\right\}</annotation></semantics></math>
are given as the averages of the values of the original sample <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="\left\{y_{m}\right\}" display="inline"><semantics id="S4.p2.6.m6.1a"><mrow id="S4.p2.6.m6.1.1.1" xref="S4.p2.6.m6.1.1.2.cmml"><mo id="S4.p2.6.m6.1.1.1.2" xref="S4.p2.6.m6.1.1.2.cmml">{</mo><msub id="S4.p2.6.m6.1.1.1.1" xref="S4.p2.6.m6.1.1.1.1.cmml"><mi id="S4.p2.6.m6.1.1.1.1.2" xref="S4.p2.6.m6.1.1.1.1.2.cmml">y</mi><mi id="S4.p2.6.m6.1.1.1.1.3" xref="S4.p2.6.m6.1.1.1.1.3.cmml">m</mi></msub><mo id="S4.p2.6.m6.1.1.1.3" xref="S4.p2.6.m6.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><set id="S4.p2.6.m6.1.1.2.cmml" xref="S4.p2.6.m6.1.1.1"><apply id="S4.p2.6.m6.1.1.1.1.cmml" xref="S4.p2.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S4.p2.6.m6.1.1.1.1.1.cmml" xref="S4.p2.6.m6.1.1.1.1">subscript</csymbol><ci id="S4.p2.6.m6.1.1.1.1.2.cmml" xref="S4.p2.6.m6.1.1.1.1.2">𝑦</ci><ci id="S4.p2.6.m6.1.1.1.1.3.cmml" xref="S4.p2.6.m6.1.1.1.1.3">𝑚</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">\left\{y_{m}\right\}</annotation></semantics></math>
by</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.4" class="ltx_math_unparsed" alttext="\begin{cases}y^{\prime}_{1}=\frac{\left(y_{1}+y_{2}\right)}{2},\\
y^{\prime}_{m}=\frac{\left(y_{m-1}+y{}_{m}+y{}_{m+1}\right)}{3},&amp;1&lt;m&lt;160\\
y^{\prime}_{160}=\frac{\left(y_{159}+y_{160}\right)}{2}.\end{cases}" display="block"><semantics id="S4.E1.m1.4a"><mrow id="S4.E1.m1.4.4"><mo id="S4.E1.m1.4.4.5">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E1.m1.4.4.4"><mtr id="S4.E1.m1.4.4.4a"><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4b"><mrow id="S4.E1.m1.1.1.1.1.1.1.2"><mrow id="S4.E1.m1.1.1.1.1.1.1.2.1"><msubsup id="S4.E1.m1.1.1.1.1.1.1.2.1.2"><mi id="S4.E1.m1.1.1.1.1.1.1.2.1.2.2.2">y</mi><mn id="S4.E1.m1.1.1.1.1.1.1.2.1.2.3">1</mn><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.2.2.3">′</mo></msubsup><mo id="S4.E1.m1.1.1.1.1.1.1.2.1.1">=</mo><mstyle displaystyle="false" id="S4.E1.m1.1.1.1.1.1.1.1"><mfrac id="S4.E1.m1.1.1.1.1.1.1.1a"><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.1"><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1"><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2">y</mi><mn id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3">1</mn></msub><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1">+</mo><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2">y</mi><mn id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3">2</mn></msub></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3">)</mo></mrow><mn id="S4.E1.m1.1.1.1.1.1.1.1.3">2</mn></mfrac></mstyle></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.2.2">,</mo></mrow></mtd><mtd id="S4.E1.m1.4.4.4c"></mtd></mtr><mtr id="S4.E1.m1.4.4.4d"><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4e"><mrow id="S4.E1.m1.2.2.2.2.1.1.1"><mrow id="S4.E1.m1.2.2.2.2.1.1.1.1"><msubsup id="S4.E1.m1.2.2.2.2.1.1.1.1.2"><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.2.2.2">y</mi><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.2.3">m</mi><mo id="S4.E1.m1.2.2.2.2.1.1.1.1.2.2.3">′</mo></msubsup><mo id="S4.E1.m1.2.2.2.2.1.1.1.1.1">=</mo><mstyle displaystyle="false" id="S4.E1.m1.2.2.2.2.1.1.1.1.3"><mfrac id="S4.E1.m1.2.2.2.2.1.1.1.1.3a"><mrow id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2"><mo id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.1">(</mo><msub id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.2"><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.2.2">y</mi><mrow id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.2.3"><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.2.3.2">m</mi><mo id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.2.3.1">−</mo><mn id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.2.3.3">1</mn></mrow></msub><mo id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.3">+</mo><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.4">y</mi><mmultiscripts id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.5"><mo id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.5.2">+</mo><mprescripts id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.5a"></mprescripts><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.5.3">m</mi><mrow id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.5b"></mrow></mmultiscripts><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.6">y</mi><mmultiscripts id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.7"><mo id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.7.2">)</mo><mprescripts id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.7a"></mprescripts><mrow id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.7.3"><mi id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.7.3.2">m</mi><mo id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.7.3.1">+</mo><mn id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.7.3.3">1</mn></mrow><mrow id="S4.E1.m1.2.2.2.2.1.1.1.1.3.2.7b"></mrow></mmultiscripts></mrow><mn id="S4.E1.m1.2.2.2.2.1.1.1.1.3.3">3</mn></mfrac></mstyle></mrow><mo id="S4.E1.m1.2.2.2.2.1.1.1.2">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4f"><mrow id="S4.E1.m1.3.3.3.3.2.1"><mn id="S4.E1.m1.3.3.3.3.2.1.2">1</mn><mo id="S4.E1.m1.3.3.3.3.2.1.3">&lt;</mo><mi id="S4.E1.m1.3.3.3.3.2.1.4">m</mi><mo id="S4.E1.m1.3.3.3.3.2.1.5">&lt;</mo><mn id="S4.E1.m1.3.3.3.3.2.1.6">160</mn></mrow></mtd></mtr><mtr id="S4.E1.m1.4.4.4g"><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4h"><mrow id="S4.E1.m1.4.4.4.4.1.1.2"><mrow id="S4.E1.m1.4.4.4.4.1.1.2.1"><msubsup id="S4.E1.m1.4.4.4.4.1.1.2.1.2"><mi id="S4.E1.m1.4.4.4.4.1.1.2.1.2.2.2">y</mi><mn id="S4.E1.m1.4.4.4.4.1.1.2.1.2.3">160</mn><mo id="S4.E1.m1.4.4.4.4.1.1.2.1.2.2.3">′</mo></msubsup><mo id="S4.E1.m1.4.4.4.4.1.1.2.1.1">=</mo><mstyle displaystyle="false" id="S4.E1.m1.4.4.4.4.1.1.1"><mfrac id="S4.E1.m1.4.4.4.4.1.1.1a"><mrow id="S4.E1.m1.4.4.4.4.1.1.1.1.1"><mo id="S4.E1.m1.4.4.4.4.1.1.1.1.1.2">(</mo><mrow id="S4.E1.m1.4.4.4.4.1.1.1.1.1.1"><msub id="S4.E1.m1.4.4.4.4.1.1.1.1.1.1.2"><mi id="S4.E1.m1.4.4.4.4.1.1.1.1.1.1.2.2">y</mi><mn id="S4.E1.m1.4.4.4.4.1.1.1.1.1.1.2.3">159</mn></msub><mo id="S4.E1.m1.4.4.4.4.1.1.1.1.1.1.1">+</mo><msub id="S4.E1.m1.4.4.4.4.1.1.1.1.1.1.3"><mi id="S4.E1.m1.4.4.4.4.1.1.1.1.1.1.3.2">y</mi><mn id="S4.E1.m1.4.4.4.4.1.1.1.1.1.1.3.3">160</mn></msub></mrow><mo id="S4.E1.m1.4.4.4.4.1.1.1.1.1.3">)</mo></mrow><mn id="S4.E1.m1.4.4.4.4.1.1.1.3">2</mn></mfrac></mstyle></mrow><mo lspace="0em" id="S4.E1.m1.4.4.4.4.1.1.2.2">.</mo></mrow></mtd><mtd id="S4.E1.m1.4.4.4i"></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex" id="S4.E1.m1.4b">\begin{cases}y^{\prime}_{1}=\frac{\left(y_{1}+y_{2}\right)}{2},\\
y^{\prime}_{m}=\frac{\left(y_{m-1}+y{}_{m}+y{}_{m+1}\right)}{3},&amp;1&lt;m&lt;160\\
y^{\prime}_{160}=\frac{\left(y_{159}+y_{160}\right)}{2}.\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2407.05701/assets/noise-masks.png" id="S4.F4.g1" class="ltx_graphics ltx_img_landscape" width="1283" height="985" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of noise masks collected
from the recordings of cell A160125#009 (<em id="S4.F4.3.1" class="ltx_emph ltx_font_italic">on top</em>) and
A150701#086 (<em id="S4.F4.4.2" class="ltx_emph ltx_font_italic">on bottom</em>).</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.7" class="ltx_p">After the previous process has taken place, one has to recombine the
averaged signal <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\left\{y^{\prime}_{m}\right\}" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.2.cmml"><mo id="S4.p3.1.m1.1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">{</mo><msubsup id="S4.p3.1.m1.1.1.1.1" xref="S4.p3.1.m1.1.1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.1.1.2.2" xref="S4.p3.1.m1.1.1.1.1.2.2.cmml">y</mi><mi id="S4.p3.1.m1.1.1.1.1.3" xref="S4.p3.1.m1.1.1.1.1.3.cmml">m</mi><mo id="S4.p3.1.m1.1.1.1.1.2.3" xref="S4.p3.1.m1.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S4.p3.1.m1.1.1.1.3" xref="S4.p3.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><set id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.1"><apply id="S4.p3.1.m1.1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1.1">subscript</csymbol><apply id="S4.p3.1.m1.1.1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.1.2.1.cmml" xref="S4.p3.1.m1.1.1.1.1">superscript</csymbol><ci id="S4.p3.1.m1.1.1.1.1.2.2.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2">𝑦</ci><ci id="S4.p3.1.m1.1.1.1.1.2.3.cmml" xref="S4.p3.1.m1.1.1.1.1.2.3">′</ci></apply><ci id="S4.p3.1.m1.1.1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.1.1.3">𝑚</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\left\{y^{\prime}_{m}\right\}</annotation></semantics></math> with a set of noise masks
<math id="S4.p3.2.m2.2" class="ltx_Math" alttext="\left\{n_{m}^{(k)}\right\}" display="inline"><semantics id="S4.p3.2.m2.2a"><mrow id="S4.p3.2.m2.2.2.1" xref="S4.p3.2.m2.2.2.2.cmml"><mo id="S4.p3.2.m2.2.2.1.2" xref="S4.p3.2.m2.2.2.2.cmml">{</mo><msubsup id="S4.p3.2.m2.2.2.1.1" xref="S4.p3.2.m2.2.2.1.1.cmml"><mi id="S4.p3.2.m2.2.2.1.1.2.2" xref="S4.p3.2.m2.2.2.1.1.2.2.cmml">n</mi><mi id="S4.p3.2.m2.2.2.1.1.2.3" xref="S4.p3.2.m2.2.2.1.1.2.3.cmml">m</mi><mrow id="S4.p3.2.m2.1.1.1.3" xref="S4.p3.2.m2.2.2.1.1.cmml"><mo stretchy="false" id="S4.p3.2.m2.1.1.1.3.1" xref="S4.p3.2.m2.2.2.1.1.cmml">(</mo><mi id="S4.p3.2.m2.1.1.1.1" xref="S4.p3.2.m2.1.1.1.1.cmml">k</mi><mo stretchy="false" id="S4.p3.2.m2.1.1.1.3.2" xref="S4.p3.2.m2.2.2.1.1.cmml">)</mo></mrow></msubsup><mo id="S4.p3.2.m2.2.2.1.3" xref="S4.p3.2.m2.2.2.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.2b"><set id="S4.p3.2.m2.2.2.2.cmml" xref="S4.p3.2.m2.2.2.1"><apply id="S4.p3.2.m2.2.2.1.1.cmml" xref="S4.p3.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S4.p3.2.m2.2.2.1.1.1.cmml" xref="S4.p3.2.m2.2.2.1.1">superscript</csymbol><apply id="S4.p3.2.m2.2.2.1.1.2.cmml" xref="S4.p3.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S4.p3.2.m2.2.2.1.1.2.1.cmml" xref="S4.p3.2.m2.2.2.1.1">subscript</csymbol><ci id="S4.p3.2.m2.2.2.1.1.2.2.cmml" xref="S4.p3.2.m2.2.2.1.1.2.2">𝑛</ci><ci id="S4.p3.2.m2.2.2.1.1.2.3.cmml" xref="S4.p3.2.m2.2.2.1.1.2.3">𝑚</ci></apply><ci id="S4.p3.2.m2.1.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1.1">𝑘</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.2c">\left\{n_{m}^{(k)}\right\}</annotation></semantics></math> previously extracted (Fig. 4). From a practical
standpoint, a good way to select such masks is to proceed with an
extraction from real recordings of experiments performed on different
days and under different environmental conditions. A noise mask is
selected extracting a segment immediately before the trigger of an
event, e.g., the noise recording from 5.5 ms to 2.5 ms before the
peak of an event. This ensures the mask contains only background noise
patterns uncorrelated with spike waveform features. Extracting masks
in this manner from multiple heterogeneous recordings provides a diverse
noise set to add variability. Moreover, considering it is quite feasible
to obtain more than <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.p3.3.m3.1a"><mn id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><cn type="integer" id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">10</annotation></semantics></math> days of recordings, then selecting just
100 noise masks from each day would yield a 1000-fold increase in
data augmentation through synthetic data generation. Indeed, combining
<math id="S4.p3.4.m4.1" class="ltx_Math" alttext="k=1000" display="inline"><semantics id="S4.p3.4.m4.1a"><mrow id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml"><mi id="S4.p3.4.m4.1.1.2" xref="S4.p3.4.m4.1.1.2.cmml">k</mi><mo id="S4.p3.4.m4.1.1.1" xref="S4.p3.4.m4.1.1.1.cmml">=</mo><mn id="S4.p3.4.m4.1.1.3" xref="S4.p3.4.m4.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><apply id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1"><eq id="S4.p3.4.m4.1.1.1.cmml" xref="S4.p3.4.m4.1.1.1"></eq><ci id="S4.p3.4.m4.1.1.2.cmml" xref="S4.p3.4.m4.1.1.2">𝑘</ci><cn type="integer" id="S4.p3.4.m4.1.1.3.cmml" xref="S4.p3.4.m4.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">k=1000</annotation></semantics></math> heterogeneous noise masks enables synthesizing 1000 distinct
versions of each averaged spike. For computational and practical reasons,
the augmentation multiplication factor <math id="S4.p3.5.m5.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.p3.5.m5.1a"><mi id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><ci id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">R</annotation></semantics></math> is typically constrained
to be considerably smaller than <math id="S4.p3.6.m6.1" class="ltx_Math" alttext="k=1000" display="inline"><semantics id="S4.p3.6.m6.1a"><mrow id="S4.p3.6.m6.1.1" xref="S4.p3.6.m6.1.1.cmml"><mi id="S4.p3.6.m6.1.1.2" xref="S4.p3.6.m6.1.1.2.cmml">k</mi><mo id="S4.p3.6.m6.1.1.1" xref="S4.p3.6.m6.1.1.1.cmml">=</mo><mn id="S4.p3.6.m6.1.1.3" xref="S4.p3.6.m6.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.6.m6.1b"><apply id="S4.p3.6.m6.1.1.cmml" xref="S4.p3.6.m6.1.1"><eq id="S4.p3.6.m6.1.1.1.cmml" xref="S4.p3.6.m6.1.1.1"></eq><ci id="S4.p3.6.m6.1.1.2.cmml" xref="S4.p3.6.m6.1.1.2">𝑘</ci><cn type="integer" id="S4.p3.6.m6.1.1.3.cmml" xref="S4.p3.6.m6.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.6.m6.1c">k=1000</annotation></semantics></math>. Nevertheless, the authors suggest
creating a noise mask pool with at least 1000 elements. Then for each
original spike, randomly select <math id="S4.p3.7.m7.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.p3.7.m7.1a"><mi id="S4.p3.7.m7.1.1" xref="S4.p3.7.m7.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.p3.7.m7.1b"><ci id="S4.p3.7.m7.1.1.cmml" xref="S4.p3.7.m7.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.7.m7.1c">R</annotation></semantics></math> masks from this large pool to
generate the synthetic augmented samples. This approach helps ensure
diversity and minimize correlations in the applied noise patterns.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.6" class="ltx_p">The generation of <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.p4.1.m1.1a"><mi id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">R</annotation></semantics></math> synthetic data is then obtained from the values
of smoothed spike <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="\left\{y^{\prime}_{m}\right\}" display="inline"><semantics id="S4.p4.2.m2.1a"><mrow id="S4.p4.2.m2.1.1.1" xref="S4.p4.2.m2.1.1.2.cmml"><mo id="S4.p4.2.m2.1.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml">{</mo><msubsup id="S4.p4.2.m2.1.1.1.1" xref="S4.p4.2.m2.1.1.1.1.cmml"><mi id="S4.p4.2.m2.1.1.1.1.2.2" xref="S4.p4.2.m2.1.1.1.1.2.2.cmml">y</mi><mi id="S4.p4.2.m2.1.1.1.1.3" xref="S4.p4.2.m2.1.1.1.1.3.cmml">m</mi><mo id="S4.p4.2.m2.1.1.1.1.2.3" xref="S4.p4.2.m2.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S4.p4.2.m2.1.1.1.3" xref="S4.p4.2.m2.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><set id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.1"><apply id="S4.p4.2.m2.1.1.1.1.cmml" xref="S4.p4.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.p4.2.m2.1.1.1.1.1.cmml" xref="S4.p4.2.m2.1.1.1.1">subscript</csymbol><apply id="S4.p4.2.m2.1.1.1.1.2.cmml" xref="S4.p4.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.p4.2.m2.1.1.1.1.2.1.cmml" xref="S4.p4.2.m2.1.1.1.1">superscript</csymbol><ci id="S4.p4.2.m2.1.1.1.1.2.2.cmml" xref="S4.p4.2.m2.1.1.1.1.2.2">𝑦</ci><ci id="S4.p4.2.m2.1.1.1.1.2.3.cmml" xref="S4.p4.2.m2.1.1.1.1.2.3">′</ci></apply><ci id="S4.p4.2.m2.1.1.1.1.3.cmml" xref="S4.p4.2.m2.1.1.1.1.3">𝑚</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">\left\{y^{\prime}_{m}\right\}</annotation></semantics></math>, that are then added
to the values of <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.p4.3.m3.1a"><mi id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><ci id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">R</annotation></semantics></math> randomly chosen noise mask <math id="S4.p4.4.m4.2" class="ltx_Math" alttext="\left\{n_{m}^{(j)}\right\}" display="inline"><semantics id="S4.p4.4.m4.2a"><mrow id="S4.p4.4.m4.2.2.1" xref="S4.p4.4.m4.2.2.2.cmml"><mo id="S4.p4.4.m4.2.2.1.2" xref="S4.p4.4.m4.2.2.2.cmml">{</mo><msubsup id="S4.p4.4.m4.2.2.1.1" xref="S4.p4.4.m4.2.2.1.1.cmml"><mi id="S4.p4.4.m4.2.2.1.1.2.2" xref="S4.p4.4.m4.2.2.1.1.2.2.cmml">n</mi><mi id="S4.p4.4.m4.2.2.1.1.2.3" xref="S4.p4.4.m4.2.2.1.1.2.3.cmml">m</mi><mrow id="S4.p4.4.m4.1.1.1.3" xref="S4.p4.4.m4.2.2.1.1.cmml"><mo stretchy="false" id="S4.p4.4.m4.1.1.1.3.1" xref="S4.p4.4.m4.2.2.1.1.cmml">(</mo><mi id="S4.p4.4.m4.1.1.1.1" xref="S4.p4.4.m4.1.1.1.1.cmml">j</mi><mo stretchy="false" id="S4.p4.4.m4.1.1.1.3.2" xref="S4.p4.4.m4.2.2.1.1.cmml">)</mo></mrow></msubsup><mo id="S4.p4.4.m4.2.2.1.3" xref="S4.p4.4.m4.2.2.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.2b"><set id="S4.p4.4.m4.2.2.2.cmml" xref="S4.p4.4.m4.2.2.1"><apply id="S4.p4.4.m4.2.2.1.1.cmml" xref="S4.p4.4.m4.2.2.1.1"><csymbol cd="ambiguous" id="S4.p4.4.m4.2.2.1.1.1.cmml" xref="S4.p4.4.m4.2.2.1.1">superscript</csymbol><apply id="S4.p4.4.m4.2.2.1.1.2.cmml" xref="S4.p4.4.m4.2.2.1.1"><csymbol cd="ambiguous" id="S4.p4.4.m4.2.2.1.1.2.1.cmml" xref="S4.p4.4.m4.2.2.1.1">subscript</csymbol><ci id="S4.p4.4.m4.2.2.1.1.2.2.cmml" xref="S4.p4.4.m4.2.2.1.1.2.2">𝑛</ci><ci id="S4.p4.4.m4.2.2.1.1.2.3.cmml" xref="S4.p4.4.m4.2.2.1.1.2.3">𝑚</ci></apply><ci id="S4.p4.4.m4.1.1.1.1.cmml" xref="S4.p4.4.m4.1.1.1.1">𝑗</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.2c">\left\{n_{m}^{(j)}\right\}</annotation></semantics></math>
where <math id="S4.p4.5.m5.3" class="ltx_Math" alttext="j\in\left\{1,...,1000\right\}" display="inline"><semantics id="S4.p4.5.m5.3a"><mrow id="S4.p4.5.m5.3.4" xref="S4.p4.5.m5.3.4.cmml"><mi id="S4.p4.5.m5.3.4.2" xref="S4.p4.5.m5.3.4.2.cmml">j</mi><mo id="S4.p4.5.m5.3.4.1" xref="S4.p4.5.m5.3.4.1.cmml">∈</mo><mrow id="S4.p4.5.m5.3.4.3.2" xref="S4.p4.5.m5.3.4.3.1.cmml"><mo id="S4.p4.5.m5.3.4.3.2.1" xref="S4.p4.5.m5.3.4.3.1.cmml">{</mo><mn id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml">1</mn><mo id="S4.p4.5.m5.3.4.3.2.2" xref="S4.p4.5.m5.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S4.p4.5.m5.2.2" xref="S4.p4.5.m5.2.2.cmml">…</mi><mo id="S4.p4.5.m5.3.4.3.2.3" xref="S4.p4.5.m5.3.4.3.1.cmml">,</mo><mn id="S4.p4.5.m5.3.3" xref="S4.p4.5.m5.3.3.cmml">1000</mn><mo id="S4.p4.5.m5.3.4.3.2.4" xref="S4.p4.5.m5.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.3b"><apply id="S4.p4.5.m5.3.4.cmml" xref="S4.p4.5.m5.3.4"><in id="S4.p4.5.m5.3.4.1.cmml" xref="S4.p4.5.m5.3.4.1"></in><ci id="S4.p4.5.m5.3.4.2.cmml" xref="S4.p4.5.m5.3.4.2">𝑗</ci><set id="S4.p4.5.m5.3.4.3.1.cmml" xref="S4.p4.5.m5.3.4.3.2"><cn type="integer" id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1">1</cn><ci id="S4.p4.5.m5.2.2.cmml" xref="S4.p4.5.m5.2.2">…</ci><cn type="integer" id="S4.p4.5.m5.3.3.cmml" xref="S4.p4.5.m5.3.3">1000</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.3c">j\in\left\{1,...,1000\right\}</annotation></semantics></math> is randomly chosen. The final
synthetic sample is thus obtained as the sample <math id="S4.p4.6.m6.2" class="ltx_math_unparsed" alttext="\left\{y^{(j)}{}_{m}\right\}_{j\in\left\{1...R\right\}}" display="inline"><semantics id="S4.p4.6.m6.2a"><msub id="S4.p4.6.m6.2.3"><mrow id="S4.p4.6.m6.2.3.2"><mo id="S4.p4.6.m6.2.3.2.1">{</mo><msup id="S4.p4.6.m6.2.3.2.2"><mi id="S4.p4.6.m6.2.3.2.2.2">y</mi><mrow id="S4.p4.6.m6.1.1.1.3"><mo stretchy="false" id="S4.p4.6.m6.1.1.1.3.1">(</mo><mi id="S4.p4.6.m6.1.1.1.1">j</mi><mo stretchy="false" id="S4.p4.6.m6.1.1.1.3.2">)</mo></mrow></msup><mmultiscripts id="S4.p4.6.m6.2.3.2.3"><mo id="S4.p4.6.m6.2.3.2.3.2">}</mo><mprescripts id="S4.p4.6.m6.2.3.2.3a"></mprescripts><mi id="S4.p4.6.m6.2.3.2.3.3">m</mi><mrow id="S4.p4.6.m6.2.3.2.3b"></mrow></mmultiscripts></mrow><mrow id="S4.p4.6.m6.2.2.1"><mi id="S4.p4.6.m6.2.2.1.3">j</mi><mo id="S4.p4.6.m6.2.2.1.2">∈</mo><mrow id="S4.p4.6.m6.2.2.1.1.1"><mo id="S4.p4.6.m6.2.2.1.1.1.2">{</mo><mrow id="S4.p4.6.m6.2.2.1.1.1.1"><mn id="S4.p4.6.m6.2.2.1.1.1.1.2">1</mn><mo lspace="0em" rspace="0em" id="S4.p4.6.m6.2.2.1.1.1.1.1">​</mo><mi mathvariant="normal" id="S4.p4.6.m6.2.2.1.1.1.1.3">…</mi><mo lspace="0em" rspace="0em" id="S4.p4.6.m6.2.2.1.1.1.1.1a">​</mo><mi id="S4.p4.6.m6.2.2.1.1.1.1.4">R</mi></mrow><mo id="S4.p4.6.m6.2.2.1.1.1.3">}</mo></mrow></mrow></msub><annotation encoding="application/x-tex" id="S4.p4.6.m6.2b">\left\{y^{(j)}{}_{m}\right\}_{j\in\left\{1...R\right\}}</annotation></semantics></math>
with</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.2" class="ltx_math_unparsed" alttext="y^{(j)}{}_{m}=y^{\prime}_{m}+\alpha\cdot n_{m}^{(j)}," display="block"><semantics id="S4.E2.m1.2a"><mrow id="S4.E2.m1.2b"><msup id="S4.E2.m1.2.3"><mi id="S4.E2.m1.2.3.2">y</mi><mrow id="S4.E2.m1.1.1.1.3"><mo stretchy="false" id="S4.E2.m1.1.1.1.3.1">(</mo><mi id="S4.E2.m1.1.1.1.1">j</mi><mo stretchy="false" id="S4.E2.m1.1.1.1.3.2">)</mo></mrow></msup><mmultiscripts id="S4.E2.m1.2.4"><mo id="S4.E2.m1.2.4.2">=</mo><mprescripts id="S4.E2.m1.2.4a"></mprescripts><mi id="S4.E2.m1.2.4.3">m</mi><mrow id="S4.E2.m1.2.4b"></mrow></mmultiscripts><msubsup id="S4.E2.m1.2.5"><mi id="S4.E2.m1.2.5.2.2">y</mi><mi id="S4.E2.m1.2.5.3">m</mi><mo id="S4.E2.m1.2.5.2.3">′</mo></msubsup><mo id="S4.E2.m1.2.6">+</mo><mi id="S4.E2.m1.2.7">α</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E2.m1.2.8">⋅</mo><msubsup id="S4.E2.m1.2.9"><mi id="S4.E2.m1.2.9.2.2">n</mi><mi id="S4.E2.m1.2.9.2.3">m</mi><mrow id="S4.E2.m1.2.2.1.3"><mo stretchy="false" id="S4.E2.m1.2.2.1.3.1">(</mo><mi id="S4.E2.m1.2.2.1.1">j</mi><mo stretchy="false" id="S4.E2.m1.2.2.1.3.2">)</mo></mrow></msubsup><mo id="S4.E2.m1.2.10">,</mo></mrow><annotation encoding="application/x-tex" id="S4.E2.m1.2c">y^{(j)}{}_{m}=y^{\prime}_{m}+\alpha\cdot n_{m}^{(j)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.p4.9" class="ltx_p">where <math id="S4.p4.7.m1.2" class="ltx_Math" alttext="\alpha\in\left[0.2,0.4\right]" display="inline"><semantics id="S4.p4.7.m1.2a"><mrow id="S4.p4.7.m1.2.3" xref="S4.p4.7.m1.2.3.cmml"><mi id="S4.p4.7.m1.2.3.2" xref="S4.p4.7.m1.2.3.2.cmml">α</mi><mo id="S4.p4.7.m1.2.3.1" xref="S4.p4.7.m1.2.3.1.cmml">∈</mo><mrow id="S4.p4.7.m1.2.3.3.2" xref="S4.p4.7.m1.2.3.3.1.cmml"><mo id="S4.p4.7.m1.2.3.3.2.1" xref="S4.p4.7.m1.2.3.3.1.cmml">[</mo><mn id="S4.p4.7.m1.1.1" xref="S4.p4.7.m1.1.1.cmml">0.2</mn><mo id="S4.p4.7.m1.2.3.3.2.2" xref="S4.p4.7.m1.2.3.3.1.cmml">,</mo><mn id="S4.p4.7.m1.2.2" xref="S4.p4.7.m1.2.2.cmml">0.4</mn><mo id="S4.p4.7.m1.2.3.3.2.3" xref="S4.p4.7.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.7.m1.2b"><apply id="S4.p4.7.m1.2.3.cmml" xref="S4.p4.7.m1.2.3"><in id="S4.p4.7.m1.2.3.1.cmml" xref="S4.p4.7.m1.2.3.1"></in><ci id="S4.p4.7.m1.2.3.2.cmml" xref="S4.p4.7.m1.2.3.2">𝛼</ci><interval closure="closed" id="S4.p4.7.m1.2.3.3.1.cmml" xref="S4.p4.7.m1.2.3.3.2"><cn type="float" id="S4.p4.7.m1.1.1.cmml" xref="S4.p4.7.m1.1.1">0.2</cn><cn type="float" id="S4.p4.7.m1.2.2.cmml" xref="S4.p4.7.m1.2.2">0.4</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.7.m1.2c">\alpha\in\left[0.2,0.4\right]</annotation></semantics></math> is a randomly generated “dumping
coefficient” experimentally found around <math id="S4.p4.8.m2.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S4.p4.8.m2.1a"><mn id="S4.p4.8.m2.1.1" xref="S4.p4.8.m2.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S4.p4.8.m2.1b"><cn type="float" id="S4.p4.8.m2.1.1.cmml" xref="S4.p4.8.m2.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.8.m2.1c">0.3</annotation></semantics></math> to modulate the
noise. The choice of this coefficient requires some clarification.
Indeed, the coefficient dumps the noise intensity to synthesize more
physiologically plausible spike waveforms. First, the background noise
was not completely removed when averaging spikes, just smoothed with
a 3-point average. Thus directly adding the full noise mask would
excessively boost the background noise compared to the original recording.
Moreover, the original noise does not influence all points of the
signal equally, but is more pronounced in slower changing current
regions. Applying the raw noise mask tends to produce unrealistic
spike shapes, e.g. double bottoms. The dumping coefficient <math id="S4.p4.9.m3.2" class="ltx_Math" alttext="\text{rand}(0.2,0.4)" display="inline"><semantics id="S4.p4.9.m3.2a"><mrow id="S4.p4.9.m3.2.3" xref="S4.p4.9.m3.2.3.cmml"><mtext id="S4.p4.9.m3.2.3.2" xref="S4.p4.9.m3.2.3.2a.cmml">rand</mtext><mo lspace="0em" rspace="0em" id="S4.p4.9.m3.2.3.1" xref="S4.p4.9.m3.2.3.1.cmml">​</mo><mrow id="S4.p4.9.m3.2.3.3.2" xref="S4.p4.9.m3.2.3.3.1.cmml"><mo stretchy="false" id="S4.p4.9.m3.2.3.3.2.1" xref="S4.p4.9.m3.2.3.3.1.cmml">(</mo><mn id="S4.p4.9.m3.1.1" xref="S4.p4.9.m3.1.1.cmml">0.2</mn><mo id="S4.p4.9.m3.2.3.3.2.2" xref="S4.p4.9.m3.2.3.3.1.cmml">,</mo><mn id="S4.p4.9.m3.2.2" xref="S4.p4.9.m3.2.2.cmml">0.4</mn><mo stretchy="false" id="S4.p4.9.m3.2.3.3.2.3" xref="S4.p4.9.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.9.m3.2b"><apply id="S4.p4.9.m3.2.3.cmml" xref="S4.p4.9.m3.2.3"><times id="S4.p4.9.m3.2.3.1.cmml" xref="S4.p4.9.m3.2.3.1"></times><ci id="S4.p4.9.m3.2.3.2a.cmml" xref="S4.p4.9.m3.2.3.2"><mtext id="S4.p4.9.m3.2.3.2.cmml" xref="S4.p4.9.m3.2.3.2">rand</mtext></ci><interval closure="open" id="S4.p4.9.m3.2.3.3.1.cmml" xref="S4.p4.9.m3.2.3.3.2"><cn type="float" id="S4.p4.9.m3.1.1.cmml" xref="S4.p4.9.m3.1.1">0.2</cn><cn type="float" id="S4.p4.9.m3.2.2.cmml" xref="S4.p4.9.m3.2.2">0.4</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.9.m3.2c">\text{rand}(0.2,0.4)</annotation></semantics></math>
was deemed a suitable range by visual inspections by an expert author
with over 30 years of experience on serotonergic spike recordings.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>A Practical Case</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">As a practical application of the previous procedure is given by the
model for serotonergic cell recognition now available on GitHub at
<span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">github.com/ neuraldl/ DLAtypicalSerotoninergicCells.git</span> .
To implement the model the authors used the following</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Original Training Data: </h3>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">The original training data for the training, validation and testing
of the models consisted in 43,327 spike samples extracted from 108
serotonergic cells and 45 non-serotonergic cells. More specifically,
the authors extracted 29,773 spikes from serotonergic cells, and 13,554 spikes
from non-serotonergic cells. In all cases, the triggering threshold
of the event was -50 pA and the spike was then sampled 1 ms before
the triggering threshold until 3 ms after (see Fig. 1). Since the
sampling rate of the original recordings was 40 kHz, every spike sample
consists of 160 values.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Non-homogenous Data:</h3>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">The non-homogenous data consisted in 24,616 samples extracted from
55 serotonergic cells and 27 non-serotonergic cells collected in experimental
days not used to obtain the training data, thus with different noise
signature. Again, the triggering threshold of the event was -50 pA
and the spike was then sampled 1 ms before the triggering threshold
until 3 ms after, yielding to 18595 spikes from serotonergic cells
and 6021 from non-serotonergic cells. These data were never part of
the training set, nor validation, nor testing set during the training,
and constituted just an additional independent test for the already
trained model.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Synthetic Data:</h3>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">The synthetic data consisted in 12,700,600 spike samples of 160 points
(simulating 4 ms at 40 kHz of sampling) arising from the 43327 original
training data samples. From the original training data recordings
the authors extracted 600 noise masks that constituted the pool for the random
noise selection for obtaining the synthetic data.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">The Architecture of the Neural Network:</h3>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p1.1" class="ltx_p">Identifying serotonergic cells is a binary classification task, where cells are categorized as either serotonergic or non-serotonergic. Convolutional neural networks (CNNs) have demonstrated remarkable performance in this domain. Inspired by the structure of the animal visual system, particularly the human brain, CNNs excel at image feature extraction, a crucial aspect of recognition tasks (Liu, 2018). These networks utilize techniques such as feedforward inhibition to mitigate problems like gradient vanishing, thereby enhancing their performance in complex pattern recognition challenges (Liu et al., 2019).</p>
</div>
<div id="S5.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p2.1" class="ltx_p">Considering these advantages, the authors opted for a CNN architecture for the somewhat atypical application of numerical pattern recognition, specifically for analyzing the electrical signals from neuronal cells. This architecture comprises a series of layers typical in image recognition with deep learning using CNNs. The implementation was carried out using the Keras library within TensorFlow 2.15. The network includes a normalization layer to stabilize learning and expedite training, two sets of a 2D convolutional layer with 32 filters each, followed by a max pooling layer with a pool size of (2x1). It also features a flatten layer that connects to a dropout layer and subsequently to dense layers, which have two output units for the binary classification task. The activation function for the convolutional layers is ReLU, while the dense layers utilize the sigmoid function, detailed in Table 1. For training, the authors employed the ’binary crossentropy’ loss function, a standard in binary classification tasks, and ’Adam’ (Adaptive Moment Estimation) as the optimizer, given its widespread use and effectiveness.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><em id="S5.T1.1.1.1.1.1" class="ltx_emph ltx_font_italic">Layer (type)</em></th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><em id="S5.T1.1.1.1.2.1" class="ltx_emph ltx_font_italic">Output Shape</em></th>
<th id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><em id="S5.T1.1.1.1.3.1" class="ltx_emph ltx_font_italic">Param #</em></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Layer Normalization</td>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">(None, 160, 2, 1)</td>
<td id="S5.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">320</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.3.2.1" class="ltx_td ltx_align_center">Conv2D</td>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_center">(None, 141, 2, 32)</td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_center">672</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.4.3.1" class="ltx_td ltx_align_center">MaxPooling2D</td>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_center">(None, 70, 2, 32)</td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<td id="S5.T1.1.5.4.1" class="ltx_td ltx_align_center">Conv2D</td>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_center">(None, 51, 2, 64)</td>
<td id="S5.T1.1.5.4.3" class="ltx_td ltx_align_center">41024</td>
</tr>
<tr id="S5.T1.1.6.5" class="ltx_tr">
<td id="S5.T1.1.6.5.1" class="ltx_td ltx_align_center">MaxPooling2D</td>
<td id="S5.T1.1.6.5.2" class="ltx_td ltx_align_center">(None, 25, 1, 64)</td>
<td id="S5.T1.1.6.5.3" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S5.T1.1.7.6" class="ltx_tr">
<td id="S5.T1.1.7.6.1" class="ltx_td ltx_align_center">Flatten</td>
<td id="S5.T1.1.7.6.2" class="ltx_td ltx_align_center">(None, 1600)</td>
<td id="S5.T1.1.7.6.3" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S5.T1.1.8.7" class="ltx_tr">
<td id="S5.T1.1.8.7.1" class="ltx_td ltx_align_center">Dropout</td>
<td id="S5.T1.1.8.7.2" class="ltx_td ltx_align_center">(None, 1600)</td>
<td id="S5.T1.1.8.7.3" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S5.T1.1.9.8" class="ltx_tr">
<td id="S5.T1.1.9.8.1" class="ltx_td ltx_align_center">Dense</td>
<td id="S5.T1.1.9.8.2" class="ltx_td ltx_align_center">(None, 2)</td>
<td id="S5.T1.1.9.8.3" class="ltx_td ltx_align_center">3202</td>
</tr>
<tr id="S5.T1.1.10.9" class="ltx_tr">
<td id="S5.T1.1.10.9.1" class="ltx_td ltx_align_center ltx_border_t"><em id="S5.T1.1.10.9.1.1" class="ltx_emph ltx_font_italic">Total Params</em></td>
<td id="S5.T1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_t">45218</td>
<td id="S5.T1.1.10.9.3" class="ltx_td ltx_border_t"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Schematic of the neural network used with kernel 20. All other models follow the same architectural structure and change only for the dimension of the kernel. The final prediction is given by the consensus of the models with kernel from 20 through 30.</figcaption>
</figure>
<div id="S5.SS0.SSS0.Px4.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p3.1" class="ltx_p">A special treatment was devoted to the kernel of the 2D convolutional
layers. Indeed, since the kernel of these layers express the ability
of the convolutional process in enlarging a specific portion of the
pattern, the authors explored a range of possible kernels between 1 to 31.
The training involving &gt; 12M spike samples required a continuous learning
implementation, where the model was trained over 200 training sessions
of 63450 synthetic spike samples. More specifically, for each one
of the 200 training sessions the 63450 synthetic spike samples (33.350
serotonergic and 30.100 non-serotonergic) the authors considered 44.415 spike
for the effective training, 9.517 spike for validation and 9.518 for
test. In each session, all models were trained on 25 epochs with a
batch size of 64.</p>
</div>
<div id="S5.SS0.SSS0.Px4.p4" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p4.1" class="ltx_p">To enhance the robustness of the model, instead of selecting a single
kernel and using one model for inference, the authors selected all models with
kernels ranging between 20 and 30 and took the consensus between the
models. This technique ensures more stability in the overall architecture
and is often considered best practice.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px5" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Results on Test Data</h3>

<div id="S5.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px5.p1.2" class="ltx_p">Being trained over 12M spikes, i.e. 6,675,300 from serotonergic spikes
and 6,025,300 originated from non-serotonergic spikes, the resulting
model has impressive metrics on the test data. More specifically,
the best training session has a test loss of <math id="S5.SS0.SSS0.Px5.p1.1.m1.2" class="ltx_Math" alttext="1,83\cdot 10^{-6}" display="inline"><semantics id="S5.SS0.SSS0.Px5.p1.1.m1.2a"><mrow id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.2.cmml"><mn id="S5.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">1</mn><mo id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.2" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.2.cmml">,</mo><mrow id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.cmml"><mn id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.2" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.2.cmml">83</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.1" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.1.cmml">⋅</mo><msup id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.cmml"><mn id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.2" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.2.cmml">10</mn><mrow id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3.cmml"><mo id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3a" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3.cmml">−</mo><mn id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3.2" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.1.m1.2b"><list id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.2.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1"><cn type="integer" id="S5.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1">1</cn><apply id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1"><ci id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.1.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.1">⋅</ci><cn type="integer" id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.2.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.2">83</cn><apply id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3">superscript</csymbol><cn type="integer" id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.2">10</cn><apply id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3"><minus id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3.1.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3"></minus><cn type="integer" id="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3.2.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.2.2.1.1.3.3.2">6</cn></apply></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.1.m1.2c">1,83\cdot 10^{-6}</annotation></semantics></math>,
accuracy of <math id="S5.SS0.SSS0.Px5.p1.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S5.SS0.SSS0.Px5.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px5.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px5.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.2.m2.1b"><cn type="integer" id="S5.SS0.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px5.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.2.m2.1c">1</annotation></semantics></math>, sensitivity at specificity 0.5 of 1, 0 False Positive
and 0 False Negative. However, these results are not deemed significant,
as overfitting not related to recording noise tends to be amplified
in the augmented dataset.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px6" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Results on Non-Homogenous Data </h3>

<div id="S5.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px6.p1.1" class="ltx_p">The most significant outcomes are on non-homogeneous data, i.e., cells
that were not utilized in training and were collected on different
days than the training data. Using this dataset, the synthetic model achieved an accuracy of 0.9375, a sensitivity at specificity 0.5 of 0.8888, an AUC of 0.9255 and an F1-Score of 0.9056.
Even though later sessions have all similar
metrics, the best training session was achieved in session 89 with
the following metrics non-homogenous data (also results on the biological model on the same dataset are reported for comparison).</p>
</div>
<div id="S5.SS0.SSS0.Px6.p2" class="ltx_para">
<table id="S5.SS0.SSS0.Px6.p2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.SS0.SSS0.Px6.p2.1.1.1" class="ltx_tr">
<th id="S5.SS0.SSS0.Px6.p2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r"><em id="S5.SS0.SSS0.Px6.p2.1.1.1.1.1" class="ltx_emph ltx_font_italic">Model</em></th>
<th id="S5.SS0.SSS0.Px6.p2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><em id="S5.SS0.SSS0.Px6.p2.1.1.1.2.1" class="ltx_emph ltx_font_italic">Accuracy</em></th>
<th id="S5.SS0.SSS0.Px6.p2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><em id="S5.SS0.SSS0.Px6.p2.1.1.1.3.1" class="ltx_emph ltx_font_italic">Sens. at Spec. 0.5</em></th>
<th id="S5.SS0.SSS0.Px6.p2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><em id="S5.SS0.SSS0.Px6.p2.1.1.1.4.1" class="ltx_emph ltx_font_italic">AUC</em></th>
<th id="S5.SS0.SSS0.Px6.p2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><em id="S5.SS0.SSS0.Px6.p2.1.1.1.5.1" class="ltx_emph ltx_font_italic">F1-Score</em></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.SS0.SSS0.Px6.p2.1.2.1" class="ltx_tr">
<th id="S5.SS0.SSS0.Px6.p2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S5.SS0.SSS0.Px6.p2.1.2.1.1.1" class="ltx_emph ltx_font_italic">Biological model</em></th>
<td id="S5.SS0.SSS0.Px6.p2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.9125</td>
<td id="S5.SS0.SSS0.Px6.p2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.8518</td>
<td id="S5.SS0.SSS0.Px6.p2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.8976</td>
<td id="S5.SS0.SSS0.Px6.p2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.8679</td>
</tr>
<tr id="S5.SS0.SSS0.Px6.p2.1.3.2" class="ltx_tr">
<th id="S5.SS0.SSS0.Px6.p2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S5.SS0.SSS0.Px6.p2.1.3.2.1.1" class="ltx_emph ltx_font_italic">Synthetic model</em></th>
<td id="S5.SS0.SSS0.Px6.p2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">0.9375</td>
<td id="S5.SS0.SSS0.Px6.p2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">0.8888</td>
<td id="S5.SS0.SSS0.Px6.p2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">0.9255</td>
<td id="S5.SS0.SSS0.Px6.p2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">0.9056</td>
</tr>
</tbody>
</table>
</div>
<div id="S5.SS0.SSS0.Px6.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px6.p3.1" class="ltx_p">A crucial indicator of performance is the model’s confusion matrix
specifically a 96.2% True Positive Rate, 3.7% False Negative Rate;
88.8% True Negative Rate, and 11.1% False Positive Rate.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conlusion and Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper presented a methodology for generating synthetic spike data to train deep learning models in identifying typical and atypical serotonergic neurons using smoothed real spike waveforms with diverse noise masks extracted from different real experiments. A practical case study demonstrated the effectiveness of the method, with a CNN model trained on the augmented dataset achieving high accuracy on non-homogeneous test data. While synthetic data have proven effective, the approach may have limitations in fully capturing the diversity of real spiking patterns. Indeed, a special care must be taken during waveform smoothing and noise intensity calibration to preserve key features and avoid creating unrealistic spikes. Moreover it is important to stress out that the method was developed and validated for serotonergic neurons in mice, and its applicability to other species and cell types might require further investigation.
Despite these limitations, the proposed approach enables the development of robust serotonergic neuron classifiers and opens up to future research (one would like to investigate advanced generative models like GANs and adaptive augmentation strategies). As experimental methods advance and more diverse serotonergic neuron datasets become available, the presented approach can be refined and extended.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">References</h2>

<div id="Sx1.p1" class="ltx_para">
<blockquote id="Sx1.p1.1" class="ltx_quote">
<p id="Sx1.p1.1.1" class="ltx_p">Calizo, L. H.; Akanwa, A.; Ma, X.; Pan, Y.; Lemos, J. C.; Craige,
C.; Heemstra, L. A.; Beck, S. G. <em id="Sx1.p1.1.1.1" class="ltx_emph ltx_font_italic">Raphe Serotonin Neurons Are
Not Homogenous: Electrophysiological, Morphological and Neurochemical
Evidence</em>. Neuropharmacology 2011, 61 (3), 524−543.</p>
<p id="Sx1.p1.1.2" class="ltx_p">Corradetti, D.; Bernardi, A.; Corradetti R.; <em id="Sx1.p1.1.2.1" class="ltx_emph ltx_font_italic">Deep Learning Models
for Atypical Serotoninergic Cells Recognition</em>. bioRxiv 2024.03.03.583157;
doi: https://doi.org/10.1101/2024.03.03.583157</p>
<p id="Sx1.p1.1.3" class="ltx_p">Hajós, M., Gartside, S.E., Villa, A.E., Sharp, T., 1995. <em id="Sx1.p1.1.3.1" class="ltx_emph ltx_font_italic">Evidence
for a repetitive (burst) firing pattern in a sub-population of 5-
hydroxytryptamine neurons in the dorsal and median raphe nuclei
of the rat</em>. Neuroscience 69(1):189-97. doi:10.1016/0306-
4522(95)00227-a.</p>
<p id="Sx1.p1.1.4" class="ltx_p">Mlinar B., Montalbano A., Piszczek L., Gross C., Corradetti R. (2016).
<em id="Sx1.p1.1.4.1" class="ltx_emph ltx_font_italic">Firing properties of genetically identified dorsal raphe serotonergic
neurons in brain slices</em>. Front. Cell Neurosci. 10:195. 10.3389/fncel.2016.00195</p>
<p id="Sx1.p1.1.5" class="ltx_p">Montalbano, A., Waider,J., Barbieri,M., Baytas,O., Lesch,K.P., Corradetti,R.,
et al (2015). <em id="Sx1.p1.1.5.1" class="ltx_emph ltx_font_italic">Cellular resilience: 5-HT neurons in Tph2 (-/-)
mice retain normal firing behaviour despite the lack of brain 5-HT</em>.
Eur. Neuropsychopharmacol. 25, 2022–2035. doi:10.1016/j.euroneuro.2015.08.021</p>
<p id="Sx1.p1.1.6" class="ltx_p">Okaty BW, Commons KG, Dymecki SM. 2019 <em id="Sx1.p1.1.6.1" class="ltx_emph ltx_font_italic">Embracing diversity in the 5-HT neuronal system</em>. Nat Rev Neurosci. 2019 20(7):397-424. doi:
10.1038/s41583-019-0151-3</p>
<p id="Sx1.p1.1.7" class="ltx_p">Liu, Y. (2018). <em id="Sx1.p1.1.7.1" class="ltx_emph ltx_font_italic">Feature Extraction and Image Recognition with
Convolutional Neural Networks</em>. Journal of Physics: Conference Series,
1087. https://doi.org/10.1088/1742-6596/1087/6/062032.</p>
<p id="Sx1.p1.1.8" class="ltx_p">Liu, L., Yang, S., &amp; Shi, D. (2019). <em id="Sx1.p1.1.8.1" class="ltx_emph ltx_font_italic">Advanced Convolutional
Neural Network With Feedforward Inhibition</em>. 2019 International Conference
on Machine Learning and Cybernetics (ICMLC), 1-5.</p>
<p id="Sx1.p1.1.9" class="ltx_p">Vandermaelen, CP., &amp; Aghajanian,G.K. (1983). <em id="Sx1.p1.1.9.1" class="ltx_emph ltx_font_italic">Electrophysiological
and pharmacological characterization of serotonergic dorsal raphe
neurons recorded extracellularly andintracellularly in rat brain slices</em>.
Brain Res. 289, 109–119. doi:10.1016/0006-8993(83)90011-2.</p>
</blockquote>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.05700" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.05701" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.05701">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.05701" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.05702" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 17:39:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
