<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.14054] CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry</title><meta property="og:description" content="The use of computer vision for product and assembly quality control is becoming ubiquitous in the manufacturing industry. Lately, it is apparent that machine learning based solutions are outperforming classical compute…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.14054">

<!--Generated on Thu Mar 14 09:05:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steven Moonen<sup id="id9.9.id1" class="ltx_sup">1</sup>, Bram Vanherle<sup id="id10.10.id2" class="ltx_sup">1</sup>, Joris de Hoog<sup id="id11.11.id3" class="ltx_sup">2</sup>, Taoufik Bourgana<sup id="id12.12.id4" class="ltx_sup">2</sup>, 
<br class="ltx_break">Abdellatif Bey-Temsamani<sup id="id13.13.id5" class="ltx_sup">2</sup>, Nick Michiels<sup id="id14.14.id6" class="ltx_sup">1</sup> 
<br class="ltx_break"><sup id="id15.15.id7" class="ltx_sup">1</sup>Hasselt University - tUL - Flanders Make, Expertise Centre for Digital Media
<br class="ltx_break"><sup id="id16.16.id8" class="ltx_sup">2</sup>Flanders Make, Gaston Geenslaan 8- B-3001 Leuven, Belgium
<br class="ltx_break"><span id="id17.17.id9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{steven.moonen,bram.vanherle, nick.michiels}@uhasselt.be</span> 
<br class="ltx_break"><span id="id18.18.id10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{joris.dehoog,taoufik.bourgana,abdellatif.bey-temsamani}@flandersmake.be </span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id19.id1" class="ltx_p">The use of computer vision for product and assembly quality control is becoming ubiquitous in the manufacturing industry. Lately, it is apparent that machine learning based solutions are outperforming classical computer vision algorithms in terms of performance and robustness. However, a main drawback is that they require sufficiently large and labeled training datasets, which are often not available or too tedious and too time consuming to acquire. This is especially true for low-volume and high-variance manufacturing. Fortunately, in this industry, CAD models of the manufactured or assembled products are available.
This paper introduces CAD2Render, a GPU-accelerated synthetic data generator based on the Unity High Definition Render Pipeline (HDRP). CAD2Render is designed to add variations in a modular fashion, making it possible for high customizable data generation, tailored to the needs of the industrial use case at hand. Although CAD2Render is specifically designed for manufacturing use cases, it can be used for other domains as well.
We validate CAD2Render by demonstrating state of the art performance in two industrial relevant setups. We demonstrate that the data generated by our approach can be used to train object detection and pose estimation models with a high enough accuracy to direct a robot. The code for CAD2Render is available at <a target="_blank" href="https://github.com/EDM-Research/CAD2Render" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/EDM-Research/CAD2Render</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2211.14054/assets/images/example_datasets/example1.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="192" height="160" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2211.14054/assets/images/example_datasets/example2.jpg" id="S1.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="192" height="160" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2211.14054/assets/images/example_datasets/example4.jpg" id="S1.F1.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="192" height="160" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Annotated training set examples of different use cases generated with CAD2Render. left: CNC fabrication, middle: compressor parts, right: tool detection.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Machine vision has been around in the industrial landscape for a couple of decades and the recent surge in popularity has made the technology a major innovation driver for manufacturers. Most of the approaches are relying on classic vision and are finetuned towards the inspection system. Although they are able to achieve great performance, they require full control of the environment where all the operating conditions stay constant. As such, they are prone to failure when variables like lighting or backgrounds might change. Machine learning algorithms, on the other hand, can be trained to handle such variations, but in turn, have some major challenges that need to be addressed before being fully adopted in this industrial domain.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A first key challenge is the need for large annotated training sets, which are tedious, time consuming and very costly to acquire. In the majority of cases, the data has to be annotated manually, which can lead to bias or errors caused by the human annotator. This limitation is more pronounced in the manufacturing industry, as they have taken some major steps forward in flexible assembly and product manufacturing, allowing them to transform their pipeline towards flexible low-volume and high-variance production. In the extreme case, each produced product can be of a different shape (e.g. prosthesis manufacturing), where there is simply no time to manually capture and annotate datasets.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2211.14054/assets/x1.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Schematic overview of CAD2Render. Environment maps, CAD files and material properties are imported in CAD2Render and used to create a high variety of 3D scene’s. Further variations to the material textures can be created by introducing defects like rust or scratches. The scenes are then rendered with a path tracer to create a dataset for training machine learning algorithms.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">A second challenge is the complex outlook of the materials used in manufacturing. Products are often made of metallic-like materials that cause detailed and complex reflections. It is very challenging for a vision technique to generalize to all the possible and complex lighting effects.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">A common approach to cope with small training datasets is data augmentation: slightly distorting the available data points to create new points that still belong to the same category. While it can provide better performance, it still requires a minimum of data and it does not take into account that the actual products are three-dimensional objects and their visual appearance is governed by complex material properties, lighting conditions, and geometrical detail. This is especially relevant in the manufacturing industry with the metallic-like materials.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper exploits the availability of CAD models and the domain knowledge of the manufacturing process, together with the recent advancements in real-time ray tracing, to propose a modular toolkit called CAD2Render. The proposed toolkit is able to automatically generate large amounts of photorealistic training images with extensive visual variations and their accompanying annotations for machine learning purposes. These annotations are generated without human errors or bias. A schematic view of the toolkit is given in Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We selected two industrial relevant algorithms for validation, i.e. bin picking by means of object detection and pose estimation, and keypoint detection. Both are important tasks for automating industrial setups, requiring complex vision algorithms. We show that we can achieve a performance capable of solving these tasks in an industrial relevant environment when training on datasets generated by CAD2Render.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Decreasing training set sizes is a popular area of research. Data augmentation is a widely adopted approach to increase the variability in datasets. By slightly distorting the few available images, new examples are created without the need of relabeling the data. The most common variations used are randomly cropping, rotating, scaling, mirroring, color balancing, and adjusting brightness of the entire collection of training pictures to create many slightly modified copies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. More recently, similar basic image-based variations are generated using deep learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. All of these modifications are limited by the information contained in the original 2D pictures, and by the fact that each modification induces a loss of quality. On the other hand, the actual products are three-dimensional and their visual appearance is governed by complex material properties, lighting conditions, and geometrical detail.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Because of the scarcity of training data, recent works propose techniques for training machine learning models purely on synthetic data and have shown that it can achieve similar results compared to SOTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. An important conclusion they draw is that realism in the synthetic data is a key factor.
Tobin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> demonstrate the importance of domain randomization when using synthetic data. In their work they show that the domain gap between real and synthetic data can be bridged by introducing enough variations in the synthetic data generation. The machine learning algorithms will see the domain gap as yet another variation of the synthetic data.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Rendering photorealistic images is a complex task and all the settings for 3D geometry, lighting and materials have to be meticulously modelled in order to achieve convincing photorealism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. In addition, rendering large datasets is a time consuming task, certainly when the generator is based on a ray tracing algorithm to generate the data. This can be a limiting problem because, in low-volume and high-variance manufacturing, new datasets need to be created in a short amount of time. To speed up this task, it can be distributed on a computer cluster or parallelized on a GPU. Kubric created by Greff et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is a data generation pipeline that is designed to both work on a single computer to facilitate prototyping or small dataset generation, as well as to run on large computer clusters to speed up the generation. This is only useful when you have access to a computer cluster.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Jeong et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> used a NeRF variant to extend a dataset of real images with new viewpoints. This limits the amount of real data required for or a sufficiently large dataset. With NeRF they are able to represent hundreds of photorealistic images in a single format, also reducing the storage size required for the dataset. The data synthesized with this technique can reach better photorealism then rendering a scene from scratch. It also doesn’t require an expert optimizing the appearance of a digital scene. However, this technique is not able to add highly customizable variations to the data.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Other approaches that use 3D rendering focus on a narrow scope of application, such as side view rendering for face detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, stereo rendering for depth estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, vehicle detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> or large scale factory simulations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. In contrast, most products in industrial manufacturing settings have complex material properties and as a result undergo intricate lighting effects when changing the viewpoint or lighting properties. To include these variations, this paper will focus on algorithms to varying this complex light propagation to cost-effectively synthesize large amounts of training data with accurate and realistic variations of lighting conditions, viewpoints, surface properties, etc. 3D CAD models provided by the manufacturers will be utilized to support this process.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>CAD2Render</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">CAD2Render is designed as a modular and highly customizable toolkit built upon the HDRP pipeline of Unity3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> for generating high quality synthetic data for deep learning purposes. It focuses on photorealism by including global illumination effects. A high level overview is provided in Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Inspired by the key insight of Tobin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, that domain randomization is a powerful tool to successfully exploit synthetic data for training deep learning models, we argue that CAD2Render should support a wide set of complex variations. CAD2Render supports variations such as model types, number of models, instancing, environments lighting, viewpoints, exposure, supporting structures, materials, material appearance, textures, etc. These variations are added in a modular fashion and can be enabled, disabled or extended in function of the use case.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Modular Variations</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To facilitate the need for broad and complex variations in the training data, we introduce a wide range of modular randomizers that can introduce different types of variations in the synthetic data. For the clarity of this paper, we have categorized the modules based on pose, lighting, appearance and miscellaneous variations. This section describes how these variations are generated.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Camera Variations</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The camera pose is randomly defined in spherical coordinates <math id="S3.SS1.SSS1.p1.1.m1.3" class="ltx_Math" alttext="(\theta,\phi,r)" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.3a"><mrow id="S3.SS1.SSS1.p1.1.m1.3.4.2" xref="S3.SS1.SSS1.p1.1.m1.3.4.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.3.4.2.1" xref="S3.SS1.SSS1.p1.1.m1.3.4.1.cmml">(</mo><mi id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">θ</mi><mo id="S3.SS1.SSS1.p1.1.m1.3.4.2.2" xref="S3.SS1.SSS1.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS1.SSS1.p1.1.m1.2.2" xref="S3.SS1.SSS1.p1.1.m1.2.2.cmml">ϕ</mi><mo id="S3.SS1.SSS1.p1.1.m1.3.4.2.3" xref="S3.SS1.SSS1.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS1.SSS1.p1.1.m1.3.3" xref="S3.SS1.SSS1.p1.1.m1.3.3.cmml">r</mi><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.3.4.2.4" xref="S3.SS1.SSS1.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.3b"><vector id="S3.SS1.SSS1.p1.1.m1.3.4.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.3.4.2"><ci id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">𝜃</ci><ci id="S3.SS1.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2">italic-ϕ</ci><ci id="S3.SS1.SSS1.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.3.3">𝑟</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.3c">(\theta,\phi,r)</annotation></semantics></math>, in a sphere around a point of interest, orientated towards this point. The user can define minimum and maximum ranges for these parameters which are then uniformly sampled within this range.
The intrinsic parameters of the camera can also be adjusted to match an existing camera. To further match a physical setup it is also possible to import exact camera poses from a BOP dataset.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Object Variations</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The object pose is randomized by automatically spawning new objects in the scene. For each dataset, the user can setup a spawning volume, which defines the 3D region where new objects can be instantiated. Furthermore, the user specifies a ”model path” that contains the actual models to be spawned, in the form of prefabs. These prefabs can be very simple, just a mesh representation of the CAD model, or can be fully tailored to the use case. CAD2Render will randomly select a set of 3D models from this folder and instantiates them in the scene. The user can specify how many random object are spawned per generated image and if each model is unique or can be instantiated multiple times. In addition, the built-in Nvidia PhysX engine can be enabled or disabled to simulate the objects falling in a natural pose. If enabled, the scene requires a supporting structure, for example a table or pallet.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2211.14054/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="291" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Complex lighting variations. Left: inter-reflections between object and and pallet. Middle: intense highlights (top) and hard shadows (bottom). Right: changes in environment lighting. </figcaption>
</figure>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Lighting Variations</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Training datasets need sufficient variation in lighting to make deep learning techniques robust for sudden changes in environmental effects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Two types of variations in light are supported by CAD2Render.
First, random high dynamic range environment maps are applied to each rendered image. The environment maps are randomly selected from a user specified path. Furthermore, randomized exposure and rotation of the environment map is supported.
Second, the user can specify additional light source prefabs, acting as templates for additional 3D light sources. During rendering, for each generated image, parameters such as the number or 3D light sources, the intensity, position, rotation and radius can be set and randomized.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">An example of lighting variations is given in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1.2 Object Variations ‣ 3.1 Modular Variations ‣ 3 CAD2Render ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The Figure shows examples of environment map variations and variations of intense highlights, shadows and color. We argue that this type of complex photorealistic variations in reflections, shadows and highlights are crucial to incorporate in the training set, because a trained pose or object detector needs to be able to differentiate between what is the actual object and what are the complex light effects that can confuse the model. The more complex light variations the model sees during training, the more it is robust to such changes in a real context. In addition, support for projector variations is available, where the projection of patterns or images can be simulated (example in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> right).</p>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Appearance Variations</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">The appearance of each instantiated object can be varied on-the-fly. The assigned material properties, in the form of normal, roughness, albedo and displacement maps, can originate from different sources.
They can be manually designed, extracted from real sample materials, extracted of the internet or (procedurally) generated (see Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> on the left). In the case of the former three, a database of existing material models can be passed to the CAD2Render toolkit. In the case of the latter, variations of
material textures can be automatically generated by the toolkit. At the moment, the toolkit supports three types of texture generators that are industrially relevant: scratches, rust and polishing lines. The settings of these generators can be tailored the the specific needs of the use case at hand. CAD2Render can randomly select and apply materials from a user defined path, similar to the environment maps. Additional variations can be set, with random parameters for HSV offsets, rust, polishing lines, scratches, etc. The database of materials can be provided by the user or can be based on online sources such as the Measured Material Library for Unity HDRP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2211.14054/assets/x3.png" id="S3.F4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="207" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example of rust and scratch variations. An extracted path material from a physical example (top left) is used in combination with a CAD file (bottom left) to generate synthetic images (middle). On top of the extracted material, we can add different material effects such as rust and scratches (right).</figcaption>
</figure>
<div id="S3.SS1.SSS4.p2" class="ltx_para">
<p id="S3.SS1.SSS4.p2.1" class="ltx_p">The simulation of rust and scratches is inspired by the work of Mihaylov <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1.4 Appearance Variations ‣ 3.1 Modular Variations ‣ 3 CAD2Render ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows examples of rust and scratch variations. In this example, we start from a basic material texture, extracted from a real physical example. The extracted material textures are adjusted to include imperfections such as rust or scratches. These variations are generated during the execution with the help of Simplex noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. The noise map is used to mark areas where the imperfections need to be generated.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2211.14054/assets/images/texture_resampling.png" id="S3.F5.g1" class="ltx_graphics ltx_img_landscape" width="538" height="319" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Texture resampling. Left: starting material texture. Middle: first iterations of copied patches. Some noticeable hard edges can be perceived. Right: refined result after 15 iterations.</figcaption>
</figure>
<div id="S3.SS1.SSS4.p3" class="ltx_para">
<p id="S3.SS1.SSS4.p3.1" class="ltx_p">To allow for more variations in material texture, we have implemented a texture resampling algorithm, based on the work of Opara et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The goal is to use an input texture and to generate a new texture that looks similar but has some distinct variations. The proposed approach rearranges the pixels of the input texture and tries to limit any obvious seams. The advantage of this technique is that any type of variation can be modeled as long as an example texture is available. Because the technique of Opara et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> is designed for offline rendering it is too slow (in the order of minutes) for synthetic data generation. To improve generation time, we have optimized the algorithm for GPU, allowing it to run at interactive frame rates. Figure <a href="#S3.F5" title="Figure 5 ‣ 3.1.4 Appearance Variations ‣ 3.1 Modular Variations ‣ 3 CAD2Render ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the approach of the resampling algorithms. First random patches are copied from an example image (on the left) to the desired output texture. This will create obvious seams where two patches meet (middle part). Then a pixel based algorithm is ran recursively to, step by step, improve the resulting texture (on the right).
Each pixel determines the difference in pixel colors from their current neighborhood compared to the neighborhood they had in the original texture. Then every pixel will calculate the neighborhood difference of multiple pixels suggested by other pixels in the neighborhood and change to the pixel with the lowest difference. This is done multiple times in a row, reducing the radius of the neighborhood with each iteration.</p>
</div>
</section>
<section id="S3.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.5 </span>Miscellaneous</h4>

<div id="S3.SS1.SSS5.p1" class="ltx_para">
<p id="S3.SS1.SSS5.p1.1" class="ltx_p">To conclude, there are some miscellaneous dataset settings that can be set, such as: image resolution, rendering profile (see Section <a href="#S3.SS2" title="3.2 Rendering Profiles and GPU Acceleration ‣ 3 CAD2Render ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>), post processing profile for white balancing, tonemapping, gamma correction, camera exposure, settings for export, number of physics frames and render frames before export, etc. It is important to note that CAD2Render is built upon Unity and the flexibility of the underlying game engine allows the user to implement or optimize any additional requirements for the use case at hand. This will possibly allow it to be applicable to other domains than the manufacturing industry as well, as long as CAD models are available.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Rendering Profiles and GPU Acceleration</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The quality and speed of the renders are highly customizable because CAD2Render is based on the High Definition Rendering Pipeline of Unity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Unity has a built-in GPU accelerated path tracer that can be used for rendering when photorealism is important. The main drawback of using pathtracing for the generation of the images is the time it takes to render. For some applications a large number of images might be needed. Complementary research has experimented with the amount of CAD2Render images needed to train object detection models and has shown that a large amount of images are beneficial when no domain knowledge is used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">DLSS 2.0 and/or NVIDIA OptiX Denoiser/Intel Open Image Denoiser can be used to reduce the time it takes to generate a converged image. To further increase the generation speed, two other rendering modes can be used: rasterization or hybrid. Rasterization relies on the classic rendering pipeline and hybrid mode is rasterization with limited ray tracing support for shadows, reflections and ambient occlusion.
Changing the rendering approach to rasterization or hybrid can have a considerable positive impact on generation time, but can introduce different types of artifacts. In Figure <a href="#S3.F6" title="Figure 6 ‣ 3.2 Rendering Profiles and GPU Acceleration ‣ 3 CAD2Render ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> some differences in artifacts are shown between the full pathtracer and the hybrid renderer. The hybrid renderer fails to show all reflections, shadows and highlights that are present in the images generated with the path tracer. The path tracer can introduce noise in areas where the result path tracer converges slowly if no denoiser is used.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Rendering Profiles and GPU Acceleration ‣ 3 CAD2Render ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives an overview of the rendering time compared to the resolution between path tracing (at 500 rays per pixel) and rasterization. All measurements were taken with the template scene of the CAD2Render repository on a RTX2070S GPU and an i7-10700KF CPU. Based on empirical observations, enabling the denoiser and rendering <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="1/10" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mn id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">/</mo><mn id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><divide id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></divide><cn type="integer" id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">1/10</annotation></semantics></math> of the samples will introduce no noticeable artifacts or noise, allowing for an additional speedup factor of 10. However, further research is required to prove this does not impact the performance of the machine learning models.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2211.14054/assets/x4.png" id="S3.F6.g1" class="ltx_graphics ltx_img_landscape" width="461" height="242" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparison between the Unity path tracer and a hybrid renderer. The Hybrid renderer fails to generate all reflections, shadows and highlights, while the path tracer can introduce noise.</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="6"><span id="S3.T1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Rendering 10.000 images</span></td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.2.2.1.1" class="ltx_text" style="font-size:80%;">Resolution</span></td>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">
<table id="S3.T1.1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.2.2.2.1.1" class="ltx_tr">
<td id="S3.T1.1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.2.2.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Path tracing</span></td>
</tr>
<tr id="S3.T1.1.2.2.2.1.2" class="ltx_tr">
<td id="S3.T1.1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.2.2.2.1.2.1.1" class="ltx_text" style="font-size:80%;">500 samples</span></td>
</tr>
<tr id="S3.T1.1.2.2.2.1.3" class="ltx_tr">
<td id="S3.T1.1.2.2.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.2.2.2.1.3.1.1" class="ltx_text" style="font-size:80%;">time (hours)</span></td>
</tr>
</table>
</td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">
<table id="S3.T1.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.2.2.3.1.1" class="ltx_tr">
<td id="S3.T1.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.2.2.3.1.1.1.1" class="ltx_text" style="font-size:80%;">Rasterization</span></td>
</tr>
<tr id="S3.T1.1.2.2.3.1.2" class="ltx_tr">
<td id="S3.T1.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.2.2.3.1.2.1.1" class="ltx_text" style="font-size:80%;">time (hours)</span></td>
</tr>
</table>
</td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S3.T1.1.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.2.2.4.1.1" class="ltx_tr">
<td id="S3.T1.1.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.2.2.4.1.1.1.1" class="ltx_text" style="font-size:80%;">Memory</span></td>
</tr>
<tr id="S3.T1.1.2.2.4.1.2" class="ltx_tr">
<td id="S3.T1.1.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.2.2.4.1.2.1.1" class="ltx_text" style="font-size:80%;">(GB)</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_l"><span id="S3.T1.1.3.3.2.1" class="ltx_text" style="font-size:80%;">DLSS</span></td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.1.3.3.3.1" class="ltx_text" style="font-size:80%;text-decoration:line-through; text-decoration-color:;">DLSS</span></td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_l"><span id="S3.T1.1.3.3.4.1" class="ltx_text" style="font-size:80%;">DLSS</span></td>
<td id="S3.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.1.3.3.5.1" class="ltx_text" style="font-size:80%;text-decoration:line-through; text-decoration-color:;">DLSS</span></td>
<td id="S3.T1.1.3.3.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.4.4.1.1" class="ltx_text" style="font-size:80%;">1280 x 720</span></td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.4.4.2.1" class="ltx_text" style="font-size:80%;">7.9</span></td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.4.4.3.1" class="ltx_text" style="font-size:80%;">7.8</span></td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.4.4.4.1" class="ltx_text" style="font-size:80%;">2.3</span></td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.4.4.5.1" class="ltx_text" style="font-size:80%;">2.3</span></td>
<td id="S3.T1.1.4.4.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.4.4.6.1" class="ltx_text" style="font-size:80%;">5.4</span></td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.5.5.1.1" class="ltx_text" style="font-size:80%;">1920 x 1080</span></td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.5.5.2.1" class="ltx_text" style="font-size:80%;">8.3</span></td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.5.5.3.1" class="ltx_text" style="font-size:80%;">13.2</span></td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.5.5.4.1" class="ltx_text" style="font-size:80%;">2.4</span></td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.5.5.5.1" class="ltx_text" style="font-size:80%;">2.4</span></td>
<td id="S3.T1.1.5.5.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.5.5.6.1" class="ltx_text" style="font-size:80%;">11.3</span></td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_right ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.6.6.1.1" class="ltx_text" style="font-size:80%;">3840 x 2160</span></td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.1.6.6.2.1" class="ltx_text" style="font-size:80%;">22.3</span></td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.1.6.6.3.1" class="ltx_text" style="font-size:80%;">30.0</span></td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.1.6.6.4.1" class="ltx_text" style="font-size:80%;">3.4</span></td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.1.6.6.5.1" class="ltx_text" style="font-size:80%;">3.6</span></td>
<td id="S3.T1.1.6.6.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.1.6.6.6.1" class="ltx_text" style="font-size:80%;">43.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Generation time and storage size for a dataset of 10.000 images. Comparison between various resolutions and render mode (path tracing compared to rasterization). Measured on a RTX2070 Super GPU and an i7-10700KF CPU at 3.80GHz.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Exporting to BOP format</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To allow for easy application of the generated datasets, CAD2Render exports the annotated dataset to the standardized BOP format <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This file format contains RGB images, object and camera poses, camera parameters, instance segmentation, depth maps and 3D models. The BOP format and its accompanying toolkit are originally designed for easy-of-use benchmarking for pose estimation, but due to the rich annotations it can be used for other tasks, such as object detection, segmentation and depth estimation.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Importing BOP for digital twin creation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">CAD2Render supports the import of existing BOP datasets as well. This is especially useful for creating a digital twin dataset of a real dataset. This feature makes it easier to research the domain gap between synthetic and real images. The Dataset of Industrial Metal Objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is one such digital twin dataset, generated by CAD2Render.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Validation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Validation of CAD2Render is done on two industrially relevant use cases: bin picking and 2D keypoint detection. The former requires solutions for both object detection and pose estimation. The latter is a useful approach to detect important landmarks. The validation results are trained solely on synthetic data and tested on real data. Bin picking is applied to metallic objects on a table. Keypoint detection is done for two use cases: tools in use in natural environments and assembly validation on a top down view of a work piece. This wide range of applications highlights the customizability of the toolkit.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2211.14054/assets/images/overview_flow.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="389" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Overview of the flow of the validation algorithm</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Bin Picking of Metal Objects</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The proposed method has been validated in a setup representing an industrial pick and place application, using a collaborative robot (cobot), equipped with a suction cup and a static camera. The identification of the objects and subsequent position and pose estimation was done with two state of the art networks: YoloV4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for object detection and PVNET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> for pose estimation. These models are applied sequentially on the input image, where the object detector will calculate the crop in which the object is found. This crop is then used as input for the pose estimator, calculating a full coordinate set for the object in the image. In the case where multiple items are found by the object detector, each crop is processed individually. Figure <a href="#S4.F7" title="Figure 7 ‣ 4 Validation ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows a high-level overview of how both networks are used. 
<br class="ltx_break">
<br class="ltx_break"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset Description</span> 
<br class="ltx_break">A large dataset of 20.000 images was generated. The images contain from 1 to 10 identical items. The item used for this study is a small stamped metal piece, approximately 7 by 7 centimeter and not symmetrical. The rendered images feature random camera angles, within a defined window of heights and angles. Also, the randomized ambient lighting was applied, as explained in Section <a href="#S3.SS1.SSS3" title="3.1.3 Lighting Variations ‣ 3.1 Modular Variations ‣ 3 CAD2Render ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>.
<br class="ltx_break">
<br class="ltx_break"><span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">Validation Setup
<br class="ltx_break"></span>Quantifying the accuracy of the combined pipeline has to be performed with the appropriate hardware considerations in mind. More precisely, an accurate calibration of the camera pinhole model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is vitally important. For this, a ChArUco board was used, which is a combination of a chequerboard and ArUco markers.This board is also used to perform the extrinsic calibration, where the relationship between camera pixels and the world geometry is established by calculating the transformation from the camera coordinate system to the world coordinate system. A reference point (or origin point) is established from which geometrical distances can be calculated.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The validation method then follows the following steps: (1) carefully place the item on the grid; (2) use the pose estimator to estimate the location of a corner of the item (loop for 10 times); (3) calculate geometrical position of this point with respect to the origin point; (4) calculate difference with known location. The camera used in this setup was an IDS UI-3280CP Rev. 2, which has a 2456x2054 pixel sensor and a global shutter. 
<br class="ltx_break">
<br class="ltx_break"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Validation Results
<br class="ltx_break"></span>The object was placed at various locations of the ChArUco board, generally at 30mm intervals. As no rotations of the object were performed, the validation is only valid for the position estimations and not for the rotation estimation. Figure <a href="#S4.F8" title="Figure 8 ‣ 4.1 Bin Picking of Metal Objects ‣ 4 Validation ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the result of the validation methodology explained in this section. It can be seen that there is a specific area in the image outside which the subsequent estimations of the keypoint show relatively large deviations.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2211.14054/assets/images/accuracy_precision.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="343" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Result of the validation methodology. The red X represents the ground-truth, while the black dots are the results of subsequent estimations
by the Pose Estimator. </figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.1" class="ltx_text" style="font-size:80%;">pos</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.2.1" class="ltx_text" style="font-size:80%;">Std Dev [mm]</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.3.1" class="ltx_text" style="font-size:80%;">Max dev [mm]</span></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.4.1" class="ltx_text" style="font-size:80%;">Min dev [mm]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">x</span></td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.2.1" class="ltx_text" style="font-size:80%;">0.91</span></td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.3.1" class="ltx_text" style="font-size:80%;">7.50</span></td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.4.1" class="ltx_text" style="font-size:80%;">0.040</span></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">y</span></td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T2.1.3.2.2.1" class="ltx_text" style="font-size:80%;">1.37</span></td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T2.1.3.2.3.1" class="ltx_text" style="font-size:80%;">9.92</span></td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T2.1.3.2.4.1" class="ltx_text" style="font-size:80%;">0.002</span></td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.4.3.1.1" class="ltx_text" style="font-size:80%;">z</span></td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.4.3.2.1" class="ltx_text" style="font-size:80%;">11.16</span></td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.4.3.3.1" class="ltx_text" style="font-size:80%;">57.90</span></td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.4.3.4.1" class="ltx_text" style="font-size:80%;">0.220</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Validation Results of the position estimation for Metal Plates on the ChArUco board.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Bin Picking of Metal Objects ‣ 4 Validation ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the statistical analysis of the combined results. It can be seen that the standard deviation in the Y-direction is slightly larger compared to the X-direction, which can be explained by the fact that the used camera sensor is not square and therefore there are more pixels in the Y-direction. This means that these pixels fall in the more heavily curved part of the lens, increasing the deviation.
More generally, it is shown that, provided good camera calibration, the models can be accurate to close to 1 mm in both X and Y directions, which will be more than good enough in most pick-and-place applications. The estimation in the Z-direction is large, however this is a classic issue with height estimations from 2D images that can be improved in the future with a multi-camera setup. 
<br class="ltx_break">
<br class="ltx_break"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Robustness against harsh light conditions</span> 
<br class="ltx_break">To demonstrate the robustness of the developed algorithms against harsh lighting conditions, a simple physical setup was conceived on which an object at a known location and orientation could be subjected to either high or low lighting conditions. A dataset was rendered following the method explained in <a href="#S3.SS1.SSS3" title="3.1.3 Lighting Variations ‣ 3.1 Modular Variations ‣ 3 CAD2Render ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>, this time using a slightly larger metallic object. A simple method of counting the saturated pixels on the surface of the object under test gave an approximate value of light intensity. From Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Bin Picking of Metal Objects ‣ 4 Validation ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we can see that under all but the most harsh conditions, the XY-deviation, calculated as the Euclidean distance between the ground-truth and the estimated location, is low. It can be observed that when the light intensity reaches more than 70%, the deviation reaches around 1cm, which can be considered to be extreme.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Intensity (%)</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">XY Deviation [cm]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.2.1.1.1" class="ltx_text" style="font-size:80%;">0.000</span></td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T3.1.2.1.2.1" class="ltx_text" style="font-size:80%;">0.276</span></td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.3.2.1.1" class="ltx_text" style="font-size:80%;">35.440</span></td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T3.1.3.2.2.1" class="ltx_text" style="font-size:80%;">0.340</span></td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.4.3.1.1" class="ltx_text" style="font-size:80%;">44.090</span></td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T3.1.4.3.2.1" class="ltx_text" style="font-size:80%;">0.742</span></td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<td id="S4.T3.1.5.4.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.5.4.1.1" class="ltx_text" style="font-size:80%;">69.990</span></td>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T3.1.5.4.2.1" class="ltx_text" style="font-size:80%;">0.565</span></td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<td id="S4.T3.1.6.5.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.6.5.1.1" class="ltx_text" style="font-size:80%;">73.170</span></td>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T3.1.6.5.2.1" class="ltx_text" style="font-size:80%;">1.460</span></td>
</tr>
<tr id="S4.T3.1.7.6" class="ltx_tr">
<td id="S4.T3.1.7.6.1" class="ltx_td ltx_align_right ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.7.6.1.1" class="ltx_text" style="font-size:80%;">83.660</span></td>
<td id="S4.T3.1.7.6.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.1.7.6.2.1" class="ltx_text" style="font-size:80%;">2.444</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Deviation of the estimated XY-location of an object under a wide range of light intensities.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>2D Keypoint Detection</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As an additional validation case, CAD2Render was used to train models for the problem of semantic 2D keypoint detection. Specifically, two problems are tackled: localizing keypoints in images of different hand tools and using keypoint detection to validate the assembly of aluminium beams. To find the landmarks, a UNET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> type architecture with intermediate supervision was trained for to generate probability maps for each semantic keypoint location. The problem of tool keypoint detection was investigated by Vanherle et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> using the CAD2Render tool, for more details consult their paper.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Tool Keypoint Detection Use Case</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">For this use-case we attempt to find the location of certain sementic keypoints of hand tools. The tools considered are a screwdriver, hammer, wrench and combination wrench. For each of these tools we find a number of keypoints by training a model for each tool.
<br class="ltx_break">
<br class="ltx_break"><span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset Description</span> 
<br class="ltx_break">To train such models, a large amount of data is needed. For each tool we collected a few textured 3D models from the internet. The 3D tools from the internet did not closely resemble the target tools, but did belong to the same class of tool. The CAD2Render toolkit was used to generate 20.000 images for each tool. The tools were randomly spawned in a space with a random environment map as background. Additionally, a few random objects from ShapeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> were also spawned in the space to simulate occlusions. For this validation case, the faster hybrid renderer was used. Figure <a href="#S4.F9" title="Figure 9 ‣ 4.2.1 Tool Keypoint Detection Use Case ‣ 4.2 2D Keypoint Detection ‣ 4 Validation ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows some examples of synthetically generated images from the tool keypoint dataset.</p>
</div>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2211.14054/assets/images/tool_keypoints/screwdriver.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="144" height="144" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2211.14054/assets/images/tool_keypoints/wrench.png" id="S4.F9.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="144" height="144" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2211.14054/assets/images/tool_keypoints/comb_wrench.png" id="S4.F9.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="144" height="144" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2211.14054/assets/images/tool_keypoints/hammer.png" id="S4.F9.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="144" height="144" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>A few examples from the tool keypoint detection dataset created by CAD2Render.</figcaption>
</figure>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p"><span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Validation Setup</span> 
<br class="ltx_break">To verify whether the datasets generated by CAD2Render are suitable to train a keypoint detection model on, we test the performance of the trained model on real images of tools. For each of the four tools we captured 50 real photographs, and manually annotated these images. To properly test the robustness of the trained models objects were photographed in wide variety of poses, lighting conditions, camera angles and backgrounds. Additionally, occlusions and truncations are introduced. The model trained on the synthetic data was then used to detect keypoint locations in the real images.
<br class="ltx_break">
<br class="ltx_break"></p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p"><span id="S4.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Validation Results</span> 
<br class="ltx_break">To measure the model’s performance we use the Percentage of Correct Keypoints (PCK) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> metric with an <math id="S4.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.SSS1.p3.1.m1.1a"><mi id="S4.SS2.SSS1.p3.1.m1.1.1" xref="S4.SS2.SSS1.p3.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.1.m1.1b"><ci id="S4.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.1.m1.1c">\alpha</annotation></semantics></math> of 1.0. The results are shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.2.1 Tool Keypoint Detection Use Case ‣ 4.2 2D Keypoint Detection ‣ 4 Validation ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The models were trained on synthetic images, created randomly without taking domain knowledge into account. Yet, these models are able to detect keypoints in the unseen real images with good accuracy. This shows that the CAD2Render toolkit is able to create synthetic images that are suitable for this problem space and that images produced by the faster hybrid rendering mode can produce good models as well. Additionally, research has shown that models trained on images generated by CAD2Render perform better on this task than models trained on images generated by simple 2D image augmentations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. This shows the benefit of using 3D information to generate training data.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1" class="ltx_tr">
<th id="S4.T4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T4.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Tool</span></th>
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T4.1.1.1.m1.1" class="ltx_Math" alttext="\textrm{PCK}_{0.1}" display="inline"><semantics id="S4.T4.1.1.1.m1.1a"><msub id="S4.T4.1.1.1.m1.1.1" xref="S4.T4.1.1.1.m1.1.1.cmml"><mtext mathsize="80%" id="S4.T4.1.1.1.m1.1.1.2" xref="S4.T4.1.1.1.m1.1.1.2a.cmml">PCK</mtext><mn mathsize="80%" id="S4.T4.1.1.1.m1.1.1.3" xref="S4.T4.1.1.1.m1.1.1.3.cmml">0.1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T4.1.1.1.m1.1.1.2a.cmml" xref="S4.T4.1.1.1.m1.1.1.2"><mtext mathsize="80%" id="S4.T4.1.1.1.m1.1.1.2.cmml" xref="S4.T4.1.1.1.m1.1.1.2">PCK</mtext></ci><cn type="float" id="S4.T4.1.1.1.m1.1.1.3.cmml" xref="S4.T4.1.1.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\textrm{PCK}_{0.1}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T4.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Screwdriver</span></th>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T4.1.2.1.2.1" class="ltx_text" style="font-size:80%;">86.1</span></td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<th id="S4.T4.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T4.1.3.2.1.1" class="ltx_text" style="font-size:80%;">Wrench</span></th>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.1.3.2.2.1" class="ltx_text" style="font-size:80%;">88.9</span></td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<th id="S4.T4.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T4.1.4.3.1.1" class="ltx_text" style="font-size:80%;">Combination Wrench</span></th>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.1.4.3.2.1" class="ltx_text" style="font-size:80%;">86.1</span></td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<th id="S4.T4.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S4.T4.1.5.4.1.1" class="ltx_text" style="font-size:80%;">Hammer</span></th>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="S4.T4.1.5.4.2.1" class="ltx_text" style="font-size:80%;">84.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Accuracy of the keypoint detection models for each tool trained on the synthetic data. Performance is measured in <math id="S4.T4.3.m1.1" class="ltx_Math" alttext="\textrm{PCK}_{0.1}" display="inline"><semantics id="S4.T4.3.m1.1b"><msub id="S4.T4.3.m1.1.1" xref="S4.T4.3.m1.1.1.cmml"><mtext id="S4.T4.3.m1.1.1.2" xref="S4.T4.3.m1.1.1.2a.cmml">PCK</mtext><mn id="S4.T4.3.m1.1.1.3" xref="S4.T4.3.m1.1.1.3.cmml">0.1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.3.m1.1c"><apply id="S4.T4.3.m1.1.1.cmml" xref="S4.T4.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.3.m1.1.1.1.cmml" xref="S4.T4.3.m1.1.1">subscript</csymbol><ci id="S4.T4.3.m1.1.1.2a.cmml" xref="S4.T4.3.m1.1.1.2"><mtext id="S4.T4.3.m1.1.1.2.cmml" xref="S4.T4.3.m1.1.1.2">PCK</mtext></ci><cn type="float" id="S4.T4.3.m1.1.1.3.cmml" xref="S4.T4.3.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.m1.1d">\textrm{PCK}_{0.1}</annotation></semantics></math> over the validation set of real images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Assembly Validation Use Case</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">To further assess the usefulness of the keypoint detector model trained on synthetic images of CAD2Render, we applied it to an additional defect detection use case, in the form of a simple assembly validation tool. The target assembly comprises of two aluminium beams, connected by two rubber insulators. These four sections are assembled on an automated line and the assembly quality is validated using a camera with high dynamic range. Figure <a href="#S5.F10" title="Figure 10 ‣ 5 Limitations and Future Work ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows a good and a bad assembly of both real and synthetic images.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">The keypoint detection model was trained for each part on the specific parts of the items that perform the insertions. During inference, the corresponding pairs for keypoints are assessed for their euclidean distance to fall below a predefined threshold. If the distance is too great, that specific insertion is assumed to be not successful and the assembly has failed. Figure <a href="#S5.F10" title="Figure 10 ‣ 5 Limitations and Future Work ‣ CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows the output of the model. When applied to all input images (26 in total) the model could successfully identify good and bad assemblies in all cases.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Although we performed extensive experiments showing the good performance of models trained on data generated by CAD2Render, a direct comparison to other methods is missing. Future work should compare the performance of models trained using our method to the performance of models trained on data generated by other state of the art methods. Additionally, parameters such as speed of rendering and ease of use should also be taken into account. Additionally, we would like to investigate the impact of the different quality rendering modes in CAD2render on the final model performance.
A number of training data variations were introduced to help improve generalization. A study on the impact of these variations on downstream performance is beyond the scope of this paper. For an in depth analysis of the impact of light and pose variations on object detection performance, for datasets generated by the CAD2Render tool, we refer to complementary research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<figure id="S5.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F10.5" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:50.8pt;height:7.199999999999999pt;vertical-align:-1.6pt;"><span class="ltx_transformed_inner" style="width:50.8pt;transform:translate(0pt,2.33pt) rotate(-0deg) ;">
<p id="S5.F10.5.1" class="ltx_p"><span id="S5.F10.5.1.1" class="ltx_text" style="font-size:80%;">good assembly</span></p>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F10.6" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:46.8pt;height:7.199999999999999pt;vertical-align:-1.6pt;"><span class="ltx_transformed_inner" style="width:46.8pt;transform:translate(0pt,2.33pt) rotate(-0deg) ;">
<p id="S5.F10.6.1" class="ltx_p"><span id="S5.F10.6.1.1" class="ltx_text" style="font-size:80%;">bad assembly</span></p>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F10.4" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S5.F10.4.5" class="ltx_text" style="font-size:80%;position:relative; bottom:36.1pt;">
<span id="S5.F10.4.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.0pt;height:87.4pt;vertical-align:6.0pt;"><span class="ltx_transformed_inner" style="width:87.4pt;transform:translate(-39.7pt,-44.7pt) rotate(-90deg) ;">
<span id="S5.F10.4.5.1.1" class="ltx_p">synthetic (CAD2Render)</span>
</span></span></span><span id="S5.F10.4.4" class="ltx_text" style="font-size:80%;"> <img src="/html/2211.14054/assets/images/assembl_val/profile_synth_correct.jpg" id="S5.F10.1.1.g1" class="ltx_graphics ltx_img_square" width="240" height="215" alt="Refer to caption">
<img src="/html/2211.14054/assets/images/assembl_val/profile_synth_defect.jpg" id="S5.F10.2.2.g2" class="ltx_graphics ltx_img_square" width="240" height="215" alt="Refer to caption"> 
<br class="ltx_break"><span id="S5.F10.4.4.1" class="ltx_text" style="position:relative; bottom:36.1pt;">
<span id="S5.F10.4.4.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.1pt;height:47.4pt;vertical-align:5.6pt;"><span class="ltx_transformed_inner" style="width:47.4pt;transform:translate(-20.14pt,-24.92pt) rotate(-90deg) ;">
<span id="S5.F10.4.4.1.1.1" class="ltx_p">real examples</span>
</span></span></span>
<img src="/html/2211.14054/assets/images/assembl_val/good_good.png" id="S5.F10.3.3.g3" class="ltx_graphics ltx_img_square" width="240" height="215" alt="Refer to caption">
<img src="/html/2211.14054/assets/images/assembl_val/bad_good.png" id="S5.F10.4.4.g4" class="ltx_graphics ltx_img_square" width="240" height="215" alt="Refer to caption">

</span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Assembly validation of good and bad assemblies. Top: synthetic examples generated with CAD2Render. Bottom: Output of the model shown on top of real examples in the case of a good validation (all points within spec) and a bad validation (one of the insertion areas show points that are too far from each other).</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper proposed a novel toolkit for synthetic data generation, that can generate a vast amount of complex photorealistic variations, including changes in lighting, appearance and pose. It is cost-effective and optimized for rendering speed on consumer hardware by exploiting the recent advancements in real-time raytracing and denoising, which is essential for fast deployment in low-volume and high-variance manufacturing. At the moment it is specifically designed for industrial use cases. However, it can be easily utilized in other domains as well, provided that there is CAD data available. Since it allows for the import and export of datasets in a standardized fashion, it can generate synthetic simulations of existing datasets, a so called digital twin. As such, it can be an enabling technology for future research on sim2real and how to close the domain gap between the real and synthetic world. Future improvements would be to include more realistic variations in the appearance of the objects based on extracted appearance of real physical examples. This would possibly reduce the sim2real domain gap that still exists in the generated data.
<br class="ltx_break"></p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Acknowledgement<span id="S6.p2.1.1.1" class="ltx_text ltx_font_medium">
This research was realized in the framework of the PILS SBO project (Products Inspection by Little Supervision), funded by Flanders Make, the strategic research Centre for the Manufacturing Industry in Belgium; and the Special Research Fund (BOF, mandate ID BOF20OWB24) of Hasselt University.
</span></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars Mescheder, Andreas Geiger,
and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Augmented reality meets computer vision: Efficient data generation
for urban driving scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. J. Comput. Vision</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 126(9):961–972, sep 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Yolov4: Optimal speed and accuracy of object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, abs/2004.10934, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang,
Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao,
Li Yi, and Fisher Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">ShapeNet: An Information-Rich 3D Model Repository.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">Technical Report arXiv:1512.03012 [cs.GR], Stanford University —
Princeton University — Toyota Technological Institute at Chicago, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Daniel Crispell, Octavian Biris, Nate Crosswhite, Jeffrey Byrne, and Joseph L
Mundy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Dataset augmentation for pose and lighting invariant face
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1704.04326</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Peter De Roovere, Steven Moonen, Nick Michiels, and Francis Wyffels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Dataset of industrial metal objects, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel
Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles
Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji,
Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz
Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M.
Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora,
Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea
Tagliasacchi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Kubric: a scalable dataset generator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Stefan Gustavson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Simplex noise demystified.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">01 2005.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Tomáš Hodaň, Frank Michel, Eric Brachmann, Wadim Kehl, Anders
Glent Buch, Dirk Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon
Zabulis, Caner Sahin, Fabian Manhardt, Federico Tombari, Tae-Kyun Kim,
Jiří Matas, and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">BOP: Benchmark for 6D object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Yoonwoo Jeong, Seungjoo Shin, Junha Lee, Chris Choy, Anima Anandkumar, Minsu
Cho, and Jaesik Park.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Perfception: Perception using radiance fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
J. Kannala and S.S. Brandt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">A generic camera model and calibration method for conventional,
wide-angle, and fish-eye lenses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">,
28(8):1335–1340, 2006.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Prabhjot Kaur, Samira Taghavi, Zhaofeng Tian, and Weisong Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">A survey on simulators for testing self-driving cars.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 Fourth International Conference on Connected and
Autonomous Driving (MetroCAD)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 62–70, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Kevin Lee and David Moloney.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Evaluation of synthetic data for deep learning stereo depth
algorithms on embedded platforms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 4th International Conference on Systems and Informatics
(ICSAI)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 170–176, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
V. Mihaylov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Rendering metals and worn or weathered metallic objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">Master’s thesis, Technical University of Denmark, Department of
Applied Mathematics and Computer Science / DTU Co, Matematiktorvet,
Building 303B, DK-2800 Kgs. Lyngby, Denmark, compute@compute.dtu.dk, 2013.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">DTU supervisor: Jeppe Revall Frisvad, jerf@dtu.dk, DTU Compute.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Yair Movshovitz-Attias, Takeo Kanade, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">How useful is photo-realistic rendering for visual learning?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In Gang Hua and Hervé Jégou, editors, </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision –
ECCV 2016 Workshops</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 202–217, Cham, 2016. Springer International
Publishing.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
NVIDIA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Nvidia isaac sim, 2022.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://developer.nvidia.com/isaac-sim" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://developer.nvidia.com/isaac-sim</a><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Anastasia Opara, Jake Shadle, and Peter Stefek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">More like this, please! texture synthesis and remixing from a single
example, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Pvnet: Pixel-wise voting network for 6dof pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Luis Perez and Jason Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">The effectiveness of data augmentation in image classification using
deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, abs/1712.04621, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In Nassir Navab, Joachim Hornegger, William M. Wells, and
Alejandro F. Frangi, editors, </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Medical Image Computing and
Computer-Assisted Intervention – MICCAI 2015</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 234–241, Cham, 2015.
Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Alireza Shafaei, James J. Little, and Mark Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Play and learn: Using video games to train computer vision models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In Richard C. Wilson, Edwin R. Hancock, and William A. P. Smith,
editors, </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the British Machine Vision Conference 2016,
BMVC 2016, York, UK, September 19-22, 2016</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">. BMVA Press, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Joshua Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Domain randomization for transferring deep neural networks from
simulation to the real world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, abs/1703.06907, 2017.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem
Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Training deep networks with synthetic data: Bridging the reality gap
by domain randomization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW)</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 1082–10828, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Unity-Technologies.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Unity technologies measured material library for high definition
render pipeline.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/Unity-Technologies/MeasuredMaterialLibraryHDRP" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/Unity-Technologies/MeasuredMaterialLibraryHDRP</a><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Unity3D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">High definition render pipeline (hdrp) - unity, 2022.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://unity.com/srp/High-Definition-Render-Pipeline" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://unity.com/srp/High-Definition-Render-Pipeline</a><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Bram Vanherle, Steven Moonen, Frank Van Reeth, and Nick Michiels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Analysis of training object detection models with synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">33rd British Machine Vision Conference 2022, BMVC 2022,
London, November 21-24, 2022</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">. BMVA Press, 2022.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">to appear.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Bram Vanherle., Jeroen Put., Nick Michiels., and Frank Van Reeth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Real-time detection of 2d tool landmarks with synthetic training
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2nd International Conference on Robotics,
Computer Vision and Intelligent Systems - ROBOVIS,</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 40–47. INSTICC,
SciTePress, 2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Bram Vanherle, Jeroen Put, Nick Michiels, and Frank Van Reeth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Detecting tool keypoints with synthetic training data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In Péter Galambos, Erdal Kayacan, and Kurosh Madani, editors,
</span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Robotics, Computer Vision and Intelligent Systems</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 190–207,
Cham, 2022. Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Sebastien C. Wong, Adam Gatt, Victor Stamatescu, and Mark D. McDonnell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Understanding data augmentation for classification: When to warp?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 International Conference on Digital Image Computing:
Techniques and Applications (DICTA)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 1–6, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, and Zhi Jin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Improved relation classification by deep recurrent neural networks
with data augmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, abs/1601.03651, 2016.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Yi Yang and Deva Ramanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Articulated human detection with flexible mixtures of parts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">,
35(12):2878–2890, 2013.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.14053" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.14054" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.14054">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.14054" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.14056" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 09:05:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
