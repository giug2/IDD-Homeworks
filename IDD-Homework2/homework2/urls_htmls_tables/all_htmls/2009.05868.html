<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2009.05868] From Federated Learning to Federated Neural Architecture Search: A Survey</title><meta property="og:description" content="Federated learning is a recently proposed distributed machine learning paradigm for privacy preservation, which has found a wide range of applications where data privacy is of primary concern. Meanwhile, neural archite…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="From Federated Learning to Federated Neural Architecture Search: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="From Federated Learning to Federated Neural Architecture Search: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2009.05868">

<!--Generated on Sat Mar  2 09:25:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">From Federated Learning to Federated Neural Architecture Search: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hangyu Zhu 
<br class="ltx_break">Department of Computer Science
<br class="ltx_break">University of Surrey
<br class="ltx_break">Guildford, GU2 7XH, UK 
<br class="ltx_break"><span id="id7.1.id1" class="ltx_text ltx_font_typewriter">hangyu.zhu@surrey.ac.uk</span> 
<br class="ltx_break"><span id="id8.2.id2" class="ltx_ERROR undefined">\And</span>Haoyu Zhang 
<br class="ltx_break">College of Information Science and Technology
<br class="ltx_break">Donghua University
<br class="ltx_break">Shanghai, 201620, China 
<br class="ltx_break"><span id="id9.3.id3" class="ltx_text ltx_font_typewriter">zhy920816@sina.cn</span> 
<br class="ltx_break"><span id="id10.4.id4" class="ltx_ERROR undefined">\And</span>Yaochu Jin 
<br class="ltx_break">Department of Computer Science
<br class="ltx_break">University of Surrey
<br class="ltx_break">Guildford, GU2 7XH, UK 
<br class="ltx_break"><span id="id11.5.id5" class="ltx_text ltx_font_typewriter">yaochu.jin@surrey.ac.uk</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Corresponding Author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">Federated learning is a recently proposed distributed machine learning paradigm for privacy preservation, which has found a wide range of applications where data privacy is of primary concern. Meanwhile, neural architecture search has become very popular in deep learning for automatically tuning the architecture and hyperparameters of deep neural networks. While both federated learning and neural architecture search are faced with many open challenges, searching for optimized neural architectures in the federated learning framework is particularly demanding. This survey paper starts with a brief introduction to federated learning, including both horizontal, vertical, and hybrid federated learning. Then, neural architecture search approaches based on reinforcement learning, evolutionary algorithms and gradient-based are presented. This is followed by a description of federated neural architecture search that has recently been proposed, which is categorized into online and offline implementations, and single- and multi-objective search approaches. Finally, remaining open research questions are outlined and promising research topics are suggested.</p>
<p id="id6.6" class="ltx_p"><em id="id6.6.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="id6.6.2" class="ltx_text ltx_font_bold">eywords</span> Federated learning  <math id="id1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\cdot</annotation></semantics></math>
Deep learning  <math id="id2.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\cdot</annotation></semantics></math>
Privacy preservation  <math id="id3.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><ci id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\cdot</annotation></semantics></math>
Neural architecture search  <math id="id4.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="id4.4.m4.1a"><mo id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><ci id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">\cdot</annotation></semantics></math>
Reinforcement learning  <math id="id5.5.m5.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="id5.5.m5.1a"><mo id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><ci id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">\cdot</annotation></semantics></math>
Evolutionary algorithm  <math id="id6.6.m6.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="id6.6.m6.1a"><mo id="id6.6.m6.1.1" xref="id6.6.m6.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id6.6.m6.1b"><ci id="id6.6.m6.1.1.cmml" xref="id6.6.m6.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m6.1c">\cdot</annotation></semantics></math>
Real-time optimization</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Deep neural networks (DNNs) have made great success in the fields of image classification, natural language processing, autonomous driving systems, and many others. However, designing DNNs with high-quality architectures usually requires to manually try a large number of different hyperparameters, which is always a tedious task requiring broad expertise in both machine learning and the application area. Therefore, neural architecture search (NAS) has become increasingly popular in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which aims to automatically search for good neural architectures.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Conventional centralized learning systems, however, requires that all training data generated on different devices be uploaded to a server or cloud for training a global model, which may give rise to serious privacy concerns. To address this concern, federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> has been proposed to protect user’s data privacy by communicating the model parameters or other model information instead of the raw data between the server and local devices. Naturally, performing NAS in a federated learning environment becomes of particular importance, although it is still in its infant stage.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">This survey aims to provide an overview of research work both on federated learning and neural architecture search, focusing, however, on the emerging area of federated neural architecture search. We categorize federated learning systems into offline and online approaches, where online federated neural architecture search is more challenging due to additional requirements on the performance of the networks during the search process and stronger constraints on the computational resources. In addition, we briefly discuss the differences between single- and multi-objective search neural architecture search methods to highlight different ways of handling multiple objectives in federated learning, such as accuracy, communication costs, model complexity and memory requirements on the local devices. Finally, we outline the main remaining challenges in federated neural architecture search.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Federated Learning</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> distinguishes itself from distributed learning in three aspects. First, the main purpose of federated learning is to protect user’s private information while distributed learning aims to accelerate training speed. Second, federated learning cannot determine the data distribution of any client devices. By contrast, distributed learning is able to arbitrarily allocate subsets of the whole learning data. Finally, federated learning faces a more challenging training environment as it may contain millions of unbalanced participating clients whose connections to the server are probably unstable. For example, edge devices like mobile phones are frequently offline.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">Federated learning is often categorized based on the distribution characteristics of the data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> which is originally used in distributed learning. Strictly speaking, federated learning does not have the concept of ’the whole dataset’, therefore, it is hard to accurately describe the federated data distribution to some extent as defined in distribute learning. In the following, we will discuss the data distributions in federated learning in greater detail.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Horizontal Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Horizontal federated learning is proposed for the scenarios in which datasets on the participating clients share the same feature space but have different samples. The name ’horizontal’ originates from instance distributed learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> as shown in Fig. <a href="#S2.F1.fig1" title="Figure 1 ‣ 2.1 Horizontal Federated Learning ‣ 2 Federated Learning ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a), where the whole dataset is horizontally partitioned over data samples and allocated to two clients. Similarly, as indicated by the part surrounded by the two dashed lines in Fig. <a href="#S2.F1.fig1" title="Figure 1 ‣ 2.1 Horizontal Federated Learning ‣ 2 Federated Learning ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b), the data can be considered as horizontally partitioned in federated learning, when different data are generated on different clients that have the same attributes (features). For instance, two hospitals in different regions may have different patients, although they performed the same tests for each patients and collected the same personal information such as the name, age, gender and address.</p>
</div>
<figure id="S2.F1.fig1" class="ltx_figure ltx_minipage ltx_align_top" style="width:433.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F1.sf1" class="ltx_figure ltx_figure_panel">
<div id="S2.F1.sf1.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:199.5pt;">
<img src="/html/2009.05868/assets/x1.png" id="S2.F1.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="176" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Instance distributed learning</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S2.F1.sf2.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:199.5pt;">
<img src="/html/2009.05868/assets/x2.png" id="S2.F1.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="278" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Horizontal federated learning</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Instance distributed learning (a) and horizontal federated learning (b).</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">There are two main differences between instance distributed learning and horizontal federated learning. First, data is typically independent and identically distributed (IID) in distributed learning but may be non-IID in horizontal federated learning. As mentioned before, distributed learning is mainly designed for reducing the training time, therefore, designers can manually allocate every subsets of the client data to be IID to enhance the convergence. However, in horizontal federated learning, the central server has no access to any raw data, which are usually non-IID on different clients. Second, global model update mechanisms are different. Instance distributed learning, such as multi-GPU training a deep neural network, does not concern too much about the communication costs between the server and the clients. Once the gradients of mini-batch data on each client are calculated, they can be uploaded immediately to update the global model parameters on the server. This global model updating approach is intrinsically not suited for horizontal federated learning because frequent upload and download of data are not desirable due to the constraints on the communication costs.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2009.05868/assets/x3.png" id="S2.F2.g1" class="ltx_graphics ltx_img_square" width="242" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Flowchart of federated learning. <math id="S2.F2.6.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.F2.6.m1.1b"><mi id="S2.F2.6.m1.1.1" xref="S2.F2.6.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.F2.6.m1.1c"><ci id="S2.F2.6.m1.1.1.cmml" xref="S2.F2.6.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m1.1d">\theta</annotation></semantics></math> is the global model parameters, <math id="S2.F2.7.m2.1" class="ltx_Math" alttext="{n_{k}}" display="inline"><semantics id="S2.F2.7.m2.1b"><msub id="S2.F2.7.m2.1.1" xref="S2.F2.7.m2.1.1.cmml"><mi id="S2.F2.7.m2.1.1.2" xref="S2.F2.7.m2.1.1.2.cmml">n</mi><mi id="S2.F2.7.m2.1.1.3" xref="S2.F2.7.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.7.m2.1c"><apply id="S2.F2.7.m2.1.1.cmml" xref="S2.F2.7.m2.1.1"><csymbol cd="ambiguous" id="S2.F2.7.m2.1.1.1.cmml" xref="S2.F2.7.m2.1.1">subscript</csymbol><ci id="S2.F2.7.m2.1.1.2.cmml" xref="S2.F2.7.m2.1.1.2">𝑛</ci><ci id="S2.F2.7.m2.1.1.3.cmml" xref="S2.F2.7.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.7.m2.1d">{n_{k}}</annotation></semantics></math> is the data size of client <math id="S2.F2.8.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.F2.8.m3.1b"><mi id="S2.F2.8.m3.1.1" xref="S2.F2.8.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.F2.8.m3.1c"><ci id="S2.F2.8.m3.1.1.cmml" xref="S2.F2.8.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.8.m3.1d">k</annotation></semantics></math>, <math id="S2.F2.9.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.F2.9.m4.1b"><mi id="S2.F2.9.m4.1.1" xref="S2.F2.9.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.F2.9.m4.1c"><ci id="S2.F2.9.m4.1.1.cmml" xref="S2.F2.9.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.9.m4.1d">K</annotation></semantics></math> is the total number of clients and <math id="S2.F2.10.m5.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.F2.10.m5.1b"><mi id="S2.F2.10.m5.1.1" xref="S2.F2.10.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.F2.10.m5.1c"><ci id="S2.F2.10.m5.1.1.cmml" xref="S2.F2.10.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.10.m5.1d">t</annotation></semantics></math> is the communication round in federated learning. We just initialize global model parameters randomly at the beginning of the communication round and use updated model parameters afterwards</figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">Typical horizontal federated learning (Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.1 Horizontal Federated Learning ‣ 2 Federated Learning ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) algorithms, such as the FedAvg proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, consist of the following main steps.</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Initialize the global model parameters on the server and download the global model to every participating (connected) clients.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Every connected clients learn the downloaded global model on its own data for several training epochs. Once completed, the updated model parameters or gradients (gradients here means the difference between the downloaded model and updated model) are uploaded to the server.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">The server aggregates all the upload the local models to update the global model.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i4.p1.1" class="ltx_p">Repeat the above two steps until convergence.</p>
</div>
</li>
</ol>
<p id="S2.SS1.p3.2" class="ltx_p">From the above steps we can find that the central server can only receive model weights or gradients of the participating clients and has no access to any local raw data. Therefore, users’ privacy is immensely protected in horizontal federated learning.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p">Horizontal federated learning has three additional main challenges compared to the traditional centralized learning: 1) it must reduce the communication resources as much as possible, 2) it needs to improve the convergence speed, and 3) it must make sure that no private information is leaked in passing the model information. Much research work has focused on reducing communication costs, such as client updates sub-sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and model quantization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. More recently, Chen <em id="S2.SS1.p4.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> propose a layer-wise asynchronous update algorithm to reduce the communication costs by decreasing the update frequency of the deep layers in the neural network. In addition, Zhu <em id="S2.SS1.p4.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> use a multi-objective evolutionary algorithm (MOEA) to simultaneously enhance the model performance and communication efficiency. Learning a good model in horizontal federated learning is not an easy task, since the training data on different clients are usually non-IID, leading to possible model divergence. In order to solve this issue, Zhao <em id="S2.SS1.p4.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> empirically explore the effect of non-IID data and provide a statistical analysis of divergence. Li <em id="S2.SS1.p4.1.4" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> propose a FedProx algorithm to alleviate negative impacts of the system’s heterogeneity by injecting a proximal term into the original loss on each client. Apart from it, an attentive aggregation method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> is used to minimize the weighted distance between the server model and client models on non-IID datasets.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.p5.2" class="ltx_p">The central server is often regarded as honest but curious (follow federated learning protocol but try to infer client data information) in horizontal federated learning, and the revealed gradients of each client may potentially leak the data information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
For this reason, Phong <em id="S2.SS1.p5.2.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> mathematically prove that model gradients (especially the first hidden weights) are proportional to the original data and adopt additive homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to encrypt and protect model gradients. In their method, the secret key is kept confidential to the server but known to all participating clients and the central server can easily get the plain model gradients as long as one of connected clients uploads its secret key. In order to mitigate this issue, secure multiparty computation (SMC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is proposed to partition an intact secret key into several key shards and each client can just hold one shard. As a result, the server must get at least <math id="S2.SS1.p5.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p5.1.m1.1a"><mi id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.1b"><ci id="S2.SS1.p5.1.m1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.1.m1.1c">t</annotation></semantics></math> shards (<math id="S2.SS1.p5.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p5.2.m2.1a"><mi id="S2.SS1.p5.2.m2.1.1" xref="S2.SS1.p5.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.2.m2.1b"><ci id="S2.SS1.p5.2.m2.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.2.m2.1c">t</annotation></semantics></math> is the threshold value) for decryption. Consequently, privacy preserving is significantly improved.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para ltx_noindent">
<p id="S2.SS1.p6.1" class="ltx_p">However, homomorphic encryption will increase the computation load, and SMC consumes much more communication resources, since encrypted model weights need to be downloaded and uploaded between the server and at least <math id="S2.SS1.p6.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p6.1.m1.1a"><mi id="S2.SS1.p6.1.m1.1.1" xref="S2.SS1.p6.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.1.m1.1b"><ci id="S2.SS1.p6.1.m1.1.1.cmml" xref="S2.SS1.p6.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.1.m1.1c">t</annotation></semantics></math> clients for partial decryption. Therefore, a more light-weight privacy preserving technique called differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> can also be used in horizontal federated learning. Such as the methods used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, a Gaussian or Laplacian noise is added to the gradients of each client before sending them to the central server. Note, however, that the learning process may be interrupted if the accountant <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> exceeds a pre-defined threshold value. Most recently, Truex <em id="S2.SS1.p6.1.1" class="ltx_emph ltx_font_italic">et al.</em> suggest a hybrid approach that combines differential privacy together with homomorphic encryption.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Vertical Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">In contrast to horizontal federated learning, vertical federated learning is applicable to the situations where the datasets share the same sample space but have different feature space, as shown by part of surrounded by the dashed lines in Fig. <a href="#S2.F3.fig1" title="Figure 3 ‣ 2.2 Vertical Federated Learning ‣ 2 Federated Learning ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b). For example, two different financial agents may have the same customers but provide different services. Different from horizontal federated learning, vertical federated learning is similar to feature distributed learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> to some extent which ’vertically’ partitions the training data, as shown in Fig. <a href="#S2.F3.fig1" title="Figure 3 ‣ 2.2 Vertical Federated Learning ‣ 2 Federated Learning ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a) upon the feature space. Moreover, the central server is often called a coordinator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> in feature distributed learning or vertical federated learning, since its main task is to calculate the total loss rather than aggregating the uploaded weights.</p>
</div>
<figure id="S2.F3.fig1" class="ltx_figure ltx_minipage ltx_align_top" style="width:433.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.sf1" class="ltx_figure ltx_figure_panel">
<div id="S2.F3.sf1.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:199.5pt;">
<img src="/html/2009.05868/assets/x4.png" id="S2.F3.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="199" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Instance distributed learning</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S2.F3.sf2.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:199.5pt;">
<img src="/html/2009.05868/assets/x5.png" id="S2.F3.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="282" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Horizontal federated learning</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Feature distributed learning (a) and vertical federated learning (b).</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">Vertical federated learning is firstly introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, in which the overall framework contains one trusted coordinator and two parties, where each party represents one client. The coordinator computes the training loss and generates encryption key pairs. Homomorphic encryption is adopted for the privacy preserving purpose and the effect of entity resolution is also discussed. More recently, a two-party architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is proposed by removing the trusted coordinator which greatly reduces the complexity of the system. A typical two-party framework of vertical federated learning using a simple logistic regression model includes the following steps:</p>
<ol id="S2.I2" class="ltx_enumerate">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p">Assume Party A contains the data labels. Party A creates a homomorphic encryption key pair and sends the public key to Party B. Both parties initialize their local model parameters according to their feature dimensions of local training data.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p">Both parties compute their local inner products of data and the model. Then, Party B sends its results to Party A.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p">Party A sums two inner products and calculates the loss function by data labels. The loss is encrypted with a public key and is sent to Party B. The model gradients of Party A are also calculated.</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I2.i4.p1" class="ltx_para ltx_noindent">
<p id="S2.I2.i4.p1.1" class="ltx_p">Party B calculates the encrypted model gradients from the received loss and encrypt. In addition, a random number is encrypted and added to the encrypted gradients. The summation should be sent to Party A for decryption.</p>
</div>
</li>
<li id="S2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S2.I2.i5.p1" class="ltx_para">
<p id="S2.I2.i5.p1.1" class="ltx_p">Party A uses a secret key to decrypt the summation value and sends it to Party B.</p>
</div>
</li>
<li id="S2.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S2.I2.i6.p1" class="ltx_para">
<p id="S2.I2.i6.p1.1" class="ltx_p">Update both model parameters of the two parties.</p>
</div>
</li>
<li id="S2.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S2.I2.i7.p1" class="ltx_para ltx_noindent">
<p id="S2.I2.i7.p1.1" class="ltx_p">Repeat Step 2 to Step 6 until convergence.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p">In Step 3, the training loss is encrypted before being sent to Party B, because it contains the information of the data labels in Party A which cannot be revealed to Party B. As a result, Party B needs to calculate its local model gradients on the encrypted loss and a Taylor approximation is commonly used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> for simplifying this computation.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.1" class="ltx_p">From the above discussions, we can see that vertical federated learning is dramatically different from horizontal federated learning. The central server in horizontal federated learning is used for model aggregation, while in vertical federated learning the server plays the role of calculating the loss or collecting features. In addition, the server can be removed in vertical federated learning, e.g., summing the training loss within one of participating parties (clients). Apart from the above, we often assume that not all parties contain the data labels in vertical federated learning, e.g., only Client B contains data labels in Fig. <a href="#S2.F3.fig1" title="Figure 3 ‣ 2.2 Vertical Federated Learning ‣ 2 Federated Learning ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b) and those parties with no data labels are not able to update their models locally. Therefore, we call the server ’coordinator’ that coordinates the feature predictions from all parties for calculating the training loss in vertical federated learning.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para ltx_noindent">
<p id="S2.SS2.p5.1" class="ltx_p">Most studies of vertical federated learning only support two parties (with or without a central coordinator) using a simple binary logistic regression model. Feng <em id="S2.SS2.p5.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> adopt the idea of multi-view learning to extend the previous scheme into a multi-participant multi-class vertical federated learning framework. Besides, Liu <em id="S2.SS2.p5.1.2" class="ltx_emph ltx_font_italic">et al.</em> introduce a federated stochastic block coordinate descent algorithm, where all participating parties update their local models for multiple times to reduce the total number of communication rounds. In addition, Chen <em id="S2.SS2.p5.1.3" class="ltx_emph ltx_font_italic">et al.</em> propose an asynchronous vertical federated learning method and differential privacy is also used for privacy protection.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Hybrid Federated Learning</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">Hybrid federated learning is more realistic in the real-world and it assumes that datasets on different clients not only have different sample spaces but also different feature spaces. Therefore, in this scenario, different parties need to share the data identity (ID) information to find the intersection part for distributed training, which is a threat to local clients’ privacy. Since participants in hybrid federated learning are often asymmetric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, for instance, some participants are small companies always requiring to protect their ID information, while some participants are large companies that have no concern about the ID privacy. Symmetric federated learning and asymmetric federated learning are illustrated in Fig. <a href="#S2.F4.fig1" title="Figure 4 ‣ 2.3 Hybrid Federated Learning ‣ 2 Federated Learning ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S2.F4.fig1" class="ltx_figure ltx_minipage ltx_align_top" style="width:433.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F4.sf1" class="ltx_figure ltx_figure_panel">
<div id="S2.F4.sf1.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:199.5pt;">
<img src="/html/2009.05868/assets/x6.png" id="S2.F4.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="462" height="280" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Symmetric federated learning</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S2.F4.sf2.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:199.5pt;">
<img src="/html/2009.05868/assets/x7.png" id="S2.F4.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="284" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Asymmetric federated learning</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>a) Symmetric federated learning, and b) Asymmetric federated learning.</figcaption>
</figure>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">Secure ID alignment protocol is significant for hybrid federated learning, such as the commonly used Private Set Intersection (PSI) protocol. In standard PSI, all participants want to collaboratively find the intersection (the part indicated by the dashed lines in Fig. <a href="#S2.F4.fig1" title="Figure 4 ‣ 2.3 Hybrid Federated Learning ‣ 2 Federated Learning ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) and keep unintersected parts private. The PSI protocols can be implemented by a classical public-key cryptosystem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> or other similar techniques.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p">Federated model training is similar to vertical federated learning, however, for asymmetric federated learning, Genuine with Dummy (GWD) approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is used to ensure the correctness of computation results.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Neural Architecture Search</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">Since the quality of deep neural networks (DNNs) heavily depends on their architecture, increasing research efforts have been committed to design of novel structures in the deep learning community. However, manually designing deep neural networks requires considerable expertise in the field of deep learning and the investigated problem, which is unrealistic for many interested users. Not until recently has automated machine learning (Auto ML), in particular neural architecture search (NAS) become very popular to allow interested users without adequate domain knowledge to profit from the success of deep neural networks. The framework of NAS methods involves three dimensions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, namely search space, search strategies, and performance estimation strategies.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">The search space is a collection of network architectures, which has a major influence on the performance of the generated networks and search efficiency. The search strategy defines the method used to automatically design the optimal network architecture. To be specific, these search strategies can be divided into at least three categories reinforcement learning (RL), evolutionary algorithms (EAs), and gradient-based (GD) methods. In addition, a few additional methods, such as random search <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, Bayesian optimization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and multinomial distribution learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, fall outside of the above categories. The search strategy aims at finding architectures that can obtain high performance on the test dataset. To guide searches effectively, these strategies utilize a performance estimation strategy to evaluate the quality of candidate architectures. Early work uses a simple way of performance estimation, for example, by iteratively training a candidate architecture on the training dataset with the stochastic gradient descent (SGD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and evaluating its performance on the validation data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Such an evaluation strategy typically results in a prohibitively high computational cost. For example, to design a good performance of neural network, the automatic evolving convolutional neural network (AE-CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> algorithm consumes 27 GPU-days and the neural architecture search approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> consumes 22400 GPU-days on the CIFAR10 dataset. Because inefficient search strategies require a large number of GPUs, many NAS methods cannot be implemented given limited computational resources. To address these challenges, much recent work dedicates to developing effective methods which can reduce the computational costs of performance evaluation, e.g., surrogate-assisted evolutionary algorithms (SAEAs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, information reuse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, one-shot neural architecture search <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, among many others.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2009.05868/assets/x8.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="277" height="227" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration of the marco and micro search spaces.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>NAS based on Reinforcement Learning</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">Early work on NAS depends on RL to search for high-performance neural architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. The design of a network model is considered as an agent’s action, which specifies the architecture of the network (i.e., a child model). The network is then trained and its performance on the validation data is returned as the agent’s reward.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.4" class="ltx_p">A policy gradient method has attempted to approximate some nondifferentiable reward function to train a model which needs parameter gradients. Zoph <em id="S3.SS1.p2.4.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> first adopt this algorithm in NAS to train a recurrent neural network (RNN) model that generates architectures. As is shown in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.1 NAS based on Reinforcement Learning ‣ 3 Neural Architecture Search ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the controller as a navigating tool to find more promising architectures in the search space. The original method in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> uses a macro search space that generates the entire network at once. As is shown in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3 Neural Architecture Search ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the whole architecture consists of <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">n</annotation></semantics></math> sequential layers where the dashed lines indicate skip connections. Hence, the macro search space aims to design the entire CNN architecture in terms of the number of hidden layers <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">n</annotation></semantics></math>, operations types (e.g. convolution), network hyper parameters (e.g., the convolutional kernel size), and the link methods (e.g. skip connections). However, this method is expensive when the data set is large. To reduce the computational cost, Zoph <em id="S3.SS1.p2.4.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> propose a more structured search space, called micro search space. The micro search space only covers repeated smaller modules, called normal cell and reduction cell, and then connects them together to form an entire network. As shown in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3 Neural Architecture Search ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, these cells are built in complex multi-branch operations (e.g. convolution). Each cell structure contains two inputs <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="h[i-1]" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">​</mo><mrow id="S3.SS1.p2.3.m3.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.2.1.cmml">[</mo><mrow id="S3.SS1.p2.3.m3.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.2.cmml">i</mi><mo id="S3.SS1.p2.3.m3.1.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml">−</mo><mn id="S3.SS1.p2.3.m3.1.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS1.p2.3.m3.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><times id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2"></times><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">ℎ</ci><apply id="S3.SS1.p2.3.m3.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p2.3.m3.1.1.1.2.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1"><minus id="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1"></minus><ci id="S3.SS1.p2.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.2">𝑖</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">h[i-1]</annotation></semantics></math> and <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="h[i-2]" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">​</mo><mrow id="S3.SS1.p2.4.m4.1.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.4.m4.1.1.1.1.2" xref="S3.SS1.p2.4.m4.1.1.1.2.1.cmml">[</mo><mrow id="S3.SS1.p2.4.m4.1.1.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.1.1.1.2" xref="S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml">i</mi><mo id="S3.SS1.p2.4.m4.1.1.1.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml">−</mo><mn id="S3.SS1.p2.4.m4.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml">2</mn></mrow><mo stretchy="false" id="S3.SS1.p2.4.m4.1.1.1.1.3" xref="S3.SS1.p2.4.m4.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><times id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2"></times><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">ℎ</ci><apply id="S3.SS1.p2.4.m4.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p2.4.m4.1.1.1.2.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1"><minus id="S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.1"></minus><ci id="S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.2">𝑖</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">h[i-2]</annotation></semantics></math> coming from two previous layers. Hence, the micro search space aims to design structures of these two types of cells. In addition, the cell structures should have a good capability of generalizing to other related tasks. For example, the proposed method searches for optimal cell structures on the CIFAR10 data set and transfers them to the ImageNet data set by stacking together multiple copies of this cell. After that, the NASNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> method is extended to a multi-objective optimization variant to simultaneously optimize the classification performance and computational cost using different scalarization parameters.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2009.05868/assets/x9.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="424" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>An overview of RL-based NAS method.</figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">Q-learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, as a class of popular RL methods, has been widely used for NAS. Baker <em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> employ an <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\epsilon</annotation></semantics></math>-greedy Q-learning strategy to train a policy that sequentially chooses a type of layers (e.g. convolutional layer, pooling layer, and fully connected layer) and their corresponding hyperparameters. Zhong <em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> extend this method to a block-wise network generation approach, which designs blocks with the same Q-learning paradigm. After that, the optimal blocks are repeated and stacked to construct the entire network architecture. To accelerate the search process, a distributed asynchronous strategy and an early-stop approach are adopted.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p">Parameter sharing introduced in efficient NAS (ENAS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> is a promising approach for speeding up the search process for RL-based NAS methods, which treats architectures as different sub-graphs (sub-net) of a larger graph (super-net) and forces all sub-graphs to share a common set of weights that have edges of this larger graph in common. Pasunuru <em id="S3.SS1.p4.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> propose a multi-task architecture search (MAS) approach based on ENAS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> for finding a cell structure that performs well across multiple tasks. Hence, the cell structure generated by NAS can transfer to a new task. Bender <em id="S3.SS1.p4.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> propose a thorough comparison between random search NAS methods and ENAS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> on a larger search spaces for image detection and classification tasks, respectively. In addition, a new reward function is suggested, which can effectively improve the quality of the generated networks and reduce the difficulty for manual hyperparameter tuning. Liu <em id="S3.SS1.p4.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> present a novel knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> approach to NAS, called architecture-aware knowledge distillation (AKD), which finds student models (compressed teacher models) that are best for distilling the given teacher model. The authors employ a RL-based NAS method with a KD-guided reward function to search for the best student model based on a given teacher model.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>NAS based on EAs</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">EAs are a class of population-based, gradient-free heuristic search paradigms, which have been widely used in solving various complex optimization problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Historically, EAs have already been used for simultaneous optimization of the topology, weights of the connections and hyperparameters of artificial neural networks (ANNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. The neuroevolution with augmenting typologies (NEAT) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> is one of the popular early methods that have shown powerful performance. However, the traditional approaches are not well suited for optimizing DNNs due to the complex network architectures and large quantities of connection weights. EA-based NAS approaches to optimizing deep network architectures have started gaining momentum again recently <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, mainly because they can simultaneously explore multiple areas of the search space and their relative insensitiveness to a local minimum <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.2 NAS based on EAs ‣ 3 Neural Architecture Search ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows a generic framework of EA-based NAS algorithms. Broadly speaking, the whole process of an EA-based NAS algorithm follows the procedure of an EA containing at least four-steps: population initialization, offspring generation, fitness evaluation, and environmental selection. Generally, each neural network in the search space is encoded as a chromosome, and crossover and mutation operations of the chromosomes are performed in the exploration. Then, each chromosomes is transformed into a corresponding neural network, and iteratively trained on the training dataset. The trained network is evaluated on the validation dataset to get their fitness value.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2009.05868/assets/x10.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="424" height="283" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>A generic EA-based NAS framework.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Xie <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> proposed a genetic CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> method that is one of the early studies using an EA for optimizing convolutional neural networks (CNNs). The genetic CNN algorithm searches over the entire architecture space and employs a fixed-length binary string to represent the connection between a number of ordered nodes (e.g. a convolutional operation). Although this early algorithm has some limitations, including a limited number of nodes as well as limited sizes and operations of convolutional filters, the generated structures have not only achieved competitive results on the CIFAR and SVHN datasets, but also shown excellent transferability to the ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">Miikkulainen <em id="S3.SS2.p3.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> propose a coevolution DeepNEAT (CoDeepNEAT) method by extending the NEAT algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> to DNNs. In CoDeepNEAT, each neural network is assembled by modules and blueprints. A coevolutionary method is adopted that evolves two populations of modules and blueprints separately, in which each module chromosome represents a small DNN. The blueprints chromosome represents a graph where each node contains a pointer to a particular module species. The assembled networks are trained and evaluated in an ordinary way of NAS. The fitness of the network is the average fitness of the entire candidate models containing the blueprints or modules. In addition, Liang <em id="S3.SS2.p3.1.2" class="ltx_emph ltx_font_italic">et al.</em> found that the CoDeepNEAT also achieves promising performance in the Omniglot multi-task learning domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p">In fact, the length of a chromosome usually represents the depth of the corresponding neural network and a fixed encoding scheme may limit the performance of the optimized network. To address this issue, some recent NAS algorithms have attempted to use a variable-length encoding scheme. Real <em id="S3.SS2.p4.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> propose a large-scale evolutionary NAS method, which utilizes a variable-length encoding method in which the network architectures can adaptively change their depths. Sun <em id="S3.SS2.p4.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> propose an AE-CNN algorithm that can fully automatically design CNN architectures, without requiring any pre-processing or post-processing. Inspired by the ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> and DenseNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, AE-CNN’s search space is defined by some predetermined building blocks, including ResNet block and DenseNet block, max pooling layer and mean pooling layer. Then, the authors design an EA-based NAS framework, including the variable-length encoding and a novel crossover and mutation operators based on the variable-length encoding, as the search strategy to search the optimal depth of the CNN. Given the nature of the variable-length encoding strategy, the algorithm employs a repair mechanism that avoids to produce invalid CNNs. Inspired by directed acyclic graph (DAG), William <em id="S3.SS2.p4.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> introduce a DAG-based encoding strategy, which can represent CNNs of an arbitrary connection structure and an unlimited depth.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p">Suganuma <em id="S3.SS2.p5.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> propose a CGP-CNN algorithm to design CNN architectures using genetic programming. The search space of CGP-CNN is represented by a DAG, where the nodes represent either convolutional blocks or concantenation operations. Then, CGP-CNN uses the Cartesian genetic programming (CGP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> encoding scheme to represent network architectures and their connectivity. This encoding scheme can represent variable-length network architectures and skip connections.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p">Most EA-based NAS methods aim at finding better topologies for DNNs while leaving the learning of weights to SGD. It is known that the SGD optimizer heavily rely on the initial values of the weights. To alleviate this problem, Sun <em id="S3.SS2.p6.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> propose an EA-based NAS method, named Evolving Deep CNNs (EvoCNN), to automatically design CNN architectures and corresponding connection weight initialization values without manual intervention. To reduce the search space, two statistical measures, including the mean and standard deviation of the connection weights, are encoded in the chromosome to represent tremendous numbers of the connection weights. In addition, the incomplete training scheme is employed to accelerate the fitness evaluation. According to the Occam’s razor theory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, the number of connection weights are also considered as an indicator to scale the quality of candidate networks.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.1" class="ltx_p">Sun <em id="S3.SS2.p7.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> use a genetic algorithm (GA) to design CNN architectures (CNN-GA). In CNN-GA, the standard convolutional layer is replaced by a novel building block, called the skip layer. The skip layer consists of two convolutional layers and one skip connection. Hence, the genotype encodes information of the skip layers and pooling layers. The fully connected layers are discarded, mainly because they easily result in the overfitting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para ltx_noindent">
<p id="S3.SS2.p8.1" class="ltx_p">Rather than generating the entire CNNs, the micro search space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> has also been successfully employed by many recent EA-based NAS algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>. Real <em id="S3.SS2.p8.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> propose an extension of the large-scale evolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, called AmoebaNet, which has achieved better results on ImageNet compared with hand-designed methods for the first time.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para ltx_noindent">
<p id="S3.SS2.p9.1" class="ltx_p">Since EAs are a class of population-based search methods, the main computational bottleneck of EA-based NAS approaches lies in evaluating the fitness of the individuals by invoking the lower-level weight optimization. One such evaluation typically takes several hours to days if the network is large and if the training dataset is huge. For instance, on the CIFAR10 datasets, the AE-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> consumed 27 GPU days, CNN-GA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> consumed 35 GPU days, and the large-scale evolutionary algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> consumed 2750 GPU days, AmoebaNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> consumed 3150 GPU days. This seriously limits the practical usability of most evolutionary NAS methods under a constrained search budget.</p>
</div>
<div id="S3.SS2.p10" class="ltx_para ltx_noindent">
<p id="S3.SS2.p10.1" class="ltx_p">Therefore, various techniques have been suggested to accelerate the fitness evaluation, such as information reuse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and SAEAs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>. SAEAs have been popular to solve computationally expensive optimization problems, which use cheap classification and regression models, e.g., radial basis function networks (RBFNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> and Gaussian process (GP) models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, to replace the expensive fitness evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>. Generally, the candidate networks are trained from a few number of expensive fitness evaluations, and then the trained networks are used to build a fitness predictors to reduce the cost of fitness evaluations. In the area of evolutionary NAS, Swersky <em id="S3.SS2.p10.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> adopt Bayesian optimization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> to speed up evolutionary optimization, which is called Freeze-thaw Bayesian optimization. Unfortunately, this algorithm is based on Markov chain Monte Carlo sampling and also suffers from high computational complexity. Sun <em id="S3.SS2.p10.1.2" class="ltx_emph ltx_font_italic">et al.</em> proposed a performance predictor termed E2EPP, which is based on a class of SAEAs method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> meant for offline data-driven evolutionary optimization of expensive engineering problems. Specifically, E2EPP builds a surrogate that can predict the quality of a candidate CNN, thereby avoiding the training of a large number of neural networks during the search process. Compared with AE-CNN, a variant of AE-CNN assisted by E2EPP (called AE-CNN+E2EPP) can reduce 230% and 214% GPU days on CIFAR100 and CIFAR10, respectively.</p>
</div>
<div id="S3.SS2.p11" class="ltx_para ltx_noindent">
<p id="S3.SS2.p11.1" class="ltx_p">Knowledge inheritance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> is another promising approach to accelerate fitness evaluations. Zhang <em id="S3.SS2.p11.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> propose an EA based on asexual reproduction to find better typologies for deep CNNs and knowledge inheritance to reduce the computation cost. Once the topology of an offspring individual is generated by its parent, the weights of offspring networks are directly copied from its parents. For edges that do not appear in its parent network, the weights are randomly initialized.</p>
</div>
<div id="S3.SS2.p12" class="ltx_para ltx_noindent">
<p id="S3.SS2.p12.1" class="ltx_p">To reduce the computational burden for fitness evaluations, another widely adopted approaches are to train and evaluate individuals using proxy metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>. The performance of the proxy models is used as the surrogate measurements to guide the evolutionary search. Such proxy metrics include reducing the width (the number of channels) and the depth (number of layers) for the intended network architecture to create a small-scale network, shortening the training time, reducing the resolution of input images, and training on a subset of the full training dataset. However, these simple proxy model constructing methods may result in a low correlation in prediction mainly because they may introduce biases in fitness estimation. Zhou <em id="S3.SS2.p12.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> have conducted extensive experiments on different combinations of proxy metrics to investigate their behaviors in maintaining the rank consistency in NAS, based on which a reliable hierarchical proxy strategy is proposed to accomplish economical neural architecture search (EcoNAS). The hierarchical proxy strategy aims at discarding less promising candidate individuals earlier with a fast proxy and estimates more promising individuals with a more expensive proxy. Hence, the EcoNAS method is able to achieve a <math id="S3.SS2.p12.1.m1.1" class="ltx_math_unparsed" alttext="400\times" display="inline"><semantics id="S3.SS2.p12.1.m1.1a"><mrow id="S3.SS2.p12.1.m1.1b"><mn id="S3.SS2.p12.1.m1.1.1">400</mn><mo lspace="0.222em" id="S3.SS2.p12.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p12.1.m1.1c">400\times</annotation></semantics></math> reduced search time in comparison to AmoebaNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> without sacrificing the performance. Lu <em id="S3.SS2.p12.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> empirically establish the trade-off between the correlation of proxy performance to true performance and the speed-up in estimation.</p>
</div>
<div id="S3.SS2.p13" class="ltx_para ltx_noindent">
<p id="S3.SS2.p13.1" class="ltx_p">Evolutionary multi-objective NAS methods considering multiple conflicting objectives have been reported. One of the earliest evolutionary multi-objective methods to design CNNs is NEMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>, which simultaneously optimizes classification performance and inference time of a network based on NSGA-II <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. Inspired by NEMO, Lu <em id="S3.SS2.p13.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> consider classification error and computational complexity as the two objectives. In addition, they empirically test multiple computational complexity metrics to measure the inference time of a network containing the number of active layers, the number of activating connections between layers, and the number of floating-point operations (FLOPs). And then, the FLOPs are used as a second conflicting objective for optimization. Moreover, a Bayesian network (BN) is adopted to learn the knowledge about promising architectures present in the search history and then guide the future exploitation in generating new architectures. Subsequently, Lu <em id="S3.SS2.p13.1.2" class="ltx_emph ltx_font_italic">et al.</em> suggest NSGANet-v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>, an extension of NSGANet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>, where a comprehensive search space including more layer operations and one more option that controls the width of the model is introduced. Dong <em id="S3.SS2.p13.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> present a DPP-Net on the basis of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> that optimizes both GPU memory usage and the model performance. Elsken <em id="S3.SS2.p13.1.4" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> proposed the LEMONADE method, which formulates the NAS as a bi-objective optimization problem that maximizes the performance and minimizes the required computational resources. Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, LEMONADE reduces computational cost through a custom-designed approximate network morphisms, which makes offspring individuals to share weights with their forerunners, avoiding training new networks from scratch. Note that evolutionary multi-objective structure optimization of shallow networks can be traced back to a decade ago <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>NAS based on GD</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Compared with the above gradient-free optimization methods, the GD-based methods (Fig. <a href="#S3.F8" title="Figure 8 ‣ 3.3 NAS based on GD ‣ 3 Neural Architecture Search ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>) have become increasingly popular recently, mainly because their search speed is much faster than RL-based and EA-based methods. Early GD-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> implement this idea for optimizing layer hyperparameters or connectivity patterns, respectively. Lorraine <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> introduce an algorithm for inexpensive GD-based hyperparameter optimization. Liu <em id="S3.SS3.p1.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> employ GD in the DARTS algorithm, which optimizes both the network weights and the architecture. The authors use relaxation tricks to make a weighted sum of candidate operations differentiable, and then apply the gradient descent method to directly train the weights. Inspired by DARTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, Dong <em id="S3.SS3.p1.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> introduce gradient-based search using the differentiable architecture sampler (GDAS) method. The authors develop a differentiable architecture sampler which samples individual architectures in a differentiable way to accelerate the architecture search procedure. Stochastic NAS (SNAS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> optimizes a probability distribution of the connections between different candidate operations. Li <em id="S3.SS3.p1.1.4" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> observe that models with a higher performance during the search phase may perform worse in the evaluation. Hence, they divided the search process into sub-problems and proposed sequential greedy architecture search (SGAS) based on DARTS, which chooses and prunes candidate operations (e.g. convolutional layers) greedily. The authors apply SGAS for CNNs and graph convolutional networks (GCNs) and have achieved competitive performance. Xu <em id="S3.SS3.p1.1.5" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> present Partially-Connected DARTS (PC-DARTS), which samples a small part of super-network to reduce the redundancy in exploring the network space. Compared with DARTS, PC-DARTS not only enjoys both faster speed and higher training stability but also a highly competitive learning performance. Gao <em id="S3.SS3.p1.1.6" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> propose the first GD-based NAS method in generative adversarial networks (GANs), called adversarial neural architecture search (AdversarialNAS), which can search the architectures of generator and discriminator simultaneously in a differentiable manner.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2009.05868/assets/x11.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="424" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>A generic pipeline GD-based NAS method.</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">One bottleneck of the above GD-based NAS methods (e.g. DARTs) is that it requires excessive GPU memory during search in that all candidate network layers must be explicitly instantiated in the GPU memory. As a result, the size of the search space is constrained. To address this issue, Wan <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> propose DMaskingNAS, a memory and computationally efficient DARTS variant. DMaskingNAS employs a masking mechanism for feature map reuse. Hence, although the search space of DMaskingNAS is expanded up to <math id="S3.SS3.p2.1.m1.1" class="ltx_math_unparsed" alttext="10^{14}\times" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1b"><msup id="S3.SS3.p2.1.m1.1.1"><mn id="S3.SS3.p2.1.m1.1.1.2">10</mn><mn id="S3.SS3.p2.1.m1.1.1.3">14</mn></msup><mo lspace="0.222em" id="S3.SS3.p2.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">10^{14}\times</annotation></semantics></math> over conventional DARTS, memory and computational costs stay nearly constant.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">Another way to address the above problem is to utilize proxy tasks, e.g., learning with only a small number of building blocks or training for a small number of epochs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>. However, these approaches cannot guarantee to be optimal on the target task due to the restricted block diversity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>. Cai <em id="S3.SS3.p3.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> proposed ProxylessNAS method, which directly designs the networks based on the target task and hardware instead of with proxy. Meanwhile, the authors used path binarization to reduced the computational cost (GPU-hours and GPU memory) of NAS to the same level of normal training. Hence, ProxylessNAS algorithm can generate network architectures on the ImageNet dateset without any proxy.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p">Most recently GD-based NAS methods are formulated as bilevel optimization problems, However, He <em id="S3.SS3.p4.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> observe that bilevel optimization in the current methods is solved based on a heuristic. For instance, solution of the problem needs to get an approximation of the second-order methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>. He <em id="S3.SS3.p4.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> demonstrate that the approximation has a superposition influence mainly because it is based on a one-step approximation of the network weights. As a result, gradient errors may cause the algorithm to fail to converge to a (locally) optimal solution. Hence, the authors propose mixed-level reformulation NAS (MiLeNAS) that uses a first-order method on the mixed-level formulation. Experimental results show that MiLeNAS has achieved higher classification accuracies than those achieved by the original bilevel optimization methods.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Federated Neural Architecture Search</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Federated NAS aims to optimize the architecture of neural network models in the federated learning environment. As discussed in Section 2, distributed model training is intrinsically more difficult than centralized training, and it becomes even more challenging for NAS problems. In this section, we would like to introduce the current research on federated NAS and discuss them from two perspectives: online and offline optimization, and single- and multi-objective optimization. It should be notice that research on federated NAS work is presently limited to horizontal federated learning and federated NAS in vertical federated learning has not been reported so far.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Offline and Online Federated Neural Architecture Search</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">Most NAS methods include two steps, i.e., search the architecture of the neural network model, and training the weights of the found neural network model afterwards. And most importantly, only the final performance matters. We define these approaches as offline NAS, because the search and training steps are typically separate and only an optimized network will be used. By contrast, online NAS requires that the architecture optimization and weight training training be done at the same time, and some of the models must be used during the search process. As a result, the performance of the models during the optimization must be acceptable.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">This concept can be easily extended to federated learning. In other words, federated NAS systems in which neural architecture search and weight training of the global model must be performed simultaneously are called online or real-time federated NAS, whilst federated NAS in which neural architecture search can be conducted at first and then the weights of the found models are trained are offline. Similarly, online federated NAS requires that the neural network models can be used during the optimization process.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">For example, the method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is a typical offline federated NAS framework using a multi-objective evolutionary algorithm. An offline evolutionary federated NAS algorithm can be summarized as follows:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.2" class="ltx_p">Initialize parents with a population size <math id="S4.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I1.i1.p1.1.m1.1a"><mi id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.m1.1b"><ci id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.1c">N</annotation></semantics></math> and each individual represents one architecture of the neural network. Construct and train <math id="S4.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I1.i1.p1.2.m2.1a"><mi id="S4.I1.i1.p1.2.m2.1.1" xref="S4.I1.i1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.2.m2.1b"><ci id="S4.I1.i1.p1.2.m2.1.1.cmml" xref="S4.I1.i1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.2.m2.1c">N</annotation></semantics></math> neural network models in federated learning with all participating clients to achieve the fitness values (e.g, validation accuracy) of the parents.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Generate <math id="S4.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I1.i2.p1.1.m1.1a"><mi id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><ci id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">N</annotation></semantics></math> offspring individuals by applying genetic operators on the parents. Construct and train all the generated offspring models for fitness evaluations in federated learning.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Combine the parent and offspring populations into one population and perform environmental selection. Select the best <math id="S4.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I1.i3.p1.1.m1.1a"><mi id="S4.I1.i3.p1.1.m1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.m1.1b"><ci id="S4.I1.i3.p1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.m1.1c">N</annotation></semantics></math> individual from the combined population as the new parents.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Repeat the above two steps until the evolutionary algorithm converges.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="S4.I1.i5.p1.1" class="ltx_p">Train the weights of the optimized neural network models in federated learning.</p>
</div>
</li>
</ol>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2009.05868/assets/x12.png" id="S4.F9.g1" class="ltx_graphics ltx_img_landscape" width="332" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Overall framework of offline federated NAS.</figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p">It can be seen that all participating clients are used for federated model training, i.e., at each generation, all participating clients must train each of the <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mi id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><ci id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">N</annotation></semantics></math> individuals for certain rounds for fitness evaluations, which significantly increases both computation and communication costs. Client sampling can be used to alleviate this issue, in which only subsets of participating clients contribute to one individual’s model training. For example in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, all the connected clients are divided into different groups and each sampled model use one group clients for local training. The overall process of this approach is summarized as follows:</p>
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Initialize the global model and a list of resource budgets in the server.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">Generate a list of simplified global models by model pruning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> based on the current global model. And then these global models are distributed to different group clients.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">For each communication round, every group of clients train their allocated group models for a number of pre-defined epochs and calculate the test accuracies on the validation datasets. Then both local test accuracies and validation data sizes are uploaded to the server. The server aggregates the uploaded local models and calculates a weighted accuracy for each group model. Remove <math id="S4.I2.i3.p1.1.m1.1" class="ltx_Math" alttext="\alpha\%" display="inline"><semantics id="S4.I2.i3.p1.1.m1.1a"><mrow id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml"><mi id="S4.I2.i3.p1.1.m1.1.1.2" xref="S4.I2.i3.p1.1.m1.1.1.2.cmml">α</mi><mo id="S4.I2.i3.p1.1.m1.1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><apply id="S4.I2.i3.p1.1.m1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.I2.i3.p1.1.m1.1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1.1">percent</csymbol><ci id="S4.I2.i3.p1.1.m1.1.1.2.cmml" xref="S4.I2.i3.p1.1.m1.1.1.2">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">\alpha\%</annotation></semantics></math> of global models with the worst test accuracies (removed global models that will not be trained and updated from the next communication round). The remaining groups of clients upload their calculated model gradients to the server for aggregation.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p">Repeat the above step for a number of pre-defined communication rounds.</p>
</div>
</li>
<li id="S4.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I2.i5.p1" class="ltx_para">
<p id="S4.I2.i5.p1.1" class="ltx_p">Replace and store the global model with the first model in the global model list.</p>
</div>
</li>
<li id="S4.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S4.I2.i6.p1" class="ltx_para">
<p id="S4.I2.i6.p1.1" class="ltx_p">Repeat the above four steps until convergence.</p>
</div>
</li>
<li id="S4.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S4.I2.i7.p1" class="ltx_para ltx_noindent">
<p id="S4.I2.i7.p1.1" class="ltx_p">Perform federated training on any stored global model.</p>
</div>
</li>
</ol>
<p id="S4.SS1.p4.2" class="ltx_p">Although the above procedure uses different architecture generation methods (model pruning) and search space compared to the evolutionary approach, it is clearly a population based offline federated NAS framework (weight training and architecture search are separate). In addition to client sampling, the authors also remove subsets of global models to further reduce the communication costs. However, the test accuracies of each global model in the list is calculated before model aggregation, which sometimes cannot represent the real test accuracies, especially for the cases when the clients’ data are particularly non-IID.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p">The overall framework of Offline federated NAS is shown in Fig. <a href="#S4.F9" title="Figure 9 ‣ 4.1 Offline and Online Federated Neural Architecture Search ‣ 4 Federated Neural Architecture Search ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and it has two main difficulties: 1) The number of communication rounds for federated model training of each individual is hard to determine. Setting a small number of communication rounds may make the individual’s model under-trained and bias the fitness evaluations. On the other hand, setting a very large number of communication rounds consumes too many communication resources. 2) Training the candidate neural network models consume additional communication resources, which should be avoided in federated learning. For the above reasons, online federated NAS frameworks need to be developed to solve above issues.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.p6.1" class="ltx_p">Online federated NAS trains the model and does the architecture optimization simultaneously (shown in Fig. <a href="#S4.F10" title="Figure 10 ‣ 4.1 Offline and Online Federated Neural Architecture Search ‣ 4 Federated Neural Architecture Search ‣ From Federated Learning to Federated Neural Architecture Search: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). To the best of our knowledge, there are currently two approaches to online federated NAS. One is gradient-based method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>, and the other is an EA-based method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>. The gradient based method adopts the idea of DARTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, which is implemented in the federated environment. The global model here is called supernet which consists of repeated directed acyclic graphs (DAGs) and each DAG contains all candidate operations. And relaxation tricks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> are used to make a weighted sum of the candidate operations differentiable so that the architecture parameters can be directly updated by the gradient descent algorithm. A brief description of this method is given below.</p>
<ol id="S4.I3" class="ltx_enumerate">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p">The server initializes the supernet and its architecture parameters.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p">All connected clients download the supernet and its architecture parameters from the server.</p>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.1" class="ltx_p">Each client trains and updates the supernet with fixed architecture parameters on mini-batch training data at first. Then update the architecture parameters with fixed model parameters on mini-batch validation data. These two procedures are performed alternately within one local training epoch.</p>
</div>
</li>
<li id="S4.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I3.i4.p1" class="ltx_para">
<p id="S4.I3.i4.p1.1" class="ltx_p">After local training for several epochs, all participating clients upload both model and architecture parameters to the server. The server performs weighted averaging upon the supernet and architect parameters.</p>
</div>
</li>
<li id="S4.I3.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I3.i5.p1" class="ltx_para ltx_noindent">
<p id="S4.I3.i5.p1.1" class="ltx_p">Repeat from step 2 to step 4 until convergence.</p>
</div>
</li>
</ol>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2009.05868/assets/x13.png" id="S4.F10.g1" class="ltx_graphics ltx_img_square" width="242" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Overall framework of online federated NAS. Client sampling are often used to ensure all offspring models are evaluated within one communication round.</figcaption>
</figure>
<div id="S4.SS1.p7" class="ltx_para ltx_noindent">
<p id="S4.SS1.p7.1" class="ltx_p">Unlike aforementioned two offline federated NAS framework, this scheme is not population based since all candidate operations are jointly optimized. In addition, architecture search and weight training of the supernet model are conducted alternately during the period of federated training. Therefore, no additional communication resources are required for training the candidate models. However, jointly optimizing the supernet on local clients requires much more computation and memory resources, which is not well suited for the edge devices like mobile phones.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para ltx_noindent">
<p id="S4.SS1.p8.1" class="ltx_p">In order to reduce the memory usage of local devices, a more light-weighted real-time evolutionary NAS framework (RT-FedEvoNAS) is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>. Different from the previous gradient based approach, RT-FedEvoNAS adopts model sampling technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite> in federated learning, where only one path of repeated cells in the global model is sampled and downloaded to local clients. As a result, both local computation and uploading costs are significantly reduced. The overall process is described as follows:</p>
<ol id="S4.I4" class="ltx_enumerate">
<li id="S4.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I4.i1.p1" class="ltx_para">
<p id="S4.I4.i1.p1.3" class="ltx_p">Initialize the supernet in the server. Generate the parent population containing <math id="S4.I4.i1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I4.i1.p1.1.m1.1a"><mi id="S4.I4.i1.p1.1.m1.1.1" xref="S4.I4.i1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I4.i1.p1.1.m1.1b"><ci id="S4.I4.i1.p1.1.m1.1.1.cmml" xref="S4.I4.i1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i1.p1.1.m1.1c">N</annotation></semantics></math> individuals, each representing a one-path subnet sampled from the supernet using a choice key. Do client sampling to allocate <math id="S4.I4.i1.p1.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.I4.i1.p1.2.m2.1a"><mi id="S4.I4.i1.p1.2.m2.1.1" xref="S4.I4.i1.p1.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.I4.i1.p1.2.m2.1b"><ci id="S4.I4.i1.p1.2.m2.1.1.cmml" xref="S4.I4.i1.p1.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i1.p1.2.m2.1c">L</annotation></semantics></math> clients evenly into <math id="S4.I4.i1.p1.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I4.i1.p1.3.m3.1a"><mi id="S4.I4.i1.p1.3.m3.1.1" xref="S4.I4.i1.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I4.i1.p1.3.m3.1b"><ci id="S4.I4.i1.p1.3.m3.1.1.cmml" xref="S4.I4.i1.p1.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i1.p1.3.m3.1c">N</annotation></semantics></math> groups.</p>
</div>
</li>
<li id="S4.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I4.i2.p1" class="ltx_para">
<p id="S4.I4.i2.p1.1" class="ltx_p">Download the subnet of each parent individual to each group of clients for training. Once the training is completed, upload the <math id="S4.I4.i2.p1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.I4.i2.p1.1.m1.1a"><mi id="S4.I4.i2.p1.1.m1.1.1" xref="S4.I4.i2.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.I4.i2.p1.1.m1.1b"><ci id="S4.I4.i2.p1.1.m1.1.1.cmml" xref="S4.I4.i2.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i2.p1.1.m1.1c">L</annotation></semantics></math> local subnets to the server for aggregation to update the supernet model.</p>
</div>
</li>
<li id="S4.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I4.i3.p1" class="ltx_para">
<p id="S4.I4.i3.p1.1" class="ltx_p">Generate <math id="S4.I4.i3.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I4.i3.p1.1.m1.1a"><mi id="S4.I4.i3.p1.1.m1.1.1" xref="S4.I4.i3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I4.i3.p1.1.m1.1b"><ci id="S4.I4.i3.p1.1.m1.1.1.cmml" xref="S4.I4.i3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i3.p1.1.m1.1c">N</annotation></semantics></math> offspring individuals using crossover and mutation. Similarly, generate a choice key for each offspring individual to sample a one-path subnet from the supernet. And then use client sampling technique to download sampled subnets (download the choice keys from the second generation) for training and uploading the trained subnets to the server for aggregation.</p>
</div>
</li>
<li id="S4.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I4.i4.p1" class="ltx_para">
<p id="S4.I4.i4.p1.1" class="ltx_p">Download the supernet together with the choice keys of all parent and offspring individuals to all participating clients to evaluate the objectives. Upload all the objective values to the server and calculate the weighted average of the validation errors for each individual.</p>
</div>
</li>
<li id="S4.I4.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I4.i5.p1" class="ltx_para">
<p id="S4.I4.i5.p1.1" class="ltx_p">Combine the parent and offspring individuals into a whole population. Perform environmental selection to select <math id="S4.I4.i5.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I4.i5.p1.1.m1.1a"><mi id="S4.I4.i5.p1.1.m1.1.1" xref="S4.I4.i5.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I4.i5.p1.1.m1.1b"><ci id="S4.I4.i5.p1.1.m1.1.1.cmml" xref="S4.I4.i5.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i5.p1.1.m1.1c">N</annotation></semantics></math> new parents.</p>
</div>
</li>
<li id="S4.I4.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S4.I4.i6.p1" class="ltx_para ltx_noindent">
<p id="S4.I4.i6.p1.1" class="ltx_p">Repeat Step 3 to Step 4 until the generation number reaches the pre-define maximum value.</p>
</div>
</li>
</ol>
<p id="S4.SS1.p8.2" class="ltx_p">Since only one-path of the supernet needs to be trained on each client, this sampling approach can significantly reduce both upload and local computation costs. There is a small detail in Step 4 that downloads all the supernet model to every client to calculate the validation accuracies, thus, only choice keys are downloaded in the next generation since the whole supernet has been already downloaded in the last generation.</p>
</div>
<div id="S4.SS1.p9" class="ltx_para ltx_noindent">
<p id="S4.SS1.p9.1" class="ltx_p">Online methods enable federated NAS systems to perform architecture search and train the model simultaneously. Both fitness evaluation thresholds, e.g. the number of communication rounds in federated learning and extra communication resources for training the searched models are not required by using online approaches, which are highly desired for federated learning. However, the search space of online federated NAS is fairly limited, which affects the diversity of the architecture search.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Single- and Multi-Objective Search</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">The aforementioned gradient based federated NAS framework only considers and optimizes the model performances, which is usually not enough for federated learning, because federated NAS is naturally a multi-objective optimization problem. In addition to the maximization of the model performance, the payload (communication costs) to be transferred between the server and clients should be minimized. Single-objective optimization often aggregates conflict objectives into one objective using hyperparameters, while Pareto approaches aim to obtain a set of models presenting trade-off relationships between the conflicting objectives.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">For example in RT-FedEvoNAS, the validation accuracy, model size and model floating point operations per second (FLOPs) of the sampled subnets are considered as the objectives need to be optimized and NSGA-II <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> is used as the basic search algorithm. Finally, after several generations of evolutionary optimization, multiple well trained subnets can be obtained chosen from the trade-off solutions based on the user’s preferences.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Open Challenges</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">Currently, research on federated NAS is still very preliminary and several challenges remain to be solved.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Horizontally Partitioned Data</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">There is no general solution that can well solve the non-IID learning degradation problems in horizontal federated learning, let alone in federated NAS. The earliest work to explore non-IID data effect is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. The authors analyze the possible reasons for divergence in global model training on non-IID data and propose a strategy to mitigate this influence by globally sharing a small part of the data across all connected clients. However, this kind of data sharing intrinsically violate the scope of privacy preserving scheme.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">The federated distillation approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> also have the potential risk of local data leakage. For the distillation, the teacher models are evaluated on mini-batches of unlabeled data on the server and their logits for mini-batch are used to train the student model on the server. The server can get a lot of local data information even on fake mini-batch data generated by local GAN generators <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">Some statistical aggregation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> are proposed to replace the original federated averaging algorithm (FedAvg). Both mathematical and experimental results prove that the proposed aggregation algorithms outperform the FedAvg on non-IID data. However, these approaches are often limited to some specific models and datasets, and it is unclear if they can show better performance for federated NAS frameworks. Hsieh <em id="S5.SS1.p3.1.1" class="ltx_emph ltx_font_italic">et al.</em> discuss the effect of non-IID data for DNNs in detail and different federated optimization methods are used upon different DNNs, such as GoogleNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>,and ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>. Experiment results show that batch normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> performs really poorly on non-IID data, but batch renormalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> and group normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> are much more robust for non-IID data, which are much more suited for federated learning. Most recently, it is shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> that ternary quantization is helpful in alleviating model divergence in federated learning, although its effectiveness remains to be validated on federated NAS.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Vertically Partitioned Data</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">Current federated NAS methods are all based on horizontal federated learning. Unlike horizontal federated learning, it is really hard to determine whether the data are IID or non-IID in vertical federated learning, since they are ’partitioned’ towards the feature space.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">Most existing vertical federated learning frameworks are built on two-party systems using simple linear models. Since only one party can hold the labels, the loss needs to be calculated on ciphertext; otherwise, the label information will be revealed. Then the gradients are very hard to calculate since the total loss is encrypted. Some approximation techniques like Taylor expansions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> are often used to simplify the gradient calculations, which, however, may introduce strong biases for complex models like DNNs.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">Overall, vertical federated NAS is totally different from horizontal federated NAS, which is in general still an unexplored research area.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Adversarial Federated Neural Architecture Search</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">Adversarial federated learning has two purposes: 1) Inference of the client data information 2) Attack the global model to conduct backdoor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite> elements or even let the model unusable. And the adversary in federated learning can be one of participating clients or the central server, since we often assume the server is honest-but-curious. Thus, the server should also be considered as a potential risk.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p">Federated learning is still fragile to white box attacks, since the model gradients and parameters still contain local data information. Geiping <em id="S5.SS3.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> showed that local images can be reconstructed from the knowledge of model parameters (or gradients) by inverting gradients techniques. In addition, an adversarial GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> generator can be developed on either the server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> or the client side <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite>. The adversary can reconstruct other participating clients’ private data, even it has no knowledge of the label information.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p">Enthoven and Al-Ars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite> summarize most defence strategies used in federated learning, which can be categorized into three types: 1) Subsample or compress the communicated gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>; 2) Differential privacy and SMC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and 3) Robustness aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> using e.g., the byzantine resilient aggregation rule <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>, <a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para ltx_noindent">
<p id="S5.SS3.p4.1" class="ltx_p">In general, finding robust model architectures in federated learning to defend against the adversarial attacks is still a hard task.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Encrypted Federated Neural Architecture Search</h3>

<div id="S5.SS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS4.p1.1" class="ltx_p">Homomorphic encryption technologies are often applied to prevent privacy leakage from the gradient information sent to the server. However, using homomorphic encryption in federated NAS system has two main difficulties.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para ltx_noindent">
<p id="S5.SS4.p2.1" class="ltx_p">First, homomorphic encryption, including encoding and encryption, is computationally expensive in federated learning. At first, all communicated model parameters need to be encoded into large inter numbers, because homomorphic encryption does not work on real numbers. Then the encoded parameters need to do modulus calculations with large prime numbers one by one. Unfortunately, modern deep neural network models contain millions of parameters, making the encryption process computationally extremely intensive. Therefore, developing a light weighted encryption method is an important yet challenging task for federated learning, let alone for federated NAS.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para ltx_noindent">
<p id="S5.SS4.p3.1" class="ltx_p">Second, the original federated encryption is introduced in which the server holds the public key and clients hold the private key. This framework is unsafe, because only one of clients uploads its secret key to the server. Therefore, a more advanced SMC approach is adopted that divides the whole secret key into several shards, and the server cannot decrypt the gradients unless it collected <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mi id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><ci id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">t</annotation></semantics></math> (secret key recover threshold) key shards. Unfortunately, the encrypted gradients must be frequently transferred between the server and clients, which significantly increases the communication costs, since the local clients can only partially decrypt the gradients through their key shards. This is a big burden to the communication resources, which needs to be solved in the future.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this survey paper, a brief overview of federated learning and NAS is provided, and a combination of both techniques, i.e., federated NAS is introduced. Given several remaining challenges in both federated learning and NAS, federated NAS becomes extremely challenging since many techniques developed in centralized NAS are no longer suited for federated NAS, and NAS will be subject to more constraints introduced by the federated learning environment. Two approaches to federated NAS are discussed, one is offline optimization and the other is online optimization. It is noted that offline evolutionary NAS methods are not applicable for many real-world scenarios, mainly because the offline approach performs architecture search and weight training separately and requires a large amount of communication costs. In addition, the performance of neural network under optimization must be acceptable for application and serious performance drop is not allowed. RT-FedEvoNAS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> offers a solution to the above challenges, although its search space is highly constrained.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">Despite that many grand challenges remain to be solved, federated NAS is of paramount practical significance for many real-world problems, where handcrafted deep neural networks may fail to work properly. We hope that this survey will help understand the promises and challenges in federated NAS, thereby triggering more research interests in developing new theories and algorithms, thereby promoting the application of AI techniques to a wider range of fields where data privacy and security is a primary concern.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter.

</span>
<span class="ltx_bibblock">Neural architecture search: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1808.05377</span>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</span>,
10(2):1–19, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence and Statistics</span>, pages 1273–1282,
2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Gong-Duo Zhang, Shen-Yi Zhao, Hao Gao, and Wu-Jun Li.

</span>
<span class="ltx_bibblock">Feature-distributed svrg for high-dimensional linear classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.03604</span>, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Reza Shokri and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security</span>, pages 1310–1321, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.05492</span>, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Sebastian Caldas, Jakub Konečny, H Brendan McMahan, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Expanding the reach of federated learning by reducing client resource
requirements.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.07210</span>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Song Han, Huizi Mao, and William J. Dally.

</span>
<span class="ltx_bibblock">Deep compression: Compressing deep neural network with pruning,
trained quantization and huffman coding.

</span>
<span class="ltx_bibblock">In Yoshua Bengio and Yann LeCun, editors, <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">4th International
Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico,
May 2-4, 2016, Conference Track Proceedings</span>, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jinjin Xu, Wenli Du, Yaochu Jin, Wangli He, and Ran Cheng.

</span>
<span class="ltx_bibblock">Ternary compression for communication-efficient federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Yang Chen, Xiaoyan Sun, and Yaochu Jin.

</span>
<span class="ltx_bibblock">Communication-efficient federated deep learning with layerwise
asynchronous model update and temporally weighted aggregation.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hangyu Zhu and Yaochu Jin.

</span>
<span class="ltx_bibblock">Multi-objective evolutionary federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
31(4):1310–1322, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.00582</span>, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.06127</span>, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Shaoxiong Ji, Shirui Pan, Guodong Long, Xue Li, Jing Jiang, and Zi Huang.

</span>
<span class="ltx_bibblock">Learning private neural language modeling with attentive aggregation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">2019 International Joint Conference on Neural Networks
(IJCNN)</span>, pages 1–8. IEEE, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning via additively homomorphic
encryption.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
13(5):1333–1345, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Frederik Armknecht, Colin Boyd, Christopher Carr, Kristian Gjøsteen, Angela
Jäschke, Christian A. Reuter, and Martin Strand.

</span>
<span class="ltx_bibblock">A guide to fully homomorphic encryption.

</span>
<span class="ltx_bibblock">Cryptology ePrint Archive, Report 2015/1192, 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://eprint.iacr.org/2015/1192" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://eprint.iacr.org/2015/1192</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yehida Lindell.

</span>
<span class="ltx_bibblock">Secure multiparty computation for privacy preserving data mining.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Encyclopedia of Data Warehousing and Mining</span>, pages
1005–1009. IGI Global, 2005.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy preserving machine learning.

</span>
<span class="ltx_bibblock">Cryptology ePrint Archive, Report 2017/281, 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://eprint.iacr.org/2017/281" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://eprint.iacr.org/2017/281</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Cynthia Dwork.

</span>
<span class="ltx_bibblock">Differential privacy: A survey of results.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">International conference on theory and applications of models
of computation</span>, pages 1–19. Springer, 2008.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Robin C Geyer, Tassilo Klein, and Moin Nabi.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client level
perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1712.07557</span>, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal
Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security</span>, pages 308–318, 2016.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini,
Guillaume Smith, and Brian Thorne.

</span>
<span class="ltx_bibblock">Private federated learning on vertically partitioned data via entity
resolution and additively homomorphic encryption.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.10677</span>, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Shengwen Yang, Bing Ren, Xuhui Zhou, and Liping Liu.

</span>
<span class="ltx_bibblock">Parallel distributed logistic regression for vertical federated
learning without third-party coordinator.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.09824</span>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang.

</span>
<span class="ltx_bibblock">A secure federated transfer learning framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE Intelligent Systems</span>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Siwei Feng and Han Yu.

</span>
<span class="ltx_bibblock">Multi-participant multi-class vertical federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.11154</span>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Yang Liu, Xiong Zhang, and Libin Wang.

</span>
<span class="ltx_bibblock">Asymmetrically vertical federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.07427</span>, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Michael J. Freedman, Kobbi Nissim, and Benny Pinkas.

</span>
<span class="ltx_bibblock">Efficient private matching and set intersection.

</span>
<span class="ltx_bibblock">In Christian Cachin and Jan L. Camenisch, editors, <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Advances in
Cryptology - EUROCRYPT 2004</span>, pages 1–19, Berlin, Heidelberg, 2004. Springer
Berlin Heidelberg.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Gang Liang and Sudarshan S. Chawathe.

</span>
<span class="ltx_bibblock">Privacy-preserving inter-database operations.

</span>
<span class="ltx_bibblock">In Hsinchun Chen, Reagan Moore, Daniel D. Zeng, and John Leavitt,
editors, <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Intelligence and Security Informatics</span>, pages 66–82, Berlin,
Heidelberg, 2004. Springer Berlin Heidelberg.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
James Bergstra and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Random search for hyper-parameter optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, 13(1):281–305, 2012.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Liam Li and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Random search and reproducibility for neural architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Uncertainty in Artificial Intelligence</span>, pages 367–377.
PMLR, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Carl Edward Rasmussen.

</span>
<span class="ltx_bibblock">Gaussian processes in machine learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Summer School on Machine Learning</span>, pages 63–71. Springer,
2003.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams.

</span>
<span class="ltx_bibblock">Freeze-thaw bayesian optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1406.3896</span>, 2014.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Xiawu Zheng, Rongrong Ji, Lang Tang, Baochang Zhang, Jianzhuang Liu, and
Qi Tian.

</span>
<span class="ltx_bibblock">Multinomial distribution learning for effective neural architecture
search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 1304–1313, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Léon Bottou.

</span>
<span class="ltx_bibblock">Stochastic gradient descent tricks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Neural networks: Tricks of the trade</span>, pages 421–436.
Springer, 2012.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Barret Zoph and Quoc V Le.

</span>
<span class="ltx_bibblock">Neural architecture search with reinforcement learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.01578</span>, 2016.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight.

</span>
<span class="ltx_bibblock">Transfer learning for low-resource neural machine translation.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1604.02201</span>, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le.

</span>
<span class="ltx_bibblock">Learning transferable architectures for scalable image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 8697–8710, 2018.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Lingxi Xie and Alan Yuille.

</span>
<span class="ltx_bibblock">Genetic cnn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 1379–1388, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yanan Sun, Bing Xue, Mengjie Zhang, and Gary G Yen.

</span>
<span class="ltx_bibblock">Completely automated cnn architecture design based on blocks.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">IEEE transactions on neural networks and learning systems</span>,
31(4):1242–1254, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen, and Jiancheng Lv.

</span>
<span class="ltx_bibblock">Automatically designing cnn architectures using the genetic algorithm
for image classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Cybernetics</span>, 2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter.

</span>
<span class="ltx_bibblock">Towards automated deep learning: Efficient joint neural architecture
and hyperparameter search.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1807.06906</span>, 2018.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yanan Sun, Handing Wang, Bing Xue, Yaochu Jin, Gary G Yen, and Mengjie Zhang.

</span>
<span class="ltx_bibblock">Surrogate-assisted evolutionary deep learning using an end-to-end
random forest-based performance predictor.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Evolutionary Computation</span>, 24(2):350–364,
2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Honglei Zhang, Serkan Kiranyaz, and Moncef Gabbouj.

</span>
<span class="ltx_bibblock">Finding better topologies for deep convolutional neural networks by
evolution.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1809.03242</span>, 2018.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Haifeng Jin, Qingquan Song, and Xia Hu.

</span>
<span class="ltx_bibblock">Efficient neural architecture search with network morphism.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.10282</span>, 9, 2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean.

</span>
<span class="ltx_bibblock">Efficient neural architecture search via parameter sharing.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.03268</span>, 2018.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and
Jian Sun.

</span>
<span class="ltx_bibblock">Single path one-shot neural architecture search with uniform
sampling.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1904.00420</span>, 2019.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Xuanyi Dong and Yi Yang.

</span>
<span class="ltx_bibblock">One-shot neural architecture search via self-evaluated template
network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 3681–3690, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Xiang Li, Chen Lin, Chuming Li, Ming Sun, Wei Wu, Junjie Yan, and Wanli Ouyang.

</span>
<span class="ltx_bibblock">Improving one-shot NAS by suppressing the posterior fading.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 13836–13845, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Shan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, and Changshui Zhang.

</span>
<span class="ltx_bibblock">GreedyNAS: Towards fast one-shot NAS with greedy supernet.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 1999–2008, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Christopher John Cornish Hellaby Watkins.

</span>
<span class="ltx_bibblock">Learning from delayed rewards.

</span>
<span class="ltx_bibblock">1989.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar.

</span>
<span class="ltx_bibblock">Designing neural network architectures using reinforcement learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.02167</span>, 2016.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Zhao Zhong, Zichen Yang, Boyang Deng, Junjie Yan, Wei Wu, Jing Shao, and
Cheng-Lin Liu.

</span>
<span class="ltx_bibblock">Blockqnn: Efficient block-wise neural network architecture
generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
2020.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Ramakanth Pasunuru and Mohit Bansal.

</span>
<span class="ltx_bibblock">Continual and multi-task architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</span>, pages 1911–1922, 2019.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan
Kindermans, and Quoc V Le.

</span>
<span class="ltx_bibblock">Can weight sharing outperform random architecture search? an
investigation with TuNAS.

</span>
<span class="ltx_bibblock">In <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 14323–14332, 2020.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Yu Liu, Xuhui Jia, Mingxing Tan, Raviteja Vemulapalli, Yukun Zhu, Bradley
Green, and Xiaogang Wang.

</span>
<span class="ltx_bibblock">Search to distill: Pearls are everywhere but not the eyes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 7539–7548, 2020.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1503.02531</span>, 2015.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Thomas Back.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Evolutionary algorithms in theory and practice: evolution
strategies, evolutionary programming, genetic algorithms</span>.

</span>
<span class="ltx_bibblock">Oxford university press, 1996.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Wolfgang Banzhaf, Peter Nordin, Robert E Keller, and Frank D Francone.

</span>
<span class="ltx_bibblock">Genetic programming—an introduction: On the automatic evolution of
computer programs and its applications, dpunkt. verlag and morgan kaufmann
publishers.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">Inc., San Francisco, California</span>, 1998.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Lothar M Schmitt.

</span>
<span class="ltx_bibblock">Theory of genetic algorithms.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">Theoretical Computer Science</span>, 259(1-2):1–61, 2001.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Xin Yao.

</span>
<span class="ltx_bibblock">Evolving artificial neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 87(9):1423–1447, 1999.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Dario Floreano, Peter Dürr, and Claudio Mattiussi.

</span>
<span class="ltx_bibblock">Neuroevolution: from architectures to learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">Evolutionary intelligence</span>, 1(1):47–62, 2008.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Kenneth O Stanley, David B D’Ambrosio, and Jason Gauci.

</span>
<span class="ltx_bibblock">A hypercube-based encoding for evolving large-scale neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Artificial life</span>, 15(2):185–212, 2009.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">An empirical exploration of recurrent network architectures.

</span>
<span class="ltx_bibblock">In <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
2342–2350, 2015.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Kenneth O Stanley and Risto Miikkulainen.

</span>
<span class="ltx_bibblock">Evolving neural networks through augmenting topologies.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">Evolutionary computation</span>, 10(2):99–127, 2002.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Edgar Galván and Peter Mooney.

</span>
<span class="ltx_bibblock">Neuroevolution in deep neural networks: Current trends and future
challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.05415</span>, 2020.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Ashraf Darwish, Aboul Ella Hassanien, and Swagatam Das.

</span>
<span class="ltx_bibblock">A survey of swarm and evolutionary computing approaches for deep
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence Review</span>, 53(3):1767–1812, 2020.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Kenneth O Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen.

</span>
<span class="ltx_bibblock">Designing neural networks through neuroevolution.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">Nature Machine Intelligence</span>, 1(1):24–35, 2019.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Handing Wang, Licheng Jiao, and Xin Yao.

</span>
<span class="ltx_bibblock">Two_arch2: An improved two-archive algorithm for many-objective
optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Evolutionary Computation</span>, 19(4):524–541,
2014.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Yanan Sun, Gary G Yen, and Zhang Yi.

</span>
<span class="ltx_bibblock">Igd indicator-based evolutionary algorithm for many-objective
optimization problems.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Evolutionary Computation</span>, 23(2):173–187,
2018.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.

</span>
<span class="ltx_bibblock">Imagenet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">International journal of computer vision</span>, 115(3):211–252,
2015.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink,
Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy,
et al.

</span>
<span class="ltx_bibblock">Evolving deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence in the Age of Neural Networks and
Brain Computing</span>, pages 293–312. Elsevier, 2019.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Jason Liang, Elliot Meyerson, and Risto Miikkulainen.

</span>
<span class="ltx_bibblock">Evolutionary architecture search for deep multitask networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">Proceedings of the Genetic and Evolutionary Computation
Conference</span>, pages 466–473, 2018.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu,
Jie Tan, Quoc V Le, and Alexey Kurakin.

</span>
<span class="ltx_bibblock">Large-scale evolution of image classifiers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
2902–2911, 2017.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.

</span>
<span class="ltx_bibblock">Densely connected convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 4700–4708, 2017.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
William Irwin-Harris, Yanan Sun, Bing Xue, and Mengjie Zhang.

</span>
<span class="ltx_bibblock">A graph-based encoding for evolutionary convolutional neural network
architecture design.

</span>
<span class="ltx_bibblock">In <span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">2019 IEEE Congress on Evolutionary Computation (CEC)</span>, pages
546–553. IEEE, 2019.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Masanori Suganuma, Shinichi Shirakawa, and Tomoharu Nagao.

</span>
<span class="ltx_bibblock">A genetic programming approach to designing convolutional neural
network architectures.

</span>
<span class="ltx_bibblock">In <span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">Proceedings of the genetic and evolutionary computation
conference</span>, pages 497–504, 2017.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Simon Harding.

</span>
<span class="ltx_bibblock">Evolution of image filters on graphics processor units using
cartesian genetic programming.

</span>
<span class="ltx_bibblock">In <span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">2008 IEEE Congress on Evolutionary Computation (IEEE World
Congress on Computational Intelligence)</span>, pages 1921–1928. IEEE, 2008.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Julian F Miller and Stephen L Smith.

</span>
<span class="ltx_bibblock">Redundancy and computational efficiency in cartesian genetic
programming.

</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Evolutionary Computation</span>, 10(2):167–174,
2006.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Yanan Sun, Bing Xue, Mengjie Zhang, and Gary G Yen.

</span>
<span class="ltx_bibblock">Evolving deep convolutional neural networks for image classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Evolutionary Computation</span>, 24(2):394–407,
2019.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Carl Edward Rasmussen and Zoubin Ghahramani.

</span>
<span class="ltx_bibblock">Occam’s razor.

</span>
<span class="ltx_bibblock">In <span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
294–300, 2001.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Douglas M Hawkins.

</span>
<span class="ltx_bibblock">The problem of overfitting.

</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">Journal of chemical information and computer sciences</span>,
44(1):1–12, 2004.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray
Kavukcuoglu.

</span>
<span class="ltx_bibblock">Hierarchical representations for efficient architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2018.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Zhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb, Erik
Goodman, and Wolfgang Banzhaf.

</span>
<span class="ltx_bibblock">Nsga-net: neural architecture search using multi-objective genetic
algorithm.

</span>
<span class="ltx_bibblock">In <span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">Proceedings of the Genetic and Evolutionary Computation
Conference</span>, pages 419–427, 2019.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le.

</span>
<span class="ltx_bibblock">Regularized evolution for image classifier architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">Proceedings of the aaai conference on artificial
intelligence</span>, volume 33, pages 4780–4789, 2019.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen
Zhang, and Wanli Ouyang.

</span>
<span class="ltx_bibblock">EcoNAS: Finding proxies for economical neural architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 11396–11404, 2020.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Zhichao Lu, Ian Whalen, Yashesh Dhebar, Kalyanmoy Deb, Erik Goodman, Wolfgang
Banzhaf, and Vishnu Naresh Boddeti.

</span>
<span class="ltx_bibblock">Multi-criterion evolutionary design of deep convolutional neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.01369</span>, 2019.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Yaochu Jin.

</span>
<span class="ltx_bibblock">Surrogate-assisted evolutionary computation: Recent advances and
future challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">Swarm and Evolutionary Computation</span>, 1(2):61–70, 2011.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
David S Broomhead and David Lowe.

</span>
<span class="ltx_bibblock">Radial basis functions, multi-variable functional interpolation and
adaptive networks.

</span>
<span class="ltx_bibblock">Technical report, Royal Signals and Radar Establishment Malvern
(United Kingdom), 1988.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Shinkyu Jeong, Mitsuhiro Murayama, and Kazuomi Yamamoto.

</span>
<span class="ltx_bibblock">Efficient optimization design method using kriging model.

</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">Journal of aircraft</span>, 42(2):413–420, 2005.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Handing Wang, Yaochu Jin, and John Doherty.

</span>
<span class="ltx_bibblock">Committee-based active learning for surrogate-assisted particle swarm
optimization of expensive problems.

</span>
<span class="ltx_bibblock"><span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">IEEE transactions on cybernetics</span>, 47(9):2664–2677, 2017.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas.

</span>
<span class="ltx_bibblock">Taking the human out of the loop: A review of bayesian optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 104(1):148–175, 2015.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Ye-Hoon Kim, Bhargava Reddy, Sojung Yun, and Chanwon Seo.

</span>
<span class="ltx_bibblock">Nemo: Neuro-evolution with multiobjective optimization of deep neural
network for speed and accuracy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">ICML 2017 AutoML Workshop</span>, 2017.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan.

</span>
<span class="ltx_bibblock">A fast and elitist multiobjective genetic algorithm: Nsga-ii.

</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">IEEE transactions on evolutionary computation</span>, 6(2):182–197,
2002.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, and Min Sun.

</span>
<span class="ltx_bibblock">Dpp-net: Device-aware progressive search for pareto-optimal neural
architectures.

</span>
<span class="ltx_bibblock">In <span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 517–531, 2018.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li,
Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.

</span>
<span class="ltx_bibblock">Progressive neural architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 19–34, 2018.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter.

</span>
<span class="ltx_bibblock">Efficient multi-objective neural architecture search via lamarckian
evolution.

</span>
<span class="ltx_bibblock"><span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.09081</span>, 2018.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen.

</span>
<span class="ltx_bibblock">Network morphism.

</span>
<span class="ltx_bibblock">In <span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
564–572, 2016.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Yaochu Jin and Bernhard Sendhoff.

</span>
<span class="ltx_bibblock">Pareto-based multiobjective machine learning: An overview and case
studies.

</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews)</span>, 38(3):397–415, 2008.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Richard Shin, Charles Packer, and Dawn Song.

</span>
<span class="ltx_bibblock">Differentiable neural network architecture search.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Karim Ahmed and Lorenzo Torresani.

</span>
<span class="ltx_bibblock">Maskconnect: Connectivity learning by gradient descent.

</span>
<span class="ltx_bibblock">In <span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 349–365, 2018.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Jiemin Fang, Yuzhu Sun, Qian Zhang, Yuan Li, Wenyu Liu, and Xinggang Wang.

</span>
<span class="ltx_bibblock">Densely connected search space for more flexible neural architecture
search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 10628–10637, 2020.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Han Cai, Ligeng Zhu, and Song Han.

</span>
<span class="ltx_bibblock">ProxylessNAS: Direct neural architecture search on target task
and hardware.

</span>
<span class="ltx_bibblock">In <span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2018.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Jonathan Lorraine, Paul Vicol, and David Duvenaud.

</span>
<span class="ltx_bibblock">Optimizing millions of hyperparameters by implicit differentiation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">International Conference on Artificial Intelligence and
Statistics</span>, pages 1540–1552, 2020.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Hanxiao Liu, Karen Simonyan, and Yiming Yang.

</span>
<span class="ltx_bibblock">Darts: Differentiable architecture search.

</span>
<span class="ltx_bibblock"><span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.09055</span>, 2018.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Xuanyi Dong and Yi Yang.

</span>
<span class="ltx_bibblock">Searching for a robust neural architecture in four gpu hours.

</span>
<span class="ltx_bibblock">In <span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on computer vision and
pattern recognition</span>, pages 1761–1770, 2019.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.

</span>
<span class="ltx_bibblock">SNAS: stochastic neural architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2018.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Guohao Li, Guocheng Qian, Itzel C Delgadillo, Matthias Muller, Ali Thabet, and
Bernard Ghanem.

</span>
<span class="ltx_bibblock">Sgas: Sequential greedy architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 1620–1630, 2020.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and
Hongkai Xiong.

</span>
<span class="ltx_bibblock">Pc-darts: Partial channel connections for memory-efficient
architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib109.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2019.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Chen Gao, Yunpeng Chen, Si Liu, Zhenxiong Tan, and Shuicheng Yan.

</span>
<span class="ltx_bibblock">AdversarialNas: Adversarial neural architecture search for GANs.

</span>
<span class="ltx_bibblock">In <span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 5680–5689, 2020.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie,
Bichen Wu, Matthew Yu, Tao Xu, Kan Chen, et al.

</span>
<span class="ltx_bibblock">Fbnetv2: Differentiable neural architecture search for spatial and
channel dimensions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib111.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 12965–12974, 2020.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Chaoyang He, Haishan Ye, Li Shen, and Tong Zhang.

</span>
<span class="ltx_bibblock">Milenas: Efficient neural architecture search via mixed-level
reformulation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib112.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 11993–12002, 2020.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Mengwei Xu, Yuxin Zhao, Kaigui Bian, Gang Huang, Qiaozhu Mei, and Xuanzhe Liu.

</span>
<span class="ltx_bibblock">Neural architecture search over decentralized data.

</span>
<span class="ltx_bibblock"><span id="bib.bib113.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.06352</span>, 2020.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.

</span>
<span class="ltx_bibblock">Thinet: A filter level pruning method for deep neural network
compression.

</span>
<span class="ltx_bibblock">In <span id="bib.bib114.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 5058–5066, 2017.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Fednas: Federated deep learning via neural architecture search.

</span>
<span class="ltx_bibblock"><span id="bib.bib115.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.08546</span>, 2020.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Hangyu Zhu and Yaochu Jin.

</span>
<span class="ltx_bibblock">Real-time federated evolutionary neural architecture search.

</span>
<span class="ltx_bibblock"><span id="bib.bib116.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.02793</span>, 2020.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Michael A Trick.

</span>
<span class="ltx_bibblock">A linear relaxation heuristic for the generalized assignment problem.

</span>
<span class="ltx_bibblock"><span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">Naval Research Logistics (NRL)</span>, 39(2):137–151, 1992.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc
Le.

</span>
<span class="ltx_bibblock">Understanding and simplifying one-shot architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib118.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
550–559, 2018.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and
Seong-Lyun Kim.

</span>
<span class="ltx_bibblock">Communication-efficient on-device machine learning: Federated
distillation and augmentation under non-iid private data.

</span>
<span class="ltx_bibblock"><span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.11479</span>, 2018.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib120.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.07242</span>, 2020.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial nets.

</span>
<span class="ltx_bibblock">In <span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
2672–2680, 2014.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Agnostic federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib122.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.00146</span>, 2019.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib123.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 1–9, 2015.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Sergey Ioffe and Christian Szegedy.

</span>
<span class="ltx_bibblock">Batch normalization: Accelerating deep network training by reducing
internal covariate shift.

</span>
<span class="ltx_bibblock"><span id="bib.bib124.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1502.03167</span>, 2015.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Sergey Ioffe.

</span>
<span class="ltx_bibblock">Batch renormalization: Towards reducing minibatch dependence in
batch-normalized models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
1945–1953, 2017.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Yuxin Wu and Kaiming He.

</span>
<span class="ltx_bibblock">Group normalization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib126.1.1" class="ltx_text ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</span>, pages 3–19, 2018.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov.

</span>
<span class="ltx_bibblock">How to backdoor federated learning.

</span>
<span class="ltx_bibblock">volume 108 of <span id="bib.bib127.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pages
2938–2948, Online, 26–28 Aug 2020. PMLR.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller.

</span>
<span class="ltx_bibblock">Inverting gradients–how easy is it to break privacy in federated
learning?

</span>
<span class="ltx_bibblock"><span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.14053</span>, 2020.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi.

</span>
<span class="ltx_bibblock">Beyond inferring class representatives: User-level privacy leakage
from federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2019-IEEE Conference on Computer
Communications</span>, pages 2512–2520. IEEE, 2019.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz.

</span>
<span class="ltx_bibblock">Deep models under the gan: information leakage from collaborative
deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security</span>, pages 603–618, 2017.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
David Enthoven and Zaid Al-Ars.

</span>
<span class="ltx_bibblock">An overview of federated deep learning privacy attacks and defensive
strategies.

</span>
<span class="ltx_bibblock"><span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.04676</span>, 2020.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang.

</span>
<span class="ltx_bibblock">Federated continual learning with adaptive parameter communication.

</span>
<span class="ltx_bibblock"><span id="bib.bib132.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.03196</span>, 2020.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui.

</span>
<span class="ltx_bibblock">Robust aggregation for federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib133.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.13445</span>, 2019.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Leslie Lamport, Robert Shostak, and Marshall Pease.

</span>
<span class="ltx_bibblock">The byzantine generals problem.

</span>
<span class="ltx_bibblock">In <span id="bib.bib134.1.1" class="ltx_text ltx_font_italic">Concurrency: the Works of Leslie Lamport</span>, pages 203–226.
2019.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Rouault.

</span>
<span class="ltx_bibblock">The hidden vulnerability of distributed learning in byzantium.

</span>
<span class="ltx_bibblock"><span id="bib.bib135.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.07927</span>, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2009.05867" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2009.05868" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2009.05868">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2009.05868" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2009.05869" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 09:25:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
