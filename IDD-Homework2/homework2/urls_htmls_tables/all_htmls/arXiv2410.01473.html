<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation</title>
<!--Generated on Wed Oct  2 12:15:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Depth Anything V2,  Sinkhole,  YOLO,  Drone,  Segment Anything Model
" lang="en" name="keywords"/>
<base href="/html/2410.01473v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S1" title="In SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S2" title="In SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S2.SS1" title="In II Related Work ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Closed Depressions Computation for Sinkhole delineation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S2.SS2" title="In II Related Work ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Sinkhole Segmentation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S2.SS3" title="In II Related Work ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Monocular Depth Estimation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S2.SS4" title="In II Related Work ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Segment Anything Model</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S3" title="In SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Materials and Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S3.SS1" title="In III Materials and Methodology ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Study Area and Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S3.SS2" title="In III Materials and Methodology ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">SinkSAM Framework Overview</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S4" title="In SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S4.SS1" title="In IV Experiments ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S4.SS2" title="In IV Experiments ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Comparison between pairs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S4.SS3" title="In IV Experiments ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Implementation Details</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5" title="In SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5.SS1" title="In V Results ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Closed Depressions by DAV2 using ”fill sinks” vs Segmentation by SAM (Comparison A: Map 2 vs Map 4)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5.SS2" title="In V Results ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Comparing Prompts: BBs from Closed Depressions (DAV2) vs BBs made by YOLOv9 (Comparison B: Map 3 vs Map 4)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5.SS3" title="In V Results ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Photogrammetric DEM vs DAV2 (Comparison C: Map 1 vs Map 2)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5.SS4" title="In V Results ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">SAM vs Fine-tuned SAM (SinkSAM) (Comparison D: Map 4 vs Map 5)</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6" title="In SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.SS1" title="In VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Refinement of Closed Depressions by RGB data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.SS2" title="In VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic"> Closed Depressions Prompting</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.SS3" title="In VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Monocular Depth Estimation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.SS4" title="In VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-D</span> </span><span class="ltx_text ltx_font_italic">Fine-tuned SAM</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S7" title="In SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Osher Rafaeli<span class="ltx_text" id="id3.1.id1" style="position:relative; bottom:2.2pt;"><span class="ltx_ERROR undefined" id="id3.1.id1.1">\orcidlink</span>0000-0002-7097-7568</span>,
Tal Svoray<span class="ltx_text" id="id4.2.id2" style="position:relative; bottom:2.2pt;"><span class="ltx_ERROR undefined" id="id4.2.id2.1">\orcidlink</span>0000-0003-2243-8532</span>
and Ariel Nahlieli<span class="ltx_text" id="id5.3.id3" style="position:relative; bottom:2.2pt;"><span class="ltx_ERROR undefined" id="id5.3.id3.1">\orcidlink</span>0009-0001-0633-1842</span>
</span><span class="ltx_author_notes">O. Rafaeli and A. Nahlieli are with the Department of Environmental, Geoinformatics and Urban Planning Sciences, Ben-Gurion University of the Negev, Israel (e-mail: osherr@post.bgu.ac.il; arielnah@post.bgu.ac.il).T. Svoray is with the Department of Environmental, Geoinformatics and Urban Planning Sciences, and the Department of Psychology, Ben-Gurion University of the Negev, Israel (e-mail: tsvoray@bgu.ac.il).</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">Soil sinkholes significantly influence soil degradation, but their irregular shapes, along with interference from shadow and vegetation, make it challenging to accurately quantify their properties using remotely sensed data. We present a novel framework for sinkhole segmentation that combines traditional topographic computations of closed depressions with the newly developed prompt-based Segment Anything Model (SAM). Within this framework, termed SinkSAM, we highlight four key improvements: (1) The integration of topographic computations with SAM enables pixel-level refinement of sinkhole boundaries segmentation; (2) A coherent mathematical prompting strategy, based on closed depressions, addresses the limitations of purely learning-based models (CNNs) in detecting and segmenting undefined sinkhole features, while improving generalization to new, unseen regions; (3) Using Depth Anything V2 monocular depth for automatic prompts eliminates photogrammetric biases, enabling sinkhole mapping without the dependence on LiDAR data; and (4) An established sinkhole database facilitates fine-tuning of SAM, improving its zero-shot performance in sinkhole segmentation. These advancements allow the deployment of SinkSAM, in an unseen test area, in the highly variable semiarid region, achieving an intersection-over-union (IoU) of 40.27% and surpassing previous results. This paper also presents the first SAM implementation for sinkhole segmentation and demonstrates the robustness of SinkSAM in extracting sinkhole maps using a single RGB image.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Depth Anything V2, Sinkhole, YOLO, Drone, Segment Anything Model

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Piping erosion and soil sinkholes have a profound impact on landform evolution and soil degradation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib1" title="">1</a>]</cite>. Many studies indicate that sinkholes can lead to irreversible damages to built areas, cause roads collapse, destroy farmlands, and induce dam piping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib2" title="">2</a>]</cite>. Monitoring soil sinkholes occurrence, and characteristics, is crucial for studying dynamics of sinkhole-prone areas and mitigating sinkhole-related hazards.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Sinkholes manifest on the Earth’s surface as depressions, which can be observed either with the naked eye or through remote sensing (RS) platforms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib3" title="">3</a>]</cite>. However, an automatic detection of soil sinkholes using RS imagery is still a challenging task that was tackled by several researchers so far <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib4" title="">4</a>]</cite>. The reasons for this research gap may be the fact that soil sinkholes are characterized by irregular shapes, suffer internal and external shadowing effect, can be masked by vegetation and are usually spatially distributed over wide regions representing a typical imbalanced data problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To tackle these challenges, ancillary topographic information may be used to improve sinkholes segmentation in RS imagery. Data sources such as LiDAR (light detection and ranging), or photogrammetric Digital Elevation Models (DEMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib6" title="">6</a>]</cite> were used for various topographic analyses. However, airborne LiDARs are rarely being used at high-resolution over wide regions, due to extremely high costs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib7" title="">7</a>]</cite>. Photogrammetric DEMs can have low accuracy in representing terrain, as they often mistakenly capture vegetation or roof structures as part of the ground surface. They are also sensitive to lighting and shadows, with poor conditions potentially causing errors in elevation data. Compared with LiDAR-derived DEMs, photogrammetric models typically have limited vertical accuracy. Additionally, they struggle to capture steep terrain, as overlapping image pairs may not fully represent slope angles. Stereo matching issues can further result in gaps or distortions in the final DEM. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">As a substitute to these two topographic data sources, recently developed monocular depth estimation (MDE), using deep learning techniques, based on a single RGB image, became central in the field of computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib9" title="">9</a>]</cite>. MDE was proven useful in 3D reconstruction and target instance segmentation. However, in RS studies, MDE is still very rarely used <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib10" title="">10</a>]</cite>, particularly geomorphological studies. This is unfortunate because MDE does not require multiple views as references, leading to improvements in both time and efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib11" title="">11</a>]</cite>. MDE was recently advanced with the Vision Transformer (ViT) based Depth anything V2 (DAV2) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib12" title="">12</a>]</cite>. DAV2 outperformed similar Monocular Depth Estimation (MDE) foundation models, providing the most capable zero-shot MDE. DAV2 was benchmarked on the new diverse DA-2K dataset, which includes aerial photos and images with fine details, indicating its high potential for estimating terrain elevation from drones.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Automated sinkhole extraction using DEMs was applied based on topographic computations known as ”fill sinks” algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib13" title="">13</a>]</cite>. Alternatively, authors applied machine learning techniques using geomorphometric variables, such as hillslope gradient and curvature, and topographic position <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib14" title="">14</a>]</cite>, to detect areas prone to sinkholes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The ”fill sinks” method was successfully applied in various studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib15" title="">15</a>]</cite>, but the features extracted by this approach often suffer from overestimation and lack precision in delineating sinkhole boundaries. Consequently, depressions identified from topographic data frequently require refinement, usually through post-processing and visual inspection of RGB imagery by humans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Although may be inaccurate, advanced topographic information can enhance segmentation models, such as Convolutional Neural Networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib17" title="">17</a>]</cite>. CNNs typically capture semantic representations through stacked convolutions and pooling operations, followed by upsampling to restore image size <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib18" title="">18</a>]</cite>. Various encoder-decoder designs were applied to sinkhole segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib19" title="">19</a>]</cite>, using DEMs and RGB images as inputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib20" title="">20</a>]</cite>. Performance of these models depends training datasets quality and the ability to generalize the models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib21" title="">21</a>]</cite>. Due to the diverse appearance of sinkholes in S data, which hampers generalization, these models often struggle to be transferred effectively between different study areas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib22" title="">22</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Recently, foundational zero-shot models have become more commonly used in computer vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib23" title="">23</a>]</cite>, enhancing predictions by generalizing based on learning underlying data concepts and relationships, rather than being limited to detect set of classes from training data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">Meta FAIR (Fundamental AI Research) team gained progress with zero-shot prompt-based image segmentation during 2023-24, garnering considerable attention in RS fields <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib25" title="">25</a>]</cite>. ViT-based SAM may conduct image segmentation with minimal human intervention. SAM only requires a bounding box, or a clicked point, for a prompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib26" title="">26</a>]</cite>. Furthermore, when SAM is guided by models such as YOLO-generated box prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib27" title="">27</a>]</cite>, or is informed by other source to a region of interest, the entire segmentation pipeline can become end-to-end automatically <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib28" title="">28</a>]</cite>.
These advancements in prompt-based classifiers and high-resolution depth estimation allow for the application of a new approach that leverages the close integration of ”fill sinks” computations in image segmentation within an auto-prompt framework (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="277" id="S1.F1.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Venn diagram illustrating the SinkSAM approach of merging Computational Topographic Models (”fill sinks”) and purely learning-based computer vision models, using prompt technology.</figcaption>
</figure>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">Utilizing this synergy, we <span class="ltx_text ltx_font_italic" id="S1.p10.1.1">aim</span> to improve sinkhole segmentation through a novel, fully-automatic framework named SinkSAM. Four operative objectives were set to achieve this aim: (1) automatically refining DEM-based computed closed depression boundaries with prompt-based segmentation of drone RGB imagery, thus addressing current requirements for visual inspection by human annotators; (2) overcoming generalization limitations of purely learning-based models by using a coherent mathematical prompting strategy through non-learnable closed depression computation; (3) comparing a photogrammetric DEM with deep learning-based DAV2 elevation estimation, to integrate the superior method into SinkSAM; and finally, (4) fine-tuning SAM using an established sinkhole dataset to achieve high performance on an unseen study site, in a noisy semiarid environment.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Closed Depressions Computation for Sinkhole delineation</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Topographic depressions, often known as closed depressions, can be extracted by subtracting elevation data of an original DEM from elevation data of a sink-free DEM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib29" title="">29</a>]</cite>. The latter can be created using ”fill sinks” algorithm as described by Planchon and Darboux in 2001 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib30" title="">30</a>]</cite>, Wang and Liu in 2006 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib31" title="">31</a>]</cite>, and by others <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib32" title="">32</a>]</cite>. In particular, the Planchon and Darboux algorithm was implemented in ArcGIS Pro as the ”fill” tool.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">”Fill sinks” was applied in many studies, for different real world conditions and sinkholes origins <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib33" title="">33</a>]</cite>. For example, it was applied to detect karst sinkholes in Kentucky using LiDAR DEMs with 1 m resolution, achieving a detection rate of 97% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib34" title="">34</a>]</cite>. Others, utilized machine learning to improve detection accuracy of karst-origin sinkholes using high-resolution topographic information from LiDAR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib14" title="">14</a>]</cite>. For example, logistic a regression model was applied to a dataset consisting morphometric characteristics derived from LiDAR-based topographic depressions, achieving an area under the curve (AUC) of 0.90 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib35" title="">35</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Fill sinks” was also applied to small soil sinkholes induced by soil piping, in conjunction with derivatives of airborne LiDARs. Success rate was satisfactory on grasslands/pastures (76% for individual forms and 80% for piping systems). However, in areas with rough topography, it yielded lower identification rate for soil pipes (45% for individual forms and 50% for piping systems) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib36" title="">36</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">To summarize, automatic sinkhole extraction methods from DEMs have two main limitations, both result in from the missing RGB information: (1) DEM-based methods tend to delineate non-sinkhole features that have a similar geometric representation in DEMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib15" title="">15</a>]</cite>; and (2) Sinkholes delineated from DEMs often have inaccurate boundaries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib37" title="">37</a>]</cite>, hampered by vegetation, mainly shrubs, within the sinkholes, consequently, DEMs often do not sufficiently represent the actual sinkhole structure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib38" title="">38</a>]</cite>. To improve DEM-based methods, researchers often filter out such noisy features in post-processing with visual inspection of RGB imagery by human annotators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Sinkhole Segmentation</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Sinkhole segmentation was applied with CNN using RGB images<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib39" title="">39</a>]</cite>. In contrast with the topographic computations described in the previous section, which are physically-based, CNN-based segmentation, as a deep learning method, relies on training data quality and model generalization to similar tasks. Soil sinkhole segmentation is challenging compared with that of karst sinkholes, due to their amorphous shapes and the presence of objects in natural scenes that can be mistaken for sinkholes, such as shadows, vegetation, and other dark pixels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib40" title="">40</a>]</cite>. Consequently, studies have reported only moderate accuracy for sinkhole detection, with Intersection over Union (IoU) values—as indicator of how well the model distinguishes objects from background—ranging 20-60%, when evaluated on unseen areas, even for large sinkholes. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib42" title="">42</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Most sinkhole segmentation studies have focused on model’s architecture and input data used during training and testing. Input data included RGB images, DEMs and their derivatives, or combinations of the two sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib43" title="">43</a>]</cite>. For example, the implementation of a simple U-Net architecture has resulted in an IoU of 45.38% and Precision of 66.29% for karst sinkholes segmentation using LiDAR DEM and aerial imagery, with spatial resolution of 1.5 m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib41" title="">41</a>]</cite>. It was also found that elevation gradient images as inputs, outperformed RGB imagery, because the latter provided weak cues for segmenting sinkholes. When applied to salt-induced, well-defined sinkholes, in drylands, an improved multi-class UNet using RGB-only aerial data proved promising, achieving IoU score of 97.08% (Drone) and F1 score of 91.23% (Satellite) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib22" title="">22</a>]</cite>. However, for small soil sinkholes in semiarid regions, the Attention U-Net applied to RGB data, and elevation and its derivatives (e.g., hillshade, slope), from photogrammetric DEM, achieved optimal scores, with a relatively low IoU of 35.27% and a Precision of 53.98% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Monocular Depth Estimation</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Deep Learning-Based Monocular Depth Estimation (MDE) model allows to quantify depth from a single RGB image using training sets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib9" title="">9</a>]</cite>. Most MDEs use an encoder–decoder architecture to minimize loss in training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib44" title="">44</a>]</cite>. Depth estimation relies on cues such as: shading, occlusion, perspective, texture variations, and object scaling, to differentiate and understand the scene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib45" title="">45</a>]</cite>. Monocular depth estimation was used for 3D reconstruction purposes, augmented reality, autonomous driving, and robotics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Due to the limited payload capacity of most drones, they are typically outfitted with only a single camera. This prevents a widespread use of depth perception methods based on LiDAR or photogrammetric DEMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib46" title="">46</a>]</cite>. As an alternative, with the success of CNNs, MDEs have become increasingly reliable <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib47" title="">47</a>]</cite>. MDEs n attention in RS, while most studies were conducted in urban environments aimed at scene reconstruction with buildings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib49" title="">49</a>]</cite>. By contrast, MDEs are still rarely used <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib10" title="">10</a>]</cite>, in studies of open environments and particularly of geomorphological processes in applications such as hillslope erosion processes, stream networks and sinkholes extractions.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Pre-trained Foundation Models have gained popularity in MDE tasks, being trained using large datasets and enabling zero-shot or fine tuning predictions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib50" title="">50</a>]</cite>. In particular, DAV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib12" title="">12</a>]</cite> introduces key advantages: (1) It overcomes limitations of image annotation by involving synthetic annotated data during training with fine detailed annotation; (2) It uses a teacher-student architecture to obtain a lightweight and accurate model. The model is trained on synthetic images. Then the teacher mode generates pseudo masks on unannotated real images. Finally, the smaller student model is trained on these pseudo-annotated images; (3) Its annotation pipeline incorporates AM for image sampling, utilizing a combination of real- and pseudo-labeled depth maps, from 62 million images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib51" title="">51</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">DAV2 is benchmarked on standard datasets e.g., KITTI, Sintel, and more, outperforming previous Depth Anything version and the MiDas V3.1 from intel labs research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib50" title="">50</a>]</cite>. A key highlight is DAV2’s superior performance, when evaluated on the DA-2K dataset, which includes eight different image categories, 9% of which are aerial imagery and fine-detail images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib12" title="">12</a>]</cite>. This indicates potential for terrain elevation estimation also from drone imagery. DAV2 was applied in a few studies of the open environment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib52" title="">52</a>]</cite>, for example, for agricultural canopy height measurements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib53" title="">53</a>]</cite>, providing a high performance solution, surpassing the current state-of-the-art.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Segment Anything Model</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">SAM is an encoder-decoder image segmentation model. SAM’s encoder is a pre-trained Vision Transformer (ViT) with weights and biases trained by 1 Billion Mask (SA-1B) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib26" title="">26</a>]</cite>. Differently from traditional encoder-decoder models, SAM includes a prompt encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib19" title="">19</a>]</cite> that is designed to use a human input of a point, or a bounding box, mapped as a vector embedding along with the image. SAM introduces three main advancements: (1) Promptable segmentation: SAM produces high-quality object masks from input prompts such as points or boxes; (2) It was trained on a dataset of 11 million images and 1.1 billion masks, created automatically by the model itself; (3) SAM exhibits strong zero-shot performance across a variety of segmentation tasks.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">SAM was applied to image processing tasks, including medical image <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib54" title="">54</a>]</cite>, shadow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib55" title="">55</a>]</cite>, and electron microscopy segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib56" title="">56</a>]</cite> crack segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib57" title="">57</a>]</cite> and road pit detection. In RS, SAM was also used for geological features analysis on planetary bodies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib58" title="">58</a>]</cite>, glacier segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib59" title="">59</a>]</cite>, road mapping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib60" title="">60</a>]</cite>, building <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib61" title="">61</a>]</cite> and solar panels segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib62" title="">62</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">SAM models are trained and tested mostly by ground photographs and videos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib63" title="">63</a>]</cite>. Due to unique properties of RS images, SAM still faces challenges when applied to RS in a zero-shot manner. When dealing with small objects, SAM may overestimate object size by including shadows in the segmented regions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib64" title="">64</a>]</cite>. To achieve better performance on RS data, SAM needs to be fine-tuned to specific tasks. Fine-tuned variants of SAM perform better on aerial and satellite datasets compared with a non-fine-tuned SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib66" title="">66</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.p4">
<p class="ltx_p" id="S2.SS4.p4.1">Furthermore, when SAM is guided by computer vision prompts or other prompt generator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib27" title="">27</a>]</cite>, the entire process becomes automated. For example, YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib68" title="">68</a>]</cite> and Grounding DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib69" title="">69</a>]</cite> models were used to generate boxes to prompt SAM. Similarly, SAM may be informed by a non-learnable approach such as Topological Data Analysis (TDA) for optimized prompts with a reduced number of points to focus on regions of interest instead of applying a uniform grid across the entire cell in biological imaging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib70" title="">70</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Materials and Methodology</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Study Area and Dataset</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">SinkSAM was applied to data from three agricultural catchments, in the northwestern Negev desert, east of the Mediterranean Sea coastline of Israel (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S3.F2" title="Figure 2 ‣ III-A Study Area and Dataset ‣ III Materials and Methodology ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>). The climate is semiarid with mean annual rainfall of 292 mm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib71" title="">71</a>]</cite>. Soils are Late Pleistocene loess, with a sandy loam/silt loam texture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib72" title="">72</a>]</cite>. Piping processes damaged many agricultural fields by induced sinkholes, resulting in a withdrawn of large areas from the cultivation cycle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib73" title="">73</a>]</cite>. Sinkholes vary in their morphology, becoming irregular features with various dimensions (Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S3.F3" title="Figure 3 ‣ III-A Study Area and Dataset ‣ III Materials and Methodology ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>). Some sinkhole’s floors are covered by vegetation causing internal and external shadowing effects. The immediate surrounding of the sinkholes, being parts of agricultural fields, changes seasonally.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.2">RGB data were collected by a drone in three sinkhole-prone subcatchmets: Tel Gama (with N=121 soil sinkholes); Asaf (N=226); and Yaen (N=216). The drone survey was conducted using a DJI Phantom 4, equipped with Real-time Kinematic (RTK) technology, providing accurate images location and navigation data. The drone carried a 12 MP RGB camera and images were captured with 60% overlap from altitudes ranging 30-40 m to produce an orthomosaic and a DEM, using photogrammetry Agisoft Metashape with a pixel length of 0.025 m. Tel Gama and Asaf, containing approximately 350 sinkholes, were used as the training and validation sets, while Yaen, an unseen site, was reserved for testing. All ortomosaics were patchified to <math alttext="512^{2}\,\text{px}^{2}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><msup id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml"><mn id="S3.SS1.p2.1.m1.1.1.2.2" xref="S3.SS1.p2.1.m1.1.1.2.2.cmml">512</mn><mn id="S3.SS1.p2.1.m1.1.1.2.3" xref="S3.SS1.p2.1.m1.1.1.2.3.cmml">2</mn></msup><mo id="S3.SS1.p2.1.m1.1.1.1" lspace="0.170em" xref="S3.SS1.p2.1.m1.1.1.1.cmml">⁢</mo><msup id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><mtext id="S3.SS1.p2.1.m1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.3.2a.cmml">px</mtext><mn id="S3.SS1.p2.1.m1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><times id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></times><apply id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.2.1.cmml" xref="S3.SS1.p2.1.m1.1.1.2">superscript</csymbol><cn id="S3.SS1.p2.1.m1.1.1.2.2.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.2.2">512</cn><cn id="S3.SS1.p2.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.2.3">2</cn></apply><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.3.2a.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2"><mtext id="S3.SS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2">px</mtext></ci><cn id="S3.SS1.p2.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">512^{2}\,\text{px}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">512 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT px start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> patches (<math alttext="12^{2}\,\text{m}^{2}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><msup id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml"><mn id="S3.SS1.p2.2.m2.1.1.2.2" xref="S3.SS1.p2.2.m2.1.1.2.2.cmml">12</mn><mn id="S3.SS1.p2.2.m2.1.1.2.3" xref="S3.SS1.p2.2.m2.1.1.2.3.cmml">2</mn></msup><mo id="S3.SS1.p2.2.m2.1.1.1" lspace="0.170em" xref="S3.SS1.p2.2.m2.1.1.1.cmml">⁢</mo><msup id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mtext id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2a.cmml">m</mtext><mn id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><times id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></times><apply id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1.2">superscript</csymbol><cn id="S3.SS1.p2.2.m2.1.1.2.2.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.2.2">12</cn><cn id="S3.SS1.p2.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.2.3">2</cn></apply><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.3.2a.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2"><mtext id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">m</mtext></ci><cn id="S3.SS1.p2.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">12^{2}\,\text{m}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">12 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>). Patches were overlaid by 50% (256 px stride) to ensure entire capture of sinkholes, at least in one patch.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="567" id="S3.F2.g1" src="extracted/5895157/study_area_with_sites.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The study area is located in the northwestern part of the Negev region, Israel. As shown below, the drone orthomosaic covers three sites prone to soil piping: Tel Gama and Asaf (Training), and Yaen (Test).</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="592" id="S3.F3.g1" src="extracted/5895157/estiamated_depth1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Input data: Patches of RGB, ground truth annotated sinkholes, photogrammetric DEM and monocular depth estimated by DAV2. As can be seen in this patch, DAV2 correctly delineates sinkhole boundaries, while in photogrammetric DEMs, small sinkholes are neglected.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">SinkSAM Framework Overview</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The newly developed SinkSAM framework, for sinkhole detection and segmentation, is illustrated in Fig. 4. The framework consists four stages: <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Stage 1</span>: depth estimation using DAV2 from a single RGB image; <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">Stage 2</span>: delineation of closed depressions. Done by: (1) ”fill sinks” execution; and (2) the subtraction of depth data of ”as is” raster, from the ”filled sinks” raster, using for both the depth data from stage 1; <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.3">Stage 3</span>: Prompts generation: a threshold is assigned to remove small sinks, and create bounding boxes from the depressions created in stage 2 for SAM usage in stage 4. Finally, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.4">stage 4</span>: SAM utilizes an image encoder, and tuned mask decoder, for final segmentation of sinkholes.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Stage one - Monocular Depth Estimation:</span>
Orthomosaic patches were processed using pre-trained DAV2 model, based on a zero-shot model, with non-metric mode. Large Vision Transformer (ViT-l) encoder was used to achieve optimal accuracy in depth estimation. The patches preserved the same resolution, and geographic data, from the original RGB images, to ensure optimal alignment. We found, by simulations, that patch size of <math alttext="512^{2}\,\text{px}^{2}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><msup id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.2.2.cmml">512</mn><mn id="S3.SS2.p2.1.m1.1.1.2.3" xref="S3.SS2.p2.1.m1.1.1.2.3.cmml">2</mn></msup><mo id="S3.SS2.p2.1.m1.1.1.1" lspace="0.170em" xref="S3.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><msup id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mtext id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2a.cmml">px</mtext><mn id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><apply id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.2">superscript</csymbol><cn id="S3.SS2.p2.1.m1.1.1.2.2.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.2.2">512</cn><cn id="S3.SS2.p2.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.2.3">2</cn></apply><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.2a.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2"><mtext id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">px</mtext></ci><cn id="S3.SS2.p2.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">512^{2}\,\text{px}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">512 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT px start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> is optimal for DAV2 estimation (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S3.F3" title="Figure 3 ‣ III-A Study Area and Dataset ‣ III Materials and Methodology ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="342" id="S3.F4.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>SinkSAM framework: <span class="ltx_text ltx_font_italic" id="S3.F4.5.1">Stage 1</span>: Depth estimation from an RGB image using DAV2. <span class="ltx_text ltx_font_italic" id="S3.F4.6.2">Stage 2</span>: ”Fill sinks” techniques and a substraction of estimated depth from a sink-free raster resulting in delineation of closed depression. <span class="ltx_text ltx_font_italic" id="S3.F4.7.3">Stage 3</span>: Prompts generation: a threshold value is used to remove small sinks and create bounding boxes for SAM. Finally, at <span class="ltx_text ltx_font_italic" id="S3.F4.8.4">Stage 4</span>, the SAM tuned model utilizes an image encoder and mask decoder to create a final sinkholes map.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Stage two - Closed Depressions Computation:</span>
The ArcGIS Pro ”Fill” tool <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib74" title="">74</a>]</cite>, was applied to fill all closed depressions of the DAV2 surface, up to their spill elevation (i.e. elevation at which water ideally flows out of the depression). This created a depression-less Depth map. The original DAV2 Depth data was then subtracted from the sinks-less raster to generate a difference raster representing depression location and depth.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Stage three - Prompts generation:</span>
The subtracted layer was filtered using two threshold values to erase small depressions. The threshold values were: depth &lt;2 (non-metric value) or area &lt;50 pixels. The subtracted rasters that were then unpatchified after filtering, reproduced a mosaic of closed depressions. Both raster layers, the RGB orthomosaic and the closed depression map, were then patchified again. Closed depressions in each patch were used to generate bounding boxes, in the next step to be used as SAM input along with corresponding RGB data.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">Stage four - Segment Anything Model (SAM):</span>
RGB patches were processed by SAM, using the corresponding prompt box extracted from the depression layer (Stage 2). Masks were generated for each bounding box separately, and then all image predictions were combined. If more than one object appeared in a single image, predictions were combined by selecting pixels with highest foreground probability. The final evaluation has resulted in a referenced GIS layer of soil sinkholes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Experimental Setup</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To evaluate SinkSAM’s performance in sinkhole detection and segmentation, we compared it with other methods, on real world data, from an untested area. The experiment includes four comparisons that were specifically designed to test the performance of each of the four SinkSAM stages: (A) closed depressions, identified through the DEM-based closed depression method, and obtained at stage 2, before being used as prompts in SAM, are compared with sinkholes predicted by SAM, using these prompts. This comparison tests the entire SinkSAM framework against a DEM-based method; (B) prompt source: SAM prompted by closed depressions bounding boxes are compared with SAM prompted by YOLOv9 bounding boxes; (C) comparing DAV2 vs photogrammetric DEMs as elevation/depth input layers for the ”fill sinks” algorithm, for testing their performance in sinkhole detection/segmentation accuracy; finally, (D) performance of SAM is compared with performance of tuned SinkSAM using bounding box prompts derived from DAV2 closed depressions.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Comparison between pairs</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Comparison A - Computed Closed Depressions vs SAM:</span> This comparison was applied to validate the entire framework. It compares the depressions identified using the closed depression method by ”fill sinks” with the final segmentation results from SAM. The aim is to determine whether image segmentation with SAM has added value within the integrated framework or the topographic computation is sufficient. Closed depressions, derived solely from depth data, were enclosed in bounding boxes and fed into SAM. This comparison allows to quantify SAM’s role in refining the identified closed depressions.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="281" id="S4.F5.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Experimental setup: Each comparison was designed to test the performance of SinkSAM Framework: (A) closed depressions, identified through the Depth-based closed depression method vs sinkholes predicted by SAM, using these prompts. This comparison tests the entire SinkSAM framework against DEM-based method; (B) prompt source: SAM prompted by closed depressions bounding boxes vs SAM prompted by YOLOv9 bounding boxes; (C) comparing DAV2 vs photogrammetric DEMs as elevation/depth input layers for the ”fill sinks” algorithm; finally, (D) performance of SAM is compared with performance of tuned SinkSAM using bounding box prompts derived from DAV2 closed depressions (CDs - Closed Depressions). </figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Comparison B - Prompts Source:</span> Two approaches for prompt generation were tested: (1) a learning-based model using NN; and (2) a non-learning, computation-based method, employing the ”fill sinks” algorithm. For the learning-based approach, we used the widely-adopted YOLO end-to-end CNN for object detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib75" title="">75</a>]</cite>. The largest YOLOv9 model (YOLOv9e) was used to generate bounding boxes. YOLO was trained on data from Asaf and Tel Gama sites. These bounding boxes were used to prompt SAM, with a confidence threshold set at 0.2, which was found here optimal for this task.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Comparison C - Depth Data:</span> To evaluate the first stage of SinkSAM, we compared DAV2 and a photogrammetric DEM as depth estimators. Both sources were tested for their accuracy in sinkholes segmentation based on computed depressions. Namely, we applied the ”fill sinks” algorithm to extract closed depressions and assessed their quality in terms of sinkhole delineation. The extracted depressions were filtered by area and depth, removing features with area &lt;50 pixels. For depth, photogrammetric DEMs were thresholded at 2 cm, while for DAV2 we used a non-metric threshold of 2. This comparison did not involve variations in SAM configurations but we tested which depth source is more effective for bounding boxes usage by SAM.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Comparison D - SAM Decoder:</span> Zero-shot SAM and tuned SAM (SinkSAM) were executed, using bounding box prompts derived from DAV2 closed depressions. The goal was to quantify the impact of tuned SAM decoder, which was trained using our sinkhole inventory from the unseen Yaen site. The experimental setup allows for a comparison of each implementation configuration, with the better-performing components being integrated into the final SinkSAM framework
(Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S4.F5" title="Figure 5 ‣ IV-B Comparison between pairs ‣ IV Experiments ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="303" id="S4.F6.g1" src="extracted/5895157/maps.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Evaluated maps of sinkhole segmentation on the unseen Yaen site: It is visually evident that the photogrammetric closed depressions miss parts of the ground truth sinkholes, while DAV2 closed depressions delineate sinkholes more precisely. Regarding the segmentation models, SAM prompted by YOLO overestimates by segmenting parts of the background as sinkholes. Whereas, the SinkSAM map (with a tuned decoder) is the most similar to the ground truth map.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Implementation Details</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">Training Setting:</span> SAM was fine-tuned using RGB images and annotated sinkholes, along with bounding boxes generated as masks to guide it during training. The model was fine-tuned using the large pretrained encoder (ViT-l). During the fine-tuning process, only parameters of SAM decoder were updated to adapt SAM to the sinkhole dataset. Freezing the encoder saves computational resources and allows larger batch size. In this fine-tuning process, we replicated the MedSAM project, which fine-tunes SAM on a dataset of medical images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib76" title="">76</a>]</cite>, and was inspired by the Encord AI teams <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib77" title="">77</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">Loss Function:</span>
A combination of the Binary Cross-Entropy Loss (L<sub class="ltx_sub" id="S4.SS3.p2.1.2">BCE</sub>) and Dice Loss (L<sub class="ltx_sub" id="S4.SS3.p2.1.3">Dice</sub>) was employed for training SAM mask decoder, as can seen in Eq. 1:</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Sx1.EGx1">
<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle L=L_{BCE}+L_{Dice}" class="ltx_Math" display="inline" id="S4.E1.m1.1"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mi id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml">L</mi><mo id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><msub id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml"><mi id="S4.E1.m1.1.1.3.2.2" xref="S4.E1.m1.1.1.3.2.2.cmml">L</mi><mrow id="S4.E1.m1.1.1.3.2.3" xref="S4.E1.m1.1.1.3.2.3.cmml"><mi id="S4.E1.m1.1.1.3.2.3.2" xref="S4.E1.m1.1.1.3.2.3.2.cmml">B</mi><mo id="S4.E1.m1.1.1.3.2.3.1" xref="S4.E1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.3.2.3.3" xref="S4.E1.m1.1.1.3.2.3.3.cmml">C</mi><mo id="S4.E1.m1.1.1.3.2.3.1a" xref="S4.E1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.3.2.3.4" xref="S4.E1.m1.1.1.3.2.3.4.cmml">E</mi></mrow></msub><mo id="S4.E1.m1.1.1.3.1" xref="S4.E1.m1.1.1.3.1.cmml">+</mo><msub id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.2" xref="S4.E1.m1.1.1.3.3.2.cmml">L</mi><mrow id="S4.E1.m1.1.1.3.3.3" xref="S4.E1.m1.1.1.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.2.cmml">D</mi><mo id="S4.E1.m1.1.1.3.3.3.1" xref="S4.E1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.3.3.3.3" xref="S4.E1.m1.1.1.3.3.3.3.cmml">i</mi><mo id="S4.E1.m1.1.1.3.3.3.1a" xref="S4.E1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.3.3.3.4" xref="S4.E1.m1.1.1.3.3.3.4.cmml">c</mi><mo id="S4.E1.m1.1.1.3.3.3.1b" xref="S4.E1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.3.3.3.5" xref="S4.E1.m1.1.1.3.3.3.5.cmml">e</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"></eq><ci id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2">𝐿</ci><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><plus id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3.1"></plus><apply id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2">𝐿</ci><apply id="S4.E1.m1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.3.2.3"><times id="S4.E1.m1.1.1.3.2.3.1.cmml" xref="S4.E1.m1.1.1.3.2.3.1"></times><ci id="S4.E1.m1.1.1.3.2.3.2.cmml" xref="S4.E1.m1.1.1.3.2.3.2">𝐵</ci><ci id="S4.E1.m1.1.1.3.2.3.3.cmml" xref="S4.E1.m1.1.1.3.2.3.3">𝐶</ci><ci id="S4.E1.m1.1.1.3.2.3.4.cmml" xref="S4.E1.m1.1.1.3.2.3.4">𝐸</ci></apply></apply><apply id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2">𝐿</ci><apply id="S4.E1.m1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3"><times id="S4.E1.m1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.1"></times><ci id="S4.E1.m1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.2">𝐷</ci><ci id="S4.E1.m1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3">𝑖</ci><ci id="S4.E1.m1.1.1.3.3.3.4.cmml" xref="S4.E1.m1.1.1.3.3.3.4">𝑐</ci><ci id="S4.E1.m1.1.1.3.3.3.5.cmml" xref="S4.E1.m1.1.1.3.3.3.5">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\displaystyle L=L_{BCE}+L_{Dice}</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.1d">italic_L = italic_L start_POSTSUBSCRIPT italic_B italic_C italic_E end_POSTSUBSCRIPT + italic_L start_POSTSUBSCRIPT italic_D italic_i italic_c italic_e end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.1">Evaluation Metrics:</span> To assess models performance, we employed the widely recognized evaluation metrics in Eq. 2:</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath ltx_font_italic" id="S4.E2X.2.1.1.1">Accuracy</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{TP+TN}{TP+FN+TN+FP}" class="ltx_Math" display="inline" id="S4.E2X.3.2.2.m1.1"><semantics id="S4.E2X.3.2.2.m1.1a"><mrow id="S4.E2X.3.2.2.m1.1.1" xref="S4.E2X.3.2.2.m1.1.1.cmml"><mi id="S4.E2X.3.2.2.m1.1.1.2" xref="S4.E2X.3.2.2.m1.1.1.2.cmml"></mi><mo id="S4.E2X.3.2.2.m1.1.1.1" xref="S4.E2X.3.2.2.m1.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S4.E2X.3.2.2.m1.1.1.3" xref="S4.E2X.3.2.2.m1.1.1.3.cmml"><mfrac id="S4.E2X.3.2.2.m1.1.1.3a" xref="S4.E2X.3.2.2.m1.1.1.3.cmml"><mrow id="S4.E2X.3.2.2.m1.1.1.3.2" xref="S4.E2X.3.2.2.m1.1.1.3.2.cmml"><mrow id="S4.E2X.3.2.2.m1.1.1.3.2.2" xref="S4.E2X.3.2.2.m1.1.1.3.2.2.cmml"><mi id="S4.E2X.3.2.2.m1.1.1.3.2.2.2" xref="S4.E2X.3.2.2.m1.1.1.3.2.2.2.cmml">T</mi><mo id="S4.E2X.3.2.2.m1.1.1.3.2.2.1" xref="S4.E2X.3.2.2.m1.1.1.3.2.2.1.cmml">⁢</mo><mi id="S4.E2X.3.2.2.m1.1.1.3.2.2.3" xref="S4.E2X.3.2.2.m1.1.1.3.2.2.3.cmml">P</mi></mrow><mo id="S4.E2X.3.2.2.m1.1.1.3.2.1" xref="S4.E2X.3.2.2.m1.1.1.3.2.1.cmml">+</mo><mrow id="S4.E2X.3.2.2.m1.1.1.3.2.3" xref="S4.E2X.3.2.2.m1.1.1.3.2.3.cmml"><mi id="S4.E2X.3.2.2.m1.1.1.3.2.3.2" xref="S4.E2X.3.2.2.m1.1.1.3.2.3.2.cmml">T</mi><mo id="S4.E2X.3.2.2.m1.1.1.3.2.3.1" xref="S4.E2X.3.2.2.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E2X.3.2.2.m1.1.1.3.2.3.3" xref="S4.E2X.3.2.2.m1.1.1.3.2.3.3.cmml">N</mi></mrow></mrow><mrow id="S4.E2X.3.2.2.m1.1.1.3.3" xref="S4.E2X.3.2.2.m1.1.1.3.3.cmml"><mrow id="S4.E2X.3.2.2.m1.1.1.3.3.2" xref="S4.E2X.3.2.2.m1.1.1.3.3.2.cmml"><mi id="S4.E2X.3.2.2.m1.1.1.3.3.2.2" xref="S4.E2X.3.2.2.m1.1.1.3.3.2.2.cmml">T</mi><mo id="S4.E2X.3.2.2.m1.1.1.3.3.2.1" xref="S4.E2X.3.2.2.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E2X.3.2.2.m1.1.1.3.3.2.3" xref="S4.E2X.3.2.2.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S4.E2X.3.2.2.m1.1.1.3.3.1" xref="S4.E2X.3.2.2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E2X.3.2.2.m1.1.1.3.3.3" xref="S4.E2X.3.2.2.m1.1.1.3.3.3.cmml"><mi id="S4.E2X.3.2.2.m1.1.1.3.3.3.2" xref="S4.E2X.3.2.2.m1.1.1.3.3.3.2.cmml">F</mi><mo id="S4.E2X.3.2.2.m1.1.1.3.3.3.1" xref="S4.E2X.3.2.2.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E2X.3.2.2.m1.1.1.3.3.3.3" xref="S4.E2X.3.2.2.m1.1.1.3.3.3.3.cmml">N</mi></mrow><mo id="S4.E2X.3.2.2.m1.1.1.3.3.1a" xref="S4.E2X.3.2.2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E2X.3.2.2.m1.1.1.3.3.4" xref="S4.E2X.3.2.2.m1.1.1.3.3.4.cmml"><mi id="S4.E2X.3.2.2.m1.1.1.3.3.4.2" xref="S4.E2X.3.2.2.m1.1.1.3.3.4.2.cmml">T</mi><mo id="S4.E2X.3.2.2.m1.1.1.3.3.4.1" xref="S4.E2X.3.2.2.m1.1.1.3.3.4.1.cmml">⁢</mo><mi id="S4.E2X.3.2.2.m1.1.1.3.3.4.3" xref="S4.E2X.3.2.2.m1.1.1.3.3.4.3.cmml">N</mi></mrow><mo id="S4.E2X.3.2.2.m1.1.1.3.3.1b" xref="S4.E2X.3.2.2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E2X.3.2.2.m1.1.1.3.3.5" xref="S4.E2X.3.2.2.m1.1.1.3.3.5.cmml"><mi id="S4.E2X.3.2.2.m1.1.1.3.3.5.2" xref="S4.E2X.3.2.2.m1.1.1.3.3.5.2.cmml">F</mi><mo id="S4.E2X.3.2.2.m1.1.1.3.3.5.1" xref="S4.E2X.3.2.2.m1.1.1.3.3.5.1.cmml">⁢</mo><mi id="S4.E2X.3.2.2.m1.1.1.3.3.5.3" xref="S4.E2X.3.2.2.m1.1.1.3.3.5.3.cmml">P</mi></mrow></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S4.E2X.3.2.2.m1.1b"><apply id="S4.E2X.3.2.2.m1.1.1.cmml" xref="S4.E2X.3.2.2.m1.1.1"><eq id="S4.E2X.3.2.2.m1.1.1.1.cmml" xref="S4.E2X.3.2.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S4.E2X.3.2.2.m1.1.1.2.cmml" xref="S4.E2X.3.2.2.m1.1.1.2">absent</csymbol><apply id="S4.E2X.3.2.2.m1.1.1.3.cmml" xref="S4.E2X.3.2.2.m1.1.1.3"><divide id="S4.E2X.3.2.2.m1.1.1.3.1.cmml" xref="S4.E2X.3.2.2.m1.1.1.3"></divide><apply id="S4.E2X.3.2.2.m1.1.1.3.2.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.2"><plus id="S4.E2X.3.2.2.m1.1.1.3.2.1.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.2.1"></plus><apply id="S4.E2X.3.2.2.m1.1.1.3.2.2.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.2.2"><times id="S4.E2X.3.2.2.m1.1.1.3.2.2.1.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.2.2.1"></times><ci id="S4.E2X.3.2.2.m1.1.1.3.2.2.2.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.2.2.2">𝑇</ci><ci id="S4.E2X.3.2.2.m1.1.1.3.2.2.3.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.2.2.3">𝑃</ci></apply><apply id="S4.E2X.3.2.2.m1.1.1.3.2.3.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.2.3"><times id="S4.E2X.3.2.2.m1.1.1.3.2.3.1.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.2.3.1"></times><ci id="S4.E2X.3.2.2.m1.1.1.3.2.3.2.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.2.3.2">𝑇</ci><ci id="S4.E2X.3.2.2.m1.1.1.3.2.3.3.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.2.3.3">𝑁</ci></apply></apply><apply id="S4.E2X.3.2.2.m1.1.1.3.3.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3"><plus id="S4.E2X.3.2.2.m1.1.1.3.3.1.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.1"></plus><apply id="S4.E2X.3.2.2.m1.1.1.3.3.2.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.2"><times id="S4.E2X.3.2.2.m1.1.1.3.3.2.1.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.2.1"></times><ci id="S4.E2X.3.2.2.m1.1.1.3.3.2.2.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.2.2">𝑇</ci><ci id="S4.E2X.3.2.2.m1.1.1.3.3.2.3.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S4.E2X.3.2.2.m1.1.1.3.3.3.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.3"><times id="S4.E2X.3.2.2.m1.1.1.3.3.3.1.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.3.1"></times><ci id="S4.E2X.3.2.2.m1.1.1.3.3.3.2.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.3.2">𝐹</ci><ci id="S4.E2X.3.2.2.m1.1.1.3.3.3.3.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.3.3">𝑁</ci></apply><apply id="S4.E2X.3.2.2.m1.1.1.3.3.4.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.4"><times id="S4.E2X.3.2.2.m1.1.1.3.3.4.1.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.4.1"></times><ci id="S4.E2X.3.2.2.m1.1.1.3.3.4.2.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.4.2">𝑇</ci><ci id="S4.E2X.3.2.2.m1.1.1.3.3.4.3.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.4.3">𝑁</ci></apply><apply id="S4.E2X.3.2.2.m1.1.1.3.3.5.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.5"><times id="S4.E2X.3.2.2.m1.1.1.3.3.5.1.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.5.1"></times><ci id="S4.E2X.3.2.2.m1.1.1.3.3.5.2.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.5.2">𝐹</ci><ci id="S4.E2X.3.2.2.m1.1.1.3.3.5.3.cmml" xref="S4.E2X.3.2.2.m1.1.1.3.3.5.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2X.3.2.2.m1.1c">\displaystyle=\frac{TP+TN}{TP+FN+TN+FP}</annotation><annotation encoding="application/x-llamapun" id="S4.E2X.3.2.2.m1.1d">= divide start_ARG italic_T italic_P + italic_T italic_N end_ARG start_ARG italic_T italic_P + italic_F italic_N + italic_T italic_N + italic_F italic_P end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="5"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E2Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath ltx_font_italic" id="S4.E2Xa.2.1.1.1">Precision</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{TP}{TP+FP}" class="ltx_Math" display="inline" id="S4.E2Xa.3.2.2.m1.1"><semantics id="S4.E2Xa.3.2.2.m1.1a"><mrow id="S4.E2Xa.3.2.2.m1.1.1" xref="S4.E2Xa.3.2.2.m1.1.1.cmml"><mi id="S4.E2Xa.3.2.2.m1.1.1.2" xref="S4.E2Xa.3.2.2.m1.1.1.2.cmml"></mi><mo id="S4.E2Xa.3.2.2.m1.1.1.1" xref="S4.E2Xa.3.2.2.m1.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S4.E2Xa.3.2.2.m1.1.1.3" xref="S4.E2Xa.3.2.2.m1.1.1.3.cmml"><mfrac id="S4.E2Xa.3.2.2.m1.1.1.3a" xref="S4.E2Xa.3.2.2.m1.1.1.3.cmml"><mrow id="S4.E2Xa.3.2.2.m1.1.1.3.2" xref="S4.E2Xa.3.2.2.m1.1.1.3.2.cmml"><mi id="S4.E2Xa.3.2.2.m1.1.1.3.2.2" xref="S4.E2Xa.3.2.2.m1.1.1.3.2.2.cmml">T</mi><mo id="S4.E2Xa.3.2.2.m1.1.1.3.2.1" xref="S4.E2Xa.3.2.2.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S4.E2Xa.3.2.2.m1.1.1.3.2.3" xref="S4.E2Xa.3.2.2.m1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S4.E2Xa.3.2.2.m1.1.1.3.3" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.cmml"><mrow id="S4.E2Xa.3.2.2.m1.1.1.3.3.2" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.2.cmml"><mi id="S4.E2Xa.3.2.2.m1.1.1.3.3.2.2" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.2.2.cmml">T</mi><mo id="S4.E2Xa.3.2.2.m1.1.1.3.3.2.1" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E2Xa.3.2.2.m1.1.1.3.3.2.3" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S4.E2Xa.3.2.2.m1.1.1.3.3.1" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E2Xa.3.2.2.m1.1.1.3.3.3" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.3.cmml"><mi id="S4.E2Xa.3.2.2.m1.1.1.3.3.3.2" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.3.2.cmml">F</mi><mo id="S4.E2Xa.3.2.2.m1.1.1.3.3.3.1" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E2Xa.3.2.2.m1.1.1.3.3.3.3" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.3.3.cmml">P</mi></mrow></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S4.E2Xa.3.2.2.m1.1b"><apply id="S4.E2Xa.3.2.2.m1.1.1.cmml" xref="S4.E2Xa.3.2.2.m1.1.1"><eq id="S4.E2Xa.3.2.2.m1.1.1.1.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S4.E2Xa.3.2.2.m1.1.1.2.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.2">absent</csymbol><apply id="S4.E2Xa.3.2.2.m1.1.1.3.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3"><divide id="S4.E2Xa.3.2.2.m1.1.1.3.1.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3"></divide><apply id="S4.E2Xa.3.2.2.m1.1.1.3.2.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.2"><times id="S4.E2Xa.3.2.2.m1.1.1.3.2.1.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.2.1"></times><ci id="S4.E2Xa.3.2.2.m1.1.1.3.2.2.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.2.2">𝑇</ci><ci id="S4.E2Xa.3.2.2.m1.1.1.3.2.3.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.2.3">𝑃</ci></apply><apply id="S4.E2Xa.3.2.2.m1.1.1.3.3.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.3"><plus id="S4.E2Xa.3.2.2.m1.1.1.3.3.1.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.1"></plus><apply id="S4.E2Xa.3.2.2.m1.1.1.3.3.2.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.2"><times id="S4.E2Xa.3.2.2.m1.1.1.3.3.2.1.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.2.1"></times><ci id="S4.E2Xa.3.2.2.m1.1.1.3.3.2.2.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.2.2">𝑇</ci><ci id="S4.E2Xa.3.2.2.m1.1.1.3.3.2.3.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S4.E2Xa.3.2.2.m1.1.1.3.3.3.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.3"><times id="S4.E2Xa.3.2.2.m1.1.1.3.3.3.1.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.3.1"></times><ci id="S4.E2Xa.3.2.2.m1.1.1.3.3.3.2.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.3.2">𝐹</ci><ci id="S4.E2Xa.3.2.2.m1.1.1.3.3.3.3.cmml" xref="S4.E2Xa.3.2.2.m1.1.1.3.3.3.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2Xa.3.2.2.m1.1c">\displaystyle=\frac{TP}{TP+FP}</annotation><annotation encoding="application/x-llamapun" id="S4.E2Xa.3.2.2.m1.1d">= divide start_ARG italic_T italic_P end_ARG start_ARG italic_T italic_P + italic_F italic_P end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E2Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath ltx_font_italic" id="S4.E2Xb.2.1.1.1">Recall</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{TP}{TP+FN}" class="ltx_Math" display="inline" id="S4.E2Xb.3.2.2.m1.1"><semantics id="S4.E2Xb.3.2.2.m1.1a"><mrow id="S4.E2Xb.3.2.2.m1.1.1" xref="S4.E2Xb.3.2.2.m1.1.1.cmml"><mi id="S4.E2Xb.3.2.2.m1.1.1.2" xref="S4.E2Xb.3.2.2.m1.1.1.2.cmml"></mi><mo id="S4.E2Xb.3.2.2.m1.1.1.1" xref="S4.E2Xb.3.2.2.m1.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S4.E2Xb.3.2.2.m1.1.1.3" xref="S4.E2Xb.3.2.2.m1.1.1.3.cmml"><mfrac id="S4.E2Xb.3.2.2.m1.1.1.3a" xref="S4.E2Xb.3.2.2.m1.1.1.3.cmml"><mrow id="S4.E2Xb.3.2.2.m1.1.1.3.2" xref="S4.E2Xb.3.2.2.m1.1.1.3.2.cmml"><mi id="S4.E2Xb.3.2.2.m1.1.1.3.2.2" xref="S4.E2Xb.3.2.2.m1.1.1.3.2.2.cmml">T</mi><mo id="S4.E2Xb.3.2.2.m1.1.1.3.2.1" xref="S4.E2Xb.3.2.2.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S4.E2Xb.3.2.2.m1.1.1.3.2.3" xref="S4.E2Xb.3.2.2.m1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S4.E2Xb.3.2.2.m1.1.1.3.3" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.cmml"><mrow id="S4.E2Xb.3.2.2.m1.1.1.3.3.2" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.2.cmml"><mi id="S4.E2Xb.3.2.2.m1.1.1.3.3.2.2" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.2.2.cmml">T</mi><mo id="S4.E2Xb.3.2.2.m1.1.1.3.3.2.1" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E2Xb.3.2.2.m1.1.1.3.3.2.3" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S4.E2Xb.3.2.2.m1.1.1.3.3.1" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E2Xb.3.2.2.m1.1.1.3.3.3" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.3.cmml"><mi id="S4.E2Xb.3.2.2.m1.1.1.3.3.3.2" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.3.2.cmml">F</mi><mo id="S4.E2Xb.3.2.2.m1.1.1.3.3.3.1" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S4.E2Xb.3.2.2.m1.1.1.3.3.3.3" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.3.3.cmml">N</mi></mrow></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S4.E2Xb.3.2.2.m1.1b"><apply id="S4.E2Xb.3.2.2.m1.1.1.cmml" xref="S4.E2Xb.3.2.2.m1.1.1"><eq id="S4.E2Xb.3.2.2.m1.1.1.1.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S4.E2Xb.3.2.2.m1.1.1.2.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.2">absent</csymbol><apply id="S4.E2Xb.3.2.2.m1.1.1.3.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3"><divide id="S4.E2Xb.3.2.2.m1.1.1.3.1.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3"></divide><apply id="S4.E2Xb.3.2.2.m1.1.1.3.2.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.2"><times id="S4.E2Xb.3.2.2.m1.1.1.3.2.1.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.2.1"></times><ci id="S4.E2Xb.3.2.2.m1.1.1.3.2.2.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.2.2">𝑇</ci><ci id="S4.E2Xb.3.2.2.m1.1.1.3.2.3.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.2.3">𝑃</ci></apply><apply id="S4.E2Xb.3.2.2.m1.1.1.3.3.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.3"><plus id="S4.E2Xb.3.2.2.m1.1.1.3.3.1.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.1"></plus><apply id="S4.E2Xb.3.2.2.m1.1.1.3.3.2.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.2"><times id="S4.E2Xb.3.2.2.m1.1.1.3.3.2.1.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.2.1"></times><ci id="S4.E2Xb.3.2.2.m1.1.1.3.3.2.2.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.2.2">𝑇</ci><ci id="S4.E2Xb.3.2.2.m1.1.1.3.3.2.3.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S4.E2Xb.3.2.2.m1.1.1.3.3.3.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.3"><times id="S4.E2Xb.3.2.2.m1.1.1.3.3.3.1.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.3.1"></times><ci id="S4.E2Xb.3.2.2.m1.1.1.3.3.3.2.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.3.2">𝐹</ci><ci id="S4.E2Xb.3.2.2.m1.1.1.3.3.3.3.cmml" xref="S4.E2Xb.3.2.2.m1.1.1.3.3.3.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2Xb.3.2.2.m1.1c">\displaystyle=\frac{TP}{TP+FN}</annotation><annotation encoding="application/x-llamapun" id="S4.E2Xb.3.2.2.m1.1d">= divide start_ARG italic_T italic_P end_ARG start_ARG italic_T italic_P + italic_F italic_N end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E2Xc">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath ltx_font_italic" id="S4.E2Xc.2.1.1.1">F1 Score</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=2\times\frac{\text{Precision}\times\text{Recall}}{\text{%
Precision}+\text{Recall}}" class="ltx_Math" display="inline" id="S4.E2Xc.3.2.2.m1.1"><semantics id="S4.E2Xc.3.2.2.m1.1a"><mrow id="S4.E2Xc.3.2.2.m1.1.1" xref="S4.E2Xc.3.2.2.m1.1.1.cmml"><mi id="S4.E2Xc.3.2.2.m1.1.1.2" xref="S4.E2Xc.3.2.2.m1.1.1.2.cmml"></mi><mo id="S4.E2Xc.3.2.2.m1.1.1.1" xref="S4.E2Xc.3.2.2.m1.1.1.1.cmml">=</mo><mrow id="S4.E2Xc.3.2.2.m1.1.1.3" xref="S4.E2Xc.3.2.2.m1.1.1.3.cmml"><mn id="S4.E2Xc.3.2.2.m1.1.1.3.2" xref="S4.E2Xc.3.2.2.m1.1.1.3.2.cmml">2</mn><mo id="S4.E2Xc.3.2.2.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S4.E2Xc.3.2.2.m1.1.1.3.1.cmml">×</mo><mstyle displaystyle="true" id="S4.E2Xc.3.2.2.m1.1.1.3.3" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.cmml"><mfrac id="S4.E2Xc.3.2.2.m1.1.1.3.3a" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.cmml"><mrow id="S4.E2Xc.3.2.2.m1.1.1.3.3.2" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.2.cmml"><mtext id="S4.E2Xc.3.2.2.m1.1.1.3.3.2.2" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.2.2a.cmml">Precision</mtext><mo id="S4.E2Xc.3.2.2.m1.1.1.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.2.1.cmml">×</mo><mtext id="S4.E2Xc.3.2.2.m1.1.1.3.3.2.3" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.2.3a.cmml">Recall</mtext></mrow><mrow id="S4.E2Xc.3.2.2.m1.1.1.3.3.3" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.3.cmml"><mtext id="S4.E2Xc.3.2.2.m1.1.1.3.3.3.2" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.3.2a.cmml">Precision</mtext><mo id="S4.E2Xc.3.2.2.m1.1.1.3.3.3.1" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.3.1.cmml">+</mo><mtext id="S4.E2Xc.3.2.2.m1.1.1.3.3.3.3" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.3.3a.cmml">Recall</mtext></mrow></mfrac></mstyle></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2Xc.3.2.2.m1.1b"><apply id="S4.E2Xc.3.2.2.m1.1.1.cmml" xref="S4.E2Xc.3.2.2.m1.1.1"><eq id="S4.E2Xc.3.2.2.m1.1.1.1.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S4.E2Xc.3.2.2.m1.1.1.2.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.2">absent</csymbol><apply id="S4.E2Xc.3.2.2.m1.1.1.3.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3"><times id="S4.E2Xc.3.2.2.m1.1.1.3.1.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.1"></times><cn id="S4.E2Xc.3.2.2.m1.1.1.3.2.cmml" type="integer" xref="S4.E2Xc.3.2.2.m1.1.1.3.2">2</cn><apply id="S4.E2Xc.3.2.2.m1.1.1.3.3.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3"><divide id="S4.E2Xc.3.2.2.m1.1.1.3.3.1.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3"></divide><apply id="S4.E2Xc.3.2.2.m1.1.1.3.3.2.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.2"><times id="S4.E2Xc.3.2.2.m1.1.1.3.3.2.1.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.2.1"></times><ci id="S4.E2Xc.3.2.2.m1.1.1.3.3.2.2a.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.2.2"><mtext id="S4.E2Xc.3.2.2.m1.1.1.3.3.2.2.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.2.2">Precision</mtext></ci><ci id="S4.E2Xc.3.2.2.m1.1.1.3.3.2.3a.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.2.3"><mtext id="S4.E2Xc.3.2.2.m1.1.1.3.3.2.3.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.2.3">Recall</mtext></ci></apply><apply id="S4.E2Xc.3.2.2.m1.1.1.3.3.3.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.3"><plus id="S4.E2Xc.3.2.2.m1.1.1.3.3.3.1.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.3.1"></plus><ci id="S4.E2Xc.3.2.2.m1.1.1.3.3.3.2a.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.3.2"><mtext id="S4.E2Xc.3.2.2.m1.1.1.3.3.3.2.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.3.2">Precision</mtext></ci><ci id="S4.E2Xc.3.2.2.m1.1.1.3.3.3.3a.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.3.3"><mtext id="S4.E2Xc.3.2.2.m1.1.1.3.3.3.3.cmml" xref="S4.E2Xc.3.2.2.m1.1.1.3.3.3.3">Recall</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2Xc.3.2.2.m1.1c">\displaystyle=2\times\frac{\text{Precision}\times\text{Recall}}{\text{%
Precision}+\text{Recall}}</annotation><annotation encoding="application/x-llamapun" id="S4.E2Xc.3.2.2.m1.1d">= 2 × divide start_ARG Precision × Recall end_ARG start_ARG Precision + Recall end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E2Xd">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath ltx_font_italic" id="S4.E2Xd.2.1.1.1">IoU</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{|A\cap B|}{|A\cup B|}" class="ltx_Math" display="inline" id="S4.E2Xd.3.2.2.m1.2"><semantics id="S4.E2Xd.3.2.2.m1.2a"><mrow id="S4.E2Xd.3.2.2.m1.2.3" xref="S4.E2Xd.3.2.2.m1.2.3.cmml"><mi id="S4.E2Xd.3.2.2.m1.2.3.2" xref="S4.E2Xd.3.2.2.m1.2.3.2.cmml"></mi><mo id="S4.E2Xd.3.2.2.m1.2.3.1" xref="S4.E2Xd.3.2.2.m1.2.3.1.cmml">=</mo><mstyle displaystyle="true" id="S4.E2Xd.3.2.2.m1.2.2" xref="S4.E2Xd.3.2.2.m1.2.2.cmml"><mfrac id="S4.E2Xd.3.2.2.m1.2.2a" xref="S4.E2Xd.3.2.2.m1.2.2.cmml"><mrow id="S4.E2Xd.3.2.2.m1.1.1.1.1" xref="S4.E2Xd.3.2.2.m1.1.1.1.2.cmml"><mo id="S4.E2Xd.3.2.2.m1.1.1.1.1.2" stretchy="false" xref="S4.E2Xd.3.2.2.m1.1.1.1.2.1.cmml">|</mo><mrow id="S4.E2Xd.3.2.2.m1.1.1.1.1.1" xref="S4.E2Xd.3.2.2.m1.1.1.1.1.1.cmml"><mi id="S4.E2Xd.3.2.2.m1.1.1.1.1.1.2" xref="S4.E2Xd.3.2.2.m1.1.1.1.1.1.2.cmml">A</mi><mo id="S4.E2Xd.3.2.2.m1.1.1.1.1.1.1" xref="S4.E2Xd.3.2.2.m1.1.1.1.1.1.1.cmml">∩</mo><mi id="S4.E2Xd.3.2.2.m1.1.1.1.1.1.3" xref="S4.E2Xd.3.2.2.m1.1.1.1.1.1.3.cmml">B</mi></mrow><mo id="S4.E2Xd.3.2.2.m1.1.1.1.1.3" stretchy="false" xref="S4.E2Xd.3.2.2.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.E2Xd.3.2.2.m1.2.2.2.1" xref="S4.E2Xd.3.2.2.m1.2.2.2.2.cmml"><mo id="S4.E2Xd.3.2.2.m1.2.2.2.1.2" stretchy="false" xref="S4.E2Xd.3.2.2.m1.2.2.2.2.1.cmml">|</mo><mrow id="S4.E2Xd.3.2.2.m1.2.2.2.1.1" xref="S4.E2Xd.3.2.2.m1.2.2.2.1.1.cmml"><mi id="S4.E2Xd.3.2.2.m1.2.2.2.1.1.2" xref="S4.E2Xd.3.2.2.m1.2.2.2.1.1.2.cmml">A</mi><mo id="S4.E2Xd.3.2.2.m1.2.2.2.1.1.1" xref="S4.E2Xd.3.2.2.m1.2.2.2.1.1.1.cmml">∪</mo><mi id="S4.E2Xd.3.2.2.m1.2.2.2.1.1.3" xref="S4.E2Xd.3.2.2.m1.2.2.2.1.1.3.cmml">B</mi></mrow><mo id="S4.E2Xd.3.2.2.m1.2.2.2.1.3" stretchy="false" xref="S4.E2Xd.3.2.2.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S4.E2Xd.3.2.2.m1.2b"><apply id="S4.E2Xd.3.2.2.m1.2.3.cmml" xref="S4.E2Xd.3.2.2.m1.2.3"><eq id="S4.E2Xd.3.2.2.m1.2.3.1.cmml" xref="S4.E2Xd.3.2.2.m1.2.3.1"></eq><csymbol cd="latexml" id="S4.E2Xd.3.2.2.m1.2.3.2.cmml" xref="S4.E2Xd.3.2.2.m1.2.3.2">absent</csymbol><apply id="S4.E2Xd.3.2.2.m1.2.2.cmml" xref="S4.E2Xd.3.2.2.m1.2.2"><divide id="S4.E2Xd.3.2.2.m1.2.2.3.cmml" xref="S4.E2Xd.3.2.2.m1.2.2"></divide><apply id="S4.E2Xd.3.2.2.m1.1.1.1.2.cmml" xref="S4.E2Xd.3.2.2.m1.1.1.1.1"><abs id="S4.E2Xd.3.2.2.m1.1.1.1.2.1.cmml" xref="S4.E2Xd.3.2.2.m1.1.1.1.1.2"></abs><apply id="S4.E2Xd.3.2.2.m1.1.1.1.1.1.cmml" xref="S4.E2Xd.3.2.2.m1.1.1.1.1.1"><intersect id="S4.E2Xd.3.2.2.m1.1.1.1.1.1.1.cmml" xref="S4.E2Xd.3.2.2.m1.1.1.1.1.1.1"></intersect><ci id="S4.E2Xd.3.2.2.m1.1.1.1.1.1.2.cmml" xref="S4.E2Xd.3.2.2.m1.1.1.1.1.1.2">𝐴</ci><ci id="S4.E2Xd.3.2.2.m1.1.1.1.1.1.3.cmml" xref="S4.E2Xd.3.2.2.m1.1.1.1.1.1.3">𝐵</ci></apply></apply><apply id="S4.E2Xd.3.2.2.m1.2.2.2.2.cmml" xref="S4.E2Xd.3.2.2.m1.2.2.2.1"><abs id="S4.E2Xd.3.2.2.m1.2.2.2.2.1.cmml" xref="S4.E2Xd.3.2.2.m1.2.2.2.1.2"></abs><apply id="S4.E2Xd.3.2.2.m1.2.2.2.1.1.cmml" xref="S4.E2Xd.3.2.2.m1.2.2.2.1.1"><union id="S4.E2Xd.3.2.2.m1.2.2.2.1.1.1.cmml" xref="S4.E2Xd.3.2.2.m1.2.2.2.1.1.1"></union><ci id="S4.E2Xd.3.2.2.m1.2.2.2.1.1.2.cmml" xref="S4.E2Xd.3.2.2.m1.2.2.2.1.1.2">𝐴</ci><ci id="S4.E2Xd.3.2.2.m1.2.2.2.1.1.3.cmml" xref="S4.E2Xd.3.2.2.m1.2.2.2.1.1.3">𝐵</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2Xd.3.2.2.m1.2c">\displaystyle=\frac{|A\cap B|}{|A\cup B|}</annotation><annotation encoding="application/x-llamapun" id="S4.E2Xd.3.2.2.m1.2d">= divide start_ARG | italic_A ∩ italic_B | end_ARG start_ARG | italic_A ∪ italic_B | end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.SS3.p5.2">where TP, TN, FP, and FN denote, respectively, the true positive, true negative, false positive, and false negative values, and <math alttext="A" class="ltx_Math" display="inline" id="S4.SS3.p5.1.m1.1"><semantics id="S4.SS3.p5.1.m1.1a"><mi id="S4.SS3.p5.1.m1.1.1" xref="S4.SS3.p5.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.1.m1.1b"><ci id="S4.SS3.p5.1.m1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p5.1.m1.1d">italic_A</annotation></semantics></math> and <math alttext="B" class="ltx_Math" display="inline" id="S4.SS3.p5.2.m2.1"><semantics id="S4.SS3.p5.2.m2.1a"><mi id="S4.SS3.p5.2.m2.1.1" xref="S4.SS3.p5.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.2.m2.1b"><ci id="S4.SS3.p5.2.m2.1.1.cmml" xref="S4.SS3.p5.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p5.2.m2.1d">italic_B</annotation></semantics></math> represent the predicted and ground truth segments, respectively. IoU denotes Intersection over Union.</p>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1">The confusion matrix for object detection was used to evaluate a model’s performance by comparing predicted masks with ground truth masks, at different IoU thresholds. It quantifies detection rate at each IoU threshold. The confusion matrix categorizes the predictions into TP, FP, and FN based on the IoU threshold.</p>
</div>
<div class="ltx_para" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.2"><span class="ltx_text ltx_font_italic" id="S4.SS3.p7.2.1">Technical details:</span> We utilized high-performance computational resources to ensure efficient training and inference of segmentation models. Specifically, We employed the Ultralytics YOLOv8.2.71 framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib78" title="">78</a>]</cite> with PyTorch 2.3.1 for YOLO and SAM. For raster analysis, we used ArcPy site package that provides a useful and productive way to perform geographic data analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib74" title="">74</a>]</cite>. All computations were executed using Python 3.9.15 with an NVIDIA RTX A4000 GPU. SAM and YOLOv9 were trained using the Adam optimizer, with a batch size of 4 images, for 25 epochs and 75 epochs, respectively, with learning rates of <math alttext="1e{-5}" class="ltx_Math" display="inline" id="S4.SS3.p7.1.m1.1"><semantics id="S4.SS3.p7.1.m1.1a"><mrow id="S4.SS3.p7.1.m1.1.1" xref="S4.SS3.p7.1.m1.1.1.cmml"><mrow id="S4.SS3.p7.1.m1.1.1.2" xref="S4.SS3.p7.1.m1.1.1.2.cmml"><mn id="S4.SS3.p7.1.m1.1.1.2.2" xref="S4.SS3.p7.1.m1.1.1.2.2.cmml">1</mn><mo id="S4.SS3.p7.1.m1.1.1.2.1" xref="S4.SS3.p7.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.SS3.p7.1.m1.1.1.2.3" xref="S4.SS3.p7.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S4.SS3.p7.1.m1.1.1.1" xref="S4.SS3.p7.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS3.p7.1.m1.1.1.3" xref="S4.SS3.p7.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p7.1.m1.1b"><apply id="S4.SS3.p7.1.m1.1.1.cmml" xref="S4.SS3.p7.1.m1.1.1"><minus id="S4.SS3.p7.1.m1.1.1.1.cmml" xref="S4.SS3.p7.1.m1.1.1.1"></minus><apply id="S4.SS3.p7.1.m1.1.1.2.cmml" xref="S4.SS3.p7.1.m1.1.1.2"><times id="S4.SS3.p7.1.m1.1.1.2.1.cmml" xref="S4.SS3.p7.1.m1.1.1.2.1"></times><cn id="S4.SS3.p7.1.m1.1.1.2.2.cmml" type="integer" xref="S4.SS3.p7.1.m1.1.1.2.2">1</cn><ci id="S4.SS3.p7.1.m1.1.1.2.3.cmml" xref="S4.SS3.p7.1.m1.1.1.2.3">𝑒</ci></apply><cn id="S4.SS3.p7.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.p7.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p7.1.m1.1c">1e{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p7.1.m1.1d">1 italic_e - 5</annotation></semantics></math> and <math alttext="2e{-3}" class="ltx_Math" display="inline" id="S4.SS3.p7.2.m2.1"><semantics id="S4.SS3.p7.2.m2.1a"><mrow id="S4.SS3.p7.2.m2.1.1" xref="S4.SS3.p7.2.m2.1.1.cmml"><mrow id="S4.SS3.p7.2.m2.1.1.2" xref="S4.SS3.p7.2.m2.1.1.2.cmml"><mn id="S4.SS3.p7.2.m2.1.1.2.2" xref="S4.SS3.p7.2.m2.1.1.2.2.cmml">2</mn><mo id="S4.SS3.p7.2.m2.1.1.2.1" xref="S4.SS3.p7.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS3.p7.2.m2.1.1.2.3" xref="S4.SS3.p7.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S4.SS3.p7.2.m2.1.1.1" xref="S4.SS3.p7.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS3.p7.2.m2.1.1.3" xref="S4.SS3.p7.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p7.2.m2.1b"><apply id="S4.SS3.p7.2.m2.1.1.cmml" xref="S4.SS3.p7.2.m2.1.1"><minus id="S4.SS3.p7.2.m2.1.1.1.cmml" xref="S4.SS3.p7.2.m2.1.1.1"></minus><apply id="S4.SS3.p7.2.m2.1.1.2.cmml" xref="S4.SS3.p7.2.m2.1.1.2"><times id="S4.SS3.p7.2.m2.1.1.2.1.cmml" xref="S4.SS3.p7.2.m2.1.1.2.1"></times><cn id="S4.SS3.p7.2.m2.1.1.2.2.cmml" type="integer" xref="S4.SS3.p7.2.m2.1.1.2.2">2</cn><ci id="S4.SS3.p7.2.m2.1.1.2.3.cmml" xref="S4.SS3.p7.2.m2.1.1.2.3">𝑒</ci></apply><cn id="S4.SS3.p7.2.m2.1.1.3.cmml" type="integer" xref="S4.SS3.p7.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p7.2.m2.1c">2e{-3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p7.2.m2.1d">2 italic_e - 3</annotation></semantics></math>. The code for implementing SinkSAM is available on GitHub at the following link: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/osherr1996/SinkSAM.git" title="">https://github.com/osherr1996/SinkSAM.git</a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Results</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Model comparison results are illustrated using <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">five sinkhole maps</span> (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S4.F6" title="Figure 6 ‣ IV-B Comparison between pairs ‣ IV Experiments ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">6</span></a>) of the unseen Yaen site, which contains a total of 216 verified sinkholes. These include <span class="ltx_text ltx_font_italic" id="S5.p1.1.2">two maps</span> of computed closed depressions generated using the ”fill sinks” algorithm: Map 1 with a photogrammetric DEM as an input; and Map 2 with Depth Anything V2 (DAV2) as an input. The second batch consists of <span class="ltx_text ltx_font_italic" id="S5.p1.1.3">three maps made by image segmentation</span>: Map 3 with SAM prompted by YOLOv9-generated boxes; Map 4 with SAM prompted by bounding boxes of closed depressions computed from DAV2 estimated depth; and Map 5 is SinkSAM with fine-tuned SAM prompted by bounding boxes of closed depressions computed from DAV2. The Results section highlights the key findings by presenting evaluation metrics from each pairwise comparison (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5.F7" title="Figure 7 ‣ V Results ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="302" id="S5.F7.g1" src="extracted/5895157/iou_and_f1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Performance at different IoU thresholds: SinkSAM outperforms all models, while photogrammetric DEM achieves the lowest performance.</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Closed Depressions by DAV2 using ”fill sinks” vs Segmentation by SAM (Comparison A: Map 2 vs Map 4)</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In this comparison, we assess the central concept of SinkSAM: the contribution of areas of interest, delineated by closed depressions derived from DAV2, using ”fill sinks”, to SAM segmentation.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">The results indicate that SAM indeed refines and improves segmentation in terms of pixel-level metrics, achieving an F1-Score of 55.56% compared with 52.34% for closed depressions, and an IoU of 38.46% compared with 35.45% for closed depressions. SAM reduces the increase rate of correctly segmented pixels, resulting in higher recall. Such improvement in matrices indicates that SAM prompted by these bounding boxes successfully refines the computed closed depressions process with the RGB sinkholes representation.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">In terms of detection, SAM outperformed the ”fill sinks” method, consistently achieving a higher true positive rate. The difference becomes more pronounced at higher IoU thresholds. For instance, at an IoU threshold of 30%, SAM correctly detected 112 sinkholes, while the ”fill sinks” method identified only 92 out of the 216 actual sinkholes at the Yaen site, as shown in the confusion matrix. SAM’s ability to maintain a higher true positive rate at higher IoU thresholds further highlights its high performance.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of Models for Sinkhole Segmentation</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.1.1.1.1">
<span class="ltx_p" id="S5.T1.1.1.1.1.1.1" style="width:136.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1.1.1">Model\Method</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.1.1.2.1">
<span class="ltx_p" id="S5.T1.1.1.1.2.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1.1.1">Prompts Source</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.1.1.3.1">
<span class="ltx_p" id="S5.T1.1.1.1.3.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.3.1.1.1">F1 (%)</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.1.1.4.1">
<span class="ltx_p" id="S5.T1.1.1.1.4.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.4.1.1.1">IoU (%)</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.1.1.5.1">
<span class="ltx_p" id="S5.T1.1.1.1.5.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.5.1.1.1">Pre. (%)</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.1.1.6.1">
<span class="ltx_p" id="S5.T1.1.1.1.6.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.6.1.1.1">Rec. (%)</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.1.1.7.1">
<span class="ltx_p" id="S5.T1.1.1.1.7.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.7.1.1.1">Acc. (%)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.2.1.1.1">
<span class="ltx_p" id="S5.T1.1.2.1.1.1.1" style="width:136.6pt;">Closed Depressions Computation (DEM)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.2.1.2.1">
<span class="ltx_p" id="S5.T1.1.2.1.2.1.1" style="width:113.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T1.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.2.1.3.1">
<span class="ltx_p" id="S5.T1.1.2.1.3.1.1" style="width:34.1pt;">20.75%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T1.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.2.1.4.1">
<span class="ltx_p" id="S5.T1.1.2.1.4.1.1" style="width:34.1pt;">11.56%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T1.1.2.1.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.2.1.5.1">
<span class="ltx_p" id="S5.T1.1.2.1.5.1.1" style="width:34.1pt;">14.07%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T1.1.2.1.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.2.1.6.1">
<span class="ltx_p" id="S5.T1.1.2.1.6.1.1" style="width:34.1pt;">39.47%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T1.1.2.1.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.2.1.7.1">
<span class="ltx_p" id="S5.T1.1.2.1.7.1.1" style="width:34.1pt;">96.40%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.3.2.1.1">
<span class="ltx_p" id="S5.T1.1.3.2.1.1.1" style="width:136.6pt;">Closed Depressions Computation (DAV2)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.3.2.2.1">
<span class="ltx_p" id="S5.T1.1.3.2.2.1.1" style="width:113.8pt;">-</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.3.2.3.1">
<span class="ltx_p" id="S5.T1.1.3.2.3.1.1" style="width:34.1pt;">52.34%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.3.2.4.1">
<span class="ltx_p" id="S5.T1.1.3.2.4.1.1" style="width:34.1pt;">35.45%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.3.2.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.3.2.5.1">
<span class="ltx_p" id="S5.T1.1.3.2.5.1.1" style="width:34.1pt;">57.43%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.3.2.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.3.2.6.1">
<span class="ltx_p" id="S5.T1.1.3.2.6.1.1" style="width:34.1pt;">48.08%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.3.2.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.3.2.7.1">
<span class="ltx_p" id="S5.T1.1.3.2.7.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.3.2.7.1.1.1">98.92%</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.4.3.1.1">
<span class="ltx_p" id="S5.T1.1.4.3.1.1.1" style="width:136.6pt;">SAM</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.4.3.2.1">
<span class="ltx_p" id="S5.T1.1.4.3.2.1.1" style="width:113.8pt;">YOLOv9 BB</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.4.3.3.1">
<span class="ltx_p" id="S5.T1.1.4.3.3.1.1" style="width:34.1pt;">43.66%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.4.3.4.1">
<span class="ltx_p" id="S5.T1.1.4.3.4.1.1" style="width:34.1pt;">27.92%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.4.3.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.4.3.5.1">
<span class="ltx_p" id="S5.T1.1.4.3.5.1.1" style="width:34.1pt;">33.54%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.4.3.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.4.3.6.1">
<span class="ltx_p" id="S5.T1.1.4.3.6.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.4.3.6.1.1.1">62.50%</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.4.3.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.4.3.7.1">
<span class="ltx_p" id="S5.T1.1.4.3.7.1.1" style="width:34.1pt;">95.02%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.5.4.1.1">
<span class="ltx_p" id="S5.T1.1.5.4.1.1.1" style="width:136.6pt;">SAM</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.5.4.2.1">
<span class="ltx_p" id="S5.T1.1.5.4.2.1.1" style="width:113.8pt;">Closed Depressions (DAV2) BBs</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.5.4.3.1">
<span class="ltx_p" id="S5.T1.1.5.4.3.1.1" style="width:34.1pt;">55.56%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.5.4.4.1">
<span class="ltx_p" id="S5.T1.1.5.4.4.1.1" style="width:34.1pt;">38.46%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.5.4.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.5.4.5.1">
<span class="ltx_p" id="S5.T1.1.5.4.5.1.1" style="width:34.1pt;">52.91%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.5.4.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.5.4.6.1">
<span class="ltx_p" id="S5.T1.1.5.4.6.1.1" style="width:34.1pt;">58.59%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T1.1.5.4.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.5.4.7.1">
<span class="ltx_p" id="S5.T1.1.5.4.7.1.1" style="width:34.1pt;">98.55%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.6.5.1.1">
<span class="ltx_p" id="S5.T1.1.6.5.1.1.1" style="width:136.6pt;">SinkSAM</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.6.5.2.1">
<span class="ltx_p" id="S5.T1.1.6.5.2.1.1" style="width:113.8pt;">Closed Depressions (DAV2) BBs</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T1.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.6.5.3.1">
<span class="ltx_p" id="S5.T1.1.6.5.3.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.6.5.3.1.1.1">57.42%</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T1.1.6.5.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.6.5.4.1">
<span class="ltx_p" id="S5.T1.1.6.5.4.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.6.5.4.1.1.1">40.27%</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T1.1.6.5.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.6.5.5.1">
<span class="ltx_p" id="S5.T1.1.6.5.5.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.6.5.5.1.1.1">66.56%</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T1.1.6.5.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.6.5.6.1">
<span class="ltx_p" id="S5.T1.1.6.5.6.1.1" style="width:34.1pt;">50.49%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T1.1.6.5.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.6.5.7.1">
<span class="ltx_p" id="S5.T1.1.6.5.7.1.1" style="width:34.1pt;">98.84%</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">A comparison of detection rates for the 216 sinkholes further highlights DAV2’s advantages. DAV2 allowed to successfully detect sinkholes, maintaining a high true positive (TP) rate across all IoU thresholds. In contrast, the use of photogrammetric DEM led to an underestimation of the sinkhole areas, resulting in a poor TP rate even at a low IoU threshold of 10% (138 true positives for DAV2 vs 38 for the DEM). At higher IoU thresholds, DAV2 continued to detect sinkholes, while the photogrammetric DEM’s detection rate dropped to zero at IoU thresholds <math alttext="&gt;" class="ltx_Math" display="inline" id="S5.SS1.p4.1.m1.1"><semantics id="S5.SS1.p4.1.m1.1a"><mo id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><gt id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.1.m1.1d">&gt;</annotation></semantics></math>40% (see confusion matrix in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5.F8" title="Figure 8 ‣ V-B Comparing Prompts: BBs from Closed Depressions (DAV2) vs BBs made by YOLOv9 (Comparison B: Map 3 vs Map 4) ‣ V Results ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Comparing Prompts: BBs from Closed Depressions (DAV2) vs BBs made by YOLOv9 (Comparison B: Map 3 vs Map 4)</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">This section presents an evaluation of SinkSAM’s prompt sources, comparing SAM prompted by DAV2-based closed depressions using the ”fill sinks” method with SAM prompted by YOLOv9. Specifically, we analyze the performance of these two prompt types in detecting and segmenting sinkhole boundaries using SAM.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">The results demonstrate that bounding boxes from DAV2 closed depressions provide more reliable prompts for SAM. For pixel-level segmentation, SAM prompted by closed depression bounding boxes achieved an F1-Score of 55.56%, compared with 43.66% for SAM prompted by YOLOv9, and an IoU of 38.46%, compared to 27.92% for YOLOv9-prompted SAM. YOLOv9 tended to overestimate sinkhole detection, causing SAM to segment more background, which resulted in false positive boxes being fed into SAM and leading to inaccurate object segmentation, as reflected in its low precision of 33.54%.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">At lower IoU thresholds, SAM prompted by closed depression bounding boxes significantly outperformed SAM prompted by YOLOv9. For example, at a 20% IoU threshold, YOLOv9 incorrectly detected 106 sinkholes, while SAM with closed depression prompts misidentified only 89. Using same threshold, YOLOv9 missed 124 sinkholes, compared with 90 missed by SAM with closed depression prompts, and 78 missed by SinkSAM (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5.F8" title="Figure 8 ‣ V-B Comparing Prompts: BBs from Closed Depressions (DAV2) vs BBs made by YOLOv9 (Comparison B: Map 3 vs Map 4) ‣ V Results ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">8</span></a>). Although YOLOv9-detected sinkholes showed a smaller drop in true positive rates at higher IoU thresholds, it still had a high number of undetected sinkholes, with 146 false positives at a 50% IoU threshold. Additionally, YOLOv9 falsely detected a large number of non-sinkholes as sinkholes.</p>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="570" id="S5.F8.g1" src="extracted/5895157/iou_treshold.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Performance of sinkhole detection models: A bar plot of the confusion matrix illustrates the performance of various models over IoU thresholds in an unseen test area. The tested area, known as the Yaen site, contains 216 ground truth sinkholes. SinkSAM outperforms all other models, while the closed depression computation using photogrammetric DEM achieved the lowest performance.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Photogrammetric DEM vs DAV2 (Comparison C: Map 1 vs Map 2)</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">DAV2 significantly outperformed the photogrammetric DEM as an input data to delineate close depressions using ”fill sinks” algorithm. The photogrammetric DEM was observed as too smooth with lower differences, between grid-cell values in the immediate vicinity of the depressions, than DAV2. Depressions delineated by DAV2 achieved an F1-Score of 52.34%, while those based on the photogrammetric DEM achieved only 20.75%. DAV2 enabled the detection of small depressions, which were clearly overlooked by the photogrammetric DEM, as can be observed in the maps (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S4.F6" title="Figure 6 ‣ IV-B Comparison between pairs ‣ IV Experiments ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">6</span></a>). DAV2 provided also a more precise delineation of the sinkhole boundaries. These differences led to improved segmentation performance when using DAV2 as input, achieving an IoU of 35.35%, compared to the photogrammetric DEM’s significantly lower IoU of just 11.56%.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">SAM vs Fine-tuned SAM (SinkSAM) (Comparison D: Map 4 vs Map 5)</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">The final version of SinkSAM, which includes a fine-tuned SAM decoder, prompted by closed depression bounding boxes derived from DAV2, was assessed here. Fine-tuning the decoder parameters of the zero-shot SAM for the sinkhole detection task led to enhanced performance in both the segmentation and detection of sinkholes.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">SinkSAM outperformed in segmentation accuracy, achieving an F1-Score of 57.42%, while the zero-shot SAM achieved 55.56%. SinkSAM also reached an IoU of 40.27%, compared with 38.46% for the un-tuned SAM. High false positive rate led to a significant difference in precision, with SinkSAM achieving 66.56%, while SAM only achieved 52.91% (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5.T1" title="TABLE I ‣ V-A Closed Depressions by DAV2 using ”fill sinks” vs Segmentation by SAM (Comparison A: Map 2 vs Map 4) ‣ V Results ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">I</span></a>).</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">In terms of sinkhole detection, SinkSAM outperformed SAM by maintaining fewer false positives and false negatives across all IoU thresholds, as demonstrated in the confusion matrix (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5.F8" title="Figure 8 ‣ V-B Comparing Prompts: BBs from Closed Depressions (DAV2) vs BBs made by YOLOv9 (Comparison B: Map 3 vs Map 4) ‣ V Results ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">8</span></a>). For instance, at an IoU threshold of 30%, SinkSAM correctly detected 112 sinkholes, compared to SAM’s 104 out of 216. At a 50% threshold, SinkSAM accurately identified 79 sinkholes, while SAM detected only 68. This experiment highlights that SinkSAM surpassed all other versions evaluated in the current study in both segmentation and detection.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The series of experiments conducted in this study showcases the key features of SinkSAM: (1) an automatic refinement of closed depressions via pixel-level RGB image segmentation; (2) prompting by closed depressions using digital elevation data; (3) an integration of monocular depth in sinkhole segmentation; and (4) fine-tuning of SAM using a high-resolution and well-established sinkhole database. These findings underscore SinkSAM’s potential to address the limitations of previous sinkhole segmentation methods.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Refinement of Closed Depressions by RGB data</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">The DEM-based sinkhole delineation method, ”fill sinks,” became widely utilized in topographic analysis and geomorphometric applications. Its integration into ArcGIS Pro GIS software made it accessible to a broad range of researchers, engineers, and soil conservationists for various purposes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib79" title="">79</a>]</cite>. As was discussed in the <span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.1">Related Work</span> section, this usage of DEM-based sinkhole delineation methods, without the support of RGB data, introduces two challenges: (1) false detection of non-sinkhole features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib15" title="">15</a>]</cite>; and (2) inaccurate segmentation of sinkhole boundaries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Consistent with previous achievements, our results demonstrate that closed depressions delineated using ”fill sinks” tend to under- or over-estimate the sinkhole boundaries. This leads to larger or smaller sinkhole area coverage than actual, resulting in many incorrectly segmented pixels. On the other hand, the integration of SinkSAM with automatic prompting, using computed closed depressions from 3D data as bounding boxes, allowed for pixel-level segmentation. This approach enhanced sinkhole detection and delineation compared to traditional DEM-based methods (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.F9" title="Figure 9 ‣ VI-A Refinement of Closed Depressions by RGB data ‣ VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">9</span></a>). That is because SinkSAM facilitates an automatic refinement of sinkhole boundaries, using an RGB true representation. An attempt to achieve that level of precision of SinkSAM, was previously accomplished only through manual inspection of RGB imagery by human annotators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib80" title="">80</a>]</cite>. This was done post-processing, after the stage of topographic computations had ended <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib37" title="">37</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S6.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="706" id="S6.F9.g1" src="extracted/5895157/Predistion_test_set.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>SAM prompted by closed depressions: SAM improved sinkhole segmentation, although closed depressions (DAV2) tend to either overestimate or underestimate sinkhole areas, leading to inaccurate bounding boxes. SAM (zero-shot), especially when prompted with smaller boxes, effectively refines the output, providing more accurate delineation of actual sinkholes (CDs - closed Depressions).</figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">By applying SAM, sinkhole segmentation was improved, with an increased IoU (from 35.45% for the ”fill sinks” algorithm to 38.46% for SAM and 40.27% for SinkSAM). This improvement is evident both when using large bounding boxes, that include both sinkholes and background, or smaller ones that partially capture sinkholes (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.F9" title="Figure 9 ‣ VI-A Refinement of Closed Depressions by RGB data ‣ VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">9</span></a>). Nonetheless, when SinkSAM was prompted by non-sinkhole features, the machine did not always filter out false positives. Instead, it often refined them by assigning low confidence. Yet, when non-sinkhole black pixels that resemble sinkholes were involved, this led to erroneous object segmentation. Future work should focus on improving prompt generation from depth information filtered based on shape and area to differentiate true sinkholes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib81" title="">81</a>]</cite>, and more integrated advanced approaches with information on slope, curvature, and other indices to improve precision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2"> Closed Depressions Prompting</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Incorporating ”fill sinks” computation with automatic prompting addresses the shortcomings of fixed learning models in identifying undefined sinkhole features and transferring knowledge to new areas (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.F11" title="Figure 11 ‣ VI-B Closed Depressions Prompting ‣ VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">11</span></a>). The performance of learnable segmentation models, such as CNNs, in handling similar tasks after training is influenced by two key factors: the model’s ability to recognize relevant patterns and the diversity and representativeness of training data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib82" title="">82</a>]</cite>. Sinkholes often present a challenge in both aspects, because many studies rely on unseen, natural data, leading to moderate accuracy, with IoU values ranging 20%-60%, even for large sinkholes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib42" title="">42</a>]</cite>. Particularly, sinkholes segmentation performance declined in a highly variable semiarid region with small and undefined soil piping sinkhole. As reported in these regions, an improved multi channel U-Net achieved an optimal IoU of 35.27% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib40" title="">40</a>]</cite>. See reports in previous studies on relatively low sinkhole segmentation performance in the <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1">Related Work</span> section.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">Soil piping sinkholes exhibit significant morphological variation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib83" title="">83</a>]</cite>, leading to differing visual appearances (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.F10" title="Figure 10 ‣ VI-B Closed Depressions Prompting ‣ VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">10</span></a>). This often causes CNN-based models to misidentify shadows and shrubs as sinkholes, leading to errors and difficulties in generalizing to new, unseen data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib84" title="">84</a>]</cite>. This issue was also confirmed by us, where YOLO frequently overestimated sinkholes due to the presence of dark pixels from other objects, resulting in false positives (see Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S5.T1" title="TABLE I ‣ V-A Closed Depressions by DAV2 using ”fill sinks” vs Segmentation by SAM (Comparison A: Map 2 vs Map 4) ‣ V Results ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">I</span></a>).</p>
</div>
<figure class="ltx_figure" id="S6.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="349" id="S6.F10.g1" src="extracted/5895157/photos.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Ground level sinkholes images from study area: Sinkholes can vary in their morphology, resulting in irregular features with different dimensions. Additionally, the base of a sinkhole’s appearance varies: some sinkhole are deep, resulting in a dark, shadowy appearance (b), while others are filled with collapsed soil, creating a bright appearance at the bottom (a). Moreover, some sinkholes contain vegetation inside them (c).</figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">To address this bias, we adopted a non-learnable prompting approach that has the potential to generalize more effectively to new tasks. A similar method was successfully applied in the biological imaging field, where function values were used to identify local image extreme. These peaks marked the locations of cells, serving as prompts for SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib70" title="">70</a>]</cite>. It was emphasized that computation-based automatic prompting is less likely to produce inconsistent or unstable decisions compared with fixed learning methods.</p>
</div>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1">SinkSAM integrates the non-learnable ”fill sinks” algorithm with RGB segmentation to accurately extract sinkhole masks. This non-learnable prompting strategy overcomes the wide variation in sinkhole appearances and the lack of robustness in CNN models when transferring to new areas, achieving significantly higher IoU (38.46% for zero-shot SAM prompted by closed depression bounding boxes compared to 27.92% for SAM prompted by YOLO bounding boxes).</p>
</div>
<figure class="ltx_figure" id="S6.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="402" id="S6.F11.g1" src="extracted/5895157/YOLOvsDeprresions2.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Closed depressions prompting: YOLO overestimates by incorrectly detecting non-sinkhole dark pixels (such as shadows and shrubs) as sinkholes. In contrast, closed depressions calculated from DAV2 provide more reliable prompts to SAM.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.5.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.6.2">Monocular Depth Estimation</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Depth information can be an essential data source for accurate sinkhole segmentation and detection. Yet, high-resolution DEMs are not always readily available <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib7" title="">7</a>]</cite>. LiDAR, while providing high-resolution ad accurate elevation data, is rarely used at the local scale due to its high cost. Conversely, as was shown in the <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.1">Related Work</span> section, low resolution LiDARs are not usable for sinkhole segmentation. For instance, a 1 m airborne LiDAR DEM was found inadequate for detecting small soil piping-related sinkholes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib36" title="">36</a>]</cite>. Photogrammetric DEMs are more accessible, but they often suffer inaccuracies and misalignment with orthomosaics, which can lead to an underrepresentation of sinkholes, making them less reliable for segmentation or even creating accurate topographic models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S6.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="618" id="S6.F12.g1" src="extracted/5895157/DEMvsMONOD.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><span class="ltx_text" id="S6.F12.2.1" style="font-size:90%;">Photogrammetric DEM vs DAV2: Sinkholes delineation from evaluated maps, compared to the ground truth sinkhole masks, DAV2 closed depressions delineate the boundaries more precisely and ”fill” the entire area of the sinkholes, while DEM closed depressions only ”fill” parts of the sinkholes and do not fill small sinkholes at all.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">Similar to previous observations, we addressed inaccuracies in sinkhole detection encountered with photogrammetric DEMs. To improve upon this, we used MDE (Monocular Depth Estimation) foundation models, which have gained traction in depth estimation tasks by enabling zero-shot or fine-tuning predictions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib50" title="">50</a>]</cite>. Notably, the state-of-the-art DAV2 model has delivered impressive results across multiple datasets, including aerial imagery, outperforming equivalent foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib12" title="">12</a>]</cite>. Since its introduction, DAV2 was applied in agricultural canopy height measurements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib53" title="">53</a>]</cite>, offering a highly efficient and high-performing solution that surpasses current methods with superior or comparable accuracy.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">Our findings demonstrate that DAV2, even in its zero-shot configuration, excels in depth estimation using a single RGB drone image as an input. DAV2 allowed to detect fine sinkholes missed by photogrammetric DEMs, as is evident in the elevation profiles (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.F13" title="Figure 13 ‣ VI-C Monocular Depth Estimation ‣ VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">13</span></a>). DAV2 also provided more precise delineation of sinkhole boundaries (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.F12" title="Figure 12 ‣ VI-C Monocular Depth Estimation ‣ VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">12</span></a>). DAV2’s depth maps are aligned geometrically with an RGB image, eliminating small offsets typically found in photogrammetric DEMs and ultimately generating more accurate prompts for SAM.</p>
</div>
<figure class="ltx_figure" id="S6.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="286" id="S6.F13.g1" src="extracted/5895157/profiles.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Monocular depth estimation: Elevation profile plots on small sinkholes. Note that DAV2 captures small sinkholes that the photogrammetric DEM fails to detect causing under estimation of sinkholes.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1">The current paper marks the first application of DAV2, on drone imagery, for detecting geomorphic features of interest. Specifically, it is the first time a raster-based algorithm has been applied to monocular depth estimation for geomorphic feature extraction, a process that traditionally done using LiDAR or photogrammetric DEMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib29" title="">29</a>]</cite>. DAV2 can be further fine-tuned on drone imagery for even more accurate sinkhole depth estimation and can also be switched to metric mode, enabling it to extract both the geometric dimensions and boundaries of sinkholes.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS4.5.1.1">VI-D</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS4.6.2">Fine-tuned SAM</span>
</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">A large sinkhole database, which includes 350 mapped sinkholes, was captured using high-resolution RGB drone imagery, by human annotators. This data enabled efficient fine-tuning of SAM, significantly improving its performance in sinkhole segmentation. As was shown in our experimental results, SinkSAM reduced the number of misclassified background pixels compared with the zero-shot SAM, (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#S6.F14" title="Figure 14 ‣ VI-D Fine-tuned SAM ‣ VI Discussion ‣ SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole Segmentation"><span class="ltx_text ltx_ref_tag">14</span></a>).</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">Zero-shot SAM was applied in RS studies with promising results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib25" title="">25</a>]</cite>. For instance, it was used to segment glaciers and geological features in a zero-shot setting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib59" title="">59</a>]</cite>. However, Zero-shot SAM is prone to over-segmentation, especially when object boundaries are amorphous and therefore unclear <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib85" title="">85</a>]</cite>.This is a methodological challenge because many objects on the Earth surface are amorphous and hard to segment from RS data. Consequently, fine-tuning is often necessary for segmentation tasks in this domain. Studies have already showed that fine-tuning improves segmentation accuracy by SAM for roads and buildings in RS imagery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS4.p3">
<p class="ltx_p" id="S6.SS4.p3.1">In line with these findings on different objects, SAM’s performance in sinkhole segmentation here is reasonable in zero-shot mode, but, it tended to segment parts of the background, such as shadows and shrubs, leading to a higher false positive rate, especially when object boundaries were fuzzy. By fine-tuning the decoder on the sinkhole dataset, we observed improvements similar to those seen with medical imagery datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01473v1#bib.bib76" title="">76</a>]</cite>. Further improvements in SinkSAM are possible with fine-tuning on a larger sinkhole database, which would help generalize its application to a wider variety of sinkholes from different origins.</p>
</div>
<figure class="ltx_figure" id="S6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="409" id="S6.F14.g1" src="extracted/5895157/fined_tuned.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>SAM vs Fine-tuned SAM: Fine-tuned SAM significantly improved segmentation performance. Zero-shot SAM tends to segment non-sinkholes, especially when prompted with large bounding boxes that capture non-sinkhole objects in the background or sinkholes boundaries is not well defined. This bias results in lower precision compared to the fine-tuned model.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This paper formulates a novel automatic SAM framework, called SinkSAM, for segmenting soil sinkholes from a single RGB image. It integrates ”fill sinks” computation with prompt-based SAM segmentation, effectively handling environments obscured by vegetation cover and shade.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">A series of four experiments established clearly the higher performance of SinkSAM compared with previous methods: (1) SinkSAM yielded pixel-level refinement of traditional DEM-based closed depression computation by adding RGB data; (2) Integrating ”fill sinks” output data to automatic prompting allowed to overcome limitations of CNN-based learnable sinkhole detection (e.g., YOLO) and enhanced sinkhole segmentation by identifying undefined sinkhole features and model transfer successfully to unseen sites; (3) The use of automatic prompts based on monocular depth, with a single RGB image only, allowed to overcome photogrammetric DEM biases and made sinkhole mapping feasible without using expensive LiDAR data. DAV2 demonstrated its strength as an AI-based substitute for 3D data input for ”fill sinks”, providing useful SAM prompts; and (4) The established sinkhole database allows fine-tuning of SAM, improving its segmentation performance.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">These results position SinkSAM as a generalized, and an accessible, framework for sinkhole segmentation. SinkSAM framework may be applied for agricultural land management, geological research, hazard mapping and risk assessment, and early warning systems. It can also be improved in three key areas: (1) improving prompt generation from depth information by employing more advanced computational methods, or topographic indices, combined with classifiers, to reduce overestimation and generate more reliable prompts for SinkSAM; (2) implementing SinkSAM with LiDAR data to compare model performance with DAV2 usage; and (3) fine-tuning SinkSAM on a larger sinkhole database to generalize its application to a variety of sinkholes from different origins.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank the Ministry of Agriculture Chief Scientist, grant number 16-17-0005, 2022, and the Negev Scholarship by the Kreitman School of Ben-Gurion University of the Negev that support Osher Rafaeli’s PhD studies.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. De Waele, F. Gutiérrez, M. Parise, and L. Plan, “Geomorphology and natural hazards in karst areas: A review,” <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Geomorphology</span>, vol. 134, no. 1, pp. 1–8, 2011.

</span>
<span class="ltx_bibblock">Geomorphology and Natural Hazards in Karst Areas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T. Waltham, F. Bell, and M. Culshaw, “Sinkholes and subsidence: Karst and cavernous rocks in engineering and construction,” <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Springer-Praxis</span>, 01 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Gutiérrez, J. P. Galve, P. Lucha, C. Castañeda, J. Bonachea, and J. Guerrero, “Integrating geomorphological mapping, trenching, insar and gpr for the identification and characterization of sinkholes: A review and application in the mantled evaporite karst of the ebro valley (ne spain),” <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Geomorphology</span>, vol. 134, no. 1-2, pp. 144–156, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
F. Gutierrez, A. H. Cooper, and K. S. Johnson, “Identification, prediction, and mitigation of sinkhole hazards in evaporite karst areas,” <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Environmental Geology</span>, vol. 53, pp. 1007–1022, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Bernatek-Jakiel and J. Poesen, “Subsurface erosion by soil piping: significance and research needs,” <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Earth-Science Reviews</span>, vol. 185, pp. 1107–1128, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
E. Pardo-Igúzquiza and P. Dowd, “The mapping of closed depressions and its contribution to the geodiversity inventory,” <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">International Journal of Geoheritage and Parks</span>, vol. 9, no. 4, pp. 480–495, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Hummel, A. Hudak, E. Uebler, M. Falkowski, and K. Megown, “A comparison of accuracy and cost of lidar versus stand exam data for landscape management on the malheur national forest,” <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Journal of forestry</span>, vol. 109, no. 5, pp. 267–273, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Uysal, A. Toprak, and N. Polat, “Dem generation with uav photogrammetry and accuracy analysis in sahitler hill,” <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Measurement</span>, vol. 73, pp. 539–543, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
C. Zhao, Q. Sun, C. Zhang, Y. Tang, and F. Qian, “Monocular depth estimation based on deep learning: An overview,” <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Science China Technological Sciences</span>, vol. 63, no. 9, pp. 1612–1627, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
V.-C. Miclea and S. Nedevschi, “Dynamic semantically guided monocular depth estimation for uav environment perception,” <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">IEEE Transactions on Geoscience and Remote Sensing</span>, vol. 62, pp. 1–11, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y. Ming, X. Meng, C. Fan, and H. Yu, “Deep learning for monocular depth estimation: A review,” <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Neurocomputing</span>, vol. 438, pp. 14–33, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, “Depth anything v2,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Sharma and K. Tiwari, “Sink removal from digital elevation model–a necessary evil for hydrological analysis,” <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Current science</span>, vol. 117, pp. 1512–1515, 11 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Zhu, A. M. Nolte, N. Jacobs, and M. Ye, “Using machine learning to identify karst sinkholes from lidar-derived topographic depressions in the bluegrass region of kentucky,” <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Journal of Hydrology</span>, vol. 588, p. 125049, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Hofierka, M. Gallay, P. Bandura, and J. Šašak, “Identification of karst sinkholes in a forested karst landscape using airborne laser scanning data and water flow analysis,” <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Geomorphology</span>, vol. 308, pp. 265–277, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Ferreira, Y. Hussain, R. Uagoda, T. Silva, and R. Cicerelli, “Uav-based doline mapping in brazilian karst: A cave heritage protection reconnaissance,” <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Open Geosciences</span>, 10 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Q. Yuan, H. Shen, T. Li, Z. Li, S. Li, Y. Jiang, H. Xu, T. Weiwei, Q. Yang, J. Wang, J. Gao, and L. Zhang, “Deep learning in environmental remote sensing: Achievements and challenges,” <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Remote Sensing of Environment</span>, vol. 241, p. 111716, 05 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">CoRR</span>, vol. abs/1505.04597, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
X. X. Zhu, D. Tuia, L. Mou, G.-S. Xia, L. Zhang, F. Xu, and F. Fraundorfer, “Deep learning in remote sensing: A comprehensive review and list of resources,” <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">IEEE Geoscience and Remote Sensing Magazine</span>, vol. 5, no. 4, pp. 8–36, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
N. V. Hoai, N. M. Dung, and S. Ro, “Sinkhole detection by deep learning and data association,” in <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">2019 Eleventh International Conference on Ubiquitous and Future Networks (ICUFN)</span>, pp. 211–213, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
P. Sharma, Y. P. S. Berwal, and W. Ghai, “Performance analysis of deep learning cnn models for disease detection in plants using image segmentation,” <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Information Processing in Agriculture</span>, vol. 7, no. 4, pp. 566–574, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
O. Alrabayah, D. Caus, R. A. Watson, H. Z. Schulten, T. Weigel, L. Rüpke, and D. Al-Halbouni, “Deep-learning-based automatic sinkhole recognition: Application to the eastern dead sea,” <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Remote Sensing</span>, vol. 16, no. 13, p. 2264, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
T. Lüddecke and A. Ecker, “Image segmentation using text and image prompts,” in <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 7086–7096, June 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su, J. Zhu, and L. Zhang, “Grounding dino: Marrying dino with grounded pre-training for open-set object detection,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
L. P. Osco, Q. Wu, E. L. de Lemos, W. N. Gonçalves, A. P. M. Ramos, J. Li, and J. M. Junior, “The segment anything model (sam) for remote sensing applications: From zero to one shot,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollár, and R. Girshick, “Segment anything,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 779–788, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Zhu, Q. Yang, and L. Xu, “Active learning enabled low-cost cell image segmentation using bounding box annotation,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
V. Zumpano, L. Pisano, and M. Parise, “An integrated framework to identify and analyze karst sinkholes,” <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Geomorphology</span>, vol. 332, pp. 213–225, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
O. Planchon and F. Darboux, “A fast, simple and versatile algorithm to fill the depressions of digital elevation models,” <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">CATENA</span>, vol. 46, no. 2, pp. 159–176, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
L. Wang and H. Liu, “An efficient method for identifying and filling surface depressions in digital elevation models for hydrologic analysis and modelling,” <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">International Journal of Geographical Information Science</span>, vol. 20, no. 2, pp. 193–213, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
L. Yong-He, Z. Wan-Chang, and X. Jing-Wen, “Another fast and simple dem depression-filling algorithm based on priority queue structure,” <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Atmospheric and Oceanic Science Letters</span>, vol. 2, no. 4, pp. 214–219, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
X. Miao, X. Qiu, S.-S. Wu, J. Luo, D. Gouzie, and H. Xie, “Developing efficient procedures for automated sinkhole extraction from lidar dems,” <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Photogrammetric Engineering and Remote Sensing</span>, vol. 79, pp. 545–554, 06 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J. Zhu, T. P. Taylor, J. C. Currens, and M. M. Crawford, “Improved karst sinkhole mapping in kentucky using lidar techniques: A pilot study in floyds fork watershed.,” <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Journal of Cave &amp; Karst Studies</span>, vol. 76, no. 3, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y. J. Kim, B. H. Nam, and H. Youn, “Sinkhole detection and characterization using lidar-derived dem with logistic regression,” <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Remote Sensing</span>, vol. 11, no. 13, p. 1592, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. Bernatek-Jakiel and M. Jakiel, “Identification of soil piping-related depressions using an airborne lidar dem: Role of land use changes,” <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Geomorphology</span>, vol. 378, p. 107591, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
H. Chen, T. Oguchi, and P. Wu, “A semi-automatic model for sinkhole identification in a karst area of zhijin county, china,” in <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">International Conference on Intelligent Earth Observing and Applications 2015</span>, vol. 9808, pp. 209–216, SPIE, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. B. Lindsay and I. F. Creed, “Distinguishing actual and artefact depressions in digital elevation data,” <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Computers &amp; Geosciences</span>, vol. 32, no. 8, pp. 1192–1204, 2006.

</span>
<span class="ltx_bibblock">Spatial Modeling for Environmental and Hazard Management.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A. Yavariabdi, H. Kusetogullari, O. Orhan, E. Uray, V. Demir, T. Celik, and E. Mendi, “Sinkholenet: A novel rgb-slope sinkhole dataset and deep weakly-supervised learning framework for sinkhole classification and localization,” <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">The Egyptian Journal of Remote Sensing and Space Sciences</span>, vol. 26, no. 4, pp. 966–973, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
N. Kariminejad, A. Mondini, M. Hosseinalizadeh, F. Golkar, and H. R. Pourghasemi, “Detection of sinkholes and landslides in a semi-arid environment using deep-learning methods, uav images, and topographical derivatives,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. U. Rafique, J. Zhu, and N. Jacobs, “Automatic segmentation of sinkholes using a convolutional neural network,” <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Earth and Space Science</span>, vol. 9, no. 2, p. e2021EA002195, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A. Mihevc and R. Mihevc, “Morphological characteristics and distribution of dolines in slovenia, a study of a lidar-based doline map of slovenia,” <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Acta Carsologica</span>, vol. 50, May 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
G. Zhu, Y. Niu, L. Ruan, and X. Zhang, “Amfenet: An adaptive multiscale feature fusion enhancement network for sinkhole detection,” <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">IEEE Geoscience and Remote Sensing Letters</span>, vol. 21, pp. 1–5, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Y. Kim, H. Jung, D. Min, and K. Sohn, “Deep monocular depth estimation via integration of global and local predictions,” <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">IEEE Transactions on Image Processing</span>, vol. 27, no. 8, pp. 4131–4144, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth estimation with left-right consistency,” in <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pp. 270–279, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
P. Pinggera, D. Pfeiffer, U. Franke, and R. Mester, “Know your limits: Accuracy of long range stereoscopic object measurements in practice,” in <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Computer Vision – ECCV 2014</span> (D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, eds.), (Cham), pp. 96–111, Springer International Publishing, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
F. Liu, C. Shen, and G. Lin, “Deep convolutional neural fields for depth estimation from a single image,” <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">CoRR</span>, vol. abs/1411.6387, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
V.-C. Miclea and S. Nedevschi, “Monocular depth estimation with improved long-range accuracy for uav environment perception,” <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">IEEE Transactions on Geoscience and Remote Sensing</span>, vol. 60, pp. 1–15, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
L. Madhuanand, F. Nex, and M. Y. Yang, “Deep learning for monocular depth estimation from uav images,” <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</span>, vol. V-2-2020, pp. 451–458, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
R. Birkl, D. Wofk, and M. Müller, “Midas v3.1 – a model zoo for robust monocular relative depth estimation,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, “Depth anything: Unleashing the power of large-scale unlabeled data,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
J. Zhang, J. Li, Y. Huang, Y. Wang, J. Zheng, L. Shen, and Z. Cao, “Towards robust monocular depth estimation in non-lambertian surfaces,” <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2408.06083</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
D. R. Cambrin, I. Corley, and P. Garza, “Depth any canopy: Leveraging depth foundation models for canopy height estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2408.04523</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
L. Zhang, X. Deng, and Y. Lu, “Segment anything model (sam) for medical image segmentation: A preliminary review,” in <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</span>, pp. 4187–4194, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
X. Zhang, C. Gu, and S. Zhu, “Sam-helps-shadow:when segment anything model meet shadow removal,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
A. Archit, S. Nair, N. Khalid, P. Hilt, V. Rajashekar, M. Freitag, S. Gupta, A. Dengel, S. Ahmed, and C. Pape, “Segment anything for microscopy,” <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">bioRxiv</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
M. Ahmadi, A. G. Lonbar, A. Sharifi, A. T. Beris, M. Nouri, and A. S. Javidi, “Application of segment anything model for civil infrastructure defect assessment,” <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2304.12600</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
I. Giannakis, A. Bhardwaj, L. Sam, and G. Leontidis, “A flexible deep learning crater detection scheme using segment anything model (sam),” <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">Icarus</span>, vol. 408, p. 115797, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
S. Shankar, L. A. Stearns, and C. van der Veen, “Semantic segmentation of glaciological features across multiple remote sensing platforms with the segment anything model (sam),” <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Journal of Glaciology</span>, pp. 1–10, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
W. Feng, F. Guan, C. Sun, and W. Xu, “Road-sam: Adapting the segment anything model to road extraction from large very-high-resolution optical remote sensing images,” <span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">IEEE Geoscience and Remote Sensing Letters</span>, vol. 21, pp. 1–5, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
X. Ma, Q. Wu, X. Zhao, X. Zhang, M.-O. Pun, and B. Huang, “Sam-assisted remote sensing imagery semantic segmentation with object and boundary constraints,” <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">IEEE Transactions on Geoscience and Remote Sensing</span>, vol. 62, pp. 1–16, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
O. Rafaeli, T. Svoray, R. Blushtein-Livnon, and A. Nahlieli, “Prompt-based segmentation at multiple resolutions and lighting conditions using segment anything model 2,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Rädle, C. Rolland, L. Gustafson, <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">et al.</span>, “Sam 2: Segment anything in images and videos,” <span class="ltx_text ltx_font_italic" id="bib.bib63.2.2">arXiv preprint arXiv:2408.00714</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
R. I. Sultan, C. Li, H. Zhu, P. Khanduri, M. Brocanelli, and D. Zhu, “Geosam: Fine-tuning sam with sparse and dense visual prompting for automated segmentation of mobility infrastructure,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
C. Hetang, H. Xue, C. Le, T. Yue, W. Wang, and Y. He, “Segment anything model for road network graph extraction,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
R. Sahay and A. Savakis, “On aligning SAM to remote sensing data,” in <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Geospatial Informatics XIV</span> (K. Palaniappan and G. Seetharaman, eds.), vol. 13037, p. 1303703, International Society for Optics and Photonics, SPIE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
S. Pandey, K.-F. Chen, and E. B. Dam, “Comprehensive multimodal segmentation in medical imaging: Combining yolov8 with sam and hq-sam models,” in <span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">Proceedings of the IEEE/CVF international conference on computer vision</span>, pp. 2592–2598, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
A. Khatua, A. Bhattacharya, A. K. Goswami, and B. H. Aithal, “Developing approaches in building classification and extraction with synergy of yolov8 and sam models,” <span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">Spatial Information Research</span>, pp. 1–20, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
F. Trujillano, G. Jimenez, E. Manrique, N. F. Kahamba, F. Okumu, N. Apollinaire, G. Carrasco-Escobar, B. Barrett, and K. Fornace, “Using image segmentation models to analyse high-resolution earth observation data: new tools to monitor disease risks in changing environments,” <span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">International Journal of Health Geographics</span>, vol. 23, no. 1, p. 13, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
R. Glatt and S. Liu, “Topological data analysis guided segment anything model prompt optimization for zero-shot segmentation in biological imaging,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Israel Meteorological Service, “Meteorological data and reports.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ims.gov.il" title="">https://ims.gov.il</a>, n.d.

</span>
<span class="ltx_bibblock">n.d.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
I. Stavi and E. Argaman, “Soil quality and aggregation in runoff water harvesting forestry systems in the semi-arid israeli negev,” <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">CATENA</span>, vol. 146, pp. 88–93, 2016.

</span>
<span class="ltx_bibblock">Dan H. Yaalon Memorial Issue.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
A. Nahlieli, T. Svoray, and E. Argaman, “Piping formation and distribution in the semi-arid northern negev environment: A new conceptual model,” <span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">CATENA</span>, vol. 213, p. 106201, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
C. E. S. R. I. Redlands, “Arcgis desktop: Release 10,” 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
C.-Y. Wang, I.-H. Yeh, and H.-Y. M. Liao, “Yolov9: Learning what you want to learn using programmable gradient information,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
J. Ma, Y. He, F. Li, L. Han, C. You, and B. Wang, “Segment anything in medical images,” <span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">Nature Communications</span>, vol. 15, Jan. 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
A. Bonnet, “Learn how to fine-tune the segment anything model (sam),” <span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">Encord Blog</span>, April 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-04.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
G. Jocher, A. Chaurasia, and J. Qiu, “Ultralytics yolo,” jan 2023.

</span>
<span class="ltx_bibblock">Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ultralytics/ultralytics" title="">https://github.com/ultralytics/ultralytics</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
J. Wall, D. Bohnenstiehl, K. Wegmann, and N. Levine, “Morphometric comparisons between automated and manual karst depression inventories in apalachicola national forest, florida, and mammoth cave national park, kentucky, usa,” <span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">Natural Hazards</span>, vol. 85, 01 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
R. Blushtein-Livnon, T. Svoray, and M. Dorman, “Performance of human annotators in object detection and segmentation of remotely sensed data,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
H. Chen, T. Oguchi, and P. Wu, “Morphometric analysis of sinkholes using a semi-automatic approach in zhijin county, china,” <span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">Arabian Journal of Geosciences</span>, vol. 11, 08 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
B. Du, Z. Wang, L. Zhang, L. Zhang, W. Liu, J. Shen, and D. Tao, “Exploring representativeness and informativeness for active learning,” <span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">IEEE Transactions on Cybernetics</span>, vol. 47, no. 1, pp. 14–26, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
O. Rafaeli, A. Nahlieli, and T. Svoray, “Dynamics of subsurface soil erosion in a semiarid region: A time-series study of sinkhole area and morphology,” <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">CATENA</span>, vol. 233, p. 107511, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Z. Jiang, S. Hu, H. Deng, N. Wang, F. Zhang, L. Wang, S. Wu, X. Wang, Z. Cao, Y. Chen, <span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">et al.</span>, “Detection and automatic identification of loess sinkholes from the perspective of lidar point clouds and deep learning algorithm,” <span class="ltx_text ltx_font_italic" id="bib.bib84.2.2">Geomorphology</span>, p. 109404, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
W. Ji, J. Li, Q. Bi, T. Liu, W. Li, and L. Cheng, “Segment anything is not always perfect: An investigation of sam on different real-world applications,” 2024.

</span>
</li>
</ul>
</section>
<figure class="ltx_float biography" id="id1">
<table class="ltx_tabular" id="id1.1">
<tr class="ltx_tr" id="id1.1.1">
<td class="ltx_td" id="id1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="125" id="id1.1.1.1.g1" src="extracted/5895157/osher_paper.jpeg" width="89"/></td>
<td class="ltx_td" id="id1.1.1.2">
<span class="ltx_inline-block" id="id1.1.1.2.1">
<span class="ltx_p" id="id1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="id1.1.1.2.1.1.1">Osher Rafaeli</span> 
received a B.Sc. in Geology from Ben-Gurion University of the Negev (BGU), Beer Sheva, Israel, in 2018.
He is currently a Ph.D. student at the Ben-Gurion University of the Negev. His main research interests include AI-based image processing and geoinformatics solutions for soil piping erosion research.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="id2">
<table class="ltx_tabular" id="id2.1">
<tr class="ltx_tr" id="id2.1.1">
<td class="ltx_td" id="id2.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="125" id="id2.1.1.1.g1" src="extracted/5895157/tal2.jpg" width="100"/></td>
<td class="ltx_td" id="id2.1.1.2">
<span class="ltx_inline-block" id="id2.1.1.2.1">
<span class="ltx_p" id="id2.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="id2.1.1.2.1.1.1">Tal Svoray</span> 
received a Ph.D. in Radar Remote Sensing of Mediterranean Vegetation from Bar Ilan University, Ramat-Gan, Israel, in 2001.
He is currently a Professor at the Ben-Gurion University of the Negev. His main research interests include object segmentation and detection, remote sensing of soil and vegetation, environmental psychology and geostatistics..</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="tab1">
<table class="ltx_tabular" id="tab1.1">
<tr class="ltx_tr" id="tab1.1.1">
<td class="ltx_td" id="tab1.1.1.1">
<span class="ltx_inline-block" id="tab1.1.1.1.1">
<span class="ltx_p" id="tab1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="tab1.1.1.1.1.1.1">Ariel Nahlieli</span> 
received a Ph.D. from the Department of Environmental, Geoinformatics and Urban Planning Sciences at Ben-Gurion University of the Negev, Beer Sheva, Israel, in 2024. He is currently focused on researching and understanding the main factors that contribute to the formation and evolution of soil piping erosion in semiarid regions.</span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  2 12:15:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
