<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Catalog of General Ethical Requirements for AI Certification</title>
<!--Generated on Thu Aug 22 10:52:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.12289v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S1" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S2" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Operationalizable minimum requirements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S3" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>European Union AI Act: a brief overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S4" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Compliance and implementation of the suggested assessments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S5" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Overall Ethical Requirements (O)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S6" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Fairness (F)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S7" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Privacy and Data Protection (P)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S8" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Safety and Robustness (SR)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S9" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Sustainability (SU)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S10" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Transparency and Explainability (T)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S11" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>Truthfulness (TR)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S12" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12 </span>Conclucing Remarks</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Catalog of General Ethical Requirements for AI Certification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Nicholas Kluge Corrêa
<br class="ltx_break"/><a class="ltx_ref ltx_href" href="mailto:kluge@uni-bonn.de" title="">kluge@uni-bonn.de</a>
&amp;Julia Maria Mönig
<br class="ltx_break"/><a class="ltx_ref ltx_href" href="mailto:moenig@uni-bonn.de" title="">moenig@uni-bonn.de</a>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This whitepaper offers normative and practical guidance for developers of artificial intelligence (AI) systems to achieve "Trustworthy AI". In it, we present overall ethical requirements and six ethical principles with value-specific recommendations for tools to implement these principles into technology. Our value-specific recommendations address the principles of <span class="ltx_text ltx_font_italic" id="id1.id1.1">fairness,</span> <span class="ltx_text ltx_font_italic" id="id1.id1.2">privacy and data protection,</span> <span class="ltx_text ltx_font_italic" id="id1.id1.3">safety and robustness,</span> <span class="ltx_text ltx_font_italic" id="id1.id1.4">sustainability,</span> <span class="ltx_text ltx_font_italic" id="id1.id1.5">transparency and explainability</span> and <span class="ltx_text ltx_font_italic" id="id1.id1.6">truthfulness</span>. For each principle, we also present examples of criteria for risk assessment and categorization of AI systems and applications in line with the categories of the European Union (EU) AI Act. Our work is aimed at stakeholders who can take it as a potential blueprint to fulfill minimum ethical requirements for trustworthy AI and AI Certification.</p>
</div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S1" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S2" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Operationalizable minimum requirements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S3" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>European Union AI Act: a brief overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S4" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Compliance and implementation of the suggested assessments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S5" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Overall Ethical Requirements (O)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S6" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Fairness (F)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S7" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Privacy and Data Protection (P)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S8" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Safety and Robustness (SR)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S9" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Sustainability (SU)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S10" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Transparency and Explainability (T)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S11" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>Truthfulness (TR)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S12" title="In Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12 </span>Conclucing Remarks</span></a></li>
</ol></nav>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S1.2.1.1" style="color:#007091;">1</span> </span><span class="ltx_text" id="S1.3.2" style="color:#007091;">Introduction</span>
</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Nowadays, our most recent AI revolution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib166" title="">166</a>]</cite> has ushered in a new era of renewed interest in the field, given its success in (1) triumphing where past paradigms have stagnated (e.g., object detection, language understanding, etc.) and (2) offering new technologies for the industry to adopt into its practices. For example, recent developments in the field of generative AI have unleashed upon the world systems that can ingest and generate content, like text, images, and video, with a remarkable level of resemblance to what human beings can produce, integrating AI increasingly into many parts of modern-day life.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While the true nature and extent of the capabilities of such systems remain a matter of debate and speculation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib144" title="">144</a>]</cite>, these same capacities have sparked several worries regarding the type of future we are creating, where intelligent systems become an ever-growing part of society. From pleas for moratoriums <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib67" title="">67</a>]</cite> to proposals of general research agendas to map and mitigate the risks associated with AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib18" title="">18</a>]</cite>, society is starting to create a consensus on the need to imbue artificial intelligence with values and norms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib164" title="">164</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this scenario, the consensus has been formulated, that we need to develop "trust" in AI, similar to the trust we have in technologies that have been in use for a longer time. We hence come to the idea of "Trustworthy AI". But what is "Trustworthy AI"? Generally speaking, we can define it as a state of development where we can trust that values and norms orient the design, development and deployment of AI systems, and we potentially have guarantees that international, national, and multinational actors are developing their technologies with respect for fundamental human rights and values <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib198" title="">198</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib174" title="">174</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib43" title="">43</a>]</cite>. Then, we could genuinely and trustingly integrate AI into society, allowing us to reap the benefits of intelligent automation. For this cause, many areas of intervention, like AI Safety <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib85" title="">85</a>]</cite>, AI Ethics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib97" title="">97</a>]</cite>, and AI Governance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib44" title="">44</a>]</cite>, are actively engaged in the development of Trustworthy AI, be that by developing techniques developers can implement in their practice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib164" title="">164</a>]</cite>, defining and exploring the ethical values that should guide our technological progress <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib39" title="">39</a>]</cite>, or creating the basis for current or future legislations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib206" title="">206</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Principles and values are at the heart of the idea of trustworthy AI development, and we can strive to achieve trustworthiness development if such principles and values are taken into account during the many stages of technological development. Even though ethical principles are many, while also being interpreted in varied ways,<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For those interested in exploring the multifaceted landscape of AI ethics, we recommend the <a class="ltx_ref ltx_href" href="https://nkluge-correa.github.io/worldwide_AI-ethics/" title="">Worldwide AI Ethics dashboard</a> by Nicholas Kluge Corrêa et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib41" title="">41</a>]</cite>, which presents an interactive tool for studying the normative discourse in over 200 ethical guidelines and other documents related to AI governance.</span></span></span> there is still agreement about (1) what the most upheld principles in the literature are and (2) what these principles mean in a general sense <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib41" title="">41</a>]</cite>. For example, we find the principle of <span class="ltx_ref ltx_ref_self">privacy</span> at the heart of the EU General Data Protection Regulation, among other data protection regulations worldwide that seek to safeguard individuals’ rights and personal data. Meanwhile, reliability and <span class="ltx_ref ltx_ref_self">safety</span> support the search for robust AI systems against adversaries and accidents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib110" title="">110</a>]</cite>. At the same time, <span class="ltx_ref ltx_ref_self">transparency</span> is the normative root of the whole field of explainable AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib146" title="">146</a>]</cite>, where we constantly seek ways to provide understandable explanations to black box processes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">While behind every ethical principle, there are human sorrows, issues, or problems, in front of these, we (can) have requirements and tools to implement them practically, when possible. Algorithmic discrimination brings the necessity of fairness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib54" title="">54</a>]</cite>. Ecological harm demands the defense of sustainable values <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib204" title="">204</a>]</cite>. The lack of human autonomy reinforces the need for human control <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib27" title="">27</a>]</cite>. The absence of reasonable explanations for high-risk scenarios brings forth the requirement for transparency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib113" title="">113</a>]</cite>. The invasive nature of our current data-hungry paradigm<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Scaling laws show the data-hungry tendencies of the foundational model, where the amount of data required to train such artifacts scales as their size increases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib88" title="">88</a>]</cite>.</span></span></span> solidifies pleas for privacy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib176" title="">176</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In this sense, this work proposes to utilize ethical principles to stipulate minimal ethical requirements for using and developing Trustworthy AI. Drawing from experts in the field, an extensive literature overview, and considering the latest advancements in AI research, we have identified six key areas that, we argue, require critical attention:<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Values are presented in alphabetical order. This order should not be interpreted as a hierarchy of importance or priority.</span></span></span> <span class="ltx_ref ltx_ref_self">Fairness</span>, <span class="ltx_ref ltx_ref_self">Privacy and Data Protection</span>, <span class="ltx_ref ltx_ref_self">Safety and Robustness</span>, <span class="ltx_ref ltx_ref_self">Sustainability</span>, <span class="ltx_ref ltx_ref_self">Transparency and Explainability</span>, and <span class="ltx_ref ltx_ref_self">Truthfulness</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">With these values in mind, our work presents to the reader a thorough examination of (1) what these principles are in the context of AI, (2) what requirements can be stipulated so that we can safeguard such values, and (3) how stakeholders can take measures to implement such requirements in their developmental practice. Ultimately, our work can help readers attain the means to aid in developing a healthy and safe AI ecosystem, providing them with a condensed and brief source of ethical and practical guidance for responsible AI development.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S2.2.1.1" style="color:#007091;">2</span> </span><span class="ltx_text" id="S2.3.2" style="color:#007091;">Operationalizable minimum requirements</span>
</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Many attempts have been made to develop methods to implement ethics in AI development, from developmental frameworks to ethical labels for Trustworthy AI systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib195" title="">195</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib77" title="">77</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib162" title="">162</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib164" title="">164</a>]</cite>, all seek to approach the "from principles to practice" problem, i.e., applying ethics in practice, be that in software development or day-to-day business <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib149" title="">149</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib77" title="">77</a>]</cite>. However, quantifying ethics has some central issues, mainly because easy yes-or-no answers do not exist regarding ethics. Most ethical dilemmas require strong contextualization, while the context might change over time in an ever-changing environment. Meanwhile, moral values are, by nature, abstract concepts that, without a humanistic approach, cannot be adequately translated into practice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib50" title="">50</a>]</cite>. Hence, coming up with operationalizable minimum requirements is not trivial and should always be approached through a contextual lens, i.e., differentially regarding application areas, levels of risk, target population, etc.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Looking at the problem through the lens of the European context, we see that the EU’s current approach, as expressed in the AI Act, is risk-based, which means that the degree of regulation that an AI system might underlie depends on its risk class (i.e., minimal, limited, high, and unacceptable), which resonates with other draft proposals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib212" title="">212</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib183" title="">183</a>]</cite>. For this work, even though the current mode of governance proposed by the AI Act is differential concerning risk, our propositions are, we argue, general enough to apply to several different risk contexts, with a caveat that for applications and systems classified as "unacceptable risk level", our only suggested requirement is the termination of the system. Meanwhile, to help the reader better frame and assess the possible risk of their system and thus account for the rigor with which our requirements should be fulfilled, we present criteria and examples for every EU AI Act risk level for every principle we work through in this document.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S3.2.1.1" style="color:#007091;">3</span> </span><span class="ltx_text" id="S3.3.2" style="color:#007091;">European Union AI Act: a brief overview</span>
</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The specific ethical requirements presented in this work are all tailored to be applicable and sensitive to the current EU AI Act. However, <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">what is the AI Act?</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The EU AI Act is a regulation proposed by the European Commission on April 21, 2021, and politically agreed upon by the European Commission, the Council of the European Union and the European Parliament on December 8, 2023. This regulation consists of rules for AI system providers and users, which detail each entity’s transparency and reporting obligations on the EU market. These requirements apply to European companies and all AI systems impacting people in the EU, regardless of where the systems are developed or deployed. Overall, this legislation is based on ideas of excellence and trust, aiming to promote research and development while guaranteeing safety and fundamental rights.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Organizations building or using AI systems are responsible for ensuring compliance with the EU AI Act. These compliance obligations depend on the risk category an AI system poses to people’s fundamental rights, i.e., <span class="ltx_text ltx_font_bold" id="S3.p3.1.1">minimal</span>, <span class="ltx_text ltx_font_bold" id="S3.p3.1.2">limited</span>, <span class="ltx_text ltx_font_bold" id="S3.p3.1.3">high</span>, and <span class="ltx_text ltx_font_bold" id="S3.p3.1.4">unacceptable</span>. Depending on the risk of an application or system, these requirements may involve procedures like:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Risk assessment surveys to profile the risk of an AI system.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Compliance assessments that prove a given system is following the AI Act.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Providing open, transparent, and accessible documentation regarding the AI system.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">Currently, most requirements are tied to systems and applications regarded as high-risk. Meanwhile, inability to comply with the rules stipulated by the AI Act may end up generating fines up to €35m for prohibited Al violations (or up to 7% of global annual turnover, whichever is higher), up to €15m for most other violations (or up to 3% of yearly global turnover, whichever is higher), and up to €7.5m for supplying incorrect info Caps on fines for SMEs<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Small and medium-sized enterprises.</span></span></span> and startups (or up to 1.5% of global annual turnover, whichever is higher).<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Moreover, other Europe-wide regulations need to be taken into account, such as the <a class="ltx_ref ltx_href" href="https://gdpr-info.eu/" title="">General Data Protection Regulation</a>, the <a class="ltx_ref ltx_href" href="https://eur-lex.europa.eu/eli/reg/2022/2065/oj" title="">Digital Services Act</a> and the <a class="ltx_ref ltx_href" href="https://digital-markets-act.ec.europa.eu/index_en" title="">Digital Markets Act</a>, which may also result in fines in cases of lack of compliance.</span></span></span> Hence, adherence to ethical requirements and the AI Act are a reality companies must come to terms with, which, besides avoiding legal fines, may bring market advantages to socially and ethically aware organizations.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S4.2.1.1" style="color:#007091;">4</span> </span><span class="ltx_text" id="S4.3.2" style="color:#007091;">Compliance and implementation of the suggested assessments</span>
</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Companies gain numerous advantages by adhering to minimal ethical requirements throughout the life cycle of AI systems. For example:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Upholding ethical standards enhances brand reputation, fostering trust among consumers and stakeholders. Trust that can translate into increased customer loyalty and market share.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Ethical AI practices mitigate legal risks, shielding companies from potential lawsuits and regulatory penalties, thereby safeguarding their financial stability.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Certain requirements, such as the development of procedures to counter model opacity or brittleness, are bound to improve the organization’s understanding of the workings and weaknesses of its system while improving overall efficiency and robustness.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">Prioritizing ethics can promote employee morale and retention, attracting top talent to contribute to innovation and growth.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">Trustworthy AI development fosters long-term sustainability by reducing the likelihood of negative societal impacts and ensuring alignment with societal values and expectations, which, in the end, safeguards the interests of stakeholders while cultivating a competitive edge in the rapidly evolving landscape of AI technology.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Hence, such recommendations should be considered advantageous additions to general quality management procedures, compliance mechanisms, or any existing assessment and audit schemes an organization may possess. Moreover, the practices suggested in this work should be considered even for applications where, for the current legislation (EU AI Act), no explicit requirements are established, given that these should not be considered only practices that are valid for compliance reasons but valid for the inherent good and benefits they may bring to the organization and overall society.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S5.2.1.1" style="color:#007091;">5</span> </span><span class="ltx_text" id="S5.3.2" style="color:#007091;">Overall Ethical Requirements (O)</span>
</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">As mentioned before, consensus on emerging values and trends in the ethics of artificial intelligence can be found in reviews of the field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib222" title="">222</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib41" title="">41</a>]</cite>. While some of these values can be specific to practices that are more easily pinpointed as requirements (e.g., Truthfulness), others are highly general and resonate with several questions and topics, from philosophy to social sciences and law (e.g., Human-Centeredness, Labor Rights). To better systematize our catalog of requirements hierarchically, we structured four overarching ethical principles, which serve as a foundation for more specific requirements. In short, these serve as foundational pillars to further support recommendations that can shape ethical decision-making processes and ensure that AI systems align with human values, rights, and societal norms:<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We highlight that the fulfillment of many of the requirements presented here and in further sections goes beyond the scope of action of developers and engineers, requiring other types of stakeholders, like policymakers, to act in a way that can enable their fulfillment (e.g., by creating policies that will incentivize their realization).</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<dl class="ltx_description" id="S5.I1">
<dt class="ltx_item" id="S5.I1.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.ix1.1.1.1">Autonomy:</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="S5.I1.ix1.p1">
<p class="ltx_p" id="S5.I1.ix1.p1.1">This principle emphasizes preserving human agency and decision-making control in interactions with AI systems. It requires that individuals are adequately informed and empowered to make autonomous choices.</p>
</div>
</dd>
<dt class="ltx_item" id="S5.I1.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.ix2.1.1.1">Beneficence:</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="S5.I1.ix2.p1">
<p class="ltx_p" id="S5.I1.ix2.p1.1">The principle of beneficence underscores the ethical obligation to promote the well-being and welfare of individuals and communities through AI technologies. AI development and deployment must prioritize positive societal impacts, such as enhancing human capabilities, fostering fairness and equity, and addressing societal challenges effectively.</p>
</div>
</dd>
<dt class="ltx_item" id="S5.I1.ix3"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.ix3.1.1.1">No Harm:</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="S5.I1.ix3.p1">
<p class="ltx_p" id="S5.I1.ix3.p1.1">This principle focuses on preventing and mitigating potential risks and harms associated with AI technologies, encompassing physical and societal dimensions. It requires proactive measures to identify, assess, and address risks to individuals, communities, and the environment.</p>
</div>
</dd>
<dt class="ltx_item" id="S5.I1.ix4"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.ix4.1.1.1">Accountability:</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="S5.I1.ix4.p1">
<p class="ltx_p" id="S5.I1.ix4.p1.1">This principle is a foundational requirement for ethical AI governance and oversight, establishing that we should trace accountability back to ourselves when we violate the above principles, whether through ignorance, incompetence, or ill intent.</p>
</div>
</dd>
</dl>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">In light of these principles, it is essential to consider the following ethical requirements when developing trustworthy AI. In the following sections, we established all other specific requirements of our work as minimal developmental criteria that seek to ground the following overall requirements.</p>
</div>
<section class="ltx_subsection" id="S5.SSx1">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement O1: Human Oversight and Control</h3>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p1">
<p class="ltx_p" id="S5.SSx1.p1.1">Stakeholders must ensure the integration of human oversight and control mechanisms into AI systems. Hence, they should adopt practices that establish clear human or human-AI intervention procedures, review, moderation, and control, particularly in critical or ethically sensitive situations, to uphold accountability and mitigate risks associated with automated decision-making. Such integration is bound to enhance the transparency and fairness of AI systems and foster public trust and confidence in their responsible use and deployment. Meanwhile, individuals should be informed when they are not interacting with a fellow human being but with a machine, enabling them to decide whether they want to engage with it. AI-generated content must be presented as such.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx2">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement O2: Impact Mitigation and Robust Deployment</h3>
<div class="ltx_para ltx_noindent" id="S5.SSx2.p1">
<p class="ltx_p" id="S5.SSx2.p1.1">Stakeholders must prioritize preventing and remedying AI’s negative impacts while ensuring robust deployment of AI systems. They must implement measures to anticipate and address potential adverse effects on individuals, communities, and society. Hence, stakeholders should adopt practices for conducting thorough risk assessments, impact assessments, evaluations, guardrail development, and, when necessary, termination. Furthermore, stakeholders should establish strategies for monitoring and evaluating the performance of AI systems in real-world contexts to facilitate continuous improvement and adaptation. Such proactive measures mitigate risks and contribute to the responsible and sustainable deployment of AI technologies in diverse settings.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx3">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement O3: Open Development and Transparent Reporting</h3>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p1">
<p class="ltx_p" id="S5.SSx3.p1.1">Stakeholders must prioritize open development practices and transparent reporting in AI projects. This requires fostering an environment where collaboration, sharing of insights, and open-source contributions are encouraged and facilitated. Hence, stakeholders should adopt practices to ensure that AI development processes, including data collection, model training, and AI-empowered services, are transparent and accessible to relevant stakeholders. Additionally, stakeholders should establish transparent reporting mechanisms to document AI system capabilities, limitations, and potential biases. Such adherence promotes accountability, facilitates peer review, and fosters public understanding and trust in AI technologies.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx4">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement O4: Privacy, Diversity and Accessibility in, by and for Design</h3>
<div class="ltx_para ltx_noindent" id="S5.SSx4.p1">
<p class="ltx_p" id="S5.SSx4.p1.1">Stakeholders must prioritize integrating privacy, diversity, and accessibility considerations into designing and deploying AI systems. This requires implementing practices that incorporate these principles throughout the AI development lifecycle. Privacy measures should ensure stakeholders handle personal data ethically, respecting individuals’ rights and expectations regarding their information. Diversity considerations should drive the development of inclusive AI systems representing diverse populations, thus preventing biases and discrimination. Additionally, accessibility by design involves designing AI systems that are usable and accessible to all individuals, including those with disabilities, ensuring equitable access to AI technologies. By incorporating these principles from the outset of the conception of a process, product, or service and throughout the research and training phase, operation, and decommissioning, stakeholders can uphold ethical standards, promote fairness, and foster inclusivity in AI technologies.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx5">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement O5: Ethical Stakeholder Inclusion, Co-creation and Diversity by Design</h3>
<div class="ltx_para ltx_noindent" id="S5.SSx5.p1">
<p class="ltx_p" id="S5.SSx5.p1.1">Stakeholders must include as many vantage points as possible in the design process of AI technologies. This necessitates actively involving all potentially affected stakeholders, such as members of vulnerable groups, in decision-making and development processes. By incorporating diverse perspectives from various stakeholders, including end-users, domain experts, ethicists, and impacted communities, stakeholders can gain comprehensive insights into AI technologies’ potential societal impacts and ethical implications. Additionally, fostering an inclusive environment encourages collaboration, increases accountability, and ensures that AI systems meet the needs and values of diverse stakeholders. This inclusive approach ultimately leads to more ethical, equitable, and socially responsible AI deployment.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx6">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement O6: Democracy and Human Rights</h3>
<div class="ltx_para ltx_noindent" id="S5.SSx6.p1">
<p class="ltx_p" id="S5.SSx6.p1.1">Stakeholders should be aware of how much they collaborate with certain institutions. A government, for instance, should verify that a company tasked with services, projects, or the production of AI products complies with human rights and their respective legislation. At the same time, stakeholders should avoid involvement with organizations that contribute to regimes that seek to topple democratic processes or violate human rights. Moreover, stakeholders must actively advocate for incorporating democratic principles and respect for human rights in developing and deploying AI systems. Furthermore, stakeholders should engage in ongoing dialogue and collaboration with civil society organizations, human rights advocates, and democratic institutions to ensure that AI technologies uphold fundamental rights and freedoms.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx6.p2">
<p class="ltx_p" id="S5.SSx6.p2.1">The development of trustworthy AI necessitates a holistic approach that integrates ethical principles into every stage of design, deployment, and operation. Requirements such as human oversight and control, impact mitigation, transparency, privacy, diversity, stakeholder inclusion, and respect for democracy and human rights serve as guiding pillars for responsible AI development. By adhering to these requirements, stakeholders can uphold accountability, foster inclusivity, mitigate risks, and promote AI technologies’ ethical and equitable deployment, ultimately contributing to a more just and socially responsible society.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx6.p3">
<p class="ltx_p" id="S5.SSx6.p3.1">However, stakeholders must know the inherent trade-offs among such requirements. While the interrelation among the principles that sustain these requirements is undeniable, no rigid hierarchical structure exists between them. Certain historical contexts and specific objectives may occasionally warrant prioritizing one principle or requirement over another, especially in extraordinary circumstances. Consequently, tensions may emerge between these, necessitating thorough discussion and ethical consideration within political and societal discourse. It is imperative to cautiously approach trade-offs between these principles, recognizing that some compromises may be ethically untenable, mainly when fundamental rights and values are at stake.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S6.2.1.1" style="color:#007091;">6</span> </span><span class="ltx_text" id="S6.3.2" style="color:#007091;">Fairness (F)</span>
</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Fairness is an immutable ethical cornerstone, serving as a guiding light across various practical contexts. This principle is rooted in our general notions of equity, justice, and impartiality, epitomized by the aspiration for equitable treatment in decision-making and dignity, which we all seek. Hence, we can define fairness as the commitment to ensuring equal opportunities and impartiality for all individuals, irrespective of differences or circumstances, especially on matters outside an individual’s control (e.g., where someone was born). In AI ethics, fairness assumes renewed significance as algorithms become the source of unequal treatment and discrimination while increasingly embedded in critical domains of human life. Hence, AI fairness embodies our (i.e., all stakeholders seeking just and fair treatment) concentrated efforts toward rectifying inherent biases entrenched within automated processes driven by AI systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib132" title="">132</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib138" title="">138</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Within the purview of automated decision-making, there are already many documented cases wherein algorithms exhibit systemic biases, manifesting in outcomes that disproportionately disadvantage certain demographic groups. For instance, in the realm of criminal justice, predictive policing algorithms have been criticized for perpetuating racial profiling and exacerbating existing disparities within the criminal justice system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib54" title="">54</a>]</cite>. Moreover, within Generative AI systems, biases can insidiously permeate through the data upon which models are trained, resulting in the generation of content imbued with stereotypes, prejudices, and cultural distortions (e.g., language models generating text that reinforces gender stereotypes or racial prejudices) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib197" title="">197</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib129" title="">129</a>]</cite>. These examples underscore the pressing need for ethical vigilance and proactive intervention to rectify algorithmic biases, ensuring that AI systems operate by following societal equity principles.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">In AI ethics, concerns regarding fairness are linked to the interplay between sensitive attributes and the behavior of AI systems. Sensitive attributes refer to characteristics such as race, gender, age, socioeconomic status, or any other feature that could potentially lead to discrimination or unequal treatment. Given that sensitive attributes or their proxies<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Proxies refer to indirect or surrogate variables used in place of sensitive attributes that may lead to discrimination outcomes in algorithmic decision-making. For example, a name can be a proxy for gender, while an address can be a proxy for race in racially segregated communities.</span></span></span> are often used as input variables in AI algorithms, how these variables affect the system is a general concern among the AI community. Perhaps it is in issues involving fairness that the old "Garbage in, Garbage out" motto becomes more pronounced <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib208" title="">208</a>]</cite>, something that also points to the fact that algorithmic discrimination is but a reflection of social inequalities which, unfortunately, have no easy technical solution. When algorithms are trained on data that reflects societal biases or historical injustices, they may inadvertently learn and perpetuate those biases, leading to unfair or discriminatory outcomes, especially for specific demographic groups associated with sensitive attributes tied to those unfair cases.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">Now, it is imperative to clarify that the scope of this work does not aim to prescribe solutions or establish requirements for addressing social inequalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib172" title="">172</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib182" title="">182</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib155" title="">155</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib66" title="">66</a>]</cite>. Instead, our primary objective lies in fostering strategies geared toward detecting, preventing, and mitigating algorithmic discrimination. Hence, our ethical requirements should be seen as something other than a catch-all solution to such a complicated and systemic problem, which, in the end, is a matter of concern that pre-dates the era of algorithms and artificial intelligence.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">To help organizations prospect the level of risk regarding their system or application, we propose the following criteria, in line with the four main categories of risk identified in the EU AI Act (examples in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S6.T1" title="Table 1 ‣ 6 Fairness (F) ‣ Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_tag">1</span></a>):</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p6">
<ol class="ltx_enumerate" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">Minimal:</span> systems with minimal risk regarding the principle of fairness do not process sensitive information or their proxies in its workings.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">Limited:</span> systems with a limited risk regarding the principle of fairness process sensitive information or their proxies in its workings but are not used for applications tied to critical services (e.g., healthcare, policing, finance).</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i3.p1.1.1">High:</span> systems that present a high risk regarding the principle of fairness process sensitive information or their proxies in its workings and are used for applications tied to critical services.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S6.I1.i4.p1">
<p class="ltx_p" id="S6.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i4.p1.1.1">Unacceptable:</span> Systems that present unacceptable risk regarding the principle of fairness process sensitive information or their proxies in its workings and are used for applications that can violate fundamental human rights and values.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S6.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Examples of application areas and their categories of risk (Fairness)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T1.1.1.1.1.1">Minimal</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T1.1.1.1.2.1">Limited</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T1.1.1.1.3.1">High</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T1.1.1.1.4.1">Unacceptable</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.1.2.1.1">Weather Forecasting</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.1.2.1.2">Music Recommendation</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.1.2.1.3">Hiring Support</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.1.2.1.4">Facial Recognition for Public Surveillance</td>
</tr>
<tr class="ltx_tr" id="S6.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S6.T1.1.3.2.1">Online Retail</td>
<td class="ltx_td ltx_align_center" id="S6.T1.1.3.2.2">Advertising Algorithms</td>
<td class="ltx_td ltx_align_center" id="S6.T1.1.3.2.3">Financial Fraud Detection</td>
<td class="ltx_td ltx_align_center" id="S6.T1.1.3.2.4">Predictive Policing</td>
</tr>
<tr class="ltx_tr" id="S6.T1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.1.4.3.1">Traffic Management</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.1.4.3.2">AI Art Generation</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.1.4.3.3">Healthcare Scheduling</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.1.4.3.4">Autonomous Weapons</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S6.p7">
<p class="ltx_p" id="S6.p7.1">The following minimal ethical requirements tie normative recommendations to implementable practices. All requirements are general and should be employed regardless of the risk category of a system.</p>
</div>
<section class="ltx_subsection" id="S6.SSx1">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement F1: Bias Analysis</h3>
<div class="ltx_para ltx_noindent" id="S6.SSx1.p1">
<p class="ltx_p" id="S6.SSx1.p1.1">Performing a bias analysis is the foundational step in improving the fairness of an AI system. By systematically examining data, features, and decision-making processes, a bias analysis helps uncover patterns of discrimination or unfairness that may be encoded within the algorithm. This initial assessment provides crucial insights into the sources and manifestations of bias, enabling stakeholders to formulate targeted strategies for mitigation and remediation. Moreover, bias analysis fosters transparency and accountability, allowing for informed decision-making regarding deploying and using AI systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SSx1.p2">
<p class="ltx_p" id="S6.SSx1.p2.1">To infer if an AI system possesses a discriminatory bias against a particular group, we can take two main approaches: statistical or causal. The statistical approach involves analyzing patterns and correlations within the data to identify disparities in outcomes across different demographic groups.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Tools like <a class="ltx_ref ltx_href" href="https://dalex.drwhy.ai/python/api/fairness/index.html" title="">Dalex</a>, <a class="ltx_ref ltx_href" href="https://www.ibm.com/opensource/open/projects/ai-fairness-360/" title="">AI Fairness 360</a>, <a class="ltx_ref ltx_href" href="https://fairlearn.org/v0.8/auto_examples/index.html" title="">Fairlearn</a>, <a class="ltx_ref ltx_href" href="https://github.com/princetonvisualai/revise-tool" title="">REVISE</a>, and <a class="ltx_ref ltx_href" href="https://pair-code.github.io/what-if-tool/" title="">What-If</a> can help you automate several process related to bias analysis, like dataset exploration.</span></span></span> We can also extend this approach to AI systems by sampling comparisons of their input-output behavior in different groups of interest. For example, using fairness metrics or heuristics<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Disparate impact, or the ”80% rule”, is a heuristic (and statistic) form to evaluate discrimination. To learn more, we recommend ”<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1412.3756" title="">Certifying and removing disparate impact</a>”.</span></span></span> to determine if a system is biased toward a particular outcome is an already established practice in machine learning Fairness, giving ready-to-use metrics that can inform stakeholders on the "fairness status" of a system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib207" title="">207</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib138" title="">138</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib13" title="">13</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>To learn more on how to apply different fairness metrics, we recommend ”<a class="ltx_ref ltx_href" href="https://dl.acm.org/doi/abs/10.1145/3194770.3194776" title="">Fairness definitions explained</a>”.</span></span></span> For example, if a credit scoring system provides good scores to an ethical majority 85% of the time but only 50% the time to an ethical minority, this might indicate a violation of statistical parity and, hence, a fairness violation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SSx1.p3">
<p class="ltx_p" id="S6.SSx1.p3.1">While these methods may assess the existence and magnitude of bias, they may fail to establish a causal connection to individual instances. In contrast, the causal approach aims to uncover the causes driving biased outcomes by examining the causal relationships between variables and outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib217" title="">217</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib133" title="">133</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib153" title="">153</a>]</cite>. By employing techniques such as causal inference or experimentation, this approach enables a deeper understanding of how certain factors contribute to disparities and allows for developing targeted interventions to address the root causes of bias. For example, suppose a text-to-image model outputs an image of a man working at a hospital when given the prompt "A working man" but produces an image of a waiter when we modify the prompt to "A working black man" through a causal analysis. In that case, we can diagnose a discriminatory bias in such a system.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>To learn more about causal fairness and how to apply it, we recommend ”<a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v162/nilforoshan22a.html" title="">Causal Conceptions of Fairness and their Consequences</a>”, which is also accompanied by practical demonstrations (Available on <a class="ltx_ref ltx_href" href="https://github.com/stanford-policylab/causal-fairness" title="">GitHub</a>).</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SSx1.p4">
<p class="ltx_p" id="S6.SSx1.p4.1">Systematic scrutiny of data, features, decision-making processes, and general system behavior can reveal biases, empowering stakeholders to devise targeted strategies for mitigation. Whether employing statistical methods to detect disparities or causal approaches to unveil underlying causes, bias analysis fosters transparency and accountability in AI deployment.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SSx2">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement F2: Open Disclosure</h3>
<div class="ltx_para ltx_noindent" id="S6.SSx2.p1">
<p class="ltx_p" id="S6.SSx2.p1.1">In tandem with bias analysis, open disclosure is a pivotal requirement in ensuring that the inherent biases of AI technologies are made clear to a broad audience of stakeholders. Hence, openly disclosing possible biases should be considered a standard practice and an absolute ethical requirement for high-risk applications. This transparency not only cultivates trust but also empowers stakeholders to assess the fairness and reliability of AI systems. Furthermore, open disclosure encourages collaboration and accountability, enabling stakeholders to collectively address and mitigate potential biases and ethical concerns. As done by organizations producing state-of-the-art AI systems, AI applications need to be accompanied by documentation that discloses information about the possible biases systems can have <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib181" title="">181</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib163" title="">163</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>Examples of AI systems that reveal the presence of potential biases can be found <a class="ltx_ref ltx_href" href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0#bias" title="">here</a> and <a class="ltx_ref ltx_href" href="https://huggingface.co/openai-community/gpt2#limitations-and-bias" title="">here</a>. As a reminder, it is essential to note that filling out a model card requires input from different roles, like the developer (e.g., a technician who runs and writes the code), the socio-technic (e.g., someone skilled at analyzing the interaction of technology and society long-term), and the project organizer (e.g., one who understands the overall scope and reach of the technology). For more details on how to structure and build such documentation, we recommend the ”<a class="ltx_ref ltx_href" href="https://huggingface.co/docs/hub/model-card-annotated" title="">Annotated Model Card Template</a>”.</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SSx3">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement F3: Rectify or Mitigate Biased Outcomes</h3>
<div class="ltx_para ltx_noindent" id="S6.SSx3.p1">
<p class="ltx_p" id="S6.SSx3.p1.1">Imagine a bustling metropolis relying on AI algorithms to optimize traffic flow. While bias analysis and open disclosure are initial steps crucial for identifying potential biases, they’re just the beginning. It’s equally vital to proactively address these biases, especially given the high stakes involved. Consider a scenario: In a city where historically marginalized neighborhoods are underserved by public transportation, an AI-powered traffic management system trained with biased samples could inadvertently exacerbate existing disparities if not carefully monitored and corrected. So, we must actively intervene beyond recognizing biases, emphasizing this requirement’s need. In short, rectifying or mitigating biased outcomes entails implementing interventions and strategies to minimize the disparate impact on marginalized or underrepresented groups. To achieve this, we can focus on two main areas of interference (regarding this requirement): data and system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SSx3.p2">
<p class="ltx_p" id="S6.SSx3.p2.1">Data quality is paramount when developing an AI system through a learning paradigm. Quality and representativeness significantly influence AI systems’ fairness and equity, as already shown by studies that strived to create fair datasets for sensitive areas of application <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib102" title="">102</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>For example, <a class="ltx_ref ltx_href" href="https://github.com/joojs/fairface?tab=readme-ov-file" title="">FairFace</a> is a face image dataset that is race-balanced. It contains 108,501 images from 7 different race groups.</span></span></span> Therefore, meticulous examination and management of training data are essential. Techniques such as data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib185" title="">185</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib158" title="">158</a>]</cite>, sampling methodologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib117" title="">117</a>]</cite>, and dataset balancing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib218" title="">218</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib82" title="">82</a>]</cite> can be instrumental in ensuring a more equitable representation of diverse demographic groups within the training data.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Tools like <a class="ltx_ref ltx_href" href="https://github.com/Trusted-AI/AIF360" title="">AI Fairness 360</a> and <a class="ltx_ref ltx_href" href="https://github.com/JerryYLi/Dataset-REPAIR" title="">REPAIR</a> have in-built methods to repair datasets to more equal distributions.</span></span></span> Moreover, ongoing monitoring and validation of data inputs are crucial to detect and address emerging biases over time.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SSx3.p3">
<p class="ltx_p" id="S6.SSx3.p3.1">In addition to data management, interacting directly with a model and implementing guardrails is vital for rectifying or mitigating biased outcomes in AI systems. In this context, model guardrails encompass techniques and strategies embedded within the system to monitor and minimize biases during inference stages. These guardrails serve as safeguards against perpetuating or amplifying biases in the data. Techniques such as adversarial debiasing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib223" title="">223</a>]</cite>, fairness-aware regularization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib101" title="">101</a>]</cite>, and fairness constraints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib220" title="">220</a>]</cite> can be integrated into a model or its larger system to promote fairness and equity.<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>One can also include automated moderation techniques to block certain types of behavior of, for example, generative systems, e.g., specifying lists of <a class="ltx_ref ltx_href" href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words" title="">banned terms</a>, <a class="ltx_ref ltx_href" href="https://github.com/LAION-AI/LAION-SAFETY" title="">NSFW detection</a> tools, and overall <a class="ltx_ref ltx_href" href="https://platform.openai.com/docs/guides/moderation/quickstart" title="">moderation APIs</a>.</span></span></span> By incorporating model guardrails into AI systems, stakeholders can proactively address biases, uphold ethical standards, and promote fairness and equity across various applications and domains.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SSx3.p4">
<p class="ltx_p" id="S6.SSx3.p4.1">By embracing these approaches, stakeholders can navigate the complexities of AI development with a commitment to fairness and equity.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SSx4">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement F4: Human Moderation</h3>
<div class="ltx_para ltx_noindent" id="S6.SSx4.p1">
<p class="ltx_p" id="S6.SSx4.p1.1">Besides all the previously established requirements, specifically in high-risk applications, the path toward automation must be accompanied by scalable human oversight and moderation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib112" title="">112</a>]</cite>. While automated techniques and algorithms are crucial in identifying and mitigating biases, in high-risk scenarios (e.g., recidivism prediction instruments), human intervention is essential in ensuring ethical and responsible AI deployment, given the abstract, nuanced, and ever-changing landscape of human normativity, which constantly clashes with AI systems as out-of-distribution cases. In short, human moderation provides a critical layer of scrutiny and accountability, allowing for the nuanced evaluation of complex ethical considerations and contextual nuances that automated systems may overlook. Moreover, human moderators can offer insights into the socio-cultural implications of AI technologies and contribute to developing contextually sensitive solutions that align with societal values and norms. For instance, consider using automated recidivism prediction algorithms in the criminal justice system. While these algorithms may excel in processing large datasets and identifying patterns, they need support to account for the complexities of individual cases and the societal context in which they operate. A human moderator, familiar with the nuances of the legal system and sensitive to social dynamics, could recognize when the algorithm’s recommendations might inadvertently perpetuate biases or unfairly target specific demographics.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SSx4.p2">
<p class="ltx_p" id="S6.SSx4.p2.1">For minimal or limited risk applications, automated mitigation strategies that use human-AI collaboration strategies can help scale human oversight to realms where human moderation does not scale (e.g., moderation of online forums) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib31" title="">31</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>To learn more about AI and human-AI moderation, we recommend ”<a class="ltx_ref ltx_href" href="https://dl.acm.org/doi/abs/10.1145/3491102.3501999" title="">Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation</a>”.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SSx4.p3">
<p class="ltx_p" id="S6.SSx4.p3.1">By integrating moderation into the AI development lifecycle, stakeholders can foster greater accountability in AI systems, ultimately advancing the goal of creating fair, inclusive, and socially responsible AI technologies.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S7.2.1.1" style="color:#007091;">7</span> </span><span class="ltx_text" id="S7.3.2" style="color:#007091;">Privacy and Data Protection (P)</span>
</h2>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Privacy is a cornerstone value, often endangered when personal data undergoes processing. When we refer to personal data, we speak of information wielded to pinpoint an individual’s identity. However, privacy surpasses mere data; it extends into realms such as decision-making and human autonomy. Safeguarding one’s privacy nurtures independence, fostering the capacity to manifest their true self. It carves out a sanctuary wherein individuals can thrive, expressing themselves freely. Although privacy concepts are ubiquitous across cultures, liberal societies prioritize privacy for fostering individual development. Consequently, it assumes a pivotal role in the framework of democratic states, comprising citizens with diverse perspectives. In essence, safeguarding privacy intertwines closely with (cyber)security measures. The mechanisms that shield a system are akin to those that fortify against data breaches or loss.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">While data protection legislation is prevalent worldwide, the emergence of artificial intelligence systems introduces novel inquiries owing to the vast quantities of data they assimilate for training and operation. Moreover, the adeptness of artificial intelligence systems in establishing connections among previously disparate data sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib43" title="">43</a>]</cite> or extracting profound insights from processed data often surpasses human capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib93" title="">93</a>]</cite>. Consequently, distinct challenges manifest concerning data processing by autonomous systems. Furthermore, data acquisition through sensors integrated into AI-driven products exacerbates the opacity surrounding data collection.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">The scope of data has evolved beyond mere alphanumeric characters stored in text files; it now encompasses diverse formats such as voice recordings, images, videos, and similar media, potentially containing personal information. Processed data may not exclusively comprise personally identifiable information; it could also include data about system performance, which warrants protection. For instance, performance data might divulge insights into a system, encroaching upon operational secrets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib43" title="">43</a>]</cite>. Risks arising from the data processing by artificial intelligence systems and their training transcend concerns solely about individual natural persons. They extend to groups, organizations, institutions, and enterprises, with the latter facing potential harm in scenarios like data breaches jeopardizing intellectual property. Moreover, deploying technologies such as facial recognition in public spaces can infringe upon an individual’s privacy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">For global enterprises seeking to engage in various markets worldwide, navigating regional legislation becomes imperative. In Europe, for instance, adherence to EU law, specifically the General Data Protection Regulation, is requisite when dealing with data subjects within EU territory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib87" title="">87</a>]</cite>. However, exemptions exist regarding data protection for research purposes. Synthetic data emerges as a viable alternative, circumventing certain challenges associated with real-world data usage. Companies may thus contemplate employing artificial data, referring to (annotated) information that mirrors real-world scenarios and is generated, for instance, through computer simulations. <span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span>For example, <a class="ltx_ref ltx_href" href="https://github.com/sdv-dev/SDV" title="">MIT’s Data to AI Lab has developed a library for synthetic data generation for tabular data</a>, now maintained by a company named DataCebo.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p5">
<p class="ltx_p" id="S7.p5.1">Certification in data protection, exemplified by seals and marks,<span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span>Some examples of such seals are <a class="ltx_ref ltx_href" href="https://trustarc.com/" title="">TrustArc</a> (formerly TrustE) and <a class="ltx_ref ltx_href" href="https://www.euprivacy-seal.com/" title="">EuroPriSe</a>.</span></span></span> serves as tangible evidence of adherence to privacy principles, particularly exemplified by the EU’s GDPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib1" title="">1</a>]</cite>. Documenting compliance with data protection standards is essential meticulously <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib87" title="">87</a>]</cite>. The GDPR explicitly prohibits the processing of special categories of personal data, encompassing information revealing sensitive aspects such as racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data for unique identification, data concerning health, or data regarding a person’s sexual life or orientation, unless exceptions outlined in Article 9(2), such as data subject consent, are applicable <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib1" title="">1</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span>Similar distinctions are also present in other legislations, such as the Brazilian GDPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib57" title="">57</a>]</cite>.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p6">
<p class="ltx_p" id="S7.p6.1">Beyond safeguarding personal data, ensuring the quality and integrity of datasets is imperative <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib4" title="">4</a>]</cite>, necessitating organizational investment in data governance. Instances like the notorious case of an AI system mislabeling black individuals as gorillas underscore the importance of rectifying flawed datasets. However, from a privacy-preserving perspective, the resolution cannot entail continually augmenting system training data or incorporating data from vulnerable populations. Such an approach would perpetuate the accumulation of extensive datasets, including sensitive information from vulnerable demographic groups.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p7">
<p class="ltx_p" id="S7.p7.1">Hence, as we navigate the landscape of privacy and data protection in the age of artificial intelligence, it becomes evident that safeguarding individual autonomy and upholding democratic principles remain paramount. The evolution of data formats and the proliferation of AI systems pose novel challenges, necessitating robust frameworks and innovative solutions. Yet, amidst these complexities, the imperative to respect privacy transcends mere compliance. It embodies a commitment to fostering trust, empowering individuals, and preserving fundamental human rights in the digital era.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p8">
<p class="ltx_p" id="S7.p8.1">To help organizations prospect the level of risk regarding their system or application, we propose the following criteria, in line with the four main categories
of risk identified in the EU AI Act (examples in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S7.T2" title="Table 2 ‣ 7 Privacy and Data Protection (P) ‣ Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_tag">2</span></a>):</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p9">
<ol class="ltx_enumerate" id="S7.I1">
<li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S7.I1.i1.p1">
<p class="ltx_p" id="S7.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i1.p1.1.1">Minimal:</span> systems with minimal risk regarding the principle of privacy do not need to process personally identifiable information for their basic functioning in any way.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S7.I1.i2.p1">
<p class="ltx_p" id="S7.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i2.p1.1.1">Limited:</span> systems with a limited risk regarding the principle of privacy process personally identifiable information in their workings but are not used for applications tied to critical services (e.g., healthcare, policing, finance).</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S7.I1.i3.p1">
<p class="ltx_p" id="S7.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i3.p1.1.1">High:</span> systems that present a high risk regarding the principle of privacy process personally identifiable information in their workings and are used for applications tied to critical services.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S7.I1.i4.p1">
<p class="ltx_p" id="S7.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i4.p1.1.1">Unacceptable:</span> Systems that present unacceptable risk regarding the principle of privacy process personally identifiable information in their workings and are used for applications that can violate fundamental human rights and values.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S7.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Examples of application areas and their categories of risk (Privacy)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S7.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S7.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S7.T2.1.1.1.1.1">Minimal</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S7.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S7.T2.1.1.1.2.1">Limited</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S7.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S7.T2.1.1.1.3.1">High</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S7.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S7.T2.1.1.1.4.1">Unacceptable</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T2.1.2.1.1">Weather Forecasting</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T2.1.2.1.2">Email Filtering</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T2.1.2.1.3">Healthcare App.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T2.1.2.1.4">Facial Recognition for Public Surveillance</td>
</tr>
<tr class="ltx_tr" id="S7.T2.1.3.2">
<td class="ltx_td ltx_align_center" id="S7.T2.1.3.2.1">Industrial Automation</td>
<td class="ltx_td ltx_align_center" id="S7.T2.1.3.2.2">Recommendation Engine</td>
<td class="ltx_td ltx_align_center" id="S7.T2.1.3.2.3">Fraud Detection</td>
<td class="ltx_td ltx_align_center" id="S7.T2.1.3.2.4">Social Credit Scoring</td>
</tr>
<tr class="ltx_tr" id="S7.T2.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T2.1.4.3.1">Translation Engines</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T2.1.4.3.2">AI Assistant</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T2.1.4.3.3">Targeted Marketing</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T2.1.4.3.4">Autonomous Weapons</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S7.p10">
<p class="ltx_p" id="S7.p10.1">The following minimal ethical requirements tie normative recommendations to implementable practices. All requirements are general and should be employed regardless of the risk category of a system.</p>
</div>
<section class="ltx_subsection" id="S7.SSx1">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement P1: Purpose Limitation and Data Minimization</h3>
<div class="ltx_para ltx_noindent" id="S7.SSx1.p1">
<p class="ltx_p" id="S7.SSx1.p1.1">Developers and implementers of artificial intelligence systems must question the necessity of processing personal data for system functionality. They should consider whether training or operating the system is feasible without relying on personal data. This imperative aligns with the principle of data minimization, which dictates that data processing should always be appropriate, pertinent, and constrained concerning its processing objective <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib1" title="">1</a>, Art. 5 (1) (b,c)]</cite>. Data minimization and purpose limitation should be coupled with a reflective examination of the true objective of the AI software <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib87" title="">87</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SSx2">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement P2: User Control and Information</h3>
<div class="ltx_para ltx_noindent" id="S7.SSx2.p1">
<p class="ltx_p" id="S7.SSx2.p1.1">Individuals must receive notification when their data undergoes processing. Users should be able to control their data to the greatest extent possible. Meaningful and informed consent is paramount; pre-checked checkboxes do not constitute valid consent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib87" title="">87</a>]</cite>. Users should have the option to opt in or, at the very least, opt out of data processing. Moreover, following the GDPR, users retain the right to revoke their consent to data processing at any juncture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib1" title="">1</a>, Art. 7 (3)]</cite>. The GDPR further establishes the "right to be forgotten," entitling individuals to request the deletion of their data, as stipulated in Article 17 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib1" title="">1</a>, Art. 17]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SSx2.p2">
<p class="ltx_p" id="S7.SSx2.p2.1">Understanding the perspectives of customers or data subjects is invaluable for specific stakeholders, enabling alignment between expectations and the actual technological capabilities of a system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib93" title="">93</a>]</cite>. Users should also be kept abreast of governmental and law enforcement involvements concerning an AI system. Information dissemination should prioritize accessibility, employing user-friendly formats like pictograms or privacy dashboards <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib87" title="">87</a>]</cite>. A comprehensive privacy policy should cater to external users and internal stakeholders such as employees and company members.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SSx3">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement P3: Data Protection by Design and Default</h3>
<div class="ltx_para ltx_noindent" id="S7.SSx3.p1">
<p class="ltx_p" id="S7.SSx3.p1.1">Data protection must be ingrained from the outset in the development of AI systems, adhering to the principle of implementing privacy protection by design. This entails incorporating various technical and organizational measures during the system’s conceptualization phase. These measures may encompass anonymization or pseudonymization (cf. requirement P4) and differential privacy—a concept aimed at ensuring privacy for individuals’ information within a group database.<span class="ltx_note ltx_role_footnote" id="footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span>Different tools from different actors, including Google and IBM, are available for using differential privacy as a privacy-preserving technique, e.g., <a class="ltx_ref ltx_href" href="https://github.com/opendp/opendp" title="">OpenDP</a>.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SSx3.p2">
<p class="ltx_p" id="S7.SSx3.p2.1">Additionally, data protection should be automatically guaranteed as the default setting. This entails preconfiguring systems with the most privacy-friendly options, safeguarding users’ privacy, and preventing inadvertent disclosure of their data to unknown entities without consent. Data protection by design and by default constitutes an obligation for data controllers as stipulated by the GDPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib1" title="">1</a>, Art. 25]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SSx4">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement P4: Anonymization and Pseudonymization</h3>
<div class="ltx_para ltx_noindent" id="S7.SSx4.p1">
<p class="ltx_p" id="S7.SSx4.p1.1">To prevent data breaches, it is imperative to utilize anonymization<span class="ltx_note ltx_role_footnote" id="footnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span>Anonymization is the process of removing any information that can link sensitive personal data back to an individual entity. Learn more in ”<a class="ltx_ref ltx_href" href="https://privacypatterns.org/patterns/Anonymity-set" title="">Anonymity Set</a>”.</span></span></span> or, at the very least, pseudonymization<span class="ltx_note ltx_role_footnote" id="footnote22"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span>Pseudonymisation is a process of removing or replacing personal identifiers from data and using placeholder values or reference numbers instead. Learn more in ”<a class="ltx_ref ltx_href" href="https://privacypatterns.org/patterns/Pseudonymous-identity" title="">Psuedonymous Identity</a>”.</span></span></span> techniques when processing personal data. When information is anonymized, data protection regulations cease to apply <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib1" title="">1</a>, Recital 26]</cite>. However, it’s essential to implement technical measures to ensure that re-identification remains unfeasible, as technological advancements or additional information releases could reverse anonymization. Preventing unwarranted and unforeseen cross-correlation of datasets is crucial <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib93" title="">93</a>]</cite>. Anonymization can also serve the data controller’s interests, particularly if a user later withdraws their consent to data processing. This approach can avert the costly retraining of the entire model in a worst-case scenario <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib164" title="">164</a>]</cite>. All data, including metadata that may facilitate the identification of an individual <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib87" title="">87</a>]</cite>, should be promptly deleted once they are no longer required. This principle extends to scenarios where the system is decommissioned <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib93" title="">93</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SSx5">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement P5: Data Protection Impact Assessment</h3>
<div class="ltx_para ltx_noindent" id="S7.SSx5.p1">
<p class="ltx_p" id="S7.SSx5.p1.1">A Data Protection Impact Assessment (DPIA) should be conducted to evaluate potential data protection risks inherent in a project, particularly when incorporating new technologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib1" title="">1</a>, Art. 35, Recital 90]</cite>. Various tools and guides are available to facilitate the execution of a DPIA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib136" title="">136</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib37" title="">37</a>]</cite>, emphasizing its necessity for sensitive data processing scenarios like profiling, public space monitoring, or automated decision-making. Stakeholders, including developers and customers of AI systems, can refer to a list compiled by the European Data Protection Supervisor to determine the need for a DPIA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib56" title="">56</a>]</cite>. All relevant parties must prioritize the protection of potentially affected individuals’ privacy. Simply relying on the AI system provider to safeguard users’ data is insufficient, as harm may arise upon system deployment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib55" title="">55</a>]</cite>. Consequently, developers/sellers and buyers of AI systems should conduct DPIAs. Additionally, prospective positive outcomes from developing and deploying AI systems should be considered <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib202" title="">202</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SSx6">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement P6: Codes of Conduct and Privacy Management</h3>
<div class="ltx_para ltx_noindent" id="S7.SSx6.p1">
<p class="ltx_p" id="S7.SSx6.p1.1">Developers and implementers of AI systems must engage in introspection regarding their business practices. At the same time, data processors should formulate codes of conduct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib1" title="">1</a>, Art. 40]</cite>. At the management level, individuals should routinely contemplate the company’s values, fostering inclusivity by involving staff members. Privacy assurance should be multifaceted, incorporating both technical and organizational measures. A well-defined privacy policy, structured as a twofold approach encompassing an "enforce strategy" aimed internally at the organization and ensuring compliance with externally communicated privacy statements, is essential <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib87" title="">87</a>]</cite>. Collectively, these components constitute a comprehensive "privacy management system" <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib87" title="">87</a>]</cite>, which can be implemented in tandem with a DPIA. This system should also encompass potential mitigation measures for addressing risks and breaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib93" title="">93</a>]</cite>. Furthermore, a broader data management strategy can aid in anticipating undesirable outcomes and contribute to ensuring the quality of datasets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S8.2.1.1" style="color:#007091;">8</span> </span><span class="ltx_text" id="S8.3.2" style="color:#007091;">Safety and Robustness (SR)</span>
</h2>
<div class="ltx_para ltx_noindent" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">The concepts of safety and robustness are closely related to another ethical principle: non-maleficence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib41" title="">41</a>]</cite>, which is a principle commonly used in the context of bioethics and medical ethics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib145" title="">145</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib72" title="">72</a>]</cite>, dictating that medical practitioners and bio researchers must do no harm or allow harm to be caused to a patient through neglect. Similarly, safety and robustness are considered an indispensable pillar of AI ethics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib100" title="">100</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib85" title="">85</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib89" title="">89</a>]</cite>, dictating that AI systems should have guarantees in relationship to their safety (avoidance of harm) and robustness (remain safe and accurate under adverse conditions). Such requirements are paramount when high-stakes applications meet the complex, dynamic, fuzzy, and noisy real world.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">In terms of safety, what we are mainly concerned with is avoiding unintended harm, i.e., accidents.<span class="ltx_note ltx_role_footnote" id="footnote23"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span>In this section, we are not considering cases where malicious actors intentionally use AI systems to provoke harm, given that such applications require only one ethical requirement (prohibition and termination). For readers interested in the landscape of potential security threats from malicious uses of artificial intelligence technologies, we recommend ”<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1802.07228" title="">The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation</a>”.</span></span></span> In almost any engineering field, we can define accidents as situations where a human designer has a specific objective or task in mind. Still, the system designed and deployed for that task produced harmful and unexpected results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib7" title="">7</a>]</cite>. In the context of AI, these events usually come in two primary forms:</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p3">
<ul class="ltx_itemize" id="S8.I1">
<li class="ltx_item" id="S8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i1.p1">
<p class="ltx_p" id="S8.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i1.p1.1.1">Side Effects:</span> unintended adverse effects that the system designers did not originally envision (e.g., a recommendation system that produces addictive behavior).</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S8.I1.i2.p1">
<p class="ltx_p" id="S8.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i2.p1.1.1">Vulnerabilities:</span> harmful behaviors that adversaries can elicit to produce negative consequences (e.g., when someone jailbreaks an AI system to perform illicit activities).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S8.p4">
<p class="ltx_p" id="S8.p4.1">Unlike side effects, vulnerabilities may have an intentional component tied to the desires of the adversary. Still, they can also happen due to distributional shifts, where vulnerabilities or hidden functionalities emerge when a system operates outside its original usage scope. That is when the concept of robustness becomes imperative. For AI systems to be considered safe, they must also be robust under adversarial or unusual scenarios. Hence, both these principles are united under this set of specific requirements. Unfortunately, as is the case in other safety-critical areas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib193" title="">193</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib64" title="">64</a>]</cite>, robust safety regulations and practices are almost only considered after a tragedy.<span class="ltx_note ltx_role_footnote" id="footnote24"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span>As the saying goes for the Aviation industries: <span class="ltx_text ltx_font_italic" id="footnote24.1">”Aviation regulations are written in blood”.</span></span></span></span> Our efforts in listing these minimal ethical requirements, in line with the AI Act, are to help organizations think in a preventive matter, which is bound to improve the workings of their technologies and the safety of all stakeholders involved.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p5">
<p class="ltx_p" id="S8.p5.1">To help organizations prospect the level of risk regarding their system or application, we propose the following criteria, in line with the four main categories of risk identified in the EU AI Act (examples in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S8.T3" title="Table 3 ‣ 8 Safety and Robustness (SR) ‣ Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_tag">3</span></a>):</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p6">
<ol class="ltx_enumerate" id="S8.I2">
<li class="ltx_item" id="S8.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S8.I2.i1.p1">
<p class="ltx_p" id="S8.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I2.i1.p1.1.1">Minimal:</span> Systems that present a minimal risk regarding the principle of Safety and Robustness are those where failure or malfunctioning of the system would not pose any direct threat to human safety or cause significant disruptions. These systems may operate in non-critical environments or have fail-safe mechanisms that mitigate potential risks to an acceptable level.</p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S8.I2.i2.p1">
<p class="ltx_p" id="S8.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I2.i2.p1.1.1">Limited:</span> Systems that present a limited risk regarding the principle of Safety and Robustness are those where failure or malfunctioning could potentially lead to minor injuries or inconveniences to individuals. While these systems are not directly tied to critical safety concerns, their failure could still adversely affect users or stakeholders.</p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S8.I2.i3.p1">
<p class="ltx_p" id="S8.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I2.i3.p1.1.1">High:</span> Systems that present a high risk regarding the principle of Safety and Robustness are those where failure or malfunctioning could result in significant harm, injury, or loss of life. These systems are often found in critical infrastructure, transportation, healthcare, or manufacturing sectors.</p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S8.I2.i4.p1">
<p class="ltx_p" id="S8.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I2.i4.p1.1.1">Unacceptable:</span> Systems that present an unacceptable risk regarding the principle of Safety and Robustness are those where failure or malfunctioning could lead to catastrophic consequences, including widespread loss of life, severe environmental damage, or irreversible harm. These systems should not be developed or deployed under any circumstances, and existing deployments should be terminated immediately.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S8.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Examples of application areas and their categories of risk (Safety and Robustness)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S8.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S8.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S8.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S8.T3.1.1.1.1.1">Minimal</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S8.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S8.T3.1.1.1.2.1">Limited</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S8.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S8.T3.1.1.1.3.1">High</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S8.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S8.T3.1.1.1.4.1">Unacceptable</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S8.T3.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T3.1.2.1.1">Auto Correct</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T3.1.2.1.2">Text-to-Image Models</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T3.1.2.1.3">Weather Forecast</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T3.1.2.1.4">Automated Policing</td>
</tr>
<tr class="ltx_tr" id="S8.T3.1.3.2">
<td class="ltx_td ltx_align_center" id="S8.T3.1.3.2.1">Translation Engines</td>
<td class="ltx_td ltx_align_center" id="S8.T3.1.3.2.2">Assisted Image Editing</td>
<td class="ltx_td ltx_align_center" id="S8.T3.1.3.2.3">Assisted Policing</td>
<td class="ltx_td ltx_align_center" id="S8.T3.1.3.2.4">Recidivism Forecasting</td>
</tr>
<tr class="ltx_tr" id="S8.T3.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T3.1.4.3.1">Optical Character Recognition</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T3.1.4.3.2">Recommendation Systems</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T3.1.4.3.3">Assisted Surgery</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T3.1.4.3.4">Automated Cyberattacks</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S8.p7">
<p class="ltx_p" id="S8.p7.1">The following minimal ethical requirements tie normative recommendations to implementable practices. All requirements are general and should be employed regardless of the risk category of a system.</p>
</div>
<section class="ltx_subsection" id="S8.SSx1">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement SR1: Robust and Open Evaluation</h3>
<div class="ltx_para ltx_noindent" id="S8.SSx1.p1">
<p class="ltx_p" id="S8.SSx1.p1.1">The first step towards safe and robust AI is how thoroughly we evaluate our systems. As the potential risks associated with AI applications increase, so should the rigor of evaluation efforts. The evaluation of AI systems concerning safety and robustness must be conducted with meticulous attention to detail and openness to scrutiny. Hence, systems categorized as minimal risk may afford more flexibility in evaluation methodologies, and applications operating in high-risk domains necessitate a more comprehensive and transparent assessment process.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx1.p2">
<p class="ltx_p" id="S8.SSx1.p2.1">For example, in the case of machine learning systems, while the vanilla train/validation/test split may be sufficient to access the reliability of an innocuous system that will operate under a very narrow domain, it may fail to explore the limitations of systems that have to work in more lose and unpredictable environments. Hence, a minimal requirement for any system categorized as high-risk should be evaluating its functionality on standard benchmarks. For organizations that do not possess their own harness of evaluations, open evaluation benchmarks should be utilized (e.g., Language Model Evaluation Harness for text generation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib69" title="">69</a>]</cite>, FACET for facial recognition applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib80" title="">80</a>]</cite>, Q-Bench for multimodal model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib215" title="">215</a>]</cite>). However, robust and reproducible evaluation should be a standard practice, regardless of the level of risk. Hence, organizations are encouraged to adopt standardized evaluation frameworks and methodologies tailored to the specific characteristics of the AI system under scrutiny. Additionally, leveraging community resources, such as benchmark datasets, evaluation protocols, and collaborative platforms, helps to stimulate a culture of openness and collaboration in the AI community.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx1.p3">
<p class="ltx_p" id="S8.SSx1.p3.1">Finally, high-risk systems should be incentivized to make evaluation procedures and results accessible to relevant stakeholders, including developers, regulators, and end-users. Exposing benchmark results is already standard practice in academia and is being adopted by the industry.<span class="ltx_note ltx_role_footnote" id="footnote25"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span>The existence of leaderboards is an example of this (e.g., <a class="ltx_ref ltx_href" href="https://crfm.stanford.edu/helm/lite/latest/#/leaderboard" title="">HELM</a> and <a class="ltx_ref ltx_href" href="https://decodingtrust.github.io/leaderboard/" title="">DecodingTrust</a> leaderboards.).</span></span></span> Moreover, minimal performance on such evaluations may become required for high-risk cases as regulation becomes more rigorous. It is imperative to prioritize robust and open evaluation practices, particularly in high-risk applications such as autonomous vehicles, medical diagnosis systems, and critical infrastructure control systems. By adhering to rigorous evaluation standards and embracing transparency, stakeholders can instill confidence in the safety and reliability of AI technologies, thereby promoting their responsible development and deployment.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SSx2">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement SR2: Red Teaming and Broader Impact Analysis</h3>
<div class="ltx_para ltx_noindent" id="S8.SSx2.p1">
<p class="ltx_p" id="S8.SSx2.p1.1">Organizations that develop or use AI systems may be unaware of their systems’ vulnerabilities or hidden functions when dealing with (1) the complexities of the real world or (2) the ingenuity of adversaries. Hence, ensuring the safety and robustness of AI systems requires proactive measures to identify these vulnerabilities, assess potential risks, and mitigate adverse impacts. That is where the idea of employing red teams comes into place.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx2.p2">
<p class="ltx_p" id="S8.SSx2.p2.1">The term red team, as the name suggests, comes from the United States of America and Cold War-era military simulations (adversarial teams were the "red team") <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib124" title="">124</a>]</cite>. Generally, it means role-playing as the adversary while conducting a vulnerability assessment. In other words, good guys pretend they are bad guys to the best of their ability, so when the actual bad guys come, the good guys are prepared. Nowadays, red teaming is a common practice in cybersecurity, where "red-teamers", also referred to as pen(etration) testers, are hired to test the security of physical locations or computer networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib2" title="">2</a>]</cite>. Finally, the AI community has adopted this practice, given that modern AI systems based on large neural networks may exhibit emergent properties and behaviors that designers cannot fully predict ahead of time.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx2.p3">
<p class="ltx_p" id="S8.SSx2.p3.1">In the context of AI, red teaming involves systematically simulating adversarial attacks, scenarios, and misuse cases to uncover weaknesses in AI systems’ behavioral processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib68" title="">68</a>]</cite>. By adopting the perspective of malicious actors or unintended users, red teams can identify vulnerabilities that traditional testing methodologies may overlook. These exercises encompass various threat vectors, including adversarial manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib74" title="">74</a>]</cite>, data poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib189" title="">189</a>]</cite>, back door attacks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib91" title="">91</a>]</cite>, and other general misbehaviors (e.g., unintended biases) and patterns that can be uncovered through adversarial techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib110" title="">110</a>]</cite>. It is essential to notice that red teaming practices are not limited to machine learning systems, given that other types of intelligent systems can also be corrupted if the larger context in which they operate is insecure. Also, given that machine learning systems do not live in a vacuum, questions related to general software and system security should also be areas to explore in the red teaming process.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx2.p4">
<p class="ltx_p" id="S8.SSx2.p4.1">Red teaming should be conducted iteratively throughout the development life cycle to validate the effectiveness of mitigation strategies and resilience measures. It’s crucial to acknowledge that red teaming is a process that varies for each system, depending on its complexity, domain, and potential threat landscape. Hence, organizations should carefully adopt measures sensitive to their context, ensuring that red-teaming efforts are tailored to the risks and challenges faced by the AI system under scrutiny.<span class="ltx_note ltx_role_footnote" id="footnote26"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span>The study ”<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2209.07858" title="">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</a>” offers a comprehensive review of the challenges related to building and administrating red teams.</span></span></span> For applications categorized as minimal risk, automated forms of red teaming, such as adversarial training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib214" title="">214</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib184" title="">184</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib9" title="">9</a>]</cite>, may suffice to uncover and address vulnerabilities. However, in the case of high-risk applications, human-led red teaming exercises become indispensable (in addition to automated forms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib161" title="">161</a>]</cite>), providing a more nuanced understanding of potential threats and ensuring comprehensive risk assessment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx2.p5">
<p class="ltx_p" id="S8.SSx2.p5.1">Finally, reporting the findings of a red teaming process should also be a requirement for high-risk situations. Currently, model reporting practices also integrate the disclosure of systems limitations and potential flaws, much like prescription drugs have warnings against substance abuse. These broader impact reports should disclose the potential social, ethical, and environmental ramifications of deploying a given AI system in real-world settings. Beyond technical metrics, organizations must consider the broader implications of their AI solutions on stakeholders, communities, and society at large (e.g., impact expectations on the job market due to technological displacement). This analysis should encompass fairness, accountability, transparency, privacy, and inclusivity to ensure that AI deployments align with ethical principles and societal values.<span class="ltx_note ltx_role_footnote" id="footnote27"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup><span class="ltx_tag ltx_tag_note">27</span>Here, we have two examples, <a class="ltx_ref ltx_href" href="https://github.com/openai/gpt-3/blob/master/model-card.md" title="">GPT-3</a> and <a class="ltx_ref ltx_href" href="https://github.com/facebookresearch/llama/blob/llama_v1/MODEL_CARD.md" title="">Llama</a>, of model cards that disclose limitations and ethical considerations.</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S8.SSx3">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement SR3: Human Oversight</h3>
<div class="ltx_para ltx_noindent" id="S8.SSx3.p1">
<p class="ltx_p" id="S8.SSx3.p1.1">As AI applications become increasingly pervasive and complex, human intervention and decision-making are critical in mitigating risks, addressing uncertainties, and upholding ethical standards. According to the EU High-Level Expert Working Group (EU HLEG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib4" title="">4</a>]</cite>, "any allocation of functions between humans and AI systems should follow human-centric design principles and leave meaningful opportunity for human choice", which entails implementing human oversight and controls over AI systems and processes. Meanwhile, the EU AI Act states that AI should "be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use". Therefore, human oversight becomes a requirement to ensure AI systems’ safety, robustness, and moral integrity are upheld throughout their lifecycle, tying a human factor to their existence.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx3.p2">
<p class="ltx_p" id="S8.SSx3.p2.1">While AI systems with minimal risk may have more lenience regarding oversight, deploying AI systems in high-stakes domains necessitates human oversight to complement automated processes and algorithms. Human oversight encompasses a range of activities, including monitoring system behavior, interpreting outputs, making critical decisions, and intervening when necessary to prevent or mitigate adverse outcomes. In short, human moderators can provide contextual understanding and domain expertise while supporting the identification of edge cases, ambiguous scenarios, and ethical dilemmas that may challenge the robustness and fairness of AI systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx3.p3">
<p class="ltx_p" id="S8.SSx3.p3.1">According to the EU HLEG, the following levels of oversight should be considered:</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx3.p4">
<ul class="ltx_itemize" id="S8.I3">
<li class="ltx_item" id="S8.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I3.i1.p1">
<p class="ltx_p" id="S8.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I3.i1.p1.1.1">Human in the loop:</span> human intervention at every stage of the AI lifecycle. For example, a radiologist may review and interpret the AI-generated diagnostic in a medical diagnosis assisted by an AI system, providing additional context and expertise before finalizing the diagnosis and treatment plan.</p>
</div>
</li>
<li class="ltx_item" id="S8.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I3.i2.p1">
<p class="ltx_p" id="S8.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I3.i2.p1.1.1">Human on the loop:</span> human intervention during the design cycle of the system and monitoring the system’s operation. For example, in an automated content moderation system for a social media platform, human moderators periodically review flagged content to ensure accuracy and fairness while providing feedback to improve the algorithm’s performance.</p>
</div>
</li>
<li class="ltx_item" id="S8.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S8.I3.i3.p1">
<p class="ltx_p" id="S8.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I3.i3.p1.1.1">Human in command:</span> the capability to oversee the overall activity of the AI system and decide when and how to use the system in any particular situation. For example, in an autonomous vehicle, the human driver retains ultimate authority over the vehicle’s operation, with the ability to override the AI system’s decisions in emergencies, such as taking manual control to avoid a collision.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx3.p5">
<p class="ltx_p" id="S8.SSx3.p5.1">Organizations should establish clear lines of responsibility, accountability, and authority for human decision-makers to operationalize these approaches by developing training programs, guidelines, and protocols to empower human stakeholders with the knowledge, skills, and resources needed to fulfill their oversight roles effectively and at scale.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S9.2.1.1" style="color:#007091;">9</span> </span><span class="ltx_text" id="S9.3.2" style="color:#007091;">Sustainability (SU)</span>
</h2>
<div class="ltx_para ltx_noindent" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">The relentless march of technology within modern society inexorably intertwines with ecological markers, profoundly influencing our planetary boundaries regarding what our world can support <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib157" title="">157</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib178" title="">178</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib94" title="">94</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib213" title="">213</a>]</cite>. Technological advancements, while promising efficiency and convenience, often come at a steep cost to the environment. From the extraction of rare earth metals to the proliferation of electronic waste, the ecological footprint of our digital age is unmistakable. At the same time, given that the current advances of AI are tied to the severe scaling of its components, like the amount of hardware (and resources to run this hardware) available, many currently question the sustainability of AI development at the scale it is being pursued <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib154" title="">154</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib177" title="">177</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib204" title="">204</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S9.p2">
<p class="ltx_p" id="S9.p2.1">If we narrow our focus to artificial intelligence, it becomes apparent that the most significant ecological impacts stem from developments tied to the deep learning paradigm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib166" title="">166</a>]</cite>. Training massive neural networks necessitates substantial computational power <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib125" title="">125</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib159" title="">159</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib52" title="">52</a>]</cite>, driving the demand for specialized hardware optimized for such tasks. However, this quest for efficiency often comes at the expense of environmental sustainability. The carbon footprint of training large neural networks is substantial, with estimates suggesting that training a single deep learning model can emit as much carbon dioxide as several cars over their lifetimes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib191" title="">191</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib128" title="">128</a>]</cite>. Additionally, the operation of data centers to support these computational tasks requires vast amounts of water for cooling, further straining already stressed water resources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib150" title="">150</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S9.p3">
<p class="ltx_p" id="S9.p3.1">Moreover, the production of specialized hardware relies heavily on extractivist practices, contributing to environmental degradation and social injustices in mining communities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib152" title="">152</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib148" title="">148</a>]</cite>. For instance, cobalt mining, a crucial component in many computing devices, has been linked to child labor and environmental pollution in regions like the Democratic Republic of Congo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib201" title="">201</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib165" title="">165</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib12" title="">12</a>]</cite>. These interconnected environmental costs underscore the urgent need for reevaluation and mitigation strategies in pursuing AI development, challenging the prevailing narrative of technological progress at any cost.</p>
</div>
<div class="ltx_para ltx_noindent" id="S9.p4">
<p class="ltx_p" id="S9.p4.1">These pressing environmental concerns must be carefully considered when developing trustworthy AI. As stakeholders in the AI field strive to ensure the ethical and responsible deployment of artificial intelligence systems, environmental sustainability must be integrated into the core principles of AI development. Failure to address these ecological impacts exacerbates environmental degradation and poses significant risks for (not so) future generations. Without concerted efforts to mitigate the ecological costs of AI development and the technological industry in general (not to mention the clothing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib36" title="">36</a>]</cite>, agricultural <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib190" title="">190</a>]</cite>, and mining industries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib141" title="">141</a>]</cite>), we risk leaving behind a legacy of depleted resources, polluted environments, and social injustices for future inhabitants of our planet. Thus, current stakeholders must acknowledge and address these challenges, paving the way for a more sustainable and equitable future where technological progress is harmonized with ecological well-being.</p>
</div>
<div class="ltx_para ltx_noindent" id="S9.p5">
<p class="ltx_p" id="S9.p5.1">Now, to promote sustainable AI development, there are two main approaches stakeholders can take <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib204" title="">204</a>]</cite>:<span class="ltx_note ltx_role_footnote" id="footnote28"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup><span class="ltx_tag ltx_tag_note">28</span>If you wish to explore the requirements for this distinction further, we recommend ”<a class="ltx_ref ltx_href" href="https://link.springer.com/article/10.1007/s43681-023-00323-3" title="">Challenging AI for Sustainability: what ought it mean?</a>”.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S9.p6">
<ul class="ltx_itemize" id="S9.I1">
<li class="ltx_item" id="S9.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S9.I1.i1.p1">
<p class="ltx_p" id="S9.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S9.I1.i1.p1.1.1">AI for sustainability:</span> applying artificial intelligence technologies to address sustainability challenges and promote sustainable development.<span class="ltx_note ltx_role_footnote" id="footnote29"><sup class="ltx_note_mark">29</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">29</sup><span class="ltx_tag ltx_tag_note">29</span>To learn more about research avenues for using AI to achieve sustainable outcomes, we recommend ”<a class="ltx_ref ltx_href" href="https://www.sciencedirect.com/science/article/pii/S0268401220300967" title="">Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda</a>”.</span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S9.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S9.I1.i2.p1">
<p class="ltx_p" id="S9.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S9.I1.i2.p1.1.1">Sustainability of AI:</span> developing and deploying artificial intelligence in an environmentally responsible manner.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S9.p7">
<p class="ltx_p" id="S9.p7.1">This work focuses on the second approach. Hence, all our suggested requirements are made with the sustainability of AI in mind. To help organizations prospect the level of risk regarding their system or application, we propose the following criteria, in line with the four main categories of risk identified in the EU AI Act (examples in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S9.T4" title="Table 4 ‣ 9 Sustainability (SU) ‣ Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_tag">4</span></a>):</p>
</div>
<div class="ltx_para ltx_noindent" id="S9.p8">
<ol class="ltx_enumerate" id="S9.I2">
<li class="ltx_item" id="S9.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="S9.I2.i1.p1">
<p class="ltx_p" id="S9.I2.i1.p1.1">Systems that present a limited risk regarding the principle of Sustainability require a negligible amount of energy to be produced and used. For these, the threshold is set to be between 1 - 50 KgCO2eq during development (i.e., around 1000 Km driven by an average gasoline-powered passenger vehicle.) and below 50 g CO2 per usage (i.e., an input-output mapping of the model).<span class="ltx_note ltx_role_footnote" id="footnote30"><sup class="ltx_note_mark">30</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">30</sup><span class="ltx_tag ltx_tag_note">30</span>This threshold is in line with the limits stipulated for car emissions by kilometer according to EU climate action laws. For more information, read ”<a class="ltx_ref ltx_href" href="https://climate.ec.europa.eu/eu-action/transport/road-transport-reducing-co2-emissions-vehicles/co2-emission-performance-standards-cars-and-vans_en" title="">CO2 emission performance standards for cars and vans</a>”.</span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S9.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S9.I2.i2.p1">
<p class="ltx_p" id="S9.I2.i2.p1.1">Systems that present a limited risk regarding the principle of Sustainability remain within a manageable level of energy consumption and environmental impact. This threshold is set to be between 50 - 250 KgCO2eq during development (i.e., around 1000 Km driven by an average gasoline-powered passenger vehicle.) and below 100 g CO2 per usage.</p>
</div>
</li>
<li class="ltx_item" id="S9.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="S9.I2.i3.p1">
<p class="ltx_p" id="S9.I2.i3.p1.1">Systems that present a high risk regarding the principle of Sustainability require specialized hardware accelerators for development or use and produce a non-negligible environmental impact. This threshold is set between 250 and 50,000 KgCO2eq during development (i.e., inside the range for the documented emissions of large neural networks)<span class="ltx_note ltx_role_footnote" id="footnote31"><sup class="ltx_note_mark">31</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">31</sup><span class="ltx_tag ltx_tag_note">31</span>This is the value reported in ”<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2211.02001" title="">Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model</a>” </span></span></span> and below 200 g CO2 per usage (i.e., twice the allowance given to the limited category).</p>
</div>
</li>
<li class="ltx_item" id="S9.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S9.I2.i4.p1">
<p class="ltx_p" id="S9.I2.i4.p1.1">Systems that present an unacceptable risk regarding the principle of Sustainability could generate emissions above 50 tons of CO2eq in a single month of training or use.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S9.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Examples of application areas and their categories of risk (Sustainability)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S9.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S9.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S9.T4.1.1.1.1.1">Minimal</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S9.T4.1.1.1.2.1">Limited</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S9.T4.1.1.1.3.1">High</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S9.T4.1.1.1.4.1">Unacceptable</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S9.T4.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T4.1.2.1.1">BERT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T4.1.2.1.2">GPT-2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T4.1.2.1.3">BLOOM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T4.1.2.1.4">&gt; 50 tCO2eq</td>
</tr>
<tr class="ltx_tr" id="S9.T4.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T4.1.3.2.1">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T4.1.3.2.2">ViT-Large</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T4.1.3.2.3">SDXL-Turbo</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T4.1.3.2.4">&gt; 50 tCO2eq</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S9.p9">
<p class="ltx_p" id="S9.p9.1">The following minimal ethical requirements tie normative recommendations to implementable practices. All requirements are general and should be employed regardless of the risk category of a system.</p>
</div>
<section class="ltx_subsection" id="S9.SSx1">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement SU1: Tracking of Environmental Markers</h3>
<div class="ltx_para ltx_noindent" id="S9.SSx1.p1">
<p class="ltx_p" id="S9.SSx1.p1.1">Tracking environmental markers such as CO2 emissions, energy use, and water consumption is essential in assessing and mitigating the ecological impacts of AI technologies. These markers serve as vital indicators of the sustainability of AI deployment. Understanding their environmental footprint becomes essential for responsible development and decision-making regarding the use and development of AI systems. Hence, tracking environmental markers is a minimal ethical requirement that should be stimulated across applications, independent of their risk.<span class="ltx_note ltx_role_footnote" id="footnote32"><sup class="ltx_note_mark">32</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">32</sup><span class="ltx_tag ltx_tag_note">32</span>To learn more about estimating environmental markers like CO2 and energy use, we recommend the ”<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1911.08354" title="">Energy Usage Reports: Environmental awareness as part of algorithmic accountability</a>”, illustrated in the methodology page of <a class="ltx_ref ltx_href" href="https://mlco2.github.io/codecarbon/methodology.html" title="">CodeCarbon’s documentation</a>.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S9.SSx1.p2">
<p class="ltx_p" id="S9.SSx1.p2.1">For example, monitoring CO2 emissions helps quantify the carbon footprint associated with AI infrastructure and operations, guiding efforts towards reducing greenhouse gas emissions. Likewise, tracking energy use provides insights into the efficiency of AI algorithms and hardware, facilitating optimizations to minimize energy consumption <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib38" title="">38</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote33"><sup class="ltx_note_mark">33</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">33</sup><span class="ltx_tag ltx_tag_note">33</span>You can use tools like <a class="ltx_ref ltx_href" href="https://github.com/mlco2/codecarbon" title="">CodeCarbon</a> and <a class="ltx_ref ltx_href" href="https://github.com/sb-ai-lab/Eco2AI" title="">Eco2AI</a> to track energy use and carbon emissions of AI experiments and any demanding computational process.</span></span></span> Meanwhile, monitoring water consumption is crucial for assessing AI technologies’ indirect environmental impacts and effects on planetary boundaries. However, while there are straightforward mechanisms and methodologies to measure energy consumption and carbon emissions of hardware and software use, more ready-to-use tools are needed to allow stakeholders to track other ecological markers, like water usage, among other resources tied to developing some types of AI systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S9.SSx1.p3">
<p class="ltx_p" id="S9.SSx1.p3.1">Hence, since carbon emissions do not account for factors like social impacts, legality, and rebound effects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib200" title="">200</a>]</cite>, other measures are required to track ecological markings. Hence, another way to track environmental markers is through lifecycle assessments (LCA). LCAs provide a holistic approach to evaluating the environmental impacts of AI technologies throughout their entire lifecycle, from raw material extraction to manufacturing, usage, and disposal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib142" title="">142</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib147" title="">147</a>]</cite>. Hence, the documentation of factors beyond carbon emissions, such as water usage, resource depletion, and waste generation, should also be considered a minimal ethical requirement. In short, LCAs can facilitate informed decision-making by identifying opportunities for eco-friendly design choices and resource-efficient practices.<span class="ltx_note ltx_role_footnote" id="footnote34"><sup class="ltx_note_mark">34</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">34</sup><span class="ltx_tag ltx_tag_note">34</span>Conducting LCAs for AI technologies poses several challenges, including data availability, methodological complexities, and uncertainty in the evaluation process. To help readers envision their own LCA reports, examples we mention are <a class="ltx_ref ltx_href" href="https://www.apple.com/environment/pdf/Apple_Environmental_Progress_Report_2023.pdf" title="">Apple’s</a> and <a class="ltx_ref ltx_href" href="https://impakter.com/index/nvidia-sustainability-report/" title="">Nvidia</a> sustainability reports, and the ”<a class="ltx_ref ltx_href" href="https://ecochain.com/knowledge/life-cycle-assessment-lca-guide/" title="">EcoChain’s Life Cycle Assessment (LCA) – Complete Beginner’s Guide</a>”.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S9.SSx1.p4">
<p class="ltx_p" id="S9.SSx1.p4.1">Lastly, the results of such tracking efforts should also be made public through comprehensive reporting, especially for applications regarded as high-risk. Reporting on ecological impacts allows for transparency and accountability in AI development and deployment, fostering trust among stakeholders and the public. It also enables policymakers, businesses, and consumers to make informed decisions, guiding efforts toward more sustainable practices and technologies.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SSx2">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement SU2: Sustainable and Open Development</h3>
<div class="ltx_para ltx_noindent" id="S9.SSx2.p1">
<p class="ltx_p" id="S9.SSx2.p1.1">For stakeholders seeking sustainable development practices within AI, embracing openness and resource efficiency is imperative. Hence, when technological development is tied to the significant expense of resources (especially in high-risk settings), a shift towards sustainable and open development methodologies should be considered a viable approach to the starting point. In other words, instead of constantly reinventing the wheel, repurposing and building upon already established frameworks and infrastructure can significantly reduce the environmental footprint associated with AI development. For example, much of the development on applied language modeling for text generation applications is rooted in open models like BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib49" title="">49</a>]</cite>, RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib121" title="">121</a>]</cite>, Mistral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib96" title="">96</a>]</cite>, and many others. These were developed once and reused/repurposed thousands of times, which can also be said for many other foundation models released with open licenses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib179" title="">179</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib120" title="">120</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib225" title="">225</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S9.SSx2.p2">
<p class="ltx_p" id="S9.SSx2.p2.1">Open access technology, especially those requiring significant building resources, presents an opportunity for leveraging existing tech through recycling and expansion. This approach conserves resources and promotes collaboration and knowledge sharing within the AI community. Moreover, by embracing open development practices, barriers to entry are lowered, allowing for a more inclusive and diverse participation in AI innovation. This fosters a sustainable and socially responsible culture of innovation, addressing the broader ethical considerations inherent in AI development.<span class="ltx_note ltx_role_footnote" id="footnote35"><sup class="ltx_note_mark">35</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">35</sup><span class="ltx_tag ltx_tag_note">35</span>Famous deep learning frameworks make available to developers a series of open-source foundations for downstream use in applications. Examples like <a class="ltx_ref ltx_href" href="https://pytorch.org/hub/" title="">PyTorch Hub</a>, <a class="ltx_ref ltx_href" href="https://huggingface.co/tasks" title="">Hugging Face</a>, <a class="ltx_ref ltx_href" href="https://github.com/pprp/timm?tab=readme-ov-file#pytorch-image-models" title="">Timm</a>, <a class="ltx_ref ltx_href" href="https://github.com/tensorflow/models/tree/master/official#tensorflow-official-models" title="">TensorFlow</a>, and <a class="ltx_ref ltx_href" href="https://www.kaggle.com/models" title="">Kaggle</a> are only a few main examples where developers can find ready-to-use foundations that do not require expensive pre-training to be redone.</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S9.SSx3">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement SU3: Sustainable and Efficient Development</h3>
<div class="ltx_para ltx_noindent" id="S9.SSx3.p1">
<p class="ltx_p" id="S9.SSx3.p1.1">Modern state-of-the-art AI systems are usually a byproduct of a paradigm that requires much computing to succeed in areas where other paradigms have failed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib194" title="">194</a>]</cite>. Hence, optimizing algorithms and models used and the hardware infrastructure supporting them in pursuing sustainable development in AI is imperative <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib192" title="">192</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib45" title="">45</a>]</cite>. For example, energy efficiency in computational systems is paramount for achieving sustainability, especially for applications categorized as high-risk (i.e., billion-parameter-sized neural networks), which, during the developmental phase, our design choices can heavily influence the resource consumption tied to their development. These choices extend to selecting components such as model architecture, training methodology, and hardware selection, which, when done wisely, are pivotal in optimizing resource consumption.</p>
</div>
<div class="ltx_para ltx_noindent" id="S9.SSx3.p2">
<p class="ltx_p" id="S9.SSx3.p2.1">Regarding model architecture, certain design choices and algorithmic implementations can significantly impact the computation and, thus, require energy consumption for things like model training, evaluation, and inference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib140" title="">140</a>]</cite>. For instance, utilizing depthwise separable convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib34" title="">34</a>]</cite> instead of standard convolutional layers in computer vision tasks can substantially decrease computation requirements without severely compromising performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib84" title="">84</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib127" title="">127</a>]</cite>. Similarly, in language models, attention mechanisms pose a computational bottleneck, given the quadradic complexity growth that scales with sequence length <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib205" title="">205</a>]</cite>, which can be alleviated through alternatives such as attention-free models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib160" title="">160</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib78" title="">78</a>]</cite> or more efficient implementations, like FlashAttention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib46" title="">46</a>]</cite>, group query attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib5" title="">5</a>]</cite>, or sliding window attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib17" title="">17</a>]</cite>, among others <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib216" title="">216</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib51" title="">51</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote36"><sup class="ltx_note_mark">36</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">36</sup><span class="ltx_tag ltx_tag_note">36</span>You can find several techniques to improve memory footprint and overall efficiency in training LLMs <a class="ltx_ref ltx_href" href="https://huggingface.co/docs/transformers/perf_infer_gpu_one" title="">here</a>, <a class="ltx_ref ltx_href" href="https://huggingface.co/docs/transformers/v4.23.1/en/perf_train_gpu_one" title="">here</a>, and <a class="ltx_ref ltx_href" href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one" title="">here</a>. For training improvements in diffusion models, check these implementations: <a class="ltx_ref ltx_href" href="https://github.com/Anima-Lab/MaskDiT" title="">MaskDiT</a> and <a class="ltx_ref ltx_href" href="https://github.com/Zhendong-Wang/Patch-Diffusion" title="">Patch Diffusion</a>.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S9.SSx3.p3">
<p class="ltx_p" id="S9.SSx3.p3.1">Dataset size also affects resource consumption. Scaling laws provide insights into the optimal dataset size relative to the model’s parameters for a fixed budget. For instance, the Chinchilla scaling laws suggest maintaining a 20:1 tokens-parameter ratio for language models under a fixed budget <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib88" title="">88</a>]</cite>. Under this constraint, one can still achieve performance while also stipulating a limit to the amount of computing to be spent.<span class="ltx_note ltx_role_footnote" id="footnote37"><sup class="ltx_note_mark">37</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">37</sup><span class="ltx_tag ltx_tag_note">37</span><a class="ltx_ref ltx_href" href="https://www.cerebras.net/model-lab/" title="">Here</a>, you can calculate training and inference estimations for the resources required to train large models according to scaling laws.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S9.SSx3.p4">
<p class="ltx_p" id="S9.SSx3.p4.1">Hardware selection is also crucial in optimizing energy consumption. State-of-the-art GPUs, though costly, significantly accelerate training runs and minimize long-term costs.<span class="ltx_note ltx_role_footnote" id="footnote38"><sup class="ltx_note_mark">38</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">38</sup><span class="ltx_tag ltx_tag_note">38</span>The <a class="ltx_ref ltx_href" href="https://mtli.github.io/gpubench/" title="">Deep Learning GPU Benchmark</a> compares GPUs in terms of their latency regarding training, inference, and complexity of the task under consideration. Stakeholders can use these results, as well as the specifications of the intended hardware, to make informed decisions regarding how to optimize energy use.</span></span></span> At the same time, efficient training techniques further enhance energy efficiency by optimizing model parameters without the resource-intensive nature of traditional methods. For example, Parameter Efficient Fine-Tuning (PEFT) methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib134" title="">134</a>]</cite>, such as Low-rank adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib90" title="">90</a>]</cite>, n-bit quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib118" title="">118</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib209" title="">209</a>]</cite>, memory efficient optimizers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib186" title="">186</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib224" title="">224</a>]</cite>, and Quantized low-rank adaptation (QLoRA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib48" title="">48</a>]</cite>, are all methods that can make training and using large models much more efficient and less resource hungry.<span class="ltx_note ltx_role_footnote" id="footnote39"><sup class="ltx_note_mark">39</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">39</sup><span class="ltx_tag ltx_tag_note">39</span>Tools like <a class="ltx_ref ltx_href" href="https://github.com/huggingface/optimum" title="">Optimum</a> provide several performance optimization tools, in line with the suggestions of this paragraph, to train and run models on targeted hardware with maximum efficiency.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S9.SSx3.p5">
<p class="ltx_p" id="S9.SSx3.p5.1">Ultimately, optimizing efficiency in computational systems involves strategic design choices in model architecture, dataset size considerations, and hardware selection, complemented by efficient training and inference techniques to minimize resource consumption. Adopting these measures is a minimal requirement for sustainable development, especially for high-risk AI systems and applications.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SSx4">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement SU4: Offsetting Policies</h3>
<div class="ltx_para ltx_noindent" id="S9.SSx4.p1">
<p class="ltx_p" id="S9.SSx4.p1.1">In pursuing sustainable and responsible AI development, offsetting policies emerge as a crucial requirement, particularly for high-risk applications. In the context of environmental sustainability, offsetting refers to counterbalancing or compensating for the negative ecological impacts of an activity by undertaking additional actions that result in positive environmental outcomes. Offsetting aims to achieve a net-zero or even a net-positive environmental impact, thereby attempting to mitigate the overall ecological footprint of a particular activity or project <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib126" title="">126</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib92" title="">92</a>]</cite>. As an example, we can mention carbon offsetting. Carbon offsetting involves investing in projects or initiatives that reduce or capture carbon dioxide (CO2) emissions equivalent to those generated by a particular activity. For instance, if a company’s operations produce a certain amount of CO2 emission, it can purchase carbon credits to fund projects such as reforestation, renewable energy development, or methane capture from landfills.</p>
</div>
<div class="ltx_para ltx_noindent" id="S9.SSx4.p2">
<p class="ltx_p" id="S9.SSx4.p2.1">Offsetting policies provide an avenue to address the inherent trade-offs between technological progress and environmental sustainability. By quantifying the ecological cost of AI development and implementation (SU1), stakeholders can implement strategies to neutralize or minimize these impacts. Additionally, offsetting policies promote accountability and transparency within the AI community. By requiring developers and organizations to assess and disclose the environmental implications of their projects, these policies foster a culture of environmental stewardship and responsible innovation.<span class="ltx_note ltx_role_footnote" id="footnote40"><sup class="ltx_note_mark">40</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">40</sup><span class="ltx_tag ltx_tag_note">40</span>You can learn more about Europe’s carbon offsetting policies in the <a class="ltx_ref ltx_href" href="https://climate.ec.europa.eu/eu-action/eu-emissions-trading-system-eu-ets_en" title="">EU Emissions Trading System (EU ETS)</a>.</span></span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S10.2.1.1" style="color:#007091;">10</span> </span><span class="ltx_text" id="S10.3.2" style="color:#007091;">Transparency and Explainability (T)</span>
</h2>
<div class="ltx_para ltx_noindent" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">Transparency, taking the definition provided by the AI Act and the analysis of 200 ethical guidelines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib41" title="">41</a>]</cite> as our basis, encompasses the principle that the development and utilization of AI technologies should be open and understandable to all stakeholders. It makes information regarding the organization’s AI practices and algorithms accessible and understandable for experts and non-experts alike. This principle necessitates that AI systems are designed and implemented to facilitate traceability and explainability, ensuring that humans are aware when interacting with AI systems. Additionally, transparency entails providing users with clear insights into the capabilities and limitations of AI systems and informing affected individuals about their rights concerning AI-generated decisions or interactions. Ultimately, transparency promotes accountability, trust, and ethical use of AI technologies within society.</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.p2">
<p class="ltx_p" id="S10.p2.4">From a practical perspective, transparency is a principle tied directly to the fields that seek to combat model opacity, like explainable AI (XAI) and mechanistic interpretability (MechInterp) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib146" title="">146</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib210" title="">210</a>]</cite>. While XAI and MechInterp share the overarching goal of enhancing AI systems’ intelligibility (especially machine learning models), they exhibit distinct methodologies and emphases. XAI primarily focuses on developing techniques to elucidate the outputs of opaque systems in an empirical fashion, where input-output relations are mapped in an observational fashion, e.g., input <math alttext="X" class="ltx_Math" display="inline" id="S10.p2.1.m1.1"><semantics id="S10.p2.1.m1.1a"><mi id="S10.p2.1.m1.1.1" xref="S10.p2.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S10.p2.1.m1.1b"><ci id="S10.p2.1.m1.1.1.cmml" xref="S10.p2.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.p2.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S10.p2.1.m1.1d">italic_X</annotation></semantics></math> generates output <math alttext="Y" class="ltx_Math" display="inline" id="S10.p2.2.m2.1"><semantics id="S10.p2.2.m2.1a"><mi id="S10.p2.2.m2.1.1" xref="S10.p2.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S10.p2.2.m2.1b"><ci id="S10.p2.2.m2.1.1.cmml" xref="S10.p2.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.p2.2.m2.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S10.p2.2.m2.1d">italic_Y</annotation></semantics></math> and components <math alttext="x_{i}andx_{j}" class="ltx_Math" display="inline" id="S10.p2.3.m3.1"><semantics id="S10.p2.3.m3.1a"><mrow id="S10.p2.3.m3.1.1" xref="S10.p2.3.m3.1.1.cmml"><msub id="S10.p2.3.m3.1.1.2" xref="S10.p2.3.m3.1.1.2.cmml"><mi id="S10.p2.3.m3.1.1.2.2" xref="S10.p2.3.m3.1.1.2.2.cmml">x</mi><mi id="S10.p2.3.m3.1.1.2.3" xref="S10.p2.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S10.p2.3.m3.1.1.1" xref="S10.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S10.p2.3.m3.1.1.3" xref="S10.p2.3.m3.1.1.3.cmml">a</mi><mo id="S10.p2.3.m3.1.1.1a" xref="S10.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S10.p2.3.m3.1.1.4" xref="S10.p2.3.m3.1.1.4.cmml">n</mi><mo id="S10.p2.3.m3.1.1.1b" xref="S10.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S10.p2.3.m3.1.1.5" xref="S10.p2.3.m3.1.1.5.cmml">d</mi><mo id="S10.p2.3.m3.1.1.1c" xref="S10.p2.3.m3.1.1.1.cmml">⁢</mo><msub id="S10.p2.3.m3.1.1.6" xref="S10.p2.3.m3.1.1.6.cmml"><mi id="S10.p2.3.m3.1.1.6.2" xref="S10.p2.3.m3.1.1.6.2.cmml">x</mi><mi id="S10.p2.3.m3.1.1.6.3" xref="S10.p2.3.m3.1.1.6.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S10.p2.3.m3.1b"><apply id="S10.p2.3.m3.1.1.cmml" xref="S10.p2.3.m3.1.1"><times id="S10.p2.3.m3.1.1.1.cmml" xref="S10.p2.3.m3.1.1.1"></times><apply id="S10.p2.3.m3.1.1.2.cmml" xref="S10.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S10.p2.3.m3.1.1.2.1.cmml" xref="S10.p2.3.m3.1.1.2">subscript</csymbol><ci id="S10.p2.3.m3.1.1.2.2.cmml" xref="S10.p2.3.m3.1.1.2.2">𝑥</ci><ci id="S10.p2.3.m3.1.1.2.3.cmml" xref="S10.p2.3.m3.1.1.2.3">𝑖</ci></apply><ci id="S10.p2.3.m3.1.1.3.cmml" xref="S10.p2.3.m3.1.1.3">𝑎</ci><ci id="S10.p2.3.m3.1.1.4.cmml" xref="S10.p2.3.m3.1.1.4">𝑛</ci><ci id="S10.p2.3.m3.1.1.5.cmml" xref="S10.p2.3.m3.1.1.5">𝑑</ci><apply id="S10.p2.3.m3.1.1.6.cmml" xref="S10.p2.3.m3.1.1.6"><csymbol cd="ambiguous" id="S10.p2.3.m3.1.1.6.1.cmml" xref="S10.p2.3.m3.1.1.6">subscript</csymbol><ci id="S10.p2.3.m3.1.1.6.2.cmml" xref="S10.p2.3.m3.1.1.6.2">𝑥</ci><ci id="S10.p2.3.m3.1.1.6.3.cmml" xref="S10.p2.3.m3.1.1.6.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.p2.3.m3.1c">x_{i}andx_{j}</annotation><annotation encoding="application/x-llamapun" id="S10.p2.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_a italic_n italic_d italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> are the major contributors for output <math alttext="Y" class="ltx_Math" display="inline" id="S10.p2.4.m4.1"><semantics id="S10.p2.4.m4.1a"><mi id="S10.p2.4.m4.1.1" xref="S10.p2.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S10.p2.4.m4.1b"><ci id="S10.p2.4.m4.1.1.cmml" xref="S10.p2.4.m4.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.p2.4.m4.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S10.p2.4.m4.1d">italic_Y</annotation></semantics></math>, particularly those involving systems that are not hand-coded but developed by a learning paradigm. Techniques within the XAI paradigm include feature importance analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib156" title="">156</a>]</cite>, and model-agnostic approaches such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib175" title="">175</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib130" title="">130</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.p3">
<p class="ltx_p" id="S10.p3.1">In contrast, MechInterp centers on uncovering the underlying causal mechanisms governing observed phenomena. Unlike XAI, which operates primarily at the level of black-box models, MechInterp delves into the intrinsic structure and dynamics of systems, aiming to extract mechanistic insights that align with domain-specific knowledge. Techniques within MechInterp encompass mainly causal intervention methods like knowledge editing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib139" title="">139</a>]</cite>, circuit search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib210" title="">210</a>]</cite>, and reverse-engineering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib151" title="">151</a>]</cite>. Despite their differences, XAI and MechInterp share several commonalities. Both recognize the importance of interpretability in fostering trust and facilitating human-machine collaboration, albeit from distinct vantage points. Furthermore, they confront similar challenges, such as the trade-off between model complexity and interpretability, the need to balance accuracy with comprehensibility, and the ethical considerations surrounding using interpretable models in high-stakes applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib180" title="">180</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.p4">
<p class="ltx_p" id="S10.p4.1">Other practices, which we could categorize as non-technical (do not involve the direct exploration of a system), are also recognized as practices in explainable and interpretable AI. Practices like documentation and reporting, be that of general information and risk assessments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib143" title="">143</a>]</cite> or more specific metrics tracked during development (e.g., carbon emissions) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib111" title="">111</a>]</cite>, improvement of AI literacy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib123" title="">123</a>]</cite>, and independent audits and reviews <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib61" title="">61</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.p5">
<p class="ltx_p" id="S10.p5.1">To help organizations prospect the level of risk regarding their system or application, we propose the following criteria, in line with the four main categories of risk identified in the EU AI Act (examples in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S10.T5" title="Table 5 ‣ 10 Transparency and Explainability (T) ‣ Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_tag">5</span></a>):</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.p6">
<ol class="ltx_enumerate" id="S10.I1">
<li class="ltx_item" id="S10.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S10.I1.i1.p1">
<p class="ltx_p" id="S10.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i1.p1.1.1">Minimal:</span> systems that present a minimal risk regarding the principle of transparency are those in which their inputs and outputs are not directly involved with human beings in any critical way. These systems are the ones in which organizations are more lenient in using black box models, given that their inner workings, as long as they are reliable and robust, do not require our complete understanding.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S10.I1.i2.p1">
<p class="ltx_p" id="S10.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i2.p1.1.1">Limited:</span> systems that present a limited risk regarding the principle of transparency are those whose inputs and outputs are directly involved with human beings, but their malfunction would only generate little harm. These systems are the ones in which organizations if chosen to utilize black box models, should provide results that can prove a minimal level of interpretability and understanding of their system.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S10.I1.i3.p1">
<p class="ltx_p" id="S10.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i3.p1.1.1">High:</span> systems that present a high risk regarding the principle of transparency are those whose inputs and outputs are directly involved with human beings, and their malfunction would generate significant harm. These systems are the ones in which organizations should refrain from using black box models, given that such systems are tied to critical infrastructure (e.g., transport, healthcare, law enforcement, etc.).</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S10.I1.i4.p1">
<p class="ltx_p" id="S10.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i4.p1.1.1">Unacceptable:</span> systems that present unacceptable risk regarding the principle of transparency are those whose inputs and outputs are directly involved with human beings in a way that their malfunction would infringe on their human rights. Organizations should refrain from developing such systems or terminating them if already deployed.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S10.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Examples of application areas and their categories of risk (Transparency)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S10.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S10.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S10.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S10.T5.1.1.1.1.1">Minimal</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S10.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S10.T5.1.1.1.2.1">Limited</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S10.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S10.T5.1.1.1.3.1">High</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S10.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S10.T5.1.1.1.4.1">Unacceptable</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S10.T5.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S10.T5.1.2.1.1">Spam Filters</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S10.T5.1.2.1.2">AI Assistants</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S10.T5.1.2.1.3">Legal Automation</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S10.T5.1.2.1.4">Social Scoring Systems</td>
</tr>
<tr class="ltx_tr" id="S10.T5.1.3.2">
<td class="ltx_td ltx_align_center" id="S10.T5.1.3.2.1">Industrial Automation</td>
<td class="ltx_td ltx_align_center" id="S10.T5.1.3.2.2">Automated Advertising</td>
<td class="ltx_td ltx_align_center" id="S10.T5.1.3.2.3">Medical Diagnostics</td>
<td class="ltx_td ltx_align_center" id="S10.T5.1.3.2.4">Remote Biometric Identification</td>
</tr>
<tr class="ltx_tr" id="S10.T5.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S10.T5.1.4.3.1">Agricultural Automation</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S10.T5.1.4.3.2">Recommendation Systems</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S10.T5.1.4.3.3">Autonomous Vehicles</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S10.T5.1.4.3.4">Automated Phishing</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S10.p7">
<p class="ltx_p" id="S10.p7.1">The following minimal ethical requirements tie normative recommendations to implementable practices. All requirements are general and should be employed regardless of the risk category of a system.</p>
</div>
<section class="ltx_subsection" id="S10.SSx1">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement T1: Explainable and Understandable Outcomes</h3>
<div class="ltx_para ltx_noindent" id="S10.SSx1.p1">
<p class="ltx_p" id="S10.SSx1.p1.1">The more risk involved in using an AI application or system, the more efforts should be made to explain its workings. Hence, systems regarded as low risks (e.g., spam filters) have the most leniency regarding these requirements. In contrast, in high-risk scenarios, stakeholders are required to invest heavily in XAI and MechInterp. By understandable explanations, we mean that meaningful attributions, correlations, and causal relations can be achieved properly and, when possible, a coherent narrative can be constructed, always keeping in mind that explanations should also have differential levels of understandability, from the expert to the layperson.</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.SSx1.p2">
<p class="ltx_p" id="S10.SSx1.p2.1">Hence, it is recommended that tools for model exploration be employed to produce such results.<span class="ltx_note ltx_role_footnote" id="footnote41"><sup class="ltx_note_mark">41</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">41</sup><span class="ltx_tag ltx_tag_note">41</span>For those technically inclined readers, we recommend ”The Building Blocks of Interpretability” as a gentle introduction to the many techniques one could employ. Available in ”<a class="ltx_ref ltx_href" href="https://distill.pub/2018/building-blocks/" title="">The Building Blocks of Interpretability</a>”.</span></span></span> For most applications, be that in computer vision, natural language processing, classification, or forecasting, there are methods, like LIME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib175" title="">175</a>]</cite>, SHAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib130" title="">130</a>]</cite>, and tools, like DALEX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib19" title="">19</a>]</cite>, CAPTUM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib108" title="">108</a>]</cite>, and ALIBI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib107" title="">107</a>]</cite>, to aid stakeholders involved in this explanatory step to achieve this goal, remembering again that these explanations should always be tailored to the audience they are intended.</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.SSx1.p3">
<p class="ltx_p" id="S10.SSx1.p3.1">Tailored explanations for AI systems are crucial, especially in high-risk scenarios, to foster trust and accountability. Leveraging tools for model exploration and techniques ensures transparent and understandable explanations, catering to diverse audiences and enhancing public confidence in AI systems.</p>
</div>
</section>
<section class="ltx_subsection" id="S10.SSx2">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement T2: Dataset Documentation</h3>
<div class="ltx_para ltx_noindent" id="S10.SSx2.p1">
<p class="ltx_p" id="S10.SSx2.p1.1">Data is a foundational element that dictates the behavior and efficacy of an AI system, making transparency regarding its characteristics paramount. Hence, a minimal ethical requirement for transparency and explainability is the documentation of datasets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.SSx2.p2">
<p class="ltx_p" id="S10.SSx2.p2.1">Dataset cards <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib167" title="">167</a>]</cite> serve as an instrument for illuminating the intricacies of the data employed in AI model development. These cards encapsulate critical details about the dataset’s composition, including its size, diversity, and provenance. By delineating the data collection methodology, annotation procedures, and potential biases within the dataset, stakeholders can gain insights into the underlying factors shaping the AI system’s performance and outputs. Moreover, dataset cards facilitate understanding the contextual nuances surrounding the data, empowering researchers and practitioners to discern the implications of utilizing specific datasets in AI model training.<span class="ltx_note ltx_role_footnote" id="footnote42"><sup class="ltx_note_mark">42</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">42</sup><span class="ltx_tag ltx_tag_note">42</span>To learn how you can produce such artifacts, we recommend ”<a class="ltx_ref ltx_href" href="https://sites.research.google/datacardsplaybook/" title="">The Data Cards Playbook</a>”.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S10.SSx2.p3">
<p class="ltx_p" id="S10.SSx2.p3.1">Ultimately, dataset documentation contributes to establishing a more robust and accountable ecosystem wherein transparent and well-documented data sources underpin data-driven insights.</p>
</div>
</section>
<section class="ltx_subsection" id="S10.SSx3">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement T3: Model Reporting</h3>
<div class="ltx_para ltx_noindent" id="S10.SSx3.p1">
<p class="ltx_p" id="S10.SSx3.p1.1">To enable all AI stakeholders to perform their designated roles, the system’s operations, intended usage, out-of-scope usage, performance, limitations, and risks should be documented clearly. Such documentation can aid in situations of failure or accidents, helping stakeholders set the scope of their responsibilities and liability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.SSx3.p2">
<p class="ltx_p" id="S10.SSx3.p2.1">This requirement can be achieved via model reporting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib143" title="">143</a>]</cite>. Model reporting facilitates documentation regarding the AI system’s operation by providing detailed insights into a given AI system, much like informational leaflets accompany pharmaceuticals. Through model cards, developers can disclose crucial information such as the architecture employed, training data sources, preprocessing techniques, and model performance metrics. Additionally, explanations regarding potential biases, limitations, and ethical considerations are elucidated, offering a comprehensive understanding of the AI system’s functioning. This transparency fosters trust and accountability and enables users to make informed decisions about deploying and utilizing AI technology. Model cards are encouraged in many public repositories of open-source AI models (e.g., Hugging Face and GitHub). At the same time, such documents almost always accompany major releases of AI models <span class="ltx_note ltx_role_footnote" id="footnote43"><sup class="ltx_note_mark">43</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">43</sup><span class="ltx_tag ltx_tag_note">43</span>Example: <a class="ltx_ref ltx_href" href="https://llama-2.ai/llama-2-model-card/" title="">Llama 2</a>. You can use <a class="ltx_ref ltx_href" href="https://huggingface.co/spaces/huggingface/Model_Cards_Writing_Tool" title="">this application</a> to fill up your model report.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S10.SSx3.p3">
<p class="ltx_p" id="S10.SSx3.p3.1">Users of AI products should be educated to request and scrutinize such documentation. At the same time, developers should be encouraged to create such documents plainly and understandably as a standard practice of their profession.</p>
</div>
</section>
<section class="ltx_subsection" id="S10.SSx4">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement T4: Risk Assessment in Black Box Scenarios</h3>
<div class="ltx_para ltx_noindent" id="S10.SSx4.p1">
<p class="ltx_p" id="S10.SSx4.p1.1">Black box models are characterized by complex internal workings often opaque to human understanding (e.g., billion parameter-sized neural networks). While these models can provide high performance in specific applications, they pose significant challenges for interpretability. Unlike more shallow linear models, where the relationship between input variables and output can be more easily understood, black box models obscure the logic behind their behavior, making it difficult for stakeholders to trust or interpret their decisions. This lack of interpretability raises concerns, particularly in high-risk settings where the consequences of errors can be severe.</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.SSx4.p2">
<p class="ltx_p" id="S10.SSx4.p2.1">Due to the inherent risks associated with black box models, as already recommended by prominent figures in the field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib180" title="">180</a>]</cite>, it’s prudent to avoid their use in high-stakes environments whenever possible. In settings where human lives, financial stability, or ethical considerations are at stake, relying solely on opaque models can be considered ethically irresponsible. Instead, transparent models or approaches prioritizing interpretability should be favored. By opting for models that offer insights into their decision-making process, stakeholders can better understand, validate, and potentially mitigate the risks associated with model errors.</p>
</div>
<div class="ltx_para ltx_noindent" id="S10.SSx4.p3">
<p class="ltx_p" id="S10.SSx4.p3.1">In cases where black box models are unavoidable due to their superior performance or lack of viable alternatives, human moderation becomes essential. Implementing human oversight and control mechanisms can help mitigate the risks associated with these models. Hence, under such circumstances, moderators should monitor the behavior of black box models and intervene when necessary. Additionally, ongoing evaluation and auditing can help ensure that the models behave as intended and do not exhibit harmful biases or errors (e.g., red teaming <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib68" title="">68</a>]</cite>). While human moderation adds complexity and costs to deploying black box models, it is a crucial safeguard in mitigating their potential risks in high-risk settings.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S11">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S11.2.1.1" style="color:#007091;">11</span> </span><span class="ltx_text" id="S11.3.2" style="color:#007091;">Truthfulness (TR)</span>
</h2>
<div class="ltx_para ltx_noindent" id="S11.p1">
<p class="ltx_p" id="S11.p1.1">The principle of truthfulness has emerged as a new matter of consideration for the field, mainly propelled by the recent strides in generative AI technologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib179" title="">179</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib86" title="">86</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib168" title="">168</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib3" title="">3</a>]</cite>. As these advancements enable AI systems to produce content that closely resembles, and in some cases, indistinguishably mirrors, human-generated content, the ethical imperative surrounding truthfulness has acquired newfound significance, which was a purely human problem until recently. This new preoccupation reflects a critical juncture wherein the traditional boundaries of truth, authenticity, and trustworthiness are redefined in light of AI’s newly found capabilities. "Truth" because of how closely AI-generated data mimics human-generated content. "Authenticity" due to our need to certify the genuineness and originality of certain types of content. And "Trustworthiness" because, in simple terms, there can be no trust where there is no truth. Hence, much debate surrounds the issues related to the question, <span class="ltx_text ltx_font_italic" id="S11.p1.1.1">"What does it mean for society when the grain of truth becomes a harder-to-find spec in a sea of artificially generated content?"</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.p2">
<p class="ltx_p" id="S11.p2.1">However, what are truthfulness, falsehoods, and lies, and how can we define them in the context of generative AI? Firstly, truthfulness is only a consideration when discussing systems that generate content, like text, images, audio, or videos. Hence, we should not confound "model incompetence" or "mistakes" with falsehood. Questions related to how accurate a system is are better defined and dealt with when working with principles like safety, reliability, and robustness. Therefore, a system that should predict class "toxic" for a piece of harmful text but ends up outputting "neutral" is not "lying" or outputting falsehood. It simply is an inaccurate system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.p3">
<p class="ltx_p" id="S11.p3.1">At the same time, to define an AI-generated falsehood we must first recognize that for this condition of untruthfulness to hold, such systems are not required to be knowledgeable <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib58" title="">58</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote44"><sup class="ltx_note_mark">44</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">44</sup><span class="ltx_tag ltx_tag_note">44</span>For more information on the matter, we recommend ”<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2110.06674" title="">Truthful AI: Developing and governing AI that does not lie</a>”.</span></span></span> In other words, if a researcher trains a language model only on flat earth literature, and while prompted to answer "What is the shape of the Earth?" it outputs "a pancake", this would not be a case of falsehood, but model incompetence. At the same time, if a language model trained with the best current literature on physics outputs a statement later disproved by advances beyond its training cut-off, that is also not a case of falsehood but, again, of incompetence or, we might even say, ignorance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.p4">
<p class="ltx_p" id="S11.p4.1">If we agree upon these conditions, we can define AI falsehoods as cases where a generative model, which possesses access to the ground truth,<span class="ltx_note ltx_role_footnote" id="footnote45"><sup class="ltx_note_mark">45</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">45</sup><span class="ltx_tag ltx_tag_note">45</span>We can define ground truth information known to be genuine, provided by direct observation and measurement (e.g., Earth is not flat). Also, we can describe it as information agreed upon by the majority, as is generally the case in matters in the humanities (e.g., Slavery is a morally abhorrent act).</span></span></span> be that contained in its knowledge base or training data, generates a piece of content untied to that fact.<span class="ltx_note ltx_role_footnote" id="footnote46"><sup class="ltx_note_mark">46</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">46</sup><span class="ltx_tag ltx_tag_note">46</span>The question regarding facts being wrong due to human incompetence is, again, beyond the scope of this principle in the context of AI and this work. At the same time, when we speak of intentional falsehoods (i.e., disinformation), the intentionality, or cause, behind the act is the human intention. We are not considering the possibility of AI systems possessing or exhibiting their ”own intentionality”.</span></span></span>. Another aspect that should also be considered an issue regarding truthfulness is when human actors use generative models to generate disinformation intentionally, like, for example, in the use of deep fakes to damage the reputation of individuals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib131" title="">131</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib135" title="">135</a>]</cite>. Hence, either in the generation of misinformation or disinformation,<span class="ltx_note ltx_role_footnote" id="footnote47"><sup class="ltx_note_mark">47</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">47</sup><span class="ltx_tag ltx_tag_note">47</span>We can define misinformation as any incorrect or misleading information (it does not require an intentional component). In contrast, disinformation is false information deliberately spread to deceive people (it requires intentionality).</span></span></span> except for cases that can be attributed to model incompetence, stakeholders should promote measures that seek to mitigate the generation of AI-empowered falsehoods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.p5">
<p class="ltx_p" id="S11.p5.1">Even though the principle of truthfulness is still not a recurring theme in many published AI Guidelines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib41" title="">41</a>]</cite>,<span class="ltx_note ltx_role_footnote" id="footnote48"><sup class="ltx_note_mark">48</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">48</sup><span class="ltx_tag ltx_tag_note">48</span>This is probably because many of these guidelines and ethical charters preceded the emergence of highly-skilled generative models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib169" title="">169</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib25" title="">25</a>]</cite>.</span></span></span> there are many reasons why the proliferation of AI-empowered misinformation and disinformation should be a concern worth addressing. Given the impact such systems may have on society (e.g., undermining electoral processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib32" title="">32</a>]</cite>) and its individuals (e.g., reputational damage and identity theft <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib22" title="">22</a>]</cite>), following minimal requirements for truthful AI becomes necessary to promote trustworthy AI. Concerning the liability and consequences that should be enforced in the case of actors trying to spread falsehoods with AI intentionally, there remains to be seen how the AI Act will be implemented in practice (e.g., what sanctions are to be imposed on those who act in an adversarial fashion?).</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.p6">
<p class="ltx_p" id="S11.p6.1">To help organizations prospect the level of risk regarding their system or application, we propose the following criteria, in line with the four main categories of risk identified in the EU AI Act (examples in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S11.T6" title="Table 6 ‣ 11 Truthfulness (TR) ‣ Catalog of General Ethical Requirements for AI Certification"><span class="ltx_text ltx_ref_tag">6</span></a>):</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.p7">
<ol class="ltx_enumerate" id="S11.I1">
<li class="ltx_item" id="S11.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S11.I1.i1.p1">
<p class="ltx_p" id="S11.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S11.I1.i1.p1.1.1">Minimal:</span> systems with minimal risk regarding the principle of truthfulness are those whose output does not require factual grounding. In other words, those are applications tied to creative and artistic work. Also, minimal-risk systems are those whose outputs can be recognized as artificial without significant effort.</p>
</div>
</li>
<li class="ltx_item" id="S11.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S11.I1.i2.p1">
<p class="ltx_p" id="S11.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S11.I1.i2.p1.1.1">Limited:</span> systems with a limited risk regarding the principle of truthfulness are those whose output requires considerable factual grounding. These systems and applications are usually tied to activities that require producing empirically, historically, and socially agreed-upon knowledge.</p>
</div>
</li>
<li class="ltx_item" id="S11.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S11.I1.i3.p1">
<p class="ltx_p" id="S11.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S11.I1.i3.p1.1.1">High:</span> systems that present a high risk regarding the principle of truthfulness are those whose output requires considerable factual grounding and produce outputs that can only be distinguished from human-generated content with significant effort.</p>
</div>
</li>
<li class="ltx_item" id="S11.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S11.I1.i4.p1">
<p class="ltx_p" id="S11.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S11.I1.i4.p1.1.1">Unacceptable:</span> Systems that present unacceptable risks regarding the principle of truthfulness are those whose output can be indistinguishable from human-generated content. Additionally, they are those that, besides being used in domains that require considerable factual grounding, through them, be by the absence of guardrails or mitigating strategies, users can intentionally deceive or manipulate others by utilizing such systems to present false information as truth, causing harm, or undermining trust in information sources in a scalable fashion. Organizations should refrain from developing such systems or terminating them if already deployed.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S11.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Examples of application areas and their categories of risk (Truthfulness)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S11.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S11.T6.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S11.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S11.T6.1.1.1.1.1">Minimal</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S11.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S11.T6.1.1.1.2.1">Limited</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S11.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S11.T6.1.1.1.3.1">High</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S11.T6.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S11.T6.1.1.1.4.1">Unacceptable</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S11.T6.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S11.T6.1.2.1.1">Art Generation</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S11.T6.1.2.1.2">Coding Assistants</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S11.T6.1.2.1.3">Photo Realistic Image Editing</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S11.T6.1.2.1.4">Deep Fakes</td>
</tr>
<tr class="ltx_tr" id="S11.T6.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S11.T6.1.3.2.1">Creative Writing Assistants</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S11.T6.1.3.2.2">Search-Engine Assistants</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S11.T6.1.3.2.3">Photo Realistic Image Generation</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S11.T6.1.3.2.4">Automated Phishing</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S11.p8">
<p class="ltx_p" id="S11.p8.1">The following minimal ethical requirements tie normative recommendations to implementable practices. All requirements are general and should be employed regardless of the risk category of a system.</p>
</div>
<section class="ltx_subsection" id="S11.SSx1">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement TR1: Disclosure and Watermarking</h3>
<div class="ltx_para ltx_noindent" id="S11.SSx1.p1">
<p class="ltx_p" id="S11.SSx1.p1.1">Nowadays, AI-driven technologies are seamlessly integrated into various facets of our daily lives, often blurring the lines between humans and machines. For instance, in video games, AI-generated conversations simulate human-like exchanges, enriching the gaming experience by creating dynamic and immersive environments. However, amid this technological marvel, it becomes imperative to underscore the significance of disclosure in AI applications, particularly those that mimic human behavior, such as text-to-image and language models, to avoid deceiving or deluding less informed stakeholders.<span class="ltx_note ltx_role_footnote" id="footnote49"><sup class="ltx_note_mark">49</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">49</sup><span class="ltx_tag ltx_tag_note">49</span>By less informed stakeholders, we mean individuals that might have a propensity to anthropomorphize artificial interactions, falling pray to systems and organizations that might seek to profit on the commoditization of relationships (e.g., AI romantic partners) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib115" title="">115</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib226" title="">226</a>]</cite>.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S11.SSx1.p2">
<p class="ltx_p" id="S11.SSx1.p2.1">Disclosure in these types of AI applications<span class="ltx_note ltx_role_footnote" id="footnote50"><sup class="ltx_note_mark">50</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">50</sup><span class="ltx_tag ltx_tag_note">50</span>Here we mean applications where over-anthropomorphization might lead to AI-human relationships <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib106" title="">106</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib221" title="">221</a>]</cite>.</span></span></span> is paramount for several reasons. Firstly, it ensures transparency and honesty in user interactions, establishing trust between users and the technology. Also, communicating to users that they are interacting with an AI system, not a human, helps manage expectations and prevents potential misunderstandings and deception. Additionally, disclosure allows users to understand the limitations of the technology they are engaging with, avoiding liability problems. For example, by warning users about potential issues like hallucinations and the risk of over-anthropomorphization inherent in AI systems, individuals can approach interactions with a critical mindset and make more informed decisions about the information they receive.</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.SSx1.p3">
<p class="ltx_p" id="S11.SSx1.p3.1">Here are some practical measures organizations can implement to promote open and honest disclosure in AI applications:</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.SSx1.p4">
<ul class="ltx_itemize" id="S11.I2">
<li class="ltx_item" id="S11.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S11.I2.i1.p1">
<p class="ltx_p" id="S11.I2.i1.p1.1">Clear and prominent labeling should indicate when users interact with AI systems rather than human counterparts. This labeling can include visible notifications or disclaimers at the onset of interactions, ensuring users are aware of the AI’s involvement.</p>
</div>
</li>
<li class="ltx_item" id="S11.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S11.I2.i2.p1">
<p class="ltx_p" id="S11.I2.i2.p1.1">Organizations should provide accessible documentation detailing the capabilities and limitations of their AI systems. This documentation should outline potential issues such as hallucinations, verbosity, repetition, and biases, empowering users to make informed decisions during their interactions. Akin to one of the requirements for transparency (T3), model cards <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib143" title="">143</a>]</cite> and similar reporting methods (Terms of Use) can complement this practice.</p>
</div>
</li>
<li class="ltx_item" id="S11.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S11.I2.i3.p1">
<p class="ltx_p" id="S11.I2.i3.p1.1">Organizations should also establish channels for feedback and complaints, which, besides fostering open communication and accountability, ensure individuals can contact a human mediator if needed (i.e., inserting a human-in-the-loop).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S11.SSx1.p5">
<p class="ltx_p" id="S11.SSx1.p5.1">Another way to promote truthfulness and disclosure is via watermarking. Watermarking is a technique extensively employed in cryptography and steganography that offers a promising avenue for enhancing veracity in generative AI applications. While steganography aims to hide secret information within digital media without changing the visible appearance, watermarking aims to embed information that verifies the owner or the authenticity of that media <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib98" title="">98</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib187" title="">187</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib211" title="">211</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib60" title="">60</a>]</cite>. In the context of generative AI, this methodology can be adapted to imbue AI-generated outputs with traceable markers, enabling the creation of digital fingerprints that allow stakeholders to track and validate AI-generated artifacts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib122" title="">122</a>]</cite>. By integrating watermarking mechanisms into the fabric of generative AI systems, researchers and stakeholders can address concerns related to misinformation, intellectual property infringement, and overall trustworthiness.</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.SSx1.p6">
<p class="ltx_p" id="S11.SSx1.p6.1">To further aid in watermarking efforts, adding identifiable metadata to AI-generated content should also be required in high-risk settings. For example, C2PA metadata is an open technical standard that allows organizations to embed metadata in media to verify its origin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib196" title="">196</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib10" title="">10</a>]</cite>. This extra step further helps create a causal link between AI-generated media and all stakeholders involved.<span class="ltx_note ltx_role_footnote" id="footnote51"><sup class="ltx_note_mark">51</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">51</sup><span class="ltx_tag ltx_tag_note">51</span>To learn more about C2PA and how to implement it, visit the <a class="ltx_ref ltx_href" href="https://c2pa.org/" title="">Coalition for Content Provenance and Authenticity</a>.</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S11.SSx2">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement TR2: Factual Grounding</h3>
<div class="ltx_para ltx_noindent" id="S11.SSx2.p1">
<p class="ltx_p" id="S11.SSx2.p1.1">In applications necessitating factual groundedness, model hallucination emerges as a prominent concern within generative AI systems. We can define hallucination in AI as a generated output containing false or misleading information presented as fact like incorrect, nonsensical, or unreal text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib137" title="">137</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib95" title="">95</a>]</cite>. This phenomenon poses significant challenges, particularly in domains where precision and reliability are paramount, such as education and journalistic reporting. The occurrence of hallucinations not only undermines the integrity of generated content but also jeopardizes user trust and confidence in AI-generated outputs. Furthermore, in contexts where decision-making relies on the information provided by AI systems, hallucinations can lead to erroneous conclusions and adverse outcomes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.SSx2.p2">
<p class="ltx_p" id="S11.SSx2.p2.1">One promising approach to combat model hallucination in generative AI systems is grounding them in trusted and curated data sources, such as external knowledge pools. This strategy, where generative AI systems are coupled to retrieval systems that search and find relevant information, is called retrieval augmented generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib116" title="">116</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib70" title="">70</a>]</cite>. Retrieval generation allows AI systems to, much like humans do, increase their knowledge and application scope by accessing large pools of curated information and, through their in-context learning skills, integrate the retrieved data into their outputs. This approach can help reduce the likelihood of hallucinations and improve the quality of generated outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib188" title="">188</a>]</cite>. This hybrid approach offers a promising avenue for combating hallucinations. RAG should be utilized in applications that require factual groundedness, given we cannot (currently) comprehend and audit how large generative models store and retrieve information from inscrutable matrices of floating point numbers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.SSx2.p3">
<p class="ltx_p" id="S11.SSx2.p3.1">While grounding generative AI in trusted data sources and adopting retrieval augmented generation techniques are crucial in mitigating model hallucination, rigorously evaluating such systems’ groundedness and factuality remains imperative. Hence, like in safety and robustness (SR1), generative systems (augmented by retrievers or not) need to be evaluated on benchmarks set to assess the propensity of a model to generate falsehoods.<span class="ltx_note ltx_role_footnote" id="footnote52"><sup class="ltx_note_mark">52</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">52</sup><span class="ltx_tag ltx_tag_note">52</span>For modalities involving language, benchmarks like <a class="ltx_ref ltx_href" href="https://github.com/sylinrl/TruthfulQA" title="">TruthfulQA</a> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib119" title="">119</a>]</cite> and <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2311.06697" title="">FactCheckQA</a> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib14" title="">14</a>]</cite> can help determine the truthfulness of a system. Meanwhile, <a class="ltx_ref ltx_href" href="https://github.com/awsaf49/artifact" title="">ArtiFact</a> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib170" title="">170</a>]</cite> <a class="ltx_ref ltx_href" href="https://github.com/SCLBD/DeepfakeBench" title="">DeepfakeBench</a> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib219" title="">219</a>]</cite> can help in dealing with applications involving computer vision. Lastly, <a class="ltx_ref ltx_href" href="https://www.kaggle.com/datasets/birdy654/deep-voice-deepfake-voice-recognition" title="">DEEP-VOICE</a> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib20" title="">20</a>]</cite> can aid in detecting and evaluating AI-generated speech.</span></span></span> This way, stakeholders can use benchmark evaluations to determine further how truthful a system is.</p>
</div>
</section>
<section class="ltx_subsection" id="S11.SSx3">
<h3 class="ltx_title ltx_title_subsection" style="color:#007091;">Requirement TR3: Fact Checking and Guardrails</h3>
<div class="ltx_para ltx_noindent" id="S11.SSx3.p1">
<p class="ltx_p" id="S11.SSx3.p1.1">When discussing fact-checking, we mean verifying the accuracy and truthfulness of claims, statements, or information circulating within various media platforms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib203" title="">203</a>]</cite>. When humans perform these activities, fact-checking involves rigorous investigation, comparison with credible sources, and analysis of evidence to determine the validity of the information in question. In applications involving high-risk settings, human moderators should be used to help flag and catalog AI-generated falsehoods, much like is already done in several media platforms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib199" title="">199</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib71" title="">71</a>]</cite>. However, as already pointed out by the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib23" title="">23</a>]</cite>, human oversight, when not augmented, has difficulties in accompanying the demands of large-scale automation. For this reason, creating automated forms of automated fact-checking guardrails should also be seen as a requirement.</p>
</div>
<div class="ltx_para ltx_noindent" id="S11.SSx3.p2">
<p class="ltx_p" id="S11.SSx3.p2.1">In the context of fact-checking, guardrails refer to a set of technical constraints designed to ensure the truthfulness of a system. For example, automated fact-checking systems can help expedite and scale human moderation while also flagging or blocking content that is deemed untruthful. Hence, to improve the scalability of human oversight, AI-empowered guardrails can further help improve the condition of a system. Using other AI tools to help us guard other AI systems is a common practice in the field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib173" title="">173</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#bib.bib171" title="">171</a>]</cite>, and stakeholders should employ them in the establishment of fail-safe procedures, especially in applications involving a limited, or high, risk.<span class="ltx_note ltx_role_footnote" id="footnote53"><sup class="ltx_note_mark">53</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">53</sup><span class="ltx_tag ltx_tag_note">53</span>AI-powered guardrails can be used to, for example, identify and debunk fabricated narratives (<a class="ltx_ref ltx_href" href="https://toolbox.google.com/factcheck/explorer" title="">Google’s FactCheck Tools</a>), improve factual consistency (<a class="ltx_ref ltx_href" href="https://github.com/citadel-ai/langcheck" title="">LangCheck</a>), detect factual inaccuracy (<a class="ltx_ref ltx_href" href="https://github.com/uptrain-ai/uptrain" title="">UpTrain</a>), and overall misinformation detection (<a class="ltx_ref ltx_href" href="https://github.com/GAIR-NLP/factool" title="">FacTool</a>).</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S11.SSx3.p3">
<p class="ltx_p" id="S11.SSx3.p3.1">By leveraging technological advancements to aid human control, fact-checking initiatives can effectively counter the proliferation of misleading content, safeguarding individuals’ power to make informed decisions and fostering a healthier information ecosystem between humans and AI.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S12">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text" id="S12.2.1.1" style="color:#007091;">12</span> </span><span class="ltx_text" id="S12.3.2" style="color:#007091;">Conclucing Remarks</span>
</h2>
<div class="ltx_para ltx_noindent" id="S12.p1">
<p class="ltx_p" id="S12.p1.1">Artificial intelligence has seamlessly woven itself into the fabric of our daily lives, from personalized recommendations on streaming platforms to intelligent assistants that fulfill our needs. However, its integration has sparked profound questions about the boundaries of technology. As AI permeates various sectors, concerns about privacy, autonomy, and many other ethical matters arise. Hence, as we navigate this era of unprecedented technological advancement, it becomes imperative to assess the implications of AI integration critically and actively shape its evolution to align with societal values and aspirations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S12.p2">
<p class="ltx_p" id="S12.p2.1">Regulating and certifying AI ensures this technology’s trustworthy and ethical deployment. However, stakeholders need clear criteria and guidelines to assess whether development is made with societal values in mind. To aid on this front, we presented two sets of ethical and practical guidance to the community in this document. While our <span class="ltx_text ltx_font_bold" id="S12.p2.1.1">overall ethical requirements</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S5" title="5 Overall Ethical Requirements (O) ‣ Catalog of General Ethical Requirements for AI Certification">O1 - O6</a>) represent general and holistic values that should serve as guiding foundations for trustworthy AI development, our <span class="ltx_text ltx_font_bold" id="S12.p2.1.2">value-specific requirements</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S6" title="6 Fairness (F) ‣ Catalog of General Ethical Requirements for AI Certification">F</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S7" title="7 Privacy and Data Protection (P) ‣ Catalog of General Ethical Requirements for AI Certification">P</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S8" title="8 Safety and Robustness (SR) ‣ Catalog of General Ethical Requirements for AI Certification">SR</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S9" title="9 Sustainability (SU) ‣ Catalog of General Ethical Requirements for AI Certification">SU</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S10" title="10 Transparency and Explainability (T) ‣ Catalog of General Ethical Requirements for AI Certification">T</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12289v1#S11" title="11 Truthfulness (TR) ‣ Catalog of General Ethical Requirements for AI Certification">TR</a>) deliver more pragmatic and implementational requirements and procedures, alongside with possible criteria of evaluation in line with the EU AI Act risk-category system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S12.p3">
<p class="ltx_p" id="S12.p3.1">At the same time, certifying AI systems requires assessing that minimal ethical requirements are fulfilled and dealt with in practice. Nevertheless, bridging the principles-practice gap is an ongoing area of research and exploration that poses a considerable challenge to applied ethics. In this work, we highlight and suggest tools that could tackle specific challenges related to trustworthy AI development to the reader. Yet, we emphasize that many other resources approach this aspect more thoroughly. Like for example, the Catalog of Tools and Metrics from the OECD,<span class="ltx_note ltx_role_footnote" id="footnote54"><sup class="ltx_note_mark">54</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">54</sup><span class="ltx_tag ltx_tag_note">54</span>The OECD’s ”<a class="ltx_ref ltx_href" href="https://oecd.ai/en/catalogue/tools" title="">Catalogue of Tools &amp; Metrics for Trustworthy AI</a>” presents tools and metrics designed to help AI actors develop and use trustworthy AI systems and applications that respect human rights and are fair, transparent, explainable, robust, secure, and safe.</span></span></span> which currently harbors more than 700 practical implementations for trustworthy AI development, be that procedural tools<span class="ltx_note ltx_role_footnote" id="footnote55"><sup class="ltx_note_mark">55</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">55</sup><span class="ltx_tag ltx_tag_note">55</span>As an example, we can mention <a class="ltx_ref ltx_href" href="https://nkluge-correa.github.io/ethical-problem-solving/" title="">Ethical Problem Solving</a>, a framework to promote the development of safe and ethical artificial intelligence via algorithmic impact assessment tools) and a recommendation methodology that culminates in an extensive developmental toolbox.</span></span></span> to aid in the lifecycle of AI systems or educational tools<span class="ltx_note ltx_role_footnote" id="footnote56"><sup class="ltx_note_mark">56</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">56</sup><span class="ltx_tag ltx_tag_note">56</span>One example is the <a class="ltx_ref ltx_href" href="https://github.com/Nkluge-correa/TeenyTinyCastle" title="">Teeny-Tiny Castle</a>, a collection of educational tutorials on using tools for AI Ethics and Safety research and application.</span></span></span> to help improve humanistic and ethical practices in IT-related fields.</p>
</div>
<div class="ltx_para ltx_noindent" id="S12.p4">
<p class="ltx_p" id="S12.p4.1">Finally, we highlight that this is an ongoing project, bound to be changed and adapted as the field progresses. Regardless, we hope our efforts can be used, expanded, and built upon by the community in search of an ever more trustworthy AI.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section" style="color:#007091;">Acknowledgments</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">"Zertifizierte KI" (Certified AI) is a KI.NRW-flagship project funded by the Ministerium für Wirtschaft, Industrie, Klimaschutz und Energie des Landes Nordrhein-Westfalen (Ministry for Economic Affairs, Industry, Climate Action and Energy of the State of North Rhine-Westphalia). We thank Sergio Genovesi and Marta Cassina for their contributions to the initial draft of the milestone, Chelsea Haramia, Was Rahman, and Christiane Schäfer for their comments in our research group meeting, Was Rahman for comments during the milestone’s editing process and Sophia Falk for comments on the whitepaper draft.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://eur-lex.europa.eu/eli/reg/2016/679/oj" title="">https://eur-lex.europa.eu/eli/reg/2016/679/oj</a>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Hussein Abbass, Axel Bender, Svetoslav Gaidow, and Paul Whitbread.

</span>
<span class="ltx_bibblock">Computational red teaming: Past, present and future.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">IEEE Computational Intelligence Magazine</span>, 6(1):30–42, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2303.08774</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
HLEG AI.

</span>
<span class="ltx_bibblock">High-level expert group on artificial intelligence.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai" title="">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.

</span>
<span class="ltx_bibblock">Gqa: Training generalized multi-query transformer models from multi-head checkpoints.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2305.13245</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Michelle A Amazeen.

</span>
<span class="ltx_bibblock">Revisiting the epistemology of fact-checking.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Critical Review</span>, 27(1):1–22, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.

</span>
<span class="ltx_bibblock">Concrete problems in ai safety.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:1606.06565</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Colorado General Assembly.

</span>
<span class="ltx_bibblock">Colorado sb21-169: Regulation prohibiting unfair discrimination in insurance.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://leg.colorado.gov/bills/sb21-169" title="">https://leg.colorado.gov/bills/sb21-169</a>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang.

</span>
<span class="ltx_bibblock">Recent advances in adversarial training for adversarial robustness.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2102.01356</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Kar Balan, Shruti Agarwal, Simon Jenni, Andy Parsons, Andrew Gilbert, and John Collomosse.

</span>
<span class="ltx_bibblock">Ekila: synthetic media provenance and attribution for generative art.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 913–922, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hubert Baniecki, Wojciech Kretowicz, Piotr Piatyszek, Jakub Wisniewski, and Przemyslaw Biecek.

</span>
<span class="ltx_bibblock">dalex: Responsible machine learning with interactive explainability and fairness in python.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Journal of Machine Learning Research</span>, 22(214):1–7, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Célestin Banza Lubaba Nkulu, Lidia Casas, Vincent Haufroid, Thierry De Putter, Nelly D Saenen, Tony Kayembe-Kitenge, Paul Musa Obadia, Daniel Kyanika Wa Mukoma, Jean-Marie Lunda Ilunga, Tim S Nawrot, et al.

</span>
<span class="ltx_bibblock">Sustainability of artisanal mining of cobalt in dr congo.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Nature sustainability</span>, 1(9):495–504, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Solon Barocas, Moritz Hardt, and Arvind Narayanan.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Fairness and machine learning: Limitations and opportunities</span>.

</span>
<span class="ltx_bibblock">MIT Press, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Vasilisa Bashlovkina, Zhaobin Kuang, Riley Matthews, Edward Clifford, Yennie Jun, William W Cohen, and Simon Baumgartner.

</span>
<span class="ltx_bibblock">Trusted source alignment in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2311.06697</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Emma Beede, Elizabeth Baylor, Fred Hersch, Anna Iurchenko, Lauren Wilcox, Paisan Ruamviboonsuk, and Laura M Vardoulakis.

</span>
<span class="ltx_bibblock">A human-centered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2020 CHI conference on human factors in computing systems</span>, pages 1–12, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varshney, and Yunfeng Zhang.

</span>
<span class="ltx_bibblock">AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:1810.01943</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Iz Beltagy, Matthew E Peters, and Arman Cohan.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2004.05150</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, et al.

</span>
<span class="ltx_bibblock">Managing ai risks in an era of rapid progress.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2310.17688</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Przemyslaw Biecek.

</span>
<span class="ltx_bibblock">Dalex: Explainers for complex predictive models in r.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Journal of Machine Learning Research</span>, 19(84):1–5, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jordan J Bird and Ahmad Lotfi.

</span>
<span class="ltx_bibblock">Real-time detection of ai-generated speech for deepfake voice conversion.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2308.12734</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Larissa Bolte, Tijs Vandemeulebroucke, and Aimee van Wynsberghe.

</span>
<span class="ltx_bibblock">From an ethics of carefulness to an ethics of desirability: Going beyond current ethics approaches to sustainable ai.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Sustainability</span>, 14(8):4472, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Johnny Botha and Heloise Pieterse.

</span>
<span class="ltx_bibblock">Fake news and deepfakes: A dangerous threat for 21st century information security.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">ICCWS 2020 15th International Conference on Cyber Warfare and Security. Academic Conferences and publishing limited</span>, page 57, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamilė Lukošiūtė, Amanda Askell, Andy Jones, Anna Chen, et al.

</span>
<span class="ltx_bibblock">Measuring progress on scalable oversight for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2211.03540</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Sarah B Boyd.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Life-cycle assessment of semiconductors</span>.

</span>
<span class="ltx_bibblock">Springer Science &amp; Business Media, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Advances in neural information processing systems</span>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with gpt-4.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2303.12712</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Rafael A Calvo, Dorian Peters, Karina Vold, and Richard M Ryan.

</span>
<span class="ltx_bibblock">Supporting human autonomy in ai systems: A framework for ethical enquiry.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Ethics of digital well-being: A multidisciplinary approach</span>, pages 31–54, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Alycia N Carey and Xintao Wu.

</span>
<span class="ltx_bibblock">The causal fairness field guide: Perspectives from social and formal sciences.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Frontiers in Big Data</span>, 5:892837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Samuel Carton, Qiaozhu Mei, and Paul Resnick.

</span>
<span class="ltx_bibblock">Feature-based explanations don’t help people detect misclassifications of online toxicity.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the international AAAI conference on web and social media</span>, volume 14, pages 95–106, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Simon Caton and Christian Haas.

</span>
<span class="ltx_bibblock">Fairness in machine learning: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">ACM Computing Surveys</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Eshwar Chandrasekharan, Chaitrali Gandhi, Matthew Wortley Mustelier, and Eric Gilbert.

</span>
<span class="ltx_bibblock">Crossmod: A cross-community learning-based system to assist reddit moderators.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Proceedings of the ACM on human-computer interaction</span>, 3(CSCW):1–30, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Bobby Chesney and Danielle Citron.

</span>
<span class="ltx_bibblock">Deep fakes: A looming challenge for privacy, democracy, and national security.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Calif. L. Rev.</span>, 107:1753, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer.

</span>
<span class="ltx_bibblock">cudnn: Efficient primitives for deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:1410.0759</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
François Chollet.

</span>
<span class="ltx_bibblock">Xception: Deep learning with depthwise separable convolutions.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 1251–1258, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Alexandra Chouldechova and Aaron Roth.

</span>
<span class="ltx_bibblock">The frontiers of fairness in machine learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:1810.08810</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Luz Claudio.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Waste couture: Environmental impact of the clothing industry</span>.

</span>
<span class="ltx_bibblock">National Institute of Environmental Health Sciences, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
CNIL.

</span>
<span class="ltx_bibblock">Pia: Analyse d’impact sur la prptection des donnees.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cnil.fr/en/open-source-pia-software-helps-carry-out-data-protection-impact-assessment" title="">https://www.cnil.fr/en/open-source-pia-software-helps-carry-out-data-protection-impact-assessment</a>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
CodeCarbon.

</span>
<span class="ltx_bibblock">Codecarbon: Track emissions from compute and recommend ways to reduce their impact on the environment.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mlco2/codecarbon" title="">https://github.com/mlco2/codecarbon</a>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Mark Coeckelbergh.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">AI ethics</span>.

</span>
<span class="ltx_bibblock">Mit Press, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Nicholas Kluge Corrêa, Nythamar Fernandes de Oliveira, and Diogo Fernando Massmann.

</span>
<span class="ltx_bibblock">Sobre a eficiência da ética como ferramenta de governança da inteligência artificial.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Veritas (Porto Alegre)</span>, 67(1):e42584–e42584, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Nicholas Kluge Corrêa, Camila Galvão, James William Santos, Carolina Del Pino, Edson Pontes Pinto, Camila Barbosa, Diogo Massmann, Rodrigo Mambrini, Luiza Galvão, Edmund Terem, et al.

</span>
<span class="ltx_bibblock">Worldwide ai ethics: A review of 200 guidelines and recommendations for ai governance.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Patterns</span>, 4(10), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
New York City Council.

</span>
<span class="ltx_bibblock">The new york city council-file#: Int 1894-2020 (law number 2021/144).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page" title="">https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page</a>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Armin B. Cremers, Alex Englander, Markus Gabriel, Dirk Hecker, Michael Mock, Maximilian Poretschkin, Julia Rosenzweig, Frauke Rostalski, Joachim Sicking, Julia Volmer, Jan Voosholz, Angelika Voss, and Stefan Wrobel.

</span>
<span class="ltx_bibblock">Trustworthy use of artificial intelligence: Priorities from a philosophical, ethical, legal, and technological viewpoint as a basis for certification of artificial intelligence.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ki.nrw/wp-content/uploads/2020/03/Whitepaper_Thrustworthy_AI.pdf" title="">https://www.ki.nrw/wp-content/uploads/2020/03/Whitepaper_Thrustworthy_AI.pdf</a>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Allan Dafoe.

</span>
<span class="ltx_bibblock">Ai governance: a research agenda.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Governance of AI Program, Future of Humanity Institute, University of Oxford: Oxford, UK</span>, 1442:1443, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Francesco Daghero, Daniele Jahier Pagliari, and Massimo Poncino.

</span>
<span class="ltx_bibblock">Energy-efficient deep learning inference on edge devices.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Advances in Computers</span>, volume 122, pages 247–301. Elsevier, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Tri Dao.

</span>
<span class="ltx_bibblock">Flashattention-2: Faster attention with better parallelism and work partitioning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2307.08691</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with io-awareness.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Advances in Neural Information Processing Systems</span>, 35:16344–16359, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms. arxiv 2023.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2305.14314</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Virginia Dignum.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Responsible artificial intelligence: how to develop and use AI in a responsible way</span>, volume 2156.

</span>
<span class="ltx_bibblock">Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang.

</span>
<span class="ltx_bibblock">Longrope: Extending llm context window beyond 2 million tokens.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2402.13753</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan.

</span>
<span class="ltx_bibblock">Measuring the carbon intensity of ai in cloud instances.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</span>, pages 1877–1894, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao, Jie Meng, Wenjie Ruan, and Xiaowei Huang.

</span>
<span class="ltx_bibblock">Building guardrails for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2402.01822</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Julia Dressel and Hany Farid.

</span>
<span class="ltx_bibblock">The accuracy, fairness, and limits of predicting recidivism.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Science advances</span>, 4(1):eaao5580, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
EDPB and EDPS.

</span>
<span class="ltx_bibblock">Edpb-edps joint opinion 5/2021 on the proposal for a regulation of the european parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://edpb.europa.eu/system/files/2021-06/edpb-edps_joint_opinion_ai_regulation_en.pdf" title="">https://edpb.europa.eu/system/files/2021-06/edpb-edps_joint_opinion_ai_regulation_en.pdf</a>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
EDPS.

</span>
<span class="ltx_bibblock">Accountability on the ground part i.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Abigayle Erickson.

</span>
<span class="ltx_bibblock">Comparative analysis of the eu’s gdpr and brazil’s lgpd: Enforcement challenges with the lgpd.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">Brook. J. Int’l L.</span>, 44:859, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders.

</span>
<span class="ltx_bibblock">Truthful ai: Developing and governing ai that does not lie.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2110.06674</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Tom Evans.

</span>
<span class="ltx_bibblock">How green is silicon valley? ecological sustainability and the high-tech industry.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Berkeley Planning Journal</span>, 17(1), 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Oleg Evsutin, Anna Melman, and Roman Meshcheryakov.

</span>
<span class="ltx_bibblock">Digital steganography and watermarking for digital images: A review of current research directions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">IEEE Access</span>, 8:166589–166611, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Gregory Falco, Ben Shneiderman, Julia Badger, Ryan Carrier, Anton Dahbura, David Danks, Martin Eling, Alwyn Goodloe, Jerry Gupta, Christopher Hart, et al.

</span>
<span class="ltx_bibblock">Governing ai safety through independent audits.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">Nature Machine Intelligence</span>, 3(7):566–571, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Sophia Falk and Aimee van Wynsberghe.

</span>
<span class="ltx_bibblock">Challenging ai for sustainability: what ought it mean?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">AI and Ethics</span>, pages 1–11, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian.

</span>
<span class="ltx_bibblock">Certifying and removing disparate impact.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</span>, pages 259–268, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Wendi Folkert.

</span>
<span class="ltx_bibblock">Assessment results regarding organization designation authorization (oda) unit member (um) independence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">Aviation Safety</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Nikolaos Fotos.

</span>
<span class="ltx_bibblock">Specification and implementation of metadata for secure image provenance information.

</span>
<span class="ltx_bibblock">Master’s thesis, Universitat Politècnica de Catalunya, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Nancy Fraser.

</span>
<span class="ltx_bibblock">Social justice in the age of identity politics: Redistribution, recognition, and participation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Geographic Thought</span>, pages 72–89. Routledge, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Future of Life Institute.

</span>
<span class="ltx_bibblock">Pause giant ai experiments: An open letter.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/" title="">https://futureoflife.org/open-letter/pause-giant-ai-experiments/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al.

</span>
<span class="ltx_bibblock">Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">arXiv preprint arXiv:2209.07858</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">A framework for few-shot language model evaluation.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/lm-evaluation-harness" title="">https://github.com/EleutherAI/lm-evaluation-harness</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2312.10997</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Dongfang Gaozhao.

</span>
<span class="ltx_bibblock">Flagging fake news on social media: An experimental study of media consumers’ identification of fake news.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">Government Information Quarterly</span>, 38(3):101591, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Raanan Gillon.

</span>
<span class="ltx_bibblock">" primum non nocere" and the principle of non-maleficence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">British medical journal (Clinical research ed.)</span>, 291(6488):130, 1985.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">Deep learning</span>.

</span>
<span class="ltx_bibblock">MIT press, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.

</span>
<span class="ltx_bibblock">Explaining and harnessing adversarial examples.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:1412.6572</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">People + ai guidebook.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pair.withgoogle.com/guidebook" title="">https://pair.withgoogle.com/guidebook</a>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Lucas Graves and Federica Cherubini.

</span>
<span class="ltx_bibblock">The rise of fact-checking sites in europe.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">Digital News Project Report</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
AI Ethics Impact Group.

</span>
<span class="ltx_bibblock">From principles to practice – an interdisciplinary framework to operationalize ai ethics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ai-ethics-impact.org/en" title="">https://www.ai-ethics-impact.org/en</a>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Albert Gu and Tri Dao.

</span>
<span class="ltx_bibblock">Mamba: Linear-time sequence modeling with selective state spaces.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2312.00752</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Joao Guerreiro and Sandra Maria Correia Loureiro.

</span>
<span class="ltx_bibblock">I am attracted to my cool smart assistant! analyzing attachment-aversion in ai-human relationships.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">Journal of Business Research</span>, 161:113863, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Laura Gustafson, Chloe Rolland, Nikhila Ravi, Quentin Duval, Aaron Adcock, Cheng-Yang Fu, Melissa Hall, and Candace Ross.

</span>
<span class="ltx_bibblock">Facet: Fairness in computer vision evaluation benchmark.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 20370–20382, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Thilo Hagendorff.

</span>
<span class="ltx_bibblock">The ethics of ai ethics: An evaluation of guidelines.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">Minds and machines</span>, 30(1):99–120, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Xudong Han, Timothy Baldwin, and Trevor Cohn.

</span>
<span class="ltx_bibblock">Balancing out bias: Achieving fairness through balanced training.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">arXiv preprint arXiv:2109.08253</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Karen Hayes and Richard Burge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">Coltan Mining in the Democratic Republic of Congo: How tantalum-using industries can commit to the reconstruction of the DRC</span>.

</span>
<span class="ltx_bibblock">Fauna &amp; Flora International Cambridge, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Yihui He, Jianing Qian, Jianren Wang, Cindy X Le, Congrui Hetang, Qi Lyu, Wenping Wang, and Tianwei Yue.

</span>
<span class="ltx_bibblock">Depth-wise decomposition for accelerating separable convolutions in efficient convolutional neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:1910.09455</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Unsolved problems in ml safety.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:2109.13916</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al.

</span>
<span class="ltx_bibblock">Imagen video: High definition video generation with diffusion models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">arXiv preprint arXiv:2210.02303</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Jaap-Henk Hoepman.

</span>
<span class="ltx_bibblock">Privacy design strategies: The little blue book.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cs.ru.nl/~jhh/publications/pds-booklet.pdf" title="">https://www.cs.ru.nl/~jhh/publications/pds-booklet.pdf</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2203.15556</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Sebastian Houben, Stephanie Abrecht, Maram Akila, Andreas Bär, Felix Brockherde, Patrick Feifel, Tim Fingscheidt, Sujan Sai Gannamaneni, Seyed Eghbal Ghobadi, Ahmed Hammam, et al.

</span>
<span class="ltx_bibblock">Inspect, understand, overcome: A survey of practical methods for ai safety.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">Deep Neural Networks and Data for Automated Driving: Robustness, Uncertainty Quantification, and Insights Towards Safety</span>, pages 3–78. Springer International Publishing Cham, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng, et al.

</span>
<span class="ltx_bibblock">Sleeper agents: Training deceptive llms that persist through safety training.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">arXiv preprint arXiv:2401.05566</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Keith Hyams and Tina Fawcett.

</span>
<span class="ltx_bibblock">The ethics of carbon offsetting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">Wiley Interdisciplinary Reviews: Climate Change</span>, 4(2):91–98, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
IEEE.

</span>
<span class="ltx_bibblock">Ieee certifaied – ontological specification for ethical privacy.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://engagestandards.ieee.org/rs/211-FYL-955/images/IEEESTD-2022%20CertifAIEd%20Privacy.pdf" title="">https://engagestandards.ieee.org/rs/211-FYL-955/images/IEEESTD-2022%20CertifAIEd%20Privacy.pdf</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Reed M Izatt, Steven R Izatt, Ronald L Bruening, Neil E Izatt, and Bruce A Moyer.

</span>
<span class="ltx_bibblock">Challenges to achievement of metal sustainability in our high-tech society.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">Chemical Society Reviews</span>, 43(8):2451–2475, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">ACM Computing Surveys</span>, 55(12):1–38, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">arXiv preprint arXiv:2310.06825</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Anna Jobin, Marcello Ienca, and Effy Vayena.

</span>
<span class="ltx_bibblock">The global landscape of ai ethics guidelines.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">Nature machine intelligence</span>, 1(9):389–399, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Neil F Johnson, Zoran Duric, and Sushil Jajodia.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib98.1.1">Information hiding: steganography and watermarking-attacks and countermeasures: steganography and watermarking: attacks and countermeasures</span>, volume 1.

</span>
<span class="ltx_bibblock">Springer Science &amp; Business Media, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Phillip Johnston and Rozi Harris.

</span>
<span class="ltx_bibblock">The boeing 737 max saga: lessons for software organizations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib99.1.1">Software Quality Professional</span>, 21(3):4–12, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Mislav Juric, Agneza Sandic, and Mario Brcic.

</span>
<span class="ltx_bibblock">Ai safety: state of the field through quantitative lens.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)</span>, pages 1254–1259. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma.

</span>
<span class="ltx_bibblock">Fairness-aware learning through regularization approach.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">2011 IEEE 11th International Conference on Data Mining Workshops</span>, pages 643–650. IEEE, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Kimmo Kärkkäinen and Jungseock Joo.

</span>
<span class="ltx_bibblock">Fairface: Face attribute dataset for balanced race, gender, and age.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">arXiv preprint arXiv:1908.04913</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo.

</span>
<span class="ltx_bibblock">Gpt-4 passes the bar exam.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib103.1.1">Available at SSRN 4389233</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Monique F Kilkenny and Kerin M Robinson.

</span>
<span class="ltx_bibblock">Data quality:“garbage in–garbage out”, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.

</span>
<span class="ltx_bibblock">A watermark for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib105.1.1">arXiv preprint arXiv:2301.10226</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Elyakim Kislev.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">Relationships 5.0: How AI, VR, and robots will reshape our emotional lives</span>.

</span>
<span class="ltx_bibblock">Oxford University Press, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Janis Klaise, Arnaud Van Looveren, Giovanni Vacanti, and Alexandru Coca.

</span>
<span class="ltx_bibblock">Alibi explain: Algorithms for explaining machine learning models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib107.1.1">Journal of Machine Learning Research</span>, 22(181):1–7, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, et al.

</span>
<span class="ltx_bibblock">Captum: A unified and generic model interpretability library for pytorch.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib108.1.1">arXiv preprint arXiv:2009.07896</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Hadas Kotek, Rikker Dockum, and David Sun.

</span>
<span class="ltx_bibblock">Gender bias and stereotypes in large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib109.1.1">Proceedings of The ACM Collective Intelligence Conference</span>, pages 12–24, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.

</span>
<span class="ltx_bibblock">Adversarial machine learning at scale.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib110.1.1">arXiv preprint arXiv:1611.01236</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.

</span>
<span class="ltx_bibblock">Quantifying the carbon emissions of machine learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib111.1.1">arXiv preprint arXiv:1910.09700</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Vivian Lai, Samuel Carton, Rajat Bhatnagar, Q Vera Liao, Yunfeng Zhang, and Chenhao Tan.

</span>
<span class="ltx_bibblock">Human-ai collaboration via conditional delegation: A case study of content moderation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib112.1.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</span>, pages 1–18, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Stefan Larsson and Fredrik Heintz.

</span>
<span class="ltx_bibblock">Transparency in artificial intelligence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib113.1.1">Internet Policy Review</span>, 9(2), 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib114.1.1">nature</span>, 521(7553):436–444, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Brenda Leong and Evan Selinger.

</span>
<span class="ltx_bibblock">Robot eyes wide shut: Understanding dishonest anthropomorphism.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib115.1.1">Proceedings of the conference on fairness, accountability, and transparency</span>, pages 299–308, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib116.1.1">Advances in Neural Information Processing Systems</span>, 33:9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Yi Li and Nuno Vasconcelos.

</span>
<span class="ltx_bibblock">Repair: Removing representation bias by dataset resampling.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib117.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 9572–9581, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.

</span>
<span class="ltx_bibblock">Awq: Activation-aware weight quantization for llm compression and acceleration.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib118.1.1">arXiv preprint arXiv:2306.00978</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib119.1.1">arXiv preprint arXiv:2109.07958</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun.

</span>
<span class="ltx_bibblock">Deplot: One-shot visual language reasoning by plot-to-table translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib120.1.1">arXiv preprint arXiv:2212.10505</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib121.1.1">arXiv preprint arXiv:1907.11692</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, and Yang Zhang.

</span>
<span class="ltx_bibblock">Watermarking diffusion model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib122.1.1">arXiv preprint arXiv:2305.12502</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Duri Long and Brian Magerko.

</span>
<span class="ltx_bibblock">What is ai literacy? competencies and design considerations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib123.1.1">Proceedings of the 2020 CHI conference on human factors in computing systems</span>, pages 1–16, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
David F Longbine.

</span>
<span class="ltx_bibblock">Red teaming: past and present.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib124.1.1">School of Advanced Military Studies, Army Command and General Staff College</span>, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Kadan Lottick, Silvia Susai, Sorelle A Friedler, and Jonathan P Wilson.

</span>
<span class="ltx_bibblock">Energy usage reports: Environmental awareness as part of algorithmic accountability.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib125.1.1">arXiv preprint arXiv:1911.08354</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Heather Lovell and Diana Liverman.

</span>
<span class="ltx_bibblock">Understanding carbon offset technologies.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib126.1.1">New Political Economy</span>, 15(2):255–273, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
Gangzhao Lu, Weizhe Zhang, and Zheng Wang.

</span>
<span class="ltx_bibblock">Optimizing depthwise separable convolution operations on gpus.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib127.1.1">IEEE Transactions on Parallel and Distributed Systems</span>, 33(1):70–87, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.

</span>
<span class="ltx_bibblock">Estimating the carbon footprint of bloom, a 176b parameter language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib128.1.1">Journal of Machine Learning Research</span>, 24(253):1–15, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite.

</span>
<span class="ltx_bibblock">Stable bias: Evaluating societal representations in diffusion models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib129.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Scott M Lundberg and Su-In Lee.

</span>
<span class="ltx_bibblock">A unified approach to interpreting model predictions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib130.1.1">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Sophie Maddocks.

</span>
<span class="ltx_bibblock">‘a deepfake porn plot intended to silence me’: exploring continuities between pornographic and ‘political’deep fakes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib131.1.1">Porn Studies</span>, 7(4):415–423, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Trisha Mahoney, Kush Varshney, and Michael Hind.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib132.1.1">AI fairness</span>.

</span>
<span class="ltx_bibblock">O’Reilly Media, Incorporated, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Karima Makhlouf, Sami Zhioua, and Catuscia Palamidessi.

</span>
<span class="ltx_bibblock">Survey on causal-based machine learning fairness notions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib133.1.1">arXiv preprint arXiv:2010.09553</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.

</span>
<span class="ltx_bibblock">Peft: State-of-the-art parameter-efficient fine-tuning methods.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/peft" title="">https://github.com/huggingface/peft</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
Karolina Mania.

</span>
<span class="ltx_bibblock">Legal protection of revenge and deepfake porn victims in the european union: Findings from a comparative legal study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib135.1.1">Trauma, Violence, &amp; Abuse</span>, 25(1):117–129, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Nicholas Martin, Michael Friedewald, Ina Schiering, Britta A. Mester, Dara Hallinan, and Meiko Jensen.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib136.1.1">The Data Protection Impact Assessment According to Article 35 GDPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald.

</span>
<span class="ltx_bibblock">On faithfulness and factuality in abstractive summarization.

</span>
<span class="ltx_bibblock">In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <span class="ltx_text ltx_font_italic" id="bib.bib137.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span>, pages 1906–1919, Online, July 2020. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan.

</span>
<span class="ltx_bibblock">A survey on bias and fairness in machine learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib138.1.1">ACM computing surveys (CSUR)</span>, 54(6):1–35, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.

</span>
<span class="ltx_bibblock">Locating and editing factual associations in gpt.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib139.1.1">Advances in Neural Information Processing Systems</span>, 35:17359–17372, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Gaurav Menghani.

</span>
<span class="ltx_bibblock">Efficient deep learning: A survey on making deep learning models smaller, faster, and better.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib140.1.1">ACM Computing Surveys</span>, 55(12):1–37, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
Albert K Mensah, Ishmail O Mahiri, Obed Owusu, Okoree D Mireku, Ishmael Wireko, and Evans A Kissi.

</span>
<span class="ltx_bibblock">Environmental impacts of mining: a study of mining communities in ghana.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib141.1.1">Applied Ecology and Environmental Sciences</span>, 3(3):81–94, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Justin Meza, Rocky Shih, Amip Shah, Parthasarathy Ranganathan, Jichuan Chang, and Cullen Bash.

</span>
<span class="ltx_bibblock">Lifecycle-based data center design.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib142.1.1">ASME International Mechanical Engineering Congress and Exposition</span>, volume 44281, pages 217–226, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.

</span>
<span class="ltx_bibblock">Model cards for model reporting.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib143.1.1">Proceedings of the conference on fairness, accountability, and transparency</span>, pages 220–229, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Melanie Mitchell.

</span>
<span class="ltx_bibblock">Debates on the nature of artificial general intelligence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib144.1.1">Science</span>, 383(6689), 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Brent Mittelstadt.

</span>
<span class="ltx_bibblock">Principles alone cannot guarantee ethical ai.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib145.1.1">Nature machine intelligence</span>, 1(11):501–507, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
Christoph Molnar.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib146.1.1">Interpretable machine learning</span>.

</span>
<span class="ltx_bibblock">Lulu. com, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Sheikh Moniruzzaman Moni, Roksana Mahmud, Karen High, and Michael Carbajales-Dale.

</span>
<span class="ltx_bibblock">Life cycle assessment of emerging technologies: A review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib147.1.1">Journal of Industrial Ecology</span>, 24(1):52–63, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
Daniel Moran, Darian McBain, Keiichiro Kanemoto, Manfred Lenzen, and Arne Geschke.

</span>
<span class="ltx_bibblock">Global supply chains of coltan: a hybrid life cycle assessment study using a social indicator.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib148.1.1">Journal of Industrial Ecology</span>, 19(3):357–365, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
Jessica Morley, Luciano Floridi, Libby Kinsey, and Anat Elhalal.

</span>
<span class="ltx_bibblock">From what to how: an initial review of publicly available ai ethics tools, methods and research to translate principles into practices.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib149.1.1">Science and engineering ethics</span>, 26(4):2141–2168, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
David Mytton.

</span>
<span class="ltx_bibblock">Data centre water consumption.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib150.1.1">npj Clean Water</span>, 4(1):11, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Progress measures for grokking via mechanistic interpretability.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib151.1.1">arXiv preprint arXiv:2301.05217</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Michael Nest.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib152.1.1">Coltan</span>, volume 3.

</span>
<span class="ltx_bibblock">Polity, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Hamed Nilforoshan, Johann D Gaebler, Ravi Shroff, and Sharad Goel.

</span>
<span class="ltx_bibblock">Causal conceptions of fairness and their consequences.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib153.1.1">International Conference on Machine Learning</span>, pages 16848–16887. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
Rohit Nishant, Mike Kennedy, and Jacqueline Corbett.

</span>
<span class="ltx_bibblock">Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib154.1.1">International Journal of Information Management</span>, 53:102104, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
Martha C Nussbaum.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib155.1.1">Creating capabilities: The human development approach</span>.

</span>
<span class="ltx_bibblock">Harvard University Press, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev.

</span>
<span class="ltx_bibblock">The building blocks of interpretability.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib156.1.1">Distill</span>, 3(3):e10, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
Daniel W O’Neill, Andrew L Fanning, William F Lamb, and Julia K Steinberger.

</span>
<span class="ltx_bibblock">A good life for all within planetary boundaries.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib157.1.1">Nature sustainability</span>, 1(2):88–95, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
Ioannis Pastaltzidis, Nikolaos Dimitriou, Katherine Quezada-Tavarez, Stergios Aidinlis, Thomas Marquenie, Agata Gurzawska, and Dimitrios Tzovaras.

</span>
<span class="ltx_bibblock">Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib158.1.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</span>, pages 2302–2314, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.

</span>
<span class="ltx_bibblock">Carbon emissions and large neural network training.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib159.1.1">arXiv preprint arXiv:2104.10350</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al.

</span>
<span class="ltx_bibblock">Rwkv: Reinventing rnns for the transformer era.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib160.1.1">arXiv preprint arXiv:2305.13048</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Red teaming language models with language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib161.1.1">arXiv preprint arXiv:2202.03286</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Christoph Peylo, Dirk Slama, Sebastian Hallensleben, Andreas Hauschke, and Stefanie Hildebrandt.

</span>
<span class="ltx_bibblock">Vcio based description of systems for ai trustworthiness characterisation, 2022.

</span>
<span class="ltx_bibblock">Accessed on September 14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.

</span>
<span class="ltx_bibblock">Sdxl: Improving latent diffusion models for high-resolution image synthesis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib163.1.1">arXiv preprint arXiv:2307.01952</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
Maximilian Poretschkin, Anna Schmitz, Maram Akila, Linara Adilova, Daniel Becker, Armin B. Cremers, Dirk Hecker, Sebastian Houben, Michael Mock, Julia Rosenzweig, Joachim Sicking, Elena Schulz, Angelika Voss, and Stefan Wrobel.

</span>
<span class="ltx_bibblock">Guideline for designing trustworthy artificial intelligence.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Olivier Pourret, Bastien Lange, Jessica Bonhoure, Gilles Colinet, Sophie Decrée, Grégory Mahy, Maxime Séleck, Mylor Shutcha, and Michel-Pierre Faucon.

</span>
<span class="ltx_bibblock">Assessment of soil metal distribution and environmental impact of mining in katanga (democratic republic of congo).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib165.1.1">Applied Geochemistry</span>, 64:43–55, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Simon JD Prince.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib166.1.1">Understanding Deep Learning</span>.

</span>
<span class="ltx_bibblock">MIT press, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Mahima Pushkarna, Andrew Zaldivar, and Dan Nanas.

</span>
<span class="ltx_bibblock">The data cards playbook.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://sites.research.google/datacardsplaybook/" title="">https://sites.research.google/datacardsplaybook/</a>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision. 2022.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib168.1.1">arXiv preprint arxiv:2212.04356</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib169.1.1">OpenAI blog</span>, 1(8):9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Md Awsafur Rahman, Bishmoy Paul, Najibul Haque Sarker, Zaber Ibn Abdul Hakim, and Shaikh Anowarul Fattah.

</span>
<span class="ltx_bibblock">Artifact: A large-scale dataset with artificial and factual images for generalizable and robust synthetic image detection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib170.1.1">arXiv preprint arXiv:2302.11970</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
Parijat Rai, Saumil Sood, Vijay K Madisetti, and Arshdeep Bahga.

</span>
<span class="ltx_bibblock">Guardian: A multi-tiered defense architecture for thwarting prompt injection attacks on llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib171.1.1">Journal of Software Engineering and Applications</span>, 17(1):43–68, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
John Rawls.

</span>
<span class="ltx_bibblock">A theory of justice.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib172.1.1">Applied Ethics</span>, pages 21–29. Routledge, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, and Jonathan Cohen.

</span>
<span class="ltx_bibblock">Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib173.1.1">arXiv preprint arXiv:2310.10501</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
Karoline Reinhardt.

</span>
<span class="ltx_bibblock">Trust and trustworthiness in ai ethics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib174.1.1">AI and Ethics</span>, 3(3):735–744, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.

</span>
<span class="ltx_bibblock">" why should i trust you?" explaining the predictions of any classifier.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib175.1.1">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</span>, pages 1135–1144, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
Neil M Richards and Jonathan H King.

</span>
<span class="ltx_bibblock">Big data ethics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib176.1.1">Wake Forest L. Rev.</span>, 49:393, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
Scott Robbins and Aimee van Wynsberghe.

</span>
<span class="ltx_bibblock">Our new artificial intelligence infrastructure: becoming locked into an unsustainable future.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib177.1.1">Sustainability</span>, 14(8):4829, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Karl-Henrik Robèrt, Göran I Broman, and George Basile.

</span>
<span class="ltx_bibblock">Analyzing the concept of planetary boundaries from a strategic sustainability perspective: how does humanity avoid tipping the planet?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib178.1.1">Ecology and Society</span>, 18(2), 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models. 2022 ieee.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib179.1.1">CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 10674–10685, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
Cynthia Rudin.

</span>
<span class="ltx_bibblock">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib180.1.1">Nature machine intelligence</span>, 1(5):206–215, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib181.1.1">Advances in Neural Information Processing Systems</span>, 35:36479–36494, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
Amartya Sen.

</span>
<span class="ltx_bibblock">The idea of justice.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib182.1.1">Journal of human development</span>, 9(3):331–342, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Senado Federal do Brasil.

</span>
<span class="ltx_bibblock">Projeto de lei n° 2338, de 2023. dispõe sobre o uso da inteligência artificial., 2023.

</span>
<span class="ltx_bibblock">Relator Senador Eduardo Gomes.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein.

</span>
<span class="ltx_bibblock">Adversarial training for free!

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib184.1.1">Advances in Neural Information Processing Systems</span>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
Shubham Sharma, Yunfeng Zhang, Jesús M Ríos Aliaga, Djallel Bouneffouf, Vinod Muthusamy, and Kush R Varshney.

</span>
<span class="ltx_bibblock">Data augmentation for discrimination prevention and bias disambiguation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib185.1.1">Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</span>, pages 358–364, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
Noam Shazeer and Mitchell Stern.

</span>
<span class="ltx_bibblock">Adafactor: Adaptive learning rates with sublinear memory cost.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib186.1.1">International Conference on Machine Learning</span>, pages 4596–4604. PMLR, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Frank Y Shih.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib187.1.1">Digital watermarking and steganography: fundamentals and techniques</span>.

</span>
<span class="ltx_bibblock">CRC press, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.

</span>
<span class="ltx_bibblock">Retrieval augmentation reduces hallucination in conversation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib188.1.1">arXiv preprint arXiv:2104.07567</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang.

</span>
<span class="ltx_bibblock">Certified defenses for data poisoning attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib189.1.1">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
Chris Stoate, Andras Baldi, Pedro Beja, ND Boatman, I Herzon, A Van Doorn, GR De Snoo, L Rakosy, and C Ramwell.

</span>
<span class="ltx_bibblock">Ecological impacts of early 21st century agricultural change in europe–a review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib190.1.1">Journal of environmental management</span>, 91(1):22–46, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
Emma Strubell, Ananya Ganesh, and Andrew McCallum.

</span>
<span class="ltx_bibblock">Energy and policy considerations for deep learning in nlp.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib191.1.1">arXiv preprint arXiv:1906.02243</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
Emma Strubell, Ananya Ganesh, and Andrew McCallum.

</span>
<span class="ltx_bibblock">Energy and policy considerations for modern deep learning research.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib192.1.1">Proceedings of the AAAI conference on artificial intelligence</span>, volume 34, pages 13693–13696, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
RL Sumwalt, B Landsberg, and J Homendy.

</span>
<span class="ltx_bibblock">Assumptions used in the safety assessment process and the effects of multiple alerts and indications on pilot performance.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib193.1.1">District of Columbia: National Transportation Safety Board</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
Richard Sutton.

</span>
<span class="ltx_bibblock">The bitter lesson.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib194.1.1">Incomplete Ideas (blog)</span>, 13(1), 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
Mariarosaria Taddeo and Luciano Floridi.

</span>
<span class="ltx_bibblock">How ai can be a force for good.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib195.1.1">Science</span>, 361(6404):751–752, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
Frederik Temmermans and Leonard Rosenthol.

</span>
<span class="ltx_bibblock">Adopting the jpeg universal metadata box format for media authenticity annotations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib196.1.1">Applications of Digital Image Processing XLIV</span>, volume 11842, pages 165–170. SPIE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
Christopher Teo, Milad Abdollahzadeh, and Ngai-Man Man Cheung.

</span>
<span class="ltx_bibblock">On measuring fairness in generative models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib197.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
Scott Thiebes, Sebastian Lins, and Ali Sunyaev.

</span>
<span class="ltx_bibblock">Trustworthy artificial intelligence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib198.1.1">Electronic Markets</span>, 31:447–464, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
Matus Tomlein, Branislav Pecher, Jakub Simko, Ivan Srba, Robert Moro, Elena Stefancova, Michal Kompan, Andrea Hrckova, Juraj Podrouzek, and Maria Bielikova.

</span>
<span class="ltx_bibblock">An audit of misinformation filter bubbles on youtube: Bubble bursting and recent behavior changes.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib199.1.1">Proceedings of the 15th ACM Conference on Recommender Systems</span>, pages 1–11, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
Bill Tomlinson, Rebecca W Black, Donald J Patterson, and Andrew W Torrance.

</span>
<span class="ltx_bibblock">The carbon emissions of writing and illustrating are lower for ai than for humans.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib200.1.1">Scientific Reports</span>, 14(1):3732, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
Nicolas Tsurukawa, Siddharth Prakash, and Andreas Manhart.

</span>
<span class="ltx_bibblock">Social impacts of artisanal cobalt mining in katanga, democratic republic of congo.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib201.1.1">Öko-Institut eV, Freiburg</span>, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
UNESCO.

</span>
<span class="ltx_bibblock">Recommendation on the ethics of artificial intelligence.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unesdoc.unesco.org/ark:/48223/pf0000381137" title="">https://unesdoc.unesco.org/ark:/48223/pf0000381137</a>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
Joseph E Uscinski and Ryden W Butler.

</span>
<span class="ltx_bibblock">The epistemology of fact checking.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib203.1.1">Critical Review</span>, 25(2):162–180, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
Aimee Van Wynsberghe.

</span>
<span class="ltx_bibblock">Sustainable ai: Ai for sustainability and the sustainability of ai.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib204.1.1">AI and Ethics</span>, 1(3):213–218, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib205.1.1">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
Michael Veale and Frederik Zuiderveen Borgesius.

</span>
<span class="ltx_bibblock">Demystifying the draft eu artificial intelligence act—analysing the good, the bad, and the unclear elements of the proposed approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib206.1.1">Computer Law Review International</span>, 22(4):97–112, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
Sahil Verma and Julia Rubin.

</span>
<span class="ltx_bibblock">Fairness definitions explained.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib207.1.1">Proceedings of the international workshop on software fairness</span>, pages 1–7, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
Bertie Vidgen and Leon Derczynski.

</span>
<span class="ltx_bibblock">Directions in abusive language training data, a systematic review: Garbage in, garbage out.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib208.1.1">Plos one</span>, 15(12):e0243300, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei.

</span>
<span class="ltx_bibblock">Bitnet: Scaling 1-bit transformers for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib209.1.1">arXiv preprint arXiv:2310.11453</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Interpretability in the wild: a circuit for indirect object identification in gpt-2 small.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib210.1.1">arXiv preprint arXiv:2211.00593</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
Peter Wayner.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib211.1.1">Disappearing cryptography: information hiding: steganography and watermarking</span>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
White House Office of Science and Technology Policy.

</span>
<span class="ltx_bibblock">Blueprint for an ai bill of rights: Making automated systems work for the american people.

</span>
<span class="ltx_bibblock">The White House, October 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
Gail Whiteman, Brian Walker, and Paolo Perego.

</span>
<span class="ltx_bibblock">Planetary boundaries: Ecological foundations for corporate sustainability.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib213.1.1">Journal of management studies</span>, 50(2):307–336, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
Eric Wong, Leslie Rice, and J Zico Kolter.

</span>
<span class="ltx_bibblock">Fast is better than free: Revisiting adversarial training.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib214.1.1">arXiv preprint arXiv:2001.03994</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al.

</span>
<span class="ltx_bibblock">Q-bench: A benchmark for general-purpose foundation models on low-level vision.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib215.1.1">arXiv preprint arXiv:2309.14181</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al.

</span>
<span class="ltx_bibblock">Effective long-context scaling of foundation models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib216.1.1">arXiv preprint arXiv:2309.16039</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
Depeng Xu, Yongkai Wu, Shuhan Yuan, Lu Zhang, and Xintao Wu.

</span>
<span class="ltx_bibblock">Achieving causal fairness through generative adversarial networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib217.1.1">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
Shen Yan, Hsien-te Kao, and Emilio Ferrara.

</span>
<span class="ltx_bibblock">Fair class balancing: Enhancing model fairness without observing sensitive attributes.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib218.1.1">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</span>, pages 1715–1724, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
Zhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, and Baoyuan Wu.

</span>
<span class="ltx_bibblock">Deepfakebench: A comprehensive benchmark of deepfake detection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib219.1.1">arXiv preprint arXiv:2307.01426</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi.

</span>
<span class="ltx_bibblock">Fairness constraints: Mechanisms for fair classification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib220.1.1">Artificial intelligence and statistics</span>, pages 962–970. PMLR, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
Syifa Izzati Zahira, Fauziah Maharani, and Wily Mohammad.

</span>
<span class="ltx_bibblock">Exploring emotional bonds: Human-ai interactions and the complexity of relationships.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib221.1.1">Serena: Journal of Artificial Intelligence Research</span>, 1(1):1–9, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
Yi Zeng, Enmeng Lu, and Cunqing Huangfu.

</span>
<span class="ltx_bibblock">Linking artificial intelligence principles.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib222.1.1">arXiv preprint arXiv:1812.04814</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell.

</span>
<span class="ltx_bibblock">Mitigating unwanted biases with adversarial learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib223.1.1">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</span>, pages 335–340, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian.

</span>
<span class="ltx_bibblock">Galore: Memory-efficient llm training by gradient low-rank projection, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang.

</span>
<span class="ltx_bibblock">Tinyllava: A framework of small-scale large multimodal models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
Anne Zimmerman, Joel Janhonen, and Emily Beer.

</span>
<span class="ltx_bibblock">Human/ai relationships: challenges, downsides, and impacts on human/human relationships.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib226.1.1">AI and Ethics</span>, pages 1–13, 2023.

</span>
</li>
</ul>
</section><div about="" class="ltx_rdf" content="artificial intelligence, tools, ethics, ethics of artificial intelligence, certification" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="AI Ethics" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="Catalog of general ethical requirements for AI Certification" property="dcterms:title"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 22 10:52:12 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
