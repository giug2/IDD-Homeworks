<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.14510] V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data</title><meta property="og:description" content="Diffusion-based generative models have recently shown remarkable image and video editing capabilities. However, local video editing, particularly removal of small attributes like glasses, remains a challenge. Existing …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.14510">

<!--Generated on Fri Jul  5 18:58:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rotem Shalev-Arkushin<sup id="id10.10.id1" class="ltx_sup"><span id="id10.10.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>    Aharon Azulay<sup id="id11.11.id2" class="ltx_sup">1</sup>    Tavi Halperin<sup id="id12.12.id3" class="ltx_sup">1</sup>     Eitan Richardson<sup id="id13.13.id4" class="ltx_sup">1</sup> 
<br class="ltx_break">        Amit H. Bermano<sup id="id14.14.id5" class="ltx_sup">2</sup>    Ohad Fried<sup id="id15.15.id6" class="ltx_sup">3</sup> 
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.9.3" class="ltx_text ltx_affiliation_institution">Lightricks<sup id="id9.9.3.1" class="ltx_sup">1</sup>   Tel Aviv University<sup id="id9.9.3.2" class="ltx_sup">2</sup>   Reichman University<sup id="id9.9.3.3" class="ltx_sup">3</sup></span>
</span>
<span class="ltx_contact ltx_role_affiliation"><a target="_blank" href="https://v-lasik.github.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://v-lasik.github.io</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id16.id1" class="ltx_p">Diffusion-based generative models have recently shown remarkable image and video editing capabilities. However, local video editing, particularly removal of small attributes like glasses, remains a challenge. Existing methods either alter the videos excessively, generate unrealistic artifacts, or fail to perform the requested edit consistently throughout the video. 
<br class="ltx_break">In this work, we focus on consistent and identity-preserving removal of glasses in videos, using it as a case study for consistent local attribute removal in videos. Due to the lack of paired data, we adopt a weakly supervised approach and generate synthetic imperfect data, using an adjusted pretrained diffusion model. We show that despite data imperfection, by learning from our generated data and leveraging the prior of pretrained diffusion models,
our model is able to perform the desired edit consistently while preserving the original video content.
Furthermore, we exemplify the generalization ability of our method to other local video editing tasks by applying it successfully to facial sticker-removal.
Our approach demonstrates significant improvement over existing methods, showcasing the potential of leveraging synthetic data and strong video priors for local video editing tasks.</p>
</div>
<figure id="S0.F1" class="ltx_figure ltx_teaserfigure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2406.14510/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="196" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Our method receives an input video of a person wearing glasses, and consistently removes the glasses, while preserving the original content and identity of that person. Our method successfully removes the glasses even when there are reflections (bottom-middle example), heavy makeup (top-right), and eye blinks (bottom-right). Red rectangles are zoomed-in at the bottom row.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S0.F1.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S0.F1.2" class="ltx_p ltx_figure_panel ltx_align_center">[]</p>
</div>
</div>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent advances in diffusion-based generative models <cite class="ltx_cite ltx_citemacro_citep">(Ho et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>; Sohl-Dickstein et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2015</a>)</cite> have demonstrated impressive capabilities in image and video editing <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>; Geyer et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2023b</a>; Kara et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Bar-Tal et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2024</a>)</cite>.
While local editing, such as removing and adding attributes without changing the rest of the content, mainly works for images, local video editing remains a challenge. Video frames often contain motion blur and challenging poses that are less common in images. Moreover, videos of people pose additional challenges, as humans are highly sensitive to subtle unrealistic “uncanny” artifacts, and our visual system is more sensitive to motion inconsistencies, so the quality bar is higher.
Existing image editing and inpainting methods <cite class="ltx_cite ltx_citemacro_citep">(Nitzan et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2024</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>; Tsaban and Passos, <a href="#bib.bib36" title="" class="ltx_ref">2023</a>; Brooks et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Couairon et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> are trained on clean images, that do not contain the aforementioned problems, and therefore do not perform well over video frames; They typically either change the original frames too much, or do not perform the requested change completely, as illustrated in <a href="#S1.F2" title="In 1. Introduction ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Video inpainting methods <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>; Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2023a</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022b</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite> typically remove objects by deleting and filling regions of the video, often disregarding the information originally present in those regions. When it comes to generating new video content behind an object that never moves, these methods tend to generate a low detailed background, or one with clear unrealistic artifacts.
As a representing use-case, we choose to tackle glasses removal, as it is a particularly challenging example; where some content is never or only partially revealed (e.g. eyes, eyebrows), and is changed across the video (e.g. eyelids change position across frames).
Since eyes contribute greatly to identity perception, a consistent and highly realistic generation is required for an identity to be preserved. However, current video inpainting works typically generate smooth results when using small masks, that do not fit the facial case well (see ProPainter <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> and FGT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite> results in <a href="#S5.F5" title="In 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Mask-free video editing methods <cite class="ltx_cite ltx_citemacro_citep">(Geyer et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2023b</a>; Khachatryan et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>; Kara et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Cong et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2024</a>)</cite> mainly focus on changing the style of the whole video, or changing the appearance of large parts of it, and not on local changes that should adhere to the original context.
Other works <cite class="ltx_cite ltx_citemacro_citep">(Kasten et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2021</a>; Bar-Tal et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite> learn an atlas for each new video, leading to long inference time. Furthermore, as mentioned in Text2Live <cite class="ltx_cite ltx_citemacro_citep">(Bar-Tal et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite>, the goal of these works is to manipulate the appearance of existing objects, not to change the composition of the elements in the scene. Hence, their method is not capable of successfully removing attributes such as glasses from faces.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we focus on removing glasses from videos as a case study for local video editing. We introduce a new method that performs this task consistently while preserving the original identity. To the best of our knowledge, no paired data exists for this task; Therefore, we cannot finetune a model directly to solve it. To tackle this problem, we generate imperfect synthetic data, and use it to train a model in a weakly supervised manner. We show that by using an adjusted pretrained image inpainting model, we can generate imperfect data pairs, and learn from them. Our method is closest to video inpainting, in the sense that it removes something from the video and replaces it with different content. However, similarly to recent video editing works, the input to our model is a mask-free image, allowing the model to use facial details that are occluded for inpainting models.
As glasses often contain colored lens and reflections that change across frames, which affect the way the eye behind them looks, they are a challenging attribute to remove from faces. Hence, we incorporate cross-frame attention layers in our data generation model, which allows aggregating information from different frames.
Still, as presented in <a href="#S1.F2" title="In 1. Introduction ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, the generated data (‘Synth data’) is imperfect and contains many artifacts, such as not always preserving the eyelid position and other features from the original frame. To overcome these challenges, we finetune a diffusion based image-to-image model over this data. This way we can leverage the prior of the pretrained model, while enhancing its capabilities with our new data. Additionally, the non-masked input allows the model to exploit the fact that it is exposed to useful information within the masked region to achieve the requested edit, while preserving the rest of the content from the original frames. Despite the imperfections in our synthetic data, a model with a strong prior can leverage its prior knowledge to outperform the training data and produce high-quality results.
Finally, to get temporally consistent results, we combine our trained model with a motion prior, and achieve state-of-the-art results on video glasses-removal, surpassing all current video editing and inpainting methods.
To demonstrate that our method can be generalized to other local video editing tasks, we successfully apply it to the task of removing stickers from faces.
We release code and both the stickers and the glasses removal datasets for further research.
</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<p id="S1.F2.16" class="ltx_p ltx_align_center"><span id="S1.F2.16.16" class="ltx_text">
<span id="S1.F2.16.16.16" class="ltx_inline-block ltx_transformed_outer" style="width:432.8pt;height:97.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S1.F2.16.16.16.16" class="ltx_p"><span id="S1.F2.16.16.16.16.16" class="ltx_text">
<span id="S1.F2.16.16.16.16.16.16" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S1.F2.16.16.16.16.16.16.17.1" class="ltx_tr">
<span id="S1.F2.16.16.16.16.16.16.17.1.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">Input</span>
<span id="S1.F2.16.16.16.16.16.16.17.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">LEDITS</span>
<span id="S1.F2.16.16.16.16.16.16.17.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">IP2P</span>
<span id="S1.F2.16.16.16.16.16.16.17.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">Lyu et al.</span>
<span id="S1.F2.16.16.16.16.16.16.17.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">SD inpaint</span>
<span id="S1.F2.16.16.16.16.16.16.17.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">CN inpaint</span>
<span id="S1.F2.16.16.16.16.16.16.17.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">Synth data</span>
<span id="S1.F2.16.16.16.16.16.16.17.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;"><span id="S1.F2.16.16.16.16.16.16.17.1.8.1" class="ltx_text ltx_font_bold">Ours</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S1.F2.8.8.8.8.8.8.8" class="ltx_tr">
<span id="S1.F2.1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/orig.jpg" id="S1.F2.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="71" height="71" alt="Refer to caption"></span>
<span id="S1.F2.2.2.2.2.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/ledits.jpg" id="S1.F2.2.2.2.2.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="71" height="71" alt="Refer to caption"></span>
<span id="S1.F2.3.3.3.3.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/ip2p.jpg" id="S1.F2.3.3.3.3.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="71" height="71" alt="Refer to caption"></span>
<span id="S1.F2.4.4.4.4.4.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/storyMY.jpg" id="S1.F2.4.4.4.4.4.4.4.4.g1" class="ltx_graphics ltx_img_square" width="71" height="71" alt="Refer to caption"></span>
<span id="S1.F2.5.5.5.5.5.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/sd_inpaint.jpg" id="S1.F2.5.5.5.5.5.5.5.5.g1" class="ltx_graphics ltx_img_square" width="71" height="71" alt="Refer to caption"></span>
<span id="S1.F2.6.6.6.6.6.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/cn_inpaint.jpg" id="S1.F2.6.6.6.6.6.6.6.6.g1" class="ltx_graphics ltx_img_square" width="71" height="71" alt="Refer to caption"></span>
<span id="S1.F2.7.7.7.7.7.7.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/synth.jpg" id="S1.F2.7.7.7.7.7.7.7.7.g1" class="ltx_graphics ltx_img_square" width="71" height="71" alt="Refer to caption"></span>
<span id="S1.F2.8.8.8.8.8.8.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/ours.jpg" id="S1.F2.8.8.8.8.8.8.8.8.g1" class="ltx_graphics ltx_img_square" width="71" height="71" alt="Refer to caption"></span></span>
<span id="S1.F2.16.16.16.16.16.16.16" class="ltx_tr">
<span id="S1.F2.9.9.9.9.9.9.9.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/orig_inset.jpg" id="S1.F2.9.9.9.9.9.9.9.1.g1" class="ltx_graphics ltx_img_landscape" width="71" height="24" alt="Refer to caption"></span>
<span id="S1.F2.10.10.10.10.10.10.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/ledits_inset.jpg" id="S1.F2.10.10.10.10.10.10.10.2.g1" class="ltx_graphics ltx_img_landscape" width="71" height="24" alt="Refer to caption"></span>
<span id="S1.F2.11.11.11.11.11.11.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/ip2p_inset.jpg" id="S1.F2.11.11.11.11.11.11.11.3.g1" class="ltx_graphics ltx_img_landscape" width="71" height="24" alt="Refer to caption"></span>
<span id="S1.F2.12.12.12.12.12.12.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/storyMY_inset.jpg" id="S1.F2.12.12.12.12.12.12.12.4.g1" class="ltx_graphics ltx_img_landscape" width="71" height="24" alt="Refer to caption"></span>
<span id="S1.F2.13.13.13.13.13.13.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/sd_inpaint_inset.jpg" id="S1.F2.13.13.13.13.13.13.13.5.g1" class="ltx_graphics ltx_img_landscape" width="71" height="24" alt="Refer to caption"></span>
<span id="S1.F2.14.14.14.14.14.14.14.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/cn_inpaint_inset.jpg" id="S1.F2.14.14.14.14.14.14.14.6.g1" class="ltx_graphics ltx_img_landscape" width="71" height="24" alt="Refer to caption"></span>
<span id="S1.F2.15.15.15.15.15.15.15.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/synth_inset.jpg" id="S1.F2.15.15.15.15.15.15.15.7.g1" class="ltx_graphics ltx_img_landscape" width="71" height="24" alt="Refer to caption"></span>
<span id="S1.F2.16.16.16.16.16.16.16.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/results/sztf/ours_inset.jpg" id="S1.F2.16.16.16.16.16.16.16.8.g1" class="ltx_graphics ltx_img_landscape" width="71" height="24" alt="Refer to caption"></span></span>
</span>
</span></span></span>
</span></span></span>


<span id="S1.F2.16.17" class="ltx_ERROR undefined">\Description</span>[]</p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>
<span id="S1.F2.18.1" class="ltx_text ltx_font_bold">Glasses-removal from a blinking eye by image editing methods</span>
Left to right: LEDITS <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, Instruct pix2pix <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, Lyu et al. <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, Stable Diffusion inpaint <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>, ControlNet inpaint <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib44" title="" class="ltx_ref">2023a</a>)</cite>, our synthetic dataset generation result, and our final result. As image editing methods expect high quality images with people looking straight to the camera, they struggle when these constraints are not met. In our dataset result ‘Synth data’, as a result of the cross-frame attention, eye artifacts appear. However, our model is still able to learn from the imperfect data and remove the glasses better than any out-of-the-box method, and better than the data it was trained on.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Image editing</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Since the advent of diffusion models for image generation and editing <cite class="ltx_cite ltx_citemacro_citep">(Ho et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, real image editing has rapidly advanced and showed remarkable results <cite class="ltx_cite ltx_citemacro_citep">(Brooks et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Couairon et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>; Tsaban and Passos, <a href="#bib.bib36" title="" class="ltx_ref">2023</a>; Hertz et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2022</a>; Tumanyan et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>, including local editing such as removing glasses from images.
Moreover, image inpainting methods <cite class="ltx_cite ltx_citemacro_citep">(Nitzan et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2024</a>; Rombach et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>; Yildirim et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>, have also shown impressive results, realistically replacing the content behind a given mask.
Additionally, prior work <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2022</a>; Lee and Lai, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> tried to tackle the task of removing glasses from images directly.
Indeed, these works perform well over images. However, as shown in <a href="#S1.F2" title="In 1. Introduction ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> they fail when applied to video frames of people, that do not always look directly at the camera, and constantly move, causing motion blur and other artifacts.
Moreover, when applied frame-by-frame, the generated content differs between frames, resulting in temporal inconsistencies.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Video editing</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Recently, video editing has also developed greatly with video-to-video translation methods <cite class="ltx_cite ltx_citemacro_citep">(Geyer et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>; Khachatryan et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>; Cong et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2024</a>; Kara et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Qi et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023</a>; Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2023b</a>)</cite> that focus on transforming the entire frame into a different style, while trying to preserve temporal consistency in the generated videos. Some of them, such as RAVE <cite class="ltx_cite ltx_citemacro_citep">(Kara et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>, or TokenFlow <cite class="ltx_cite ltx_citemacro_citep">(Geyer et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite> with SDEdit <cite class="ltx_cite ltx_citemacro_citep">(Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> also perform local editing. However, in the case of glasses-removal, where a person moves across the video and the requested edit is small and delicate, many artifacts such as face deformations and inconsistencies are generated. Examples are presented in <a href="#S5.F5" title="In 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a> and in video results in the supplementary material.
Another line of works is atlas-based video editing <cite class="ltx_cite ltx_citemacro_citep">(Kasten et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2021</a>; Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>; Bar-Tal et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2022</a>; Suhail et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>. These methods learn an atlas for each video, to apply changes to either the background or foreground of the video. The initial atlas reconstruction requires excessive computational resources and long running times. Moreover, as these methods were designed to change the appearance of existing objects, they are not meant for adding or removing attributes that were not part of the original video, such as glasses, and do not perform these changes well. Additionally, while theoretically allowing to remove layers from a video, these methods are limited to relatively simple motion and to static backgrounds.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Video inpainting</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Video inpainting is the task of consistently filling-in new content behind a given mask throughout a video <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2023a</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2022</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>; Gu et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022b</a>)</cite>. Such methods work well when the object moves throughout the video, and the model is able to fill-in the background from other video frames, where the background is visible. However, they struggle with filling-in completely occluded areas, especially when they are part of the foreground, such as faces. Particularly, areas behind glasses are only partially revealed by head motion, and sometimes glasses cause optical deformations that hide the background. Slight changes to face features in the “background” of glasses may yield unrealistic results, or alter the face identity, as shown in the ProPainter <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> and FGT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite> results in <a href="#S5.F5" title="In 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. While such changes to smooth backgrounds may not significantly affect the perceptual quality of the results, in foreground areas and particularly in faces they are detrimental.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Learning from synthesized data</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Deep learning models require large amount of data to learn from, however paired data is not always available for the task at hand. Hence, several works <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022a</a>; Peebles et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2022</a>; Ravuri and Vinyals, <a href="#bib.bib31" title="" class="ltx_ref">2019</a>; Brooks et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Lyu et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite> use generative models for data generation to train models. These works usually rely on the high quality of the generated data, and learn solely from it. In this work, we accept that generated data is usually imperfect, and is insufficient to achieve the required result on its own, thus we take advantage of the strong priors of pretrained models and use our generated data for fine-tuning. This way, the model learns to generate results that are superior to the data it was trained on, while performing the relevant task.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Lyu et al. <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite> specifically explores synthetic data for the task of glasses-removal from images. However, their data acquisition process requires face scanning and 3D data, which is hard to acquire, unlike our method which does not require any special equipment or effort.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Method</h2>

<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F3.16" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:125.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.3pt,19.6pt) scale(0.76039,0.76039) ;">
<p id="S3.F3.16.16" class="ltx_p"><span id="S3.F3.16.16.16" class="ltx_text">
<span id="S3.F3.16.16.16.16" class="ltx_inline-block ltx_transformed_outer" style="width:570.3pt;height:164.9pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F3.16.16.16.16.16" class="ltx_p"><span id="S3.F3.16.16.16.16.16.16" class="ltx_text">
<span id="S3.F3.16.16.16.16.16.16.16" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S3.F3.16.16.16.16.16.16.16.17.1" class="ltx_tr">
<span id="S3.F3.16.16.16.16.16.16.16.17.1.1" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">Input</span>
<span id="S3.F3.16.16.16.16.16.16.16.17.1.2" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">Masked input</span>
<span id="S3.F3.16.16.16.16.16.16.16.17.1.3" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">w/o CF attn</span>
<span id="S3.F3.16.16.16.16.16.16.16.17.1.4" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">w/ CF attn</span>
<span id="S3.F3.16.16.16.16.16.16.16.17.1.5" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">Input</span>
<span id="S3.F3.16.16.16.16.16.16.16.17.1.6" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">Masked input</span>
<span id="S3.F3.16.16.16.16.16.16.16.17.1.7" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">w/o CF attn</span>
<span id="S3.F3.16.16.16.16.16.16.16.17.1.8" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">w/ CF attn</span></span>
<span id="S3.F3.8.8.8.8.8.8.8.8" class="ltx_tr">
<span id="S3.F3.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x2.png" id="S3.F3.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="50" height="59" alt="Refer to caption"></span>
<span id="S3.F3.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x3.png" id="S3.F3.2.2.2.2.2.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="50" height="58" alt="Refer to caption"></span>
<span id="S3.F3.3.3.3.3.3.3.3.3.3" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x4.png" id="S3.F3.3.3.3.3.3.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="50" height="59" alt="Refer to caption"></span>
<span id="S3.F3.4.4.4.4.4.4.4.4.4" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x5.png" id="S3.F3.4.4.4.4.4.4.4.4.4.g1" class="ltx_graphics ltx_img_square" width="50" height="59" alt="Refer to caption"></span>
<span id="S3.F3.5.5.5.5.5.5.5.5.5" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x6.png" id="S3.F3.5.5.5.5.5.5.5.5.5.g1" class="ltx_graphics ltx_img_square" width="50" height="60" alt="Refer to caption"></span>
<span id="S3.F3.6.6.6.6.6.6.6.6.6" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x7.png" id="S3.F3.6.6.6.6.6.6.6.6.6.g1" class="ltx_graphics ltx_img_square" width="50" height="60" alt="Refer to caption"></span>
<span id="S3.F3.7.7.7.7.7.7.7.7.7" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x8.png" id="S3.F3.7.7.7.7.7.7.7.7.7.g1" class="ltx_graphics ltx_img_square" width="50" height="60" alt="Refer to caption"></span>
<span id="S3.F3.8.8.8.8.8.8.8.8.8" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x9.png" id="S3.F3.8.8.8.8.8.8.8.8.8.g1" class="ltx_graphics ltx_img_square" width="50" height="60" alt="Refer to caption"></span></span>
<span id="S3.F3.16.16.16.16.16.16.16.16" class="ltx_tr">
<span id="S3.F3.9.9.9.9.9.9.9.9.1" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x10.png" id="S3.F3.9.9.9.9.9.9.9.9.1.g1" class="ltx_graphics ltx_img_landscape" width="50" height="25" alt="Refer to caption"></span>
<span id="S3.F3.10.10.10.10.10.10.10.10.2" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x11.png" id="S3.F3.10.10.10.10.10.10.10.10.2.g1" class="ltx_graphics ltx_img_landscape" width="50" height="26" alt="Refer to caption"></span>
<span id="S3.F3.11.11.11.11.11.11.11.11.3" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x12.png" id="S3.F3.11.11.11.11.11.11.11.11.3.g1" class="ltx_graphics ltx_img_landscape" width="50" height="26" alt="Refer to caption"></span>
<span id="S3.F3.12.12.12.12.12.12.12.12.4" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x13.png" id="S3.F3.12.12.12.12.12.12.12.12.4.g1" class="ltx_graphics ltx_img_landscape" width="50" height="26" alt="Refer to caption"></span>
<span id="S3.F3.13.13.13.13.13.13.13.13.5" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x14.png" id="S3.F3.13.13.13.13.13.13.13.13.5.g1" class="ltx_graphics ltx_img_landscape" width="50" height="26" alt="Refer to caption"></span>
<span id="S3.F3.14.14.14.14.14.14.14.14.6" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x15.png" id="S3.F3.14.14.14.14.14.14.14.14.6.g1" class="ltx_graphics ltx_img_landscape" width="50" height="26" alt="Refer to caption"></span>
<span id="S3.F3.15.15.15.15.15.15.15.15.7" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x16.png" id="S3.F3.15.15.15.15.15.15.15.15.7.g1" class="ltx_graphics ltx_img_landscape" width="50" height="26" alt="Refer to caption"></span>
<span id="S3.F3.16.16.16.16.16.16.16.16.8" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;"><img src="/html/2406.14510/assets/x17.png" id="S3.F3.16.16.16.16.16.16.16.16.8.g1" class="ltx_graphics ltx_img_landscape" width="50" height="26" alt="Refer to caption"></span></span>
<span id="S3.F3.16.16.16.16.16.16.16.18.2" class="ltx_tr">
<span id="S3.F3.16.16.16.16.16.16.16.18.2.1" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">input</span>
<span id="S3.F3.16.16.16.16.16.16.16.18.2.2" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">inaccurate mask</span>
<span id="S3.F3.16.16.16.16.16.16.16.18.2.3" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">glasses remnants</span>
<span id="S3.F3.16.16.16.16.16.16.16.18.2.4" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">no remnants</span>
<span id="S3.F3.16.16.16.16.16.16.16.18.2.5" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">input</span>
<span id="S3.F3.16.16.16.16.16.16.16.18.2.6" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">masked input</span>
<span id="S3.F3.16.16.16.16.16.16.16.18.2.7" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">reflections</span>
<span id="S3.F3.16.16.16.16.16.16.16.18.2.8" class="ltx_td ltx_align_center" style="padding:-2.5pt 2.0pt;">no reflections</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>
<span id="S3.F3.18.1" class="ltx_text ltx_font_bold">Cross-frame attention importance in data generation.</span> Cross-frame attention helps removing glasses remnants, even when the mask is not perfect (left example) and reducing glasses reflections (right example).
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F3.19" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F3.20" class="ltx_p ltx_figure_panel ltx_align_center">[] </p>
</div>
</div>
</figure>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2406.14510/assets/x18.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="250" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span><span id="S3.F4.4.1" class="ltx_text ltx_font_bold">Method overview:</span> <span id="S3.F4.5.2" class="ltx_text ltx_font_bold">Step 1:</span> we create an imperfect synthetic paired dataset by generating glasses masks for each video frame and inpainting it. We inpaint each frame using an adjusted version ControlNet inpaint <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>)</cite>. We replace the self-attention layers with cross-frame attention (cf attn) and use blending between the generated latent images and the noised masked original latent images at each diffusion step. The generated data in the first step is imperfect; e.g. in the middle frame, the person blinks, however its generated pair has open eyes. Nevertheless, the data is good enough for finetuning an image-to-image diffusion model and achieving satisfactory results, due to the strong prior of the model.
<span id="S3.F4.6.3" class="ltx_text ltx_font_bold">Step 2:</span> Given our trained model for the task of removing glasses from images, we incorporate it with a motion prior module to generate temporally consistent videos without glasses from previously unseen videos. To obtain the original frame colors, at each diffusion step we blend the generated frames with the noised original masked latent images, and before decoding, we apply an Inside-Out Normalization (ION), to better align the statistics within the masked area and the area outside of the mask.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F4.7" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F4.8" class="ltx_p ltx_figure_panel ltx_align_center">[]</p>
</div>
</div>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Given an input video of a person wearing glasses, our goal is to remove the glasses while preserving all other information.
As illustrated in <a href="#S3.F4" title="In 3. Method ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, our approach to removing glasses from videos consists of three stages: data generation, training, and editing.
First, as no paired data is available for this task, we generate a synthetic paired dataset using videos of people wearing glasses.
Next, we finetune an image-to-image diffusion model over our synthetic dataset, to get realistic video frames without glasses, where all other parts of the frame remain similar to the original frame, and the identity of the edited person is preserved.
Finally, we incorporate our trained model with a pretrained motion module, into a video editing pipeline, to obtain temporally consistent videos without glasses.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Paired Data Generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As illustrated in <a href="#S1.F2" title="In 1. Introduction ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, current image editing and inpainting methods do not perform well on the task of removing glasses from video frames. Therefore, we wish to train a model for this task. However, to the best of our knowledge, no relevant paired dataset currently exists. Hence, we create a synthetic dataset of paired video frames, with and without glasses, using videos from the CelebV-Text dataset <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2023b</a>)</cite> where people wear glasses.
For each video frame, we generate a glasses mask using a face parser <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite> that identifies the glasses in it. The eyes are a key component in the identity of a person, and the current position of the eyelids is crucial to generating a result that is consistent with the original video, that might contain eye-closure and blinking.
Therefore, we want to give the model information about the eyes and eyelids, to keep their original appearance. To do that, we make eye “holes” in the glasses masks, using the identified eye landmarks. Then, we inpaint the glasses area by applying an adjusted inpainting diffusion model over each video frame and its generated mask.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.2" class="ltx_p">We make two adjustments to the inpainting model: First, to achieve smoother and more realistic transitions between the frame and the inpainted part, we blend the latent feature vectors with a noised encoded version of the masked original frame at each diffusion step, as suggested in Blended Latent Diffusion <cite class="ltx_cite ltx_citemacro_citep">(Avrahami et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>.
Second, to increase the edit consistency across frames from the same video, we
replace the self-attention layers of the model with a cross-frame attention <cite class="ltx_cite ltx_citemacro_citep">(Khachatryan et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>.
For each video frame, we perform a cross-frame attention with <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">k</annotation></semantics></math> reference frames from the same video. We use multiple reference frames as sometimes information such as the eyebrows or the eye color is hidden behind the glasses or their reflections. Using cross-frame attention with multiple frames allows for generalization from all <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">k</annotation></semantics></math> frames.
For this purpose, we adopt the cross-frame attention mechanism suggested by TokenFlow <cite class="ltx_cite ltx_citemacro_citep">(Geyer et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.6" class="ltx_Math" alttext="\text{softmax}(\frac{Q\cdot[K_{0},...,K_{k}]^{T}}{\sqrt{d}})\cdot[V_{1},...,V_{k}]" display="block"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml"><mrow id="S3.E1.m1.6.6.4" xref="S3.E1.m1.6.6.4.cmml"><mtext id="S3.E1.m1.6.6.4.2" xref="S3.E1.m1.6.6.4.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.4.1" xref="S3.E1.m1.6.6.4.1.cmml">​</mo><mrow id="S3.E1.m1.6.6.4.3.2" xref="S3.E1.m1.3.3.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.4.3.2.1" xref="S3.E1.m1.3.3.cmml">(</mo><mfrac id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mi id="S3.E1.m1.3.3.3.5" xref="S3.E1.m1.3.3.3.5.cmml">Q</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.3.3.3.4" xref="S3.E1.m1.3.3.3.4.cmml">⋅</mo><msup id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml"><mrow id="S3.E1.m1.3.3.3.3.2.2" xref="S3.E1.m1.3.3.3.3.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.3.2.2.3" xref="S3.E1.m1.3.3.3.3.2.3.cmml">[</mo><msub id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.2.cmml">K</mi><mn id="S3.E1.m1.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S3.E1.m1.3.3.3.3.2.2.4" xref="S3.E1.m1.3.3.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">…</mi><mo id="S3.E1.m1.3.3.3.3.2.2.5" xref="S3.E1.m1.3.3.3.3.2.3.cmml">,</mo><msub id="S3.E1.m1.3.3.3.3.2.2.2" xref="S3.E1.m1.3.3.3.3.2.2.2.cmml"><mi id="S3.E1.m1.3.3.3.3.2.2.2.2" xref="S3.E1.m1.3.3.3.3.2.2.2.2.cmml">K</mi><mi id="S3.E1.m1.3.3.3.3.2.2.2.3" xref="S3.E1.m1.3.3.3.3.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E1.m1.3.3.3.3.2.2.6" xref="S3.E1.m1.3.3.3.3.2.3.cmml">]</mo></mrow><mi id="S3.E1.m1.3.3.3.3.4" xref="S3.E1.m1.3.3.3.3.4.cmml">T</mi></msup></mrow><msqrt id="S3.E1.m1.3.3.5" xref="S3.E1.m1.3.3.5.cmml"><mi id="S3.E1.m1.3.3.5.2" xref="S3.E1.m1.3.3.5.2.cmml">d</mi></msqrt></mfrac><mo rspace="0.055em" stretchy="false" id="S3.E1.m1.6.6.4.3.2.2" xref="S3.E1.m1.3.3.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E1.m1.6.6.3" xref="S3.E1.m1.6.6.3.cmml">⋅</mo><mrow id="S3.E1.m1.6.6.2.2" xref="S3.E1.m1.6.6.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.2.2.3" xref="S3.E1.m1.6.6.2.3.cmml">[</mo><msub id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.2.cmml">V</mi><mn id="S3.E1.m1.5.5.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.3.cmml">1</mn></msub><mo id="S3.E1.m1.6.6.2.2.4" xref="S3.E1.m1.6.6.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">…</mi><mo id="S3.E1.m1.6.6.2.2.5" xref="S3.E1.m1.6.6.2.3.cmml">,</mo><msub id="S3.E1.m1.6.6.2.2.2" xref="S3.E1.m1.6.6.2.2.2.cmml"><mi id="S3.E1.m1.6.6.2.2.2.2" xref="S3.E1.m1.6.6.2.2.2.2.cmml">V</mi><mi id="S3.E1.m1.6.6.2.2.2.3" xref="S3.E1.m1.6.6.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E1.m1.6.6.2.2.6" xref="S3.E1.m1.6.6.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"><ci id="S3.E1.m1.6.6.3.cmml" xref="S3.E1.m1.6.6.3">⋅</ci><apply id="S3.E1.m1.6.6.4.cmml" xref="S3.E1.m1.6.6.4"><times id="S3.E1.m1.6.6.4.1.cmml" xref="S3.E1.m1.6.6.4.1"></times><ci id="S3.E1.m1.6.6.4.2a.cmml" xref="S3.E1.m1.6.6.4.2"><mtext id="S3.E1.m1.6.6.4.2.cmml" xref="S3.E1.m1.6.6.4.2">softmax</mtext></ci><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.6.6.4.3.2"><divide id="S3.E1.m1.3.3.4.cmml" xref="S3.E1.m1.6.6.4.3.2"></divide><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><ci id="S3.E1.m1.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.4">⋅</ci><ci id="S3.E1.m1.3.3.3.5.cmml" xref="S3.E1.m1.3.3.3.5">𝑄</ci><apply id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3">superscript</csymbol><list id="S3.E1.m1.3.3.3.3.2.3.cmml" xref="S3.E1.m1.3.3.3.3.2.2"><apply id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.2">𝐾</ci><cn type="integer" id="S3.E1.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3">0</cn></apply><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">…</ci><apply id="S3.E1.m1.3.3.3.3.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.2.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.3.3.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.2.2.2.2">𝐾</ci><ci id="S3.E1.m1.3.3.3.3.2.2.2.3.cmml" xref="S3.E1.m1.3.3.3.3.2.2.2.3">𝑘</ci></apply></list><ci id="S3.E1.m1.3.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.3.4">𝑇</ci></apply></apply><apply id="S3.E1.m1.3.3.5.cmml" xref="S3.E1.m1.3.3.5"><root id="S3.E1.m1.3.3.5a.cmml" xref="S3.E1.m1.3.3.5"></root><ci id="S3.E1.m1.3.3.5.2.cmml" xref="S3.E1.m1.3.3.5.2">𝑑</ci></apply></apply></apply><list id="S3.E1.m1.6.6.2.3.cmml" xref="S3.E1.m1.6.6.2.2"><apply id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.2">𝑉</ci><cn type="integer" id="S3.E1.m1.5.5.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.3">1</cn></apply><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">…</ci><apply id="S3.E1.m1.6.6.2.2.2.cmml" xref="S3.E1.m1.6.6.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.2.2.2.1.cmml" xref="S3.E1.m1.6.6.2.2.2">subscript</csymbol><ci id="S3.E1.m1.6.6.2.2.2.2.cmml" xref="S3.E1.m1.6.6.2.2.2.2">𝑉</ci><ci id="S3.E1.m1.6.6.2.2.2.3.cmml" xref="S3.E1.m1.6.6.2.2.2.3">𝑘</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">\text{softmax}(\frac{Q\cdot[K_{0},...,K_{k}]^{T}}{\sqrt{d}})\cdot[V_{1},...,V_{k}]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.6" class="ltx_p">where each query frame (<math id="S3.SS1.p2.3.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p2.3.m1.1a"><mi id="S3.SS1.p2.3.m1.1.1" xref="S3.SS1.p2.3.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m1.1b"><ci id="S3.SS1.p2.3.m1.1.1.cmml" xref="S3.SS1.p2.3.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m1.1c">Q</annotation></semantics></math>) attends to <math id="S3.SS1.p2.4.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.4.m2.1a"><mi id="S3.SS1.p2.4.m2.1.1" xref="S3.SS1.p2.4.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m2.1b"><ci id="S3.SS1.p2.4.m2.1.1.cmml" xref="S3.SS1.p2.4.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m2.1c">k</annotation></semantics></math> different values (<math id="S3.SS1.p2.5.m3.1" class="ltx_Math" alttext="V_{i}" display="inline"><semantics id="S3.SS1.p2.5.m3.1a"><msub id="S3.SS1.p2.5.m3.1.1" xref="S3.SS1.p2.5.m3.1.1.cmml"><mi id="S3.SS1.p2.5.m3.1.1.2" xref="S3.SS1.p2.5.m3.1.1.2.cmml">V</mi><mi id="S3.SS1.p2.5.m3.1.1.3" xref="S3.SS1.p2.5.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m3.1b"><apply id="S3.SS1.p2.5.m3.1.1.cmml" xref="S3.SS1.p2.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m3.1.1.1.cmml" xref="S3.SS1.p2.5.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m3.1.1.2.cmml" xref="S3.SS1.p2.5.m3.1.1.2">𝑉</ci><ci id="S3.SS1.p2.5.m3.1.1.3.cmml" xref="S3.SS1.p2.5.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m3.1c">V_{i}</annotation></semantics></math>), corresponding to <math id="S3.SS1.p2.6.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.6.m4.1a"><mi id="S3.SS1.p2.6.m4.1.1" xref="S3.SS1.p2.6.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m4.1b"><ci id="S3.SS1.p2.6.m4.1.1.cmml" xref="S3.SS1.p2.6.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m4.1c">k</annotation></semantics></math> different reference frames in our case.
As a result, even when face parts are occluded,
the information about the occluded parts can be retrieved from the reference frames if they are revealed there.
An example of occluding reflections is presented on the right side of <a href="#S3.F3" title="In 3. Method ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. In this example, there are reflections over the glasses, hiding the true eye colors behind them. Without using cross-frame attention, reflections and artifacts are present on the output result. However, when using cross-frame attention, the true eye color is preserved and no reflections are shown in the result.
Moreover, using cross-frame attention also helps when the masks are not exact and do not cover all of the glasses in all video frames, as in the left example of <a href="#S3.F3" title="In 3. Method ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. In this case, glasses remnants appear when no cross-frame attention is used, due to an incomplete mask. However, they are removed when we use cross-frame attention, where the masks of the reference frames cover the uncovered part.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Model fine tuning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">After the data generation stage, we obtain pairs of frames with and without glasses. However, our generated frames are not temporally consistent, and also contain per-frame artifacts in many cases, as featured in <a href="#S1.F2" title="In 1. Introduction ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S3.F4" title="Figure 4 ‣ 3. Method ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. For example, the eyelid positions, i.e. closed or open, sometimes change after the inpainting process.
Despite that, the generated data is good enough for finetuning a pretrained image-to-image diffusion model for the task of removing glasses from faces.
These models have strong prior for generating realistic looking images that are similar to the original ones and preserve their small and delicate details. Therefore, after finetuning, the model learns the task of removing glasses from our data, while preserving fine details such as eye color and eyelid positions.
</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Video editing pipeline</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">To consistently remove glasses from unseen videos, we integrate our trained model with a pretrained motion module <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite>.
When applied directly to video frames, this model’s results often exhibit slightly different colors from the original input. To overcome this issue, we can use the masks from the data generation stage to retrieve the original values on areas that are outside of the mask. For a smooth result, at each diffusion step we perform a gradual blending between the noised masked input and the generated result, such that the area within the mask changes completely, and the areas that surround the mask change less, gradually decreasing the amount of change as the pixels are further from the mask.
Additionally, inspired by AdaIN <cite class="ltx_cite ltx_citemacro_citep">(Huang and Belongie, <a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite>, we apply a new normalization function, we dub Inside-Out Normalization (ION), where we aim to align the statistics of the masked area with those of the non-masked area.
Formally, we calculate the mean and standard deviation of the masked area and the non-masked area, <math id="S3.SS3.p1.1.m1.2" class="ltx_Math" alttext="\mu_{m},\sigma_{m}" display="inline"><semantics id="S3.SS3.p1.1.m1.2a"><mrow id="S3.SS3.p1.1.m1.2.2.2" xref="S3.SS3.p1.1.m1.2.2.3.cmml"><msub id="S3.SS3.p1.1.m1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml">μ</mi><mi id="S3.SS3.p1.1.m1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.3.cmml">m</mi></msub><mo id="S3.SS3.p1.1.m1.2.2.2.3" xref="S3.SS3.p1.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS3.p1.1.m1.2.2.2.2" xref="S3.SS3.p1.1.m1.2.2.2.2.cmml"><mi id="S3.SS3.p1.1.m1.2.2.2.2.2" xref="S3.SS3.p1.1.m1.2.2.2.2.2.cmml">σ</mi><mi id="S3.SS3.p1.1.m1.2.2.2.2.3" xref="S3.SS3.p1.1.m1.2.2.2.2.3.cmml">m</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.2b"><list id="S3.SS3.p1.1.m1.2.2.3.cmml" xref="S3.SS3.p1.1.m1.2.2.2"><apply id="S3.SS3.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.2">𝜇</ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.3">𝑚</ci></apply><apply id="S3.SS3.p1.1.m1.2.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.2">𝜎</ci><ci id="S3.SS3.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.3">𝑚</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.2c">\mu_{m},\sigma_{m}</annotation></semantics></math> and <math id="S3.SS3.p1.2.m2.2" class="ltx_Math" alttext="\mu_{\bar{m}},\sigma_{\bar{m}}" display="inline"><semantics id="S3.SS3.p1.2.m2.2a"><mrow id="S3.SS3.p1.2.m2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.3.cmml"><msub id="S3.SS3.p1.2.m2.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.1.1.2.cmml">μ</mi><mover accent="true" id="S3.SS3.p1.2.m2.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.3.2" xref="S3.SS3.p1.2.m2.1.1.1.1.3.2.cmml">m</mi><mo id="S3.SS3.p1.2.m2.1.1.1.1.3.1" xref="S3.SS3.p1.2.m2.1.1.1.1.3.1.cmml">¯</mo></mover></msub><mo id="S3.SS3.p1.2.m2.2.2.2.3" xref="S3.SS3.p1.2.m2.2.2.3.cmml">,</mo><msub id="S3.SS3.p1.2.m2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.cmml"><mi id="S3.SS3.p1.2.m2.2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.2.cmml">σ</mi><mover accent="true" id="S3.SS3.p1.2.m2.2.2.2.2.3" xref="S3.SS3.p1.2.m2.2.2.2.2.3.cmml"><mi id="S3.SS3.p1.2.m2.2.2.2.2.3.2" xref="S3.SS3.p1.2.m2.2.2.2.2.3.2.cmml">m</mi><mo id="S3.SS3.p1.2.m2.2.2.2.2.3.1" xref="S3.SS3.p1.2.m2.2.2.2.2.3.1.cmml">¯</mo></mover></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.2b"><list id="S3.SS3.p1.2.m2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.2.2"><apply id="S3.SS3.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.2">𝜇</ci><apply id="S3.SS3.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.3"><ci id="S3.SS3.p1.2.m2.1.1.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.3.1">¯</ci><ci id="S3.SS3.p1.2.m2.1.1.1.1.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.3.2">𝑚</ci></apply></apply><apply id="S3.SS3.p1.2.m2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.2.2.2.2.1.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2">𝜎</ci><apply id="S3.SS3.p1.2.m2.2.2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.3"><ci id="S3.SS3.p1.2.m2.2.2.2.2.3.1.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.3.1">¯</ci><ci id="S3.SS3.p1.2.m2.2.2.2.2.3.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.3.2">𝑚</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.2c">\mu_{\bar{m}},\sigma_{\bar{m}}</annotation></semantics></math> respectively, and we normalize the values of the latent features in the area inside the mask by calculating:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="ION(x)=\sigma_{\bar{m}}\frac{x-\mu_{m}}{\sigma_{m}}+\mu_{\bar{m}}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.2" xref="S3.E2.m1.1.2.cmml"><mrow id="S3.E2.m1.1.2.2" xref="S3.E2.m1.1.2.2.cmml"><mi id="S3.E2.m1.1.2.2.2" xref="S3.E2.m1.1.2.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.2.1" xref="S3.E2.m1.1.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.2.2.3" xref="S3.E2.m1.1.2.2.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.2.1a" xref="S3.E2.m1.1.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.2.2.4" xref="S3.E2.m1.1.2.2.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.2.1b" xref="S3.E2.m1.1.2.2.1.cmml">​</mo><mrow id="S3.E2.m1.1.2.2.5.2" xref="S3.E2.m1.1.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.1.2.2.5.2.1" xref="S3.E2.m1.1.2.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.E2.m1.1.2.2.5.2.2" xref="S3.E2.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.2.1" xref="S3.E2.m1.1.2.1.cmml">=</mo><mrow id="S3.E2.m1.1.2.3" xref="S3.E2.m1.1.2.3.cmml"><mrow id="S3.E2.m1.1.2.3.2" xref="S3.E2.m1.1.2.3.2.cmml"><msub id="S3.E2.m1.1.2.3.2.2" xref="S3.E2.m1.1.2.3.2.2.cmml"><mi id="S3.E2.m1.1.2.3.2.2.2" xref="S3.E2.m1.1.2.3.2.2.2.cmml">σ</mi><mover accent="true" id="S3.E2.m1.1.2.3.2.2.3" xref="S3.E2.m1.1.2.3.2.2.3.cmml"><mi id="S3.E2.m1.1.2.3.2.2.3.2" xref="S3.E2.m1.1.2.3.2.2.3.2.cmml">m</mi><mo id="S3.E2.m1.1.2.3.2.2.3.1" xref="S3.E2.m1.1.2.3.2.2.3.1.cmml">¯</mo></mover></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.3.2.1" xref="S3.E2.m1.1.2.3.2.1.cmml">​</mo><mfrac id="S3.E2.m1.1.2.3.2.3" xref="S3.E2.m1.1.2.3.2.3.cmml"><mrow id="S3.E2.m1.1.2.3.2.3.2" xref="S3.E2.m1.1.2.3.2.3.2.cmml"><mi id="S3.E2.m1.1.2.3.2.3.2.2" xref="S3.E2.m1.1.2.3.2.3.2.2.cmml">x</mi><mo id="S3.E2.m1.1.2.3.2.3.2.1" xref="S3.E2.m1.1.2.3.2.3.2.1.cmml">−</mo><msub id="S3.E2.m1.1.2.3.2.3.2.3" xref="S3.E2.m1.1.2.3.2.3.2.3.cmml"><mi id="S3.E2.m1.1.2.3.2.3.2.3.2" xref="S3.E2.m1.1.2.3.2.3.2.3.2.cmml">μ</mi><mi id="S3.E2.m1.1.2.3.2.3.2.3.3" xref="S3.E2.m1.1.2.3.2.3.2.3.3.cmml">m</mi></msub></mrow><msub id="S3.E2.m1.1.2.3.2.3.3" xref="S3.E2.m1.1.2.3.2.3.3.cmml"><mi id="S3.E2.m1.1.2.3.2.3.3.2" xref="S3.E2.m1.1.2.3.2.3.3.2.cmml">σ</mi><mi id="S3.E2.m1.1.2.3.2.3.3.3" xref="S3.E2.m1.1.2.3.2.3.3.3.cmml">m</mi></msub></mfrac></mrow><mo id="S3.E2.m1.1.2.3.1" xref="S3.E2.m1.1.2.3.1.cmml">+</mo><msub id="S3.E2.m1.1.2.3.3" xref="S3.E2.m1.1.2.3.3.cmml"><mi id="S3.E2.m1.1.2.3.3.2" xref="S3.E2.m1.1.2.3.3.2.cmml">μ</mi><mover accent="true" id="S3.E2.m1.1.2.3.3.3" xref="S3.E2.m1.1.2.3.3.3.cmml"><mi id="S3.E2.m1.1.2.3.3.3.2" xref="S3.E2.m1.1.2.3.3.3.2.cmml">m</mi><mo id="S3.E2.m1.1.2.3.3.3.1" xref="S3.E2.m1.1.2.3.3.3.1.cmml">¯</mo></mover></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.2.cmml" xref="S3.E2.m1.1.2"><eq id="S3.E2.m1.1.2.1.cmml" xref="S3.E2.m1.1.2.1"></eq><apply id="S3.E2.m1.1.2.2.cmml" xref="S3.E2.m1.1.2.2"><times id="S3.E2.m1.1.2.2.1.cmml" xref="S3.E2.m1.1.2.2.1"></times><ci id="S3.E2.m1.1.2.2.2.cmml" xref="S3.E2.m1.1.2.2.2">𝐼</ci><ci id="S3.E2.m1.1.2.2.3.cmml" xref="S3.E2.m1.1.2.2.3">𝑂</ci><ci id="S3.E2.m1.1.2.2.4.cmml" xref="S3.E2.m1.1.2.2.4">𝑁</ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑥</ci></apply><apply id="S3.E2.m1.1.2.3.cmml" xref="S3.E2.m1.1.2.3"><plus id="S3.E2.m1.1.2.3.1.cmml" xref="S3.E2.m1.1.2.3.1"></plus><apply id="S3.E2.m1.1.2.3.2.cmml" xref="S3.E2.m1.1.2.3.2"><times id="S3.E2.m1.1.2.3.2.1.cmml" xref="S3.E2.m1.1.2.3.2.1"></times><apply id="S3.E2.m1.1.2.3.2.2.cmml" xref="S3.E2.m1.1.2.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.2.3.2.2.1.cmml" xref="S3.E2.m1.1.2.3.2.2">subscript</csymbol><ci id="S3.E2.m1.1.2.3.2.2.2.cmml" xref="S3.E2.m1.1.2.3.2.2.2">𝜎</ci><apply id="S3.E2.m1.1.2.3.2.2.3.cmml" xref="S3.E2.m1.1.2.3.2.2.3"><ci id="S3.E2.m1.1.2.3.2.2.3.1.cmml" xref="S3.E2.m1.1.2.3.2.2.3.1">¯</ci><ci id="S3.E2.m1.1.2.3.2.2.3.2.cmml" xref="S3.E2.m1.1.2.3.2.2.3.2">𝑚</ci></apply></apply><apply id="S3.E2.m1.1.2.3.2.3.cmml" xref="S3.E2.m1.1.2.3.2.3"><divide id="S3.E2.m1.1.2.3.2.3.1.cmml" xref="S3.E2.m1.1.2.3.2.3"></divide><apply id="S3.E2.m1.1.2.3.2.3.2.cmml" xref="S3.E2.m1.1.2.3.2.3.2"><minus id="S3.E2.m1.1.2.3.2.3.2.1.cmml" xref="S3.E2.m1.1.2.3.2.3.2.1"></minus><ci id="S3.E2.m1.1.2.3.2.3.2.2.cmml" xref="S3.E2.m1.1.2.3.2.3.2.2">𝑥</ci><apply id="S3.E2.m1.1.2.3.2.3.2.3.cmml" xref="S3.E2.m1.1.2.3.2.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.2.3.2.3.2.3.1.cmml" xref="S3.E2.m1.1.2.3.2.3.2.3">subscript</csymbol><ci id="S3.E2.m1.1.2.3.2.3.2.3.2.cmml" xref="S3.E2.m1.1.2.3.2.3.2.3.2">𝜇</ci><ci id="S3.E2.m1.1.2.3.2.3.2.3.3.cmml" xref="S3.E2.m1.1.2.3.2.3.2.3.3">𝑚</ci></apply></apply><apply id="S3.E2.m1.1.2.3.2.3.3.cmml" xref="S3.E2.m1.1.2.3.2.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.2.3.2.3.3.1.cmml" xref="S3.E2.m1.1.2.3.2.3.3">subscript</csymbol><ci id="S3.E2.m1.1.2.3.2.3.3.2.cmml" xref="S3.E2.m1.1.2.3.2.3.3.2">𝜎</ci><ci id="S3.E2.m1.1.2.3.2.3.3.3.cmml" xref="S3.E2.m1.1.2.3.2.3.3.3">𝑚</ci></apply></apply></apply><apply id="S3.E2.m1.1.2.3.3.cmml" xref="S3.E2.m1.1.2.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.2.3.3.1.cmml" xref="S3.E2.m1.1.2.3.3">subscript</csymbol><ci id="S3.E2.m1.1.2.3.3.2.cmml" xref="S3.E2.m1.1.2.3.3.2">𝜇</ci><apply id="S3.E2.m1.1.2.3.3.3.cmml" xref="S3.E2.m1.1.2.3.3.3"><ci id="S3.E2.m1.1.2.3.3.3.1.cmml" xref="S3.E2.m1.1.2.3.3.3.1">¯</ci><ci id="S3.E2.m1.1.2.3.3.3.2.cmml" xref="S3.E2.m1.1.2.3.3.3.2">𝑚</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">ION(x)=\sigma_{\bar{m}}\frac{x-\mu_{m}}{\sigma_{m}}+\mu_{\bar{m}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">ION allows a smooth transition between the areas inside and outside of the mask by moving the statistics of the latent masked area toward those of the non-masked area.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Implementation Details</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Data generation &amp; training specifics.</span>
For the glasses masks generation, we use Facer <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite>
with <span id="S4.p1.1.2" class="ltx_text ltx_font_typewriter">retinaface/mobilenet</span> as a face detector, <span id="S4.p1.1.3" class="ltx_text ltx_font_typewriter">farl/celebm/448</span> as a face parser, and
<span id="S4.p1.1.4" class="ltx_text ltx_font_typewriter">farl/ibug300w/448</span> as a face aligner. We first find the glasses mask using the parser, and then make eye holes in it based on the eye landmarks found by the face aligner. To generate the holes, for each eye, we connect the eye landmarks into one connected component, dilate it with a (10,10) kernel, so that we keep enough of the eyes information, and remove the component from the mask. Additionally, after resizing the mask to match the size of the latent vectors, we dilate the mask with a kernel of (3,3), and then blur it with a (3,3) kernel as well, to make sure we include all glasses pixels,
and not
too much from the rest of the image.
For the inpainting process, we use CN inpaint <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>)</cite> as our model, with latent blending with a blending ratio of 0.9.
We use 2 reference frames for the cross-frame attention: the first and middle frames in each video, since usually the person moves and changes position throughout the video so more information is gathered this way. If more memory is available, more reference frames will probably give better results. Moreover, for consistency we also use the same noise (encoding of the first reference frame) for all the video.
We use CN Tile <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>)</cite> as the model we finetune over our dataset, with batch size 8, learning rate <math id="S4.p1.1.m1.3" class="ltx_Math" alttext="110-5" display="inline"><semantics id="S4.p1.1.m1.3a"><mrow id="S4.p1.1.m1.3.3.3" xref="S4.p1.1.m1.3.3.3.cmml"><mn id="S4.p1.1.m1.1.1.1.1.1.1.1" xref="S4.p1.1.m1.3.3.3.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.2.2.2.2.2.2.2" xref="S4.p1.1.m1.3.3.3.cmml">​</mo><mrow id="S4.p1.1.m1.3.3.3.3.3.3.3" xref="S4.p1.1.m1.3.3.3.cmml"><mi mathvariant="normal" id="S4.p1.1.m1.3.3.3.3.3.3.3.3" xref="S4.p1.1.m1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.3.3.3.3.3.3.3.2" xref="S4.p1.1.m1.3.3.3.cmml">​</mo><mrow id="S4.p1.1.m1.3.3.3.3.3.3.3.1.1.1.1.1.2" xref="S4.p1.1.m1.3.3.3.cmml"><mo id="S4.p1.1.m1.3.3.3.3.3.3.3.1.1.1.1.1.2a" xref="S4.p1.1.m1.3.3.3.cmml">−</mo><mn id="S4.p1.1.m1.3.3.3.3.3.3.3.1.1.1.1.1.2.2" xref="S4.p1.1.m1.3.3.3.cmml">5</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.3b"><csymbol cd="latexml" id="S4.p1.1.m1.3.3.3.cmml" xref="S4.p1.1.m1.3.3.3">1E-5</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.3c">110-5</annotation></semantics></math>. To avoid learning the artifacts of our imperfect data, and avoid forgetting the prior knowledge of CN Tile, we stop the training at an early stage, as suggested by DVP <cite class="ltx_cite ltx_citemacro_citep">(Lei et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Editing pipeline specifics.</span>
We use the pretrained motion module of AnimateDiff <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite> with context length 16, context overlap 4 as our motion prior module.
As shown in the right example of <a href="#S6.F9" title="In 6. Limitations ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>, the motion module tends to smooth the frames to get a more temporally consistent result, hence sometimes the results using this module get blurry. To avoid blurriness, we use only some of the motion layers and not all of them. Specifically, we remove the first 5 output motion layers.
This way, we get a more realistic and less blurry video.
For blending we use gradual values between 0 and 0.7 as mask values, so that pixels outside of the mask can also change a little bit for a smoother result.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Original colors vs. glasses removal trade-off.</span>
As an option in our video editing pipeline, we blend original pixels back into areas outside the masked region. If we do not blend them back, the colors in the edited video may not match those of the original video exactly. However, as the masks are not perfect, glasses remnants are sometimes left within the non-masked region, causing glasses remnants to appear in the result when using masks. To avoid it, we use dilation and different blending values outside of the mask. If the original colors are not as important and glasses-removal is of higher priority, one can use higher dilation for the mask, lower mask blending values, or even not use the masks at all.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">General.</span>
We use Stable Diffusion 1.5 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">rombach2021highresolution</span>)</cite>
as our backbone in all the method steps, as at the time of development there was no compatible motion module for SDXL.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experiments</h2>

<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F5.48" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:372.9pt;height:642.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(36.6pt,-63.1pt) scale(1.24460014755377,1.24460014755377) ;">
<table id="S5.F5.48.48" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F5.6.6.6" class="ltx_tr">
<th id="S5.F5.6.6.6.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F5.6.6.6.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F5.6.6.6.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:24.2pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:24.2pt;transform:translate(-7.69pt,-13.56pt) rotate(-90deg) ;">
<span id="S5.F5.6.6.6.7.1.1.1" class="ltx_p">Input</span>
</span></span></span></th>
<td id="S5.F5.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/original/00008.jpg" id="S5.F5.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/original/00021.jpg" id="S5.F5.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.3.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/original/00031.jpg" id="S5.F5.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.4.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/original/00005.png" id="S5.F5.4.4.4.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.5.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/original/00014.png" id="S5.F5.5.5.5.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.6.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/original/00018.png" id="S5.F5.6.6.6.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.12.12.12" class="ltx_tr">
<th id="S5.F5.12.12.12.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F5.12.12.12.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F5.12.12.12.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:42.6pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:42.5pt;transform:translate(-17.85pt,-24.68pt) rotate(-90deg) ;">
<span id="S5.F5.12.12.12.7.1.1.1" class="ltx_p">T2V-Zero</span>
</span></span></span></th>
<td id="S5.F5.7.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/Text2Video-Zero/00008.jpg" id="S5.F5.7.7.7.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.8.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/Text2Video-Zero/00021.jpg" id="S5.F5.8.8.8.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.9.9.9.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/Text2Video-Zero/00031.jpg" id="S5.F5.9.9.9.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.10.10.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/Text2Video-Zero/00004.jpg" id="S5.F5.10.10.10.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.11.11.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/Text2Video-Zero/00013.jpg" id="S5.F5.11.11.11.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.12.12.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/Text2Video-Zero/00017.jpg" id="S5.F5.12.12.12.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.18.18.18" class="ltx_tr">
<th id="S5.F5.18.18.18.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F5.18.18.18.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F5.18.18.18.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:47.7pt;vertical-align:6.9pt;"><span class="ltx_transformed_inner" style="width:47.6pt;transform:translate(-20.35pt,-27.29pt) rotate(-90deg) ;">
<span id="S5.F5.18.18.18.7.1.1.1" class="ltx_p">TokenFlow</span>
</span></span></span></th>
<td id="S5.F5.13.13.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/TokenFlow/00008.png" id="S5.F5.13.13.13.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.14.14.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/TokenFlow/00021.png" id="S5.F5.14.14.14.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.15.15.15.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/TokenFlow/00031.png" id="S5.F5.15.15.15.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.16.16.16.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/TokenFlow/00004.png" id="S5.F5.16.16.16.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.17.17.17.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/TokenFlow/00013.png" id="S5.F5.17.17.17.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.18.18.18.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/TokenFlow/00017.png" id="S5.F5.18.18.18.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.24.24.24" class="ltx_tr">
<th id="S5.F5.24.24.24.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F5.24.24.24.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F5.24.24.24.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:28.1pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:28.1pt;transform:translate(-10.61pt,-17.44pt) rotate(-90deg) ;">
<span id="S5.F5.24.24.24.7.1.1.1" class="ltx_p">RAVE</span>
</span></span></span></th>
<td id="S5.F5.19.19.19.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/RAVE/frame_8.png" id="S5.F5.19.19.19.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.20.20.20.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/RAVE/frame_21.png" id="S5.F5.20.20.20.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.21.21.21.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/RAVE/frame_31.png" id="S5.F5.21.21.21.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.22.22.22.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/RAVE/frame_4.png" id="S5.F5.22.22.22.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.23.23.23.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/RAVE/frame_13.png" id="S5.F5.23.23.23.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.24.24.24.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/RAVE/frame_17.png" id="S5.F5.24.24.24.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.30.30.30" class="ltx_tr">
<th id="S5.F5.30.30.30.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F5.30.30.30.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F5.30.30.30.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:48.9pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:48.9pt;transform:translate(-20.06pt,-25.92pt) rotate(-90deg) ;">
<span id="S5.F5.30.30.30.7.1.1.1" class="ltx_p">CN inpaint</span>
</span></span></span></th>
<td id="S5.F5.25.25.25.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/CN_inpaint/00008.jpg" id="S5.F5.25.25.25.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.26.26.26.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/CN_inpaint/00021.jpg" id="S5.F5.26.26.26.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.27.27.27.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/CN_inpaint/00031.jpg" id="S5.F5.27.27.27.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.28.28.28.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/CN_inpaint/00004.jpg" id="S5.F5.28.28.28.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.29.29.29.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/CN_inpaint/00013.jpg" id="S5.F5.29.29.29.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.30.30.30.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/CN_inpaint/00017.jpg" id="S5.F5.30.30.30.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.36.36.36" class="ltx_tr">
<th id="S5.F5.36.36.36.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F5.36.36.36.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F5.36.36.36.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:21.4pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:21.3pt;transform:translate(-7.24pt,-14.08pt) rotate(-90deg) ;">
<span id="S5.F5.36.36.36.7.1.1.1" class="ltx_p">FGT</span>
</span></span></span></th>
<td id="S5.F5.31.31.31.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/FGT/00008.png" id="S5.F5.31.31.31.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.32.32.32.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/FGT/00021.png" id="S5.F5.32.32.32.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.33.33.33.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/FGT/00031.png" id="S5.F5.33.33.33.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.34.34.34.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/FGT/00004.png" id="S5.F5.34.34.34.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.35.35.35.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/FGT/00013.png" id="S5.F5.35.35.35.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.36.36.36.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/FGT/00017.png" id="S5.F5.36.36.36.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.42.42.42" class="ltx_tr">
<th id="S5.F5.42.42.42.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F5.42.42.42.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F5.42.42.42.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:47.6pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:47.6pt;transform:translate(-20.36pt,-27.19pt) rotate(-90deg) ;">
<span id="S5.F5.42.42.42.7.1.1.1" class="ltx_p">ProPainter</span>
</span></span></span></th>
<td id="S5.F5.37.37.37.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/ProPainter/00008.jpg" id="S5.F5.37.37.37.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.38.38.38.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/ProPainter/00021.jpg" id="S5.F5.38.38.38.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.39.39.39.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/ProPainter/00031.jpg" id="S5.F5.39.39.39.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.40.40.40.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/ProPainter/00004.jpg" id="S5.F5.40.40.40.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.41.41.41.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/ProPainter/00013.jpg" id="S5.F5.41.41.41.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.42.42.42.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/ProPainter/00017.jpg" id="S5.F5.42.42.42.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.48.48.48" class="ltx_tr">
<th id="S5.F5.48.48.48.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F5.48.48.48.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F5.48.48.48.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:21.2pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:21.2pt;transform:translate(-7.18pt,-14.01pt) rotate(-90deg) ;">
<span id="S5.F5.48.48.48.7.1.1.1" class="ltx_p"><span id="S5.F5.48.48.48.7.1.1.1.1" class="ltx_text ltx_font_bold">Ours</span></span>
</span></span></span></th>
<td id="S5.F5.43.43.43.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/ours_masked/00008.png" id="S5.F5.43.43.43.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.44.44.44.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/ours_masked/00021.png" id="S5.F5.44.44.44.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.45.45.45.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/59TJ/ours_masked/00031.png" id="S5.F5.45.45.45.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.46.46.46.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/ours_masked/00004.png" id="S5.F5.46.46.46.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.47.47.47.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/ours_masked/00013.png" id="S5.F5.47.47.47.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F5.48.48.48.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/3Zlr/ours_masked/00017.png" id="S5.F5.48.48.48.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span><span id="S5.F5.50.1" class="ltx_text ltx_font_bold">Visual comparisons:</span> We compare our results to different video editing and inpainting methods. Other methods often struggle with glasses-removal, and even when they do remove the glasses, they tend to leave glasses remnants (e.g. RAVE right example), generate artifacts (e.g. FGT, ProPainter examples, TokenFlow left example), do not preserve the identity of the person (e.g. RAVE Left example), or their eyelids position (e.g. RAVE both examples).
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F5.51" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F5.52" class="ltx_p ltx_figure_panel ltx_align_center">[]</p>
</div>
</div>
</figure>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S5.F6.48" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:372.9pt;height:642.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(36.6pt,-63.1pt) scale(1.24460014755377,1.24460014755377) ;">
<table id="S5.F6.48.48" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F6.6.6.6" class="ltx_tr">
<th id="S5.F6.6.6.6.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F6.6.6.6.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F6.6.6.6.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:24.2pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:24.2pt;transform:translate(-7.69pt,-13.56pt) rotate(-90deg) ;">
<span id="S5.F6.6.6.6.7.1.1.1" class="ltx_p">Input</span>
</span></span></span></th>
<td id="S5.F6.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/original/00006.jpg" id="S5.F6.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/original/00010.jpg" id="S5.F6.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.3.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/original/00030.jpg" id="S5.F6.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.4.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/original/00000.jpg" id="S5.F6.4.4.4.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.5.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/original/00030.jpg" id="S5.F6.5.5.5.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.6.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/original/00017.jpg" id="S5.F6.6.6.6.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F6.12.12.12" class="ltx_tr">
<th id="S5.F6.12.12.12.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F6.12.12.12.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F6.12.12.12.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:42.6pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:42.5pt;transform:translate(-17.85pt,-24.68pt) rotate(-90deg) ;">
<span id="S5.F6.12.12.12.7.1.1.1" class="ltx_p">T2V-Zero</span>
</span></span></span></th>
<td id="S5.F6.7.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/Text2Video-Zero/00006.jpg" id="S5.F6.7.7.7.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.8.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/Text2Video-Zero/00010.jpg" id="S5.F6.8.8.8.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.9.9.9.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/Text2Video-Zero/00030.jpg" id="S5.F6.9.9.9.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.10.10.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/Text2Video-Zero/00000.jpg" id="S5.F6.10.10.10.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.11.11.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/Text2Video-Zero/00030.jpg" id="S5.F6.11.11.11.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.12.12.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/Text2Video-Zero/00017.jpg" id="S5.F6.12.12.12.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F6.18.18.18" class="ltx_tr">
<th id="S5.F6.18.18.18.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F6.18.18.18.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F6.18.18.18.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:47.7pt;vertical-align:6.9pt;"><span class="ltx_transformed_inner" style="width:47.6pt;transform:translate(-20.35pt,-27.29pt) rotate(-90deg) ;">
<span id="S5.F6.18.18.18.7.1.1.1" class="ltx_p">TokenFlow</span>
</span></span></span></th>
<td id="S5.F6.13.13.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/TokenFlow/00005.png" id="S5.F6.13.13.13.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.14.14.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/TokenFlow/00010.png" id="S5.F6.14.14.14.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.15.15.15.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/TokenFlow/00030.png" id="S5.F6.15.15.15.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.16.16.16.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/TokenFlow/00000.png" id="S5.F6.16.16.16.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.17.17.17.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/TokenFlow/00030.png" id="S5.F6.17.17.17.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.18.18.18.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/TokenFlow/00017.png" id="S5.F6.18.18.18.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F6.24.24.24" class="ltx_tr">
<th id="S5.F6.24.24.24.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F6.24.24.24.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F6.24.24.24.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:28.1pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:28.1pt;transform:translate(-10.61pt,-17.44pt) rotate(-90deg) ;">
<span id="S5.F6.24.24.24.7.1.1.1" class="ltx_p">RAVE</span>
</span></span></span></th>
<td id="S5.F6.19.19.19.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/RAVE/frame_6.png" id="S5.F6.19.19.19.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.20.20.20.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/RAVE/frame_10.png" id="S5.F6.20.20.20.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.21.21.21.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/RAVE/frame_30.png" id="S5.F6.21.21.21.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.22.22.22.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/RAVE/frame_0.png" id="S5.F6.22.22.22.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.23.23.23.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/RAVE/frame_30.png" id="S5.F6.23.23.23.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.24.24.24.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/RAVE/frame_17.png" id="S5.F6.24.24.24.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F6.30.30.30" class="ltx_tr">
<th id="S5.F6.30.30.30.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F6.30.30.30.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F6.30.30.30.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:48.9pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:48.9pt;transform:translate(-20.06pt,-25.92pt) rotate(-90deg) ;">
<span id="S5.F6.30.30.30.7.1.1.1" class="ltx_p">CN inpaint</span>
</span></span></span></th>
<td id="S5.F6.25.25.25.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/CN_inpaint/00006.jpg" id="S5.F6.25.25.25.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.26.26.26.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/CN_inpaint/00010.jpg" id="S5.F6.26.26.26.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.27.27.27.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/CN_inpaint/00030.jpg" id="S5.F6.27.27.27.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.28.28.28.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/CN_inpaint/00000.jpg" id="S5.F6.28.28.28.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.29.29.29.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/CN_inpaint/00030.jpg" id="S5.F6.29.29.29.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.30.30.30.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/CN_inpaint/00017.jpg" id="S5.F6.30.30.30.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F6.36.36.36" class="ltx_tr">
<th id="S5.F6.36.36.36.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F6.36.36.36.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F6.36.36.36.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:21.4pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:21.3pt;transform:translate(-7.24pt,-14.08pt) rotate(-90deg) ;">
<span id="S5.F6.36.36.36.7.1.1.1" class="ltx_p">FGT</span>
</span></span></span></th>
<td id="S5.F6.31.31.31.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/FGT/00006.png" id="S5.F6.31.31.31.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.32.32.32.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/FGT/00010.png" id="S5.F6.32.32.32.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.33.33.33.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/FGT/00030.png" id="S5.F6.33.33.33.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.34.34.34.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/FGT/00000.png" id="S5.F6.34.34.34.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.35.35.35.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/FGT/00030.png" id="S5.F6.35.35.35.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.36.36.36.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/FGT/00017.png" id="S5.F6.36.36.36.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F6.42.42.42" class="ltx_tr">
<th id="S5.F6.42.42.42.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F6.42.42.42.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F6.42.42.42.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:47.6pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:47.6pt;transform:translate(-20.36pt,-27.19pt) rotate(-90deg) ;">
<span id="S5.F6.42.42.42.7.1.1.1" class="ltx_p">ProPainter</span>
</span></span></span></th>
<td id="S5.F6.37.37.37.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/ProPainter/00006.jpg" id="S5.F6.37.37.37.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.38.38.38.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/ProPainter/00010.jpg" id="S5.F6.38.38.38.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.39.39.39.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/ProPainter/00030.jpg" id="S5.F6.39.39.39.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.40.40.40.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/ProPainter/00000.jpg" id="S5.F6.40.40.40.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.41.41.41.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/ProPainter/00030.jpg" id="S5.F6.41.41.41.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.42.42.42.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/ProPainter/00017.jpg" id="S5.F6.42.42.42.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
<tr id="S5.F6.48.48.48" class="ltx_tr">
<th id="S5.F6.48.48.48.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding-left:0.1pt;padding-right:0.1pt;"><span id="S5.F6.48.48.48.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F6.48.48.48.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:21.2pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:21.2pt;transform:translate(-7.18pt,-14.01pt) rotate(-90deg) ;">
<span id="S5.F6.48.48.48.7.1.1.1" class="ltx_p"><span id="S5.F6.48.48.48.7.1.1.1.1" class="ltx_text ltx_font_bold">Ours</span></span>
</span></span></span></th>
<td id="S5.F6.43.43.43.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/ours_masked/00006.png" id="S5.F6.43.43.43.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.44.44.44.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/ours_masked/00010.png" id="S5.F6.44.44.44.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.45.45.45.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/Xbi2B_1hDls_1/ours_masked/00030.png" id="S5.F6.45.45.45.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.46.46.46.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/ours_masked/00000.png" id="S5.F6.46.46.46.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.47.47.47.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/ours_masked/00030.png" id="S5.F6.47.47.47.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
<td id="S5.F6.48.48.48.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.1pt;padding-right:0.1pt;"><img src="/html/2406.14510/assets/Assets/results/YY_k56xXlK0_1/ours_masked/00017.png" id="S5.F6.48.48.48.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span><span id="S5.F6.50.1" class="ltx_text ltx_font_bold">Visual comparison:</span> We compare our results to different video editing and inpainting methods. Other methods often struggle with glasses-removal, and even when they do remove the glasses, they tend to either change the identity completely (e.g. TokenFlow right example), generate artifacts such as black areas around the eyes (e.g. FGT, ProPainter both examples), or do not preserve the eyelids position (e.g. TokenFlow, RAVE right example)</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F6.51" class="ltx_p ltx_figure_panel ltx_align_center">.

<span id="S5.F6.51.1" class="ltx_ERROR undefined">\Description</span>[] </p>
</div>
</div>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We evaluate our results both qualitatively and quantitatively, testing three aspects: 
<br class="ltx_break">1. <span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Fidelity</span> to the required edit, i.e glasses-removal. 
<br class="ltx_break">2. <span id="S5.p1.1.2" class="ltx_text ltx_font_bold">Identity and content preservation</span> of the original video. We want to remove the glasses while leaving the rest of the video intact. 
<br class="ltx_break">3. <span id="S5.p1.1.3" class="ltx_text ltx_font_bold">Realism</span> of the result, by means of temporal consistency and realism per-frame.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">As we perform local video editing, we compare our results with the results of the SOTA video editing methods:
TokenFlow <cite class="ltx_cite ltx_citemacro_citep">(Geyer et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>, RAVE <cite class="ltx_cite ltx_citemacro_citep">(Kara et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>, and Text2Video-Zero <cite class="ltx_cite ltx_citemacro_citep">(Khachatryan et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. TokenFlow and Text2Video-Zero incorporate SDEdit <cite class="ltx_cite ltx_citemacro_citep">(Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> and instructPix2Pix <cite class="ltx_cite ltx_citemacro_citep">(Brooks et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, respectively, to allow for local attribute editing, and RAVE uses CN “depth-zoe” <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>)</cite> for that purpose.
We also compare our model to the SOTA video inpainting works ProPainter <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> and FGT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>
as our work is similar to inpainting works in the sense that it tries to replace some part of the video. Additionally, we compare our results to the results of our video editing pipeline with CN inpaint <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>)</cite> instead of our trained model, to emphasize that existing image inpainting models do not perform well enough on this task, even when combined in our video editing pipeline.
Our quantitative and qualitative evaluations, including a comprehensive user study, demonstrate that our results are favored over all other methods across all tested aspects.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Experimental details</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We generate our data pairs as described in <a href="#S3.SS1" title="3.1. Paired Data Generation ‣ 3. Method ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a> from the dataset CelebV-Text <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2023b</a>)</cite>. We train our model over 1296 of those videos, and test over 144 unseen videos.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Qualitative evaluation</h3>

<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S5.F7.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F7.1.1" class="ltx_tr">
<td id="S5.F7.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2406.14510/assets/x19.png" id="S5.F7.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="307" alt="Refer to caption"></td>
</tr>
<tr id="S5.F7.2.2" class="ltx_tr">
<td id="S5.F7.2.2.1" class="ltx_td ltx_align_center"><img src="/html/2406.14510/assets/x20.png" id="S5.F7.2.2.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="307" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span><span id="S5.F7.4.1" class="ltx_text ltx_font_bold">User study results:</span> First, we ask users which models remove the glasses from the input video (top). Then, when both models remove the glasses, we ask
which one better preserves the identity of the person, which one is more realistic, and which result has less remnants of glasses (bottom). Compared to all model examined, the users preferred our results through all measured aspects.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F7.5" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F7.6" class="ltx_p ltx_figure_panel ltx_align_center">[]</p>
</div>
</div>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Visual results of our method are presented in <a href="#S5.F5" title="In 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S5.F6" title="Figure 6 ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and in the supplementary material.
As illustrated in the figures, existing video editing and inpainting methods struggle with performing the required local edit, i.e., removing the glasses of the person. Moreover, even when they do remove the glasses, they tend to generate artifacts and unrealistic results, leave glasses remnants, or do not preserve the identity of the person or its original eyelid positions. In contrast, our method successfully removes the glasses while preserving the identity and content of the original video.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">We perform a thorough user study to compare our results with the results of TokenFlow, RAVE, ProPainter,
and our editing pipeline with CN inpaint. In the user study, we do not compare our results to those of Text2Video-Zero,
as it
only removes the glasses from the input video in 4% of the test videos, as featured in <a href="#S5.F5" title="In 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The user study tests three aspects: glasses-removal, resemblance to the identity in the original video, and realism of the result.
For each video in the survey, we ask the users in which videos are the glasses removed. If both models removed the glasses for that video, we additionally ask which video contains less remnants of glasses, which one looks more realistic, and which one better preserves the identity of the person in the original video.
We ask for remnants of glasses, because we noted that even when other models remove the glasses from the video, they often leave parts of the glasses or their reflections, e.g. as presented in the results of RAVE in <a href="#S5.F5" title="In 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>.
The user study contained side-by-side video pairs of our results vs. the results of different models for the same input (11 per model, 44 pairs in total), and it was answered by 57 users.
The results of our user study are presented in <a href="#S5.F7" title="In 5.2. Qualitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>.
The top table shows the percentage of videos for which the users thought each model removed the glasses from the input video. The users thought our model removed the glasses in 98.9% of the cases, more than any other model.
The bottom table shows the percentage of users that preferred our results over the tested models.
<a href="#S5.F7" title="In 5.2. Qualitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a> shows that the users preferred our results over all other models, in all the measured aspects: identity preservation, realism, and quality of glasses-removal.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Quantitative evaluation</h3>

<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span><span id="S5.T1.18.1" class="ltx_text ltx_font_bold">Quantitative results:</span> We compare our results to different video inpainting and editing methods, where CN inpaint<sup id="S5.T1.19.2" class="ltx_sup">∗</sup> is CN inpaint embedded in our video editing pipeline instead of our model. We present results for two versions of our model — with and without masks, as elaborated in <a href="#S5.SS3" title="5.3. Quantitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>.
We test the fidelity of the results by checking <math id="S5.T1.7.m2.1" class="ltx_Math" alttext="\Delta G" display="inline"><semantics id="S5.T1.7.m2.1b"><mrow id="S5.T1.7.m2.1.1" xref="S5.T1.7.m2.1.1.cmml"><mi mathvariant="normal" id="S5.T1.7.m2.1.1.2" xref="S5.T1.7.m2.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S5.T1.7.m2.1.1.1" xref="S5.T1.7.m2.1.1.1.cmml">​</mo><mi id="S5.T1.7.m2.1.1.3" xref="S5.T1.7.m2.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.7.m2.1c"><apply id="S5.T1.7.m2.1.1.cmml" xref="S5.T1.7.m2.1.1"><times id="S5.T1.7.m2.1.1.1.cmml" xref="S5.T1.7.m2.1.1.1"></times><ci id="S5.T1.7.m2.1.1.2.cmml" xref="S5.T1.7.m2.1.1.2">Δ</ci><ci id="S5.T1.7.m2.1.1.3.cmml" xref="S5.T1.7.m2.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.m2.1d">\Delta G</annotation></semantics></math>, the average difference between glasses pixels in the original video frames and the edited one.
We test the identity preservation of the edited video using <math id="S5.T1.8.m3.1" class="ltx_Math" alttext="ID" display="inline"><semantics id="S5.T1.8.m3.1b"><mrow id="S5.T1.8.m3.1.1" xref="S5.T1.8.m3.1.1.cmml"><mi id="S5.T1.8.m3.1.1.2" xref="S5.T1.8.m3.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.T1.8.m3.1.1.1" xref="S5.T1.8.m3.1.1.1.cmml">​</mo><mi id="S5.T1.8.m3.1.1.3" xref="S5.T1.8.m3.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.8.m3.1c"><apply id="S5.T1.8.m3.1.1.cmml" xref="S5.T1.8.m3.1.1"><times id="S5.T1.8.m3.1.1.1.cmml" xref="S5.T1.8.m3.1.1.1"></times><ci id="S5.T1.8.m3.1.1.2.cmml" xref="S5.T1.8.m3.1.1.2">𝐼</ci><ci id="S5.T1.8.m3.1.1.3.cmml" xref="S5.T1.8.m3.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.m3.1d">ID</annotation></semantics></math> score, and the tradeoff between them using <math id="S5.T1.9.m4.1" class="ltx_Math" alttext="ID\cdot\Delta G" display="inline"><semantics id="S5.T1.9.m4.1b"><mrow id="S5.T1.9.m4.1.1" xref="S5.T1.9.m4.1.1.cmml"><mrow id="S5.T1.9.m4.1.1.2" xref="S5.T1.9.m4.1.1.2.cmml"><mrow id="S5.T1.9.m4.1.1.2.2" xref="S5.T1.9.m4.1.1.2.2.cmml"><mi id="S5.T1.9.m4.1.1.2.2.2" xref="S5.T1.9.m4.1.1.2.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.T1.9.m4.1.1.2.2.1" xref="S5.T1.9.m4.1.1.2.2.1.cmml">​</mo><mi id="S5.T1.9.m4.1.1.2.2.3" xref="S5.T1.9.m4.1.1.2.2.3.cmml">D</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S5.T1.9.m4.1.1.2.1" xref="S5.T1.9.m4.1.1.2.1.cmml">⋅</mo><mi mathvariant="normal" id="S5.T1.9.m4.1.1.2.3" xref="S5.T1.9.m4.1.1.2.3.cmml">Δ</mi></mrow><mo lspace="0em" rspace="0em" id="S5.T1.9.m4.1.1.1" xref="S5.T1.9.m4.1.1.1.cmml">​</mo><mi id="S5.T1.9.m4.1.1.3" xref="S5.T1.9.m4.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.9.m4.1c"><apply id="S5.T1.9.m4.1.1.cmml" xref="S5.T1.9.m4.1.1"><times id="S5.T1.9.m4.1.1.1.cmml" xref="S5.T1.9.m4.1.1.1"></times><apply id="S5.T1.9.m4.1.1.2.cmml" xref="S5.T1.9.m4.1.1.2"><ci id="S5.T1.9.m4.1.1.2.1.cmml" xref="S5.T1.9.m4.1.1.2.1">⋅</ci><apply id="S5.T1.9.m4.1.1.2.2.cmml" xref="S5.T1.9.m4.1.1.2.2"><times id="S5.T1.9.m4.1.1.2.2.1.cmml" xref="S5.T1.9.m4.1.1.2.2.1"></times><ci id="S5.T1.9.m4.1.1.2.2.2.cmml" xref="S5.T1.9.m4.1.1.2.2.2">𝐼</ci><ci id="S5.T1.9.m4.1.1.2.2.3.cmml" xref="S5.T1.9.m4.1.1.2.2.3">𝐷</ci></apply><ci id="S5.T1.9.m4.1.1.2.3.cmml" xref="S5.T1.9.m4.1.1.2.3">Δ</ci></apply><ci id="S5.T1.9.m4.1.1.3.cmml" xref="S5.T1.9.m4.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.m4.1d">ID\cdot\Delta G</annotation></semantics></math>. Moreover, we test the temporal consistency of the generated videos using the optical flow warp error <math id="S5.T1.10.m5.1" class="ltx_Math" alttext="E_{warp}" display="inline"><semantics id="S5.T1.10.m5.1b"><msub id="S5.T1.10.m5.1.1" xref="S5.T1.10.m5.1.1.cmml"><mi id="S5.T1.10.m5.1.1.2" xref="S5.T1.10.m5.1.1.2.cmml">E</mi><mrow id="S5.T1.10.m5.1.1.3" xref="S5.T1.10.m5.1.1.3.cmml"><mi id="S5.T1.10.m5.1.1.3.2" xref="S5.T1.10.m5.1.1.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S5.T1.10.m5.1.1.3.1" xref="S5.T1.10.m5.1.1.3.1.cmml">​</mo><mi id="S5.T1.10.m5.1.1.3.3" xref="S5.T1.10.m5.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.T1.10.m5.1.1.3.1b" xref="S5.T1.10.m5.1.1.3.1.cmml">​</mo><mi id="S5.T1.10.m5.1.1.3.4" xref="S5.T1.10.m5.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.T1.10.m5.1.1.3.1c" xref="S5.T1.10.m5.1.1.3.1.cmml">​</mo><mi id="S5.T1.10.m5.1.1.3.5" xref="S5.T1.10.m5.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.T1.10.m5.1c"><apply id="S5.T1.10.m5.1.1.cmml" xref="S5.T1.10.m5.1.1"><csymbol cd="ambiguous" id="S5.T1.10.m5.1.1.1.cmml" xref="S5.T1.10.m5.1.1">subscript</csymbol><ci id="S5.T1.10.m5.1.1.2.cmml" xref="S5.T1.10.m5.1.1.2">𝐸</ci><apply id="S5.T1.10.m5.1.1.3.cmml" xref="S5.T1.10.m5.1.1.3"><times id="S5.T1.10.m5.1.1.3.1.cmml" xref="S5.T1.10.m5.1.1.3.1"></times><ci id="S5.T1.10.m5.1.1.3.2.cmml" xref="S5.T1.10.m5.1.1.3.2">𝑤</ci><ci id="S5.T1.10.m5.1.1.3.3.cmml" xref="S5.T1.10.m5.1.1.3.3">𝑎</ci><ci id="S5.T1.10.m5.1.1.3.4.cmml" xref="S5.T1.10.m5.1.1.3.4">𝑟</ci><ci id="S5.T1.10.m5.1.1.3.5.cmml" xref="S5.T1.10.m5.1.1.3.5">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.m5.1d">E_{warp}</annotation></semantics></math>.
</figcaption>
<div id="S5.T1.15" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:152.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.7pt,5.1pt) scale(0.93661,0.93661) ;">
<p id="S5.T1.15.5" class="ltx_p"><span id="S5.T1.15.5.5" class="ltx_text">
<span id="S5.T1.15.5.5.5" class="ltx_inline-block ltx_transformed_outer" style="width:463.0pt;height:163pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.T1.15.5.5.5.5" class="ltx_p"><span id="S5.T1.15.5.5.5.5.5" class="ltx_text">



<span id="S5.T1.15.5.5.5.5.5.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T1.14.4.4.4.4.4.4.4" class="ltx_tr">
<span id="S5.T1.14.4.4.4.4.4.4.4.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</span>
<span id="S5.T1.11.1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\Delta G_{(\times.01)}\uparrow" display="inline"><semantics id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.cmml"><mrow id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.cmml"><mi mathvariant="normal" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.2" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.1" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.1.cmml">​</mo><msub id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.3" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.3.cmml"><mi id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.3.2" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.3.2.cmml">G</mi><mrow id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.2" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.cmml"><mi id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.2" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml">×</mo><mn id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3.cmml">.01</mn></mrow><mo stretchy="false" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.3" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msub></mrow><mo stretchy="false" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.1" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.1.cmml">↑</mo><mi id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.3" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1b"><apply id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2"><ci id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.1.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.1">↑</ci><apply id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2"><times id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.1.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.1"></times><ci id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.2.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.2">Δ</ci><apply id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.3.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.3"><csymbol cd="ambiguous" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.3.1.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.3">subscript</csymbol><ci id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.3.2.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.2.3.2">𝐺</ci><apply id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1"><times id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1"></times><csymbol cd="latexml" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.2.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.2">absent</csymbol><cn type="float" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.3">.01</cn></apply></apply></apply><csymbol cd="latexml" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.3.cmml" xref="S5.T1.11.1.1.1.1.1.1.1.1.m1.1.2.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.1.1.1.1.1.1.1.1.m1.1c">\Delta G_{(\times.01)}\uparrow</annotation></semantics></math></span>
<span id="S5.T1.12.2.2.2.2.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="ID_{(\times.1)}\uparrow" display="inline"><semantics id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1a"><mrow id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.cmml"><mrow id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.cmml"><mi id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.2" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.1" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.1.cmml">​</mo><msub id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.3" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.3.cmml"><mi id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.3.2" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.3.2.cmml">D</mi><mrow id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.2" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.cmml"><mi id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.2" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.1" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.1.cmml">×</mo><mn id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.3" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.3.cmml">.1</mn></mrow><mo stretchy="false" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.3" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.cmml">)</mo></mrow></msub></mrow><mo stretchy="false" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.1" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.1.cmml">↑</mo><mi id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.3" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1b"><apply id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2"><ci id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.1.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.1">↑</ci><apply id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2"><times id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.1.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.1"></times><ci id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.2.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.2">𝐼</ci><apply id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.3.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.3"><csymbol cd="ambiguous" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.3.1.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.3">subscript</csymbol><ci id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.3.2.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.2.3.2">𝐷</ci><apply id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1"><times id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.1.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.1"></times><csymbol cd="latexml" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.2.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.2">absent</csymbol><cn type="float" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.3.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.1.1.1.1.3">.1</cn></apply></apply></apply><csymbol cd="latexml" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.3.cmml" xref="S5.T1.12.2.2.2.2.2.2.2.2.m1.1.2.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.12.2.2.2.2.2.2.2.2.m1.1c">ID_{(\times.1)}\uparrow</annotation></semantics></math></span>
<span id="S5.T1.13.3.3.3.3.3.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1" class="ltx_Math" alttext="ID\cdot\Delta G_{(\times.01)}\uparrow" display="inline"><semantics id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1a"><mrow id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.cmml"><mrow id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.cmml"><mrow id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.cmml"><mrow id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.cmml"><mi id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.2" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.1" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.1.cmml">​</mo><mi id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.3" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.3.cmml">D</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.1" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.1.cmml">⋅</mo><mi mathvariant="normal" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.3" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.3.cmml">Δ</mi></mrow><mo lspace="0em" rspace="0em" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.1" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.1.cmml">​</mo><msub id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.3" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.3.cmml"><mi id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.3.2" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.3.2.cmml">G</mi><mrow id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.2" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.cmml"><mi id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.2" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.1" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.1.cmml">×</mo><mn id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.3" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.3.cmml">.01</mn></mrow><mo stretchy="false" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.3" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.cmml">)</mo></mrow></msub></mrow><mo stretchy="false" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.1" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.1.cmml">↑</mo><mi id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.3" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1b"><apply id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2"><ci id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.1.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.1">↑</ci><apply id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2"><times id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.1.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.1"></times><apply id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2"><ci id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.1.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.1">⋅</ci><apply id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2"><times id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.1.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.1"></times><ci id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.2.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.2">𝐼</ci><ci id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.3.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.2.3">𝐷</ci></apply><ci id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.3.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.2.3">Δ</ci></apply><apply id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.3.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.3"><csymbol cd="ambiguous" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.3.1.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.3">subscript</csymbol><ci id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.3.2.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.2.3.2">𝐺</ci><apply id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1"><times id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.1.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.1"></times><csymbol cd="latexml" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.2.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.2">absent</csymbol><cn type="float" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.3.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.1.1.1.1.3">.01</cn></apply></apply></apply><csymbol cd="latexml" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.3.cmml" xref="S5.T1.13.3.3.3.3.3.3.3.3.m1.1.2.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.13.3.3.3.3.3.3.3.3.m1.1c">ID\cdot\Delta G_{(\times.01)}\uparrow</annotation></semantics></math></span>
<span id="S5.T1.14.4.4.4.4.4.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2" class="ltx_Math" alttext="E_{warp(\times 10^{-4})}\downarrow" display="inline"><semantics id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2a"><mrow id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.cmml"><msub id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.2" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.2.cmml"><mi id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.2.2" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.2.2.cmml">E</mi><mrow id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.3.cmml"><mrow id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.cmml"><mi id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.2" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.1" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.3" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.1a" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.4" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.1b" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.5" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.5.cmml">p</mi></mrow><mo id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.3" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.3.cmml">⁣</mo><mrow id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.2" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.cmml"><mi id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.2" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.1" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.1.cmml">×</mo><msup id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.cmml"><mn id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.2" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.2.cmml">10</mn><mrow id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3.cmml"><mo id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3a" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3.cmml">−</mo><mn id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3.2" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><mo stretchy="false" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.3" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></msub><mo stretchy="false" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.1" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.1.cmml">↓</mo><mi id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.3" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2b"><apply id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3"><ci id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.1.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.1">↓</ci><apply id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.2.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.2"><csymbol cd="ambiguous" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.2.1.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.2">subscript</csymbol><ci id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.2.2.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.2.2">𝐸</ci><list id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.3.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2"><apply id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1"><times id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.1.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.1"></times><ci id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.2.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.2">𝑤</ci><ci id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.3.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.3">𝑎</ci><ci id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.4.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.4">𝑟</ci><ci id="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.5.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.1.1.1.1.1.5">𝑝</ci></apply><apply id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1"><times id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.1.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.1"></times><csymbol cd="latexml" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.2.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.2">absent</csymbol><apply id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.1.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3">superscript</csymbol><cn type="integer" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.2.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.2">10</cn><apply id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3"><minus id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3.1.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3"></minus><cn type="integer" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3.2.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.2.2.2.2.1.1.3.3.2">4</cn></apply></apply></apply></list></apply><csymbol cd="latexml" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.3.cmml" xref="S5.T1.14.4.4.4.4.4.4.4.4.m1.2.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.14.4.4.4.4.4.4.4.4.m1.2c">E_{warp(\times 10^{-4})}\downarrow</annotation></semantics></math></span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T1.15.5.5.5.5.5.5.6.1" class="ltx_tr">
<span id="S5.T1.15.5.5.5.5.5.5.6.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t">ProPainter <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S5.T1.15.5.5.5.5.5.5.6.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t"><span id="S5.T1.15.5.5.5.5.5.5.6.1.2.1" class="ltx_text ltx_font_bold">4.0</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.6.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">7.5</span>
<span id="S5.T1.15.5.5.5.5.5.5.6.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t"><span id="S5.T1.15.5.5.5.5.5.5.6.1.4.1" class="ltx_text ltx_font_bold">3.0</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.6.1.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">3.9</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.7.2" class="ltx_tr">
<span id="S5.T1.15.5.5.5.5.5.5.7.2.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row">FGT <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite></span>
<span id="S5.T1.15.5.5.5.5.5.5.7.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S5.T1.15.5.5.5.5.5.5.7.2.2.1" class="ltx_text ltx_font_bold">4.0</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.7.2.3" class="ltx_td ltx_nopad_l ltx_align_center">7.1</span>
<span id="S5.T1.15.5.5.5.5.5.5.7.2.4" class="ltx_td ltx_nopad_l ltx_align_center">2.8</span>
<span id="S5.T1.15.5.5.5.5.5.5.7.2.5" class="ltx_td ltx_nopad_l ltx_align_center">4.0</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.5" class="ltx_tr">
<span id="S5.T1.15.5.5.5.5.5.5.5.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row">CN inpaint<sup id="S5.T1.15.5.5.5.5.5.5.5.1.1" class="ltx_sup">∗</sup> <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib44" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S5.T1.15.5.5.5.5.5.5.5.2" class="ltx_td ltx_nopad_l ltx_align_center">3.0</span>
<span id="S5.T1.15.5.5.5.5.5.5.5.3" class="ltx_td ltx_nopad_l ltx_align_center">8.0</span>
<span id="S5.T1.15.5.5.5.5.5.5.5.4" class="ltx_td ltx_nopad_l ltx_align_center">2.3</span>
<span id="S5.T1.15.5.5.5.5.5.5.5.5" class="ltx_td ltx_nopad_l ltx_align_center">4.1</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.8.3" class="ltx_tr">
<span id="S5.T1.15.5.5.5.5.5.5.8.3.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row">TokenFlow<cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S5.T1.15.5.5.5.5.5.5.8.3.2" class="ltx_td ltx_nopad_l ltx_align_center">2.0</span>
<span id="S5.T1.15.5.5.5.5.5.5.8.3.3" class="ltx_td ltx_nopad_l ltx_align_center">6.9</span>
<span id="S5.T1.15.5.5.5.5.5.5.8.3.4" class="ltx_td ltx_nopad_l ltx_align_center">1.4</span>
<span id="S5.T1.15.5.5.5.5.5.5.8.3.5" class="ltx_td ltx_nopad_l ltx_align_center">5.7</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.9.4" class="ltx_tr">
<span id="S5.T1.15.5.5.5.5.5.5.9.4.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row">RAVE <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S5.T1.15.5.5.5.5.5.5.9.4.2" class="ltx_td ltx_nopad_l ltx_align_center">2.0</span>
<span id="S5.T1.15.5.5.5.5.5.5.9.4.3" class="ltx_td ltx_nopad_l ltx_align_center">7.4</span>
<span id="S5.T1.15.5.5.5.5.5.5.9.4.4" class="ltx_td ltx_nopad_l ltx_align_center">1.5</span>
<span id="S5.T1.15.5.5.5.5.5.5.9.4.5" class="ltx_td ltx_nopad_l ltx_align_center">6.8</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.10.5" class="ltx_tr">
<span id="S5.T1.15.5.5.5.5.5.5.10.5.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row">T2V-Zero <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S5.T1.15.5.5.5.5.5.5.10.5.2" class="ltx_td ltx_nopad_l ltx_align_center">0.3</span>
<span id="S5.T1.15.5.5.5.5.5.5.10.5.3" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S5.T1.15.5.5.5.5.5.5.10.5.3.1" class="ltx_text ltx_font_bold">9.0</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.10.5.4" class="ltx_td ltx_nopad_l ltx_align_center">0.3</span>
<span id="S5.T1.15.5.5.5.5.5.5.10.5.5" class="ltx_td ltx_nopad_l ltx_align_center">8.1</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.11.6" class="ltx_tr">
<span id="S5.T1.15.5.5.5.5.5.5.11.6.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row"><span id="S5.T1.15.5.5.5.5.5.5.11.6.1.1" class="ltx_text ltx_font_bold">V-LASIK (ours)</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.11.6.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S5.T1.15.5.5.5.5.5.5.11.6.2.1" class="ltx_text ltx_font_bold">4.0</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.11.6.3" class="ltx_td ltx_nopad_l ltx_align_center">7.6</span>
<span id="S5.T1.15.5.5.5.5.5.5.11.6.4" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S5.T1.15.5.5.5.5.5.5.11.6.4.1" class="ltx_text ltx_font_bold">3.0</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.11.6.5" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S5.T1.15.5.5.5.5.5.5.11.6.5.1" class="ltx_text ltx_font_bold">3.6</span></span></span>
<span id="S5.T1.15.5.5.5.5.5.5.12.7" class="ltx_tr">
<span id="S5.T1.15.5.5.5.5.5.5.12.7.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T1.15.5.5.5.5.5.5.12.7.1.1" class="ltx_text ltx_font_bold">V-LASIK masked (ours)</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.12.7.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S5.T1.15.5.5.5.5.5.5.12.7.2.1" class="ltx_text ltx_font_bold">4.0</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.12.7.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">7.6</span>
<span id="S5.T1.15.5.5.5.5.5.5.12.7.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S5.T1.15.5.5.5.5.5.5.12.7.4.1" class="ltx_text ltx_font_bold">3.0</span></span>
<span id="S5.T1.15.5.5.5.5.5.5.12.7.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">4.2</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We compare the results of different video editing and inpainting methods to the results of our model in two versions — with and without masks. As mentioned in <a href="#S4" title="4. Implementation Details ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a> both options exist for our model and there is a tradeoff between them. The masked version better preserves the original video colors, while the non-masked version better removes the glasses and thus is more temporally consistent. These differences are small, hence as shown in <a href="#S5.T1" title="In 5.3. Quantitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, they only slightly affect the quantitative results.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.2" class="ltx_p"><span id="S5.SS3.p2.2.1" class="ltx_text ltx_font_bold">Fidelity:</span> to test the fidelity of our results, we measure the average difference between the number of pixels with glasses in the original videos vs. the edited ones. To find the pixels that contain glasses in an image, we use a face parser <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite> that detects glasses, and apply it to each video frame. Then, we calculate the average difference between the number of pixels with glasses in the original frame and the edited one, normalized by the total number of pixels per frame, and report it as <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="\Delta G" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p2.1.m1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><times id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1"></times><ci id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">Δ</ci><ci id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">\Delta G</annotation></semantics></math> in <a href="#S5.T1" title="In 5.3. Quantitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
As the <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="\Delta G" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mrow id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="S5.SS3.p2.2.m2.1.1.2" xref="S5.SS3.p2.2.m2.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p2.2.m2.1.1.1" xref="S5.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS3.p2.2.m2.1.1.3" xref="S5.SS3.p2.2.m2.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"><times id="S5.SS3.p2.2.m2.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1.1"></times><ci id="S5.SS3.p2.2.m2.1.1.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2">Δ</ci><ci id="S5.SS3.p2.2.m2.1.1.3.cmml" xref="S5.SS3.p2.2.m2.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">\Delta G</annotation></semantics></math> scores in <a href="#S5.T1" title="In 5.3. Quantitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> show, our method removes the glasses better than all other video editing methods, and is on-par with the inpainting methods ProPainter <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> and FGT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>. However, as illustrated in <a href="#S5.F5" title="In 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S5.F6" title="Figure 6 ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and by the results of our user study in <a href="#S5.F7" title="In 5.2. Qualitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>, although they remove the glasses from most videos, as our method does, they often generate unrealistic results with artifacts around the eyes, which our model does not generate.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.6" class="ltx_p"><span id="S5.SS3.p3.6.1" class="ltx_text ltx_font_bold">Identity preservation:</span> to test identity preservation, we use an ID score (<math id="S5.SS3.p3.1.m1.1" class="ltx_Math" alttext="ID" display="inline"><semantics id="S5.SS3.p3.1.m1.1a"><mrow id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml"><mi id="S5.SS3.p3.1.m1.1.1.2" xref="S5.SS3.p3.1.m1.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p3.1.m1.1.1.1" xref="S5.SS3.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS3.p3.1.m1.1.1.3" xref="S5.SS3.p3.1.m1.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><apply id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1"><times id="S5.SS3.p3.1.m1.1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1.1"></times><ci id="S5.SS3.p3.1.m1.1.1.2.cmml" xref="S5.SS3.p3.1.m1.1.1.2">𝐼</ci><ci id="S5.SS3.p3.1.m1.1.1.3.cmml" xref="S5.SS3.p3.1.m1.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">ID</annotation></semantics></math> in <a href="#S5.T1" title="In 5.3. Quantitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>), which is defined by the average cosine similarity between the face embeddings of the video frames, generated by the face recognition model Arcface <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite>.
We note that neither of these metrics is complete on its own, as an unchanged video would get a perfect <math id="S5.SS3.p3.2.m2.1" class="ltx_Math" alttext="ID" display="inline"><semantics id="S5.SS3.p3.2.m2.1a"><mrow id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml"><mi id="S5.SS3.p3.2.m2.1.1.2" xref="S5.SS3.p3.2.m2.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p3.2.m2.1.1.1" xref="S5.SS3.p3.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS3.p3.2.m2.1.1.3" xref="S5.SS3.p3.2.m2.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><apply id="S5.SS3.p3.2.m2.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1"><times id="S5.SS3.p3.2.m2.1.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1.1"></times><ci id="S5.SS3.p3.2.m2.1.1.2.cmml" xref="S5.SS3.p3.2.m2.1.1.2">𝐼</ci><ci id="S5.SS3.p3.2.m2.1.1.3.cmml" xref="S5.SS3.p3.2.m2.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">ID</annotation></semantics></math> score, and a random video without glasses would get a very high <math id="S5.SS3.p3.3.m3.1" class="ltx_Math" alttext="\Delta G" display="inline"><semantics id="S5.SS3.p3.3.m3.1a"><mrow id="S5.SS3.p3.3.m3.1.1" xref="S5.SS3.p3.3.m3.1.1.cmml"><mi mathvariant="normal" id="S5.SS3.p3.3.m3.1.1.2" xref="S5.SS3.p3.3.m3.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p3.3.m3.1.1.1" xref="S5.SS3.p3.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS3.p3.3.m3.1.1.3" xref="S5.SS3.p3.3.m3.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.3.m3.1b"><apply id="S5.SS3.p3.3.m3.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1"><times id="S5.SS3.p3.3.m3.1.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1.1"></times><ci id="S5.SS3.p3.3.m3.1.1.2.cmml" xref="S5.SS3.p3.3.m3.1.1.2">Δ</ci><ci id="S5.SS3.p3.3.m3.1.1.3.cmml" xref="S5.SS3.p3.3.m3.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.3.m3.1c">\Delta G</annotation></semantics></math> score.
For example, as Text2Video-Zero <cite class="ltx_cite ltx_citemacro_citep">(Khachatryan et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> does not remove the glasses from most videos, and does not change the videos by much, it achieves a very high <math id="S5.SS3.p3.4.m4.1" class="ltx_Math" alttext="ID" display="inline"><semantics id="S5.SS3.p3.4.m4.1a"><mrow id="S5.SS3.p3.4.m4.1.1" xref="S5.SS3.p3.4.m4.1.1.cmml"><mi id="S5.SS3.p3.4.m4.1.1.2" xref="S5.SS3.p3.4.m4.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p3.4.m4.1.1.1" xref="S5.SS3.p3.4.m4.1.1.1.cmml">​</mo><mi id="S5.SS3.p3.4.m4.1.1.3" xref="S5.SS3.p3.4.m4.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.4.m4.1b"><apply id="S5.SS3.p3.4.m4.1.1.cmml" xref="S5.SS3.p3.4.m4.1.1"><times id="S5.SS3.p3.4.m4.1.1.1.cmml" xref="S5.SS3.p3.4.m4.1.1.1"></times><ci id="S5.SS3.p3.4.m4.1.1.2.cmml" xref="S5.SS3.p3.4.m4.1.1.2">𝐼</ci><ci id="S5.SS3.p3.4.m4.1.1.3.cmml" xref="S5.SS3.p3.4.m4.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.4.m4.1c">ID</annotation></semantics></math> score.
For this reason, we follow prior work <cite class="ltx_cite ltx_citemacro_citep">(Kara et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">cong2023flatten</span>)</cite> and also look at <math id="S5.SS3.p3.5.m5.1" class="ltx_Math" alttext="ID\cdot\Delta G" display="inline"><semantics id="S5.SS3.p3.5.m5.1a"><mrow id="S5.SS3.p3.5.m5.1.1" xref="S5.SS3.p3.5.m5.1.1.cmml"><mrow id="S5.SS3.p3.5.m5.1.1.2" xref="S5.SS3.p3.5.m5.1.1.2.cmml"><mrow id="S5.SS3.p3.5.m5.1.1.2.2" xref="S5.SS3.p3.5.m5.1.1.2.2.cmml"><mi id="S5.SS3.p3.5.m5.1.1.2.2.2" xref="S5.SS3.p3.5.m5.1.1.2.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p3.5.m5.1.1.2.2.1" xref="S5.SS3.p3.5.m5.1.1.2.2.1.cmml">​</mo><mi id="S5.SS3.p3.5.m5.1.1.2.2.3" xref="S5.SS3.p3.5.m5.1.1.2.2.3.cmml">D</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S5.SS3.p3.5.m5.1.1.2.1" xref="S5.SS3.p3.5.m5.1.1.2.1.cmml">⋅</mo><mi mathvariant="normal" id="S5.SS3.p3.5.m5.1.1.2.3" xref="S5.SS3.p3.5.m5.1.1.2.3.cmml">Δ</mi></mrow><mo lspace="0em" rspace="0em" id="S5.SS3.p3.5.m5.1.1.1" xref="S5.SS3.p3.5.m5.1.1.1.cmml">​</mo><mi id="S5.SS3.p3.5.m5.1.1.3" xref="S5.SS3.p3.5.m5.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.5.m5.1b"><apply id="S5.SS3.p3.5.m5.1.1.cmml" xref="S5.SS3.p3.5.m5.1.1"><times id="S5.SS3.p3.5.m5.1.1.1.cmml" xref="S5.SS3.p3.5.m5.1.1.1"></times><apply id="S5.SS3.p3.5.m5.1.1.2.cmml" xref="S5.SS3.p3.5.m5.1.1.2"><ci id="S5.SS3.p3.5.m5.1.1.2.1.cmml" xref="S5.SS3.p3.5.m5.1.1.2.1">⋅</ci><apply id="S5.SS3.p3.5.m5.1.1.2.2.cmml" xref="S5.SS3.p3.5.m5.1.1.2.2"><times id="S5.SS3.p3.5.m5.1.1.2.2.1.cmml" xref="S5.SS3.p3.5.m5.1.1.2.2.1"></times><ci id="S5.SS3.p3.5.m5.1.1.2.2.2.cmml" xref="S5.SS3.p3.5.m5.1.1.2.2.2">𝐼</ci><ci id="S5.SS3.p3.5.m5.1.1.2.2.3.cmml" xref="S5.SS3.p3.5.m5.1.1.2.2.3">𝐷</ci></apply><ci id="S5.SS3.p3.5.m5.1.1.2.3.cmml" xref="S5.SS3.p3.5.m5.1.1.2.3">Δ</ci></apply><ci id="S5.SS3.p3.5.m5.1.1.3.cmml" xref="S5.SS3.p3.5.m5.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.5.m5.1c">ID\cdot\Delta G</annotation></semantics></math>, which quantifies the trade-off between removing the glasses from the video, and remaining faithful to the identity of the person in the original video.
When looking at <math id="S5.SS3.p3.6.m6.1" class="ltx_Math" alttext="ID\cdot\Delta G" display="inline"><semantics id="S5.SS3.p3.6.m6.1a"><mrow id="S5.SS3.p3.6.m6.1.1" xref="S5.SS3.p3.6.m6.1.1.cmml"><mrow id="S5.SS3.p3.6.m6.1.1.2" xref="S5.SS3.p3.6.m6.1.1.2.cmml"><mrow id="S5.SS3.p3.6.m6.1.1.2.2" xref="S5.SS3.p3.6.m6.1.1.2.2.cmml"><mi id="S5.SS3.p3.6.m6.1.1.2.2.2" xref="S5.SS3.p3.6.m6.1.1.2.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p3.6.m6.1.1.2.2.1" xref="S5.SS3.p3.6.m6.1.1.2.2.1.cmml">​</mo><mi id="S5.SS3.p3.6.m6.1.1.2.2.3" xref="S5.SS3.p3.6.m6.1.1.2.2.3.cmml">D</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S5.SS3.p3.6.m6.1.1.2.1" xref="S5.SS3.p3.6.m6.1.1.2.1.cmml">⋅</mo><mi mathvariant="normal" id="S5.SS3.p3.6.m6.1.1.2.3" xref="S5.SS3.p3.6.m6.1.1.2.3.cmml">Δ</mi></mrow><mo lspace="0em" rspace="0em" id="S5.SS3.p3.6.m6.1.1.1" xref="S5.SS3.p3.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS3.p3.6.m6.1.1.3" xref="S5.SS3.p3.6.m6.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.6.m6.1b"><apply id="S5.SS3.p3.6.m6.1.1.cmml" xref="S5.SS3.p3.6.m6.1.1"><times id="S5.SS3.p3.6.m6.1.1.1.cmml" xref="S5.SS3.p3.6.m6.1.1.1"></times><apply id="S5.SS3.p3.6.m6.1.1.2.cmml" xref="S5.SS3.p3.6.m6.1.1.2"><ci id="S5.SS3.p3.6.m6.1.1.2.1.cmml" xref="S5.SS3.p3.6.m6.1.1.2.1">⋅</ci><apply id="S5.SS3.p3.6.m6.1.1.2.2.cmml" xref="S5.SS3.p3.6.m6.1.1.2.2"><times id="S5.SS3.p3.6.m6.1.1.2.2.1.cmml" xref="S5.SS3.p3.6.m6.1.1.2.2.1"></times><ci id="S5.SS3.p3.6.m6.1.1.2.2.2.cmml" xref="S5.SS3.p3.6.m6.1.1.2.2.2">𝐼</ci><ci id="S5.SS3.p3.6.m6.1.1.2.2.3.cmml" xref="S5.SS3.p3.6.m6.1.1.2.2.3">𝐷</ci></apply><ci id="S5.SS3.p3.6.m6.1.1.2.3.cmml" xref="S5.SS3.p3.6.m6.1.1.2.3">Δ</ci></apply><ci id="S5.SS3.p3.6.m6.1.1.3.cmml" xref="S5.SS3.p3.6.m6.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.6.m6.1c">ID\cdot\Delta G</annotation></semantics></math>, our model achieves the best results, together with ProPainter <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>. However, as mentioned, the results of our user-study in <a href="#S5.F7" title="In 5.2. Qualitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a> show that our model outperforms ProPainter and all other methods both in terms of glasses-removal, and in terms of realism and identity preservation.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p"><span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_bold">Temporal consistency:</span> to evaluate temporal consistency, we follow previous work <cite class="ltx_cite ltx_citemacro_citep">(Geyer et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>; Lei et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2020</a>; Lai et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite> and calculate the warp error (<math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="E_{warp}" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><msub id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml"><mi id="S5.SS3.p4.1.m1.1.1.2" xref="S5.SS3.p4.1.m1.1.1.2.cmml">E</mi><mrow id="S5.SS3.p4.1.m1.1.1.3" xref="S5.SS3.p4.1.m1.1.1.3.cmml"><mi id="S5.SS3.p4.1.m1.1.1.3.2" xref="S5.SS3.p4.1.m1.1.1.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p4.1.m1.1.1.3.1" xref="S5.SS3.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS3.p4.1.m1.1.1.3.3" xref="S5.SS3.p4.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p4.1.m1.1.1.3.1a" xref="S5.SS3.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS3.p4.1.m1.1.1.3.4" xref="S5.SS3.p4.1.m1.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p4.1.m1.1.1.3.1b" xref="S5.SS3.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS3.p4.1.m1.1.1.3.5" xref="S5.SS3.p4.1.m1.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><apply id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.1.m1.1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p4.1.m1.1.1.2.cmml" xref="S5.SS3.p4.1.m1.1.1.2">𝐸</ci><apply id="S5.SS3.p4.1.m1.1.1.3.cmml" xref="S5.SS3.p4.1.m1.1.1.3"><times id="S5.SS3.p4.1.m1.1.1.3.1.cmml" xref="S5.SS3.p4.1.m1.1.1.3.1"></times><ci id="S5.SS3.p4.1.m1.1.1.3.2.cmml" xref="S5.SS3.p4.1.m1.1.1.3.2">𝑤</ci><ci id="S5.SS3.p4.1.m1.1.1.3.3.cmml" xref="S5.SS3.p4.1.m1.1.1.3.3">𝑎</ci><ci id="S5.SS3.p4.1.m1.1.1.3.4.cmml" xref="S5.SS3.p4.1.m1.1.1.3.4">𝑟</ci><ci id="S5.SS3.p4.1.m1.1.1.3.5.cmml" xref="S5.SS3.p4.1.m1.1.1.3.5">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">E_{warp}</annotation></semantics></math> in <a href="#S5.T1" title="In 5.3. Quantitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>). We use RAFT <cite class="ltx_cite ltx_citemacro_citep">(Teed and Deng, <a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite> to calculate optical flow between each consecutive pair of frames, warp the former towards the latter, and calculate a masked MSE loss (masking occlusions) between the warped frame and the second frame. As shown in <a href="#S5.T1" title="In 5.3. Quantitative evaluation ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, the non-masked version of our model produces the most temporally consistent results, compared to all other methods.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Generalization</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">In this work, we focus on glasses as a case study. We chose glasses as they are a particularly complex task, as explained in <a href="#S1" title="1. Introduction ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
However, our method can be generalized to other types of local video editing.</p>
</div>
<section id="S5.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.1. </span>Stickers</h4>

<figure id="S5.F8" class="ltx_figure">
<p id="S5.F8.12" class="ltx_p ltx_align_center"><span id="S5.F8.12.12" class="ltx_text">
<span id="S5.F8.12.12.12" class="ltx_inline-block ltx_transformed_outer" style="width:300.3pt;height:110.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.F8.12.12.12.12" class="ltx_p"><span id="S5.F8.12.12.12.12.12" class="ltx_text">
<span id="S5.F8.12.12.12.12.12.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S5.F8.6.6.6.6.6.6.6" class="ltx_tr">
<span id="S5.F8.6.6.6.6.6.6.6.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding:-2.5pt 0.0pt;"><span id="S5.F8.6.6.6.6.6.6.6.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F8.6.6.6.6.6.6.6.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:24.2pt;vertical-align:6.8pt;"><span class="ltx_transformed_inner" style="width:24.2pt;transform:translate(-7.69pt,-13.56pt) rotate(-90deg) ;">
<span id="S5.F8.6.6.6.6.6.6.6.7.1.1.1" class="ltx_p">Input</span>
</span></span></span></span>
<span id="S5.F8.1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_oPoq6CvPFIU_7/original/00011.jpg" id="S5.F8.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span>
<span id="S5.F8.2.2.2.2.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_oPoq6CvPFIU_7/original/00030.jpg" id="S5.F8.2.2.2.2.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span>
<span id="S5.F8.3.3.3.3.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_WXyGorVqLBA_0/original/00001.jpg" id="S5.F8.3.3.3.3.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span>
<span id="S5.F8.4.4.4.4.4.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_WXyGorVqLBA_0/original/00006.jpg" id="S5.F8.4.4.4.4.4.4.4.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span>
<span id="S5.F8.5.5.5.5.5.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_o_loGu4viQE_10/original/00017.jpg" id="S5.F8.5.5.5.5.5.5.5.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span>
<span id="S5.F8.6.6.6.6.6.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_o_loGu4viQE_10/original/00027.jpg" id="S5.F8.6.6.6.6.6.6.6.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span></span>
<span id="S5.F8.12.12.12.12.12.12.12" class="ltx_tr">
<span id="S5.F8.12.12.12.12.12.12.12.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" style="padding:-2.5pt 0.0pt;"><span id="S5.F8.12.12.12.12.12.12.12.7.1" class="ltx_text" style="position:relative; bottom:17.0pt;">
<span id="S5.F8.12.12.12.12.12.12.12.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:28pt;vertical-align:6.9pt;"><span class="ltx_transformed_inner" style="width:28.0pt;transform:translate(-10.51pt,-17.46pt) rotate(-90deg) ;">
<span id="S5.F8.12.12.12.12.12.12.12.7.1.1.1" class="ltx_p">Result</span>
</span></span></span></span>
<span id="S5.F8.7.7.7.7.7.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_oPoq6CvPFIU_7/ours/00010.png" id="S5.F8.7.7.7.7.7.7.7.1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span>
<span id="S5.F8.8.8.8.8.8.8.8.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_oPoq6CvPFIU_7/ours/00029.png" id="S5.F8.8.8.8.8.8.8.8.2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span>
<span id="S5.F8.9.9.9.9.9.9.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_WXyGorVqLBA_0/ours/00000.png" id="S5.F8.9.9.9.9.9.9.9.3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span>
<span id="S5.F8.10.10.10.10.10.10.10.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_WXyGorVqLBA_0/ours/00005.png" id="S5.F8.10.10.10.10.10.10.10.4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span>
<span id="S5.F8.11.11.11.11.11.11.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_o_loGu4viQE_10/ours/00017.png" id="S5.F8.11.11.11.11.11.11.11.5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span>
<span id="S5.F8.12.12.12.12.12.12.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 0.0pt;"><img src="/html/2406.14510/assets/Assets/results/stickers/0_o_loGu4viQE_10/ours/00027.png" id="S5.F8.12.12.12.12.12.12.12.6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption"></span></span>
</span>
</span></span></span>
</span></span></span>

<span id="S5.F8.12.13" class="ltx_ERROR undefined">\Description</span>[]</p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>
<span id="S5.F8.14.1" class="ltx_text ltx_font_bold">Stickers results.</span> These examples show the generalization ability of our method. We apply it to removing facial stickers from videos, and show our model successfully removes different stickers from different locations over the face.
</figcaption>
</figure>
<div id="S5.SS4.SSS1.p1" class="ltx_para">
<p id="S5.SS4.SSS1.p1.1" class="ltx_p">To show our method works for a different use-case, we generate a synthetic dataset of videos by applying stickers from the Stickers dataset <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite> on faces from the CelebV-Text <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2023b</a>)</cite> dataset, imitating real facial stickers, tattoos or synthetic features added by social-media apps, and show that our method is able to remove them.
For this task, we detect the face in each frame, and estimate its 3D shape using a trained neural network that predicts the coefficients of the Basel 3D face model <cite class="ltx_cite ltx_citemacro_citep">(Gerig et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>. Then, we project and render the sticker texture on the original frame. For each generated clip, we sample one sticker and place it at a random position in the <math id="S5.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="uv" display="inline"><semantics id="S5.SS4.SSS1.p1.1.m1.1a"><mrow id="S5.SS4.SSS1.p1.1.m1.1.1" xref="S5.SS4.SSS1.p1.1.m1.1.1.cmml"><mi id="S5.SS4.SSS1.p1.1.m1.1.1.2" xref="S5.SS4.SSS1.p1.1.m1.1.1.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.SS4.SSS1.p1.1.m1.1.1.1" xref="S5.SS4.SSS1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS4.SSS1.p1.1.m1.1.1.3" xref="S5.SS4.SSS1.p1.1.m1.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.1.m1.1b"><apply id="S5.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1"><times id="S5.SS4.SSS1.p1.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1.1"></times><ci id="S5.SS4.SSS1.p1.1.m1.1.1.2.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1.2">𝑢</ci><ci id="S5.SS4.SSS1.p1.1.m1.1.1.3.cmml" xref="S5.SS4.SSS1.p1.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.1.m1.1c">uv</annotation></semantics></math> coordinate space, which is mapped to the surface of the face. We generate 1274 clips this way, each 40-frames long.</p>
</div>
<div id="S5.SS4.SSS1.p2" class="ltx_para">
<p id="S5.SS4.SSS1.p2.1" class="ltx_p">After the dataset is created, we train the same image-to-image model over the stickers dataset and run our video editing pipeline using this model over the test videos. Some results are presented in <a href="#S5.F8" title="In 5.4.1. Stickers ‣ 5.4. Generalization ‣ 5. Experiments ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>. As shown, our model is able to seamlessly remove the stickers from the videos.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Limitations</h2>

<figure id="S6.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S6.F9.6" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:93pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.8pt,1.5pt) scale(0.96945,0.96945) ;">
<p id="S6.F9.6.6" class="ltx_p"><span id="S6.F9.6.6.6" class="ltx_text">
<span id="S6.F9.6.6.6.6" class="ltx_inline-block ltx_transformed_outer" style="width:447.3pt;height:95.9pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S6.F9.6.6.6.6.6" class="ltx_p"><span id="S6.F9.6.6.6.6.6.6" class="ltx_text">
<span id="S6.F9.6.6.6.6.6.6.6" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S6.F9.6.6.6.6.6.6.6.7.1" class="ltx_tr">
<span id="S6.F9.6.6.6.6.6.6.6.7.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">Input</span>
<span id="S6.F9.6.6.6.6.6.6.6.7.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">Output</span>
<span id="S6.F9.6.6.6.6.6.6.6.7.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">Input</span>
<span id="S6.F9.6.6.6.6.6.6.6.7.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">Output</span>
<span id="S6.F9.6.6.6.6.6.6.6.7.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">No MM</span>
<span id="S6.F9.6.6.6.6.6.6.6.7.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:-2.5pt 1.0pt;">MM</span></span>
</span>
<span class="ltx_tbody">
<span id="S6.F9.6.6.6.6.6.6.6.6" class="ltx_tr">
<span id="S6.F9.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/limitations/Tw0x_orig_00023.jpg" id="S6.F9.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="99" height="99" alt="Refer to caption"></span>
<span id="S6.F9.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/limitations/Tw0x_ours_00023.png" id="S6.F9.2.2.2.2.2.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="99" height="99" alt="Refer to caption"></span>
<span id="S6.F9.3.3.3.3.3.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/limitations/rbWV_orig_00015.png" id="S6.F9.3.3.3.3.3.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="99" height="99" alt="Refer to caption"></span>
<span id="S6.F9.4.4.4.4.4.4.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/limitations/rbWV_ours_00014.png" id="S6.F9.4.4.4.4.4.4.4.4.4.g1" class="ltx_graphics ltx_img_square" width="99" height="99" alt="Refer to caption"></span>
<span id="S6.F9.5.5.5.5.5.5.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/limitations/rmm_0_no_AD.png" id="S6.F9.5.5.5.5.5.5.5.5.5.g1" class="ltx_graphics ltx_img_square" width="99" height="99" alt="Refer to caption"></span>
<span id="S6.F9.6.6.6.6.6.6.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-2.5pt 1.0pt;"><img src="/html/2406.14510/assets/Assets/limitations/rmm_0_AD.png" id="S6.F9.6.6.6.6.6.6.6.6.6.g1" class="ltx_graphics ltx_img_square" width="99" height="99" alt="Refer to caption"></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>
<span id="S6.F9.8.1" class="ltx_text ltx_font_bold">Limitations.</span> Left: strong reflections. Middle: Dark sunglasses. In these cases, as the eyes are not exposed, the model cannot guess the right eye color. Moreover, our data was not clean and contained sunglasses videos which were treated the same way as eye glasses in the data generation process. Hence, the model generates dark areas instead of eyes for dark sunglasses videos. Right: Eye blurriness that comes from the motion module.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S6.F9.9" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S6.F9.10" class="ltx_p ltx_figure_panel ltx_align_center">[]</p>
</div>
</div>
</figure>
<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Although our method removes glasses reflections in many cases, it struggles with strong reflections, as in the left example in <a href="#S6.F9" title="In 6. Limitations ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>, where the true eyes are never visible.
Additionally, our method struggles with dark sunglasses, as in the middle example in <a href="#S6.F9" title="In 6. Limitations ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>, where identifying eye color is difficult.
Moreover, as current glasses detectors do not differ between eye and sun glasses, our dataset was not clean and contained sunglasses videos as well. As we used the same kind of masks for all our data, if the input is a video with sunglasses, our model often generates dark areas where the eyes should be, as is shown the middle example in <a href="#S6.F9" title="In 6. Limitations ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>. This could be solved by cleaning the data and only training over eye-glasses videos.
Finally, the motion module tends to smooth the frames to get a more temporally consistent result. As a result, our outputs also tend to be a bit blurry, as shown in the right example in <a href="#S6.F9" title="In 6. Limitations ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>. As discussed in <a href="#S4" title="4. Implementation Details ‣ V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, by using only some of the motion layers, we reduce the blurriness to a minimum.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Societal impact</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">While the goal of this work is to allow editing of owned or licensed videos only, we acknowledge the fact it may be misused to altering videos without consent, and contribute to the spread of misinformation. We condemn such usage, and we are actively working on systems that detect synthetic and edited media.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Discussion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We explored the potential of local video editing through learning from imperfect synthetic data without paired data. Our results surpass existing methods, consistently and realistically removing glasses from videos while preserving the individual’s identity.
We focus on the challenging case of removing glasses from videos, however we show our method can also be applied to other local video editing tasks such as removing stickers from faces. We hypothesize it would work similarly for any other local attribute, and we encourage future work to pursue this direction.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Acknowledgements</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">This project was funded in part by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme, grant agreement No. 802774 (iEXTRACT), and by ISF grant number 1337/22.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Avrahami et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Omri Avrahami, Ohad Fried, and Dani Lischinski. 2023.

</span>
<span class="ltx_bibblock">Blended latent diffusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Graphics (TOG)</em> 42, 4 (2023), 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bar-Tal et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al<span id="bib.bib3.3.1" class="ltx_text">.</span> 2024.

</span>
<span class="ltx_bibblock">Lumiere: A space-time diffusion model for video generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.12945</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bar-Tal et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. 2022.

</span>
<span class="ltx_bibblock">Text2live: Text-driven layered image and video editing. In <em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>. Springer, 707–723.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brooks et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2023.

</span>
<span class="ltx_bibblock">Instructpix2pix: Learning to follow image editing instructions. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 18392–18402.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cong et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Bodo Rosenhahn, Tao Xiang, and Sen He. 2024.

</span>
<span class="ltx_bibblock">FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Couairon et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. 2023.

</span>
<span class="ltx_bibblock">DiffEdit: Diffusion-based Semantic Image Editing with Mask Guidance. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">ICLR 2023 (Eleventh International Conference on Learning Representations)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. 2019.

</span>
<span class="ltx_bibblock">Arcface: Additive angular margin loss for deep face recognition. In <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 4690–4699.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerig et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Thomas Gerig, Andreas Morel-Forster, Clemens Blumer, Bernhard Egger, Marcel Luthi, Sandro Schönborn, and Thomas Vetter. 2018.

</span>
<span class="ltx_bibblock">Morphable face models-an open framework. In <em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">2018 13th IEEE international conference on automatic face &amp; gesture recognition (FG 2018)</em>. IEEE, 75–82.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geyer et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. 2023.

</span>
<span class="ltx_bibblock">TokenFlow: Consistent Diffusion Features for Consistent Video Editing. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Bohai Gu, Yongsheng Yu, Heng Fan, and Libo Zhang. 2023.

</span>
<span class="ltx_bibblock">Flow-Guided Diffusion for Video Inpainting.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.15368</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2024.

</span>
<span class="ltx_bibblock">AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hertz et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. 2022.

</span>
<span class="ltx_bibblock">Prompt-to-Prompt Image Editing with Cross-Attention Control. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 33 (2020), 6840–6851.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Belongie (2017)</span>
<span class="ltx_bibblock">
Xun Huang and Serge Belongie. 2017.

</span>
<span class="ltx_bibblock">Arbitrary style transfer in real-time with adaptive instance normalization. In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>. 1501–1510.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kara et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M Rehg, and Pinar Yanardag. 2023.

</span>
<span class="ltx_bibblock">RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.04524</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasten et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. 2021.

</span>
<span class="ltx_bibblock">Layered neural atlases for consistent video editing.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Graphics (TOG)</em> 40, 6 (2021), 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khachatryan et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. 2023.

</span>
<span class="ltx_bibblock">Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 15954–15964.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. 2018.

</span>
<span class="ltx_bibblock">Learning blind video temporal consistency. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision (ECCV)</em>. 170–185.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee and Lai (2020)</span>
<span class="ltx_bibblock">
Yu-Hui Lee and Shang-Hong Lai. 2020.

</span>
<span class="ltx_bibblock">Byeglassesgan: Identity preserving eyeglasses removal for face images. In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIX 16</em>. Springer, 243–258.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Chenyang Lei, Yazhou Xing, and Qifeng Chen. 2020.

</span>
<span class="ltx_bibblock">Blind video temporal consistency via deep video prior.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 33 (2020), 1083–1093.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja Fidler, and Antonio Torralba. 2022a.

</span>
<span class="ltx_bibblock">BigDatasetGAN: Synthesizing ImageNet with pixel-wise annotations. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 21330–21340.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and Ming-Ming Cheng. 2022b.

</span>
<span class="ltx_bibblock">Towards an end-to-end framework for flow-guided video inpainting. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 17562–17571.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T Freeman, and Michael Rubinstein. 2021.

</span>
<span class="ltx_bibblock">Omnimatte: Associating objects and their effects in video. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 4507–4515.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Junfeng Lyu, Zhibo Wang, and Feng Xu. 2022.

</span>
<span class="ltx_bibblock">Portrait eyeglasses and shadow removal by leveraging 3d synthetic data. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 3429–3439.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2021.

</span>
<span class="ltx_bibblock">SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Thao Nguyen, Anh Tran, and Minh Hoai. 2021.

</span>
<span class="ltx_bibblock">Lipstick ain’t enough: Beyond Color Matching for In-the-Wild Makeup Transfer. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nitzan et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Yotam Nitzan, Zongze Wu, Richard Zhang, Eli Shechtman, Daniel Cohen-Or, Taesung Park, and Michaël Gharbi. 2024.

</span>
<span class="ltx_bibblock">Lazy Diffusion Transformer for Interactive Image Editing.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.12382</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peebles et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
William Peebles, Jun-Yan Zhu, Richard Zhang, Antonio Torralba, Alexei A Efros, and Eli Shechtman. 2022.

</span>
<span class="ltx_bibblock">Gan-supervised dense visual alignment. In <em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 13470–13481.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. 2023.

</span>
<span class="ltx_bibblock">Fatezero: Fusing attentions for zero-shot text-based video editing. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 15932–15942.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravuri and Vinyals (2019)</span>
<span class="ltx_bibblock">
Suman Ravuri and Oriol Vinyals. 2019.

</span>
<span class="ltx_bibblock">Classification accuracy score for conditional generative models.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models. In <em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 10684–10695.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sohl-Dickstein et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015.

</span>
<span class="ltx_bibblock">Deep unsupervised learning using nonequilibrium thermodynamics. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 2256–2265.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suhail et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Mohammed Suhail, Erika Lu, Zhengqi Li, Noah Snavely, Leonid Sigal, and Forrester Cole. 2023.

</span>
<span class="ltx_bibblock">Omnimatte3D: Associating Objects and Their Effects in Unconstrained Monocular Video. In <em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 630–639.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teed and Deng (2020)</span>
<span class="ltx_bibblock">
Zachary Teed and Jia Deng. 2020.

</span>
<span class="ltx_bibblock">Raft: Recurrent all-pairs field transforms for optical flow. In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16</em>. Springer, 402–419.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsaban and Passos (2023)</span>
<span class="ltx_bibblock">
Linoy Tsaban and Apolinário Passos. 2023.

</span>
<span class="ltx_bibblock">Ledits: Real image editing with ddpm inversion and semantic guidance.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.00522</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tumanyan et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. 2023.

</span>
<span class="ltx_bibblock">Plug-and-play diffusion features for text-driven image-to-image translation. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 1921–1930.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2023.

</span>
<span class="ltx_bibblock">Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In <em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 7623–7633.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change Loy. 2023.

</span>
<span class="ltx_bibblock">Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation. In <em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">ACM SIGGRAPH Asia Conference Proceedings</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yildirim et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ahmet Burak Yildirim, Vedat Baday, Erkut Erdem, Aykut Erdem, and Aysegul Dundar. 2023.

</span>
<span class="ltx_bibblock">Inst-Inpaint: Instructing to Remove Objects with Diffusion Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03246</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and Wayne Wu. 2023b.

</span>
<span class="ltx_bibblock">CelebV-Text: A Large-Scale Facial Text-Video Dataset. In <em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 14805–14814.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. 2023a.

</span>
<span class="ltx_bibblock">Inpaint anything: Segment anything meets image inpainting.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.06790</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Kaidong Zhang, Jingjing Fu, and Dong Liu. 2022.

</span>
<span class="ltx_bibblock">Flow-guided transformer for video inpainting. In <em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>. Springer, 74–90.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023a.

</span>
<span class="ltx_bibblock">Adding Conditional Control to Text-to-Image Diffusion Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, XIAOPENG ZHANG, Wangmeng Zuo, and Qi Tian. 2023b.

</span>
<span class="ltx_bibblock">ControlVideo: Training-free Controllable Text-to-video Generation. In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, and Fang Wen. 2022.

</span>
<span class="ltx_bibblock">General facial representation learning in a visual-linguistic manner. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 18697–18709.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Shangchen Zhou, Chongyi Li, Kelvin C.K Chan, and Chen Change Loy. 2023.

</span>
<span class="ltx_bibblock">ProPainter: Improving Propagation and Transformer for Video Inpainting. In <em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">Proceedings of IEEE International Conference on Computer Vision (ICCV)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.14509" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.14510" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.14510">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.14510" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.14511" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 18:58:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
