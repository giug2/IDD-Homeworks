<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.11848] How to find a good image-text embedding for remote sensing visual question answering?</title><meta property="og:description" content="Visual question answering (VQA) has recently been introduced to remote sensing to make information extraction from overhead imagery more accessible to everyone. VQA considers a question (in natural language, therefore â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="How to find a good image-text embedding for remote sensing visual question answering?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="How to find a good image-text embedding for remote sensing visual question answering?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.11848">

<!--Generated on Sat Mar  2 03:36:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Earth observation Text-image Models Deep learning Data fusion">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Environmental Computational Science and Earth Observation Laboratory (ECEO), EPFL, Sion, Switzerland </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Laboratoire dâ€™Informatique Paris Descartes (LIPADE), UniversitÃ© de Paris, France </span></span></span><span id="id1a" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span><math id="id1.m1.1" class="ltx_Math" alttext="\mathrm{\Phi}" display="inline"><semantics id="id1.m1.1b"><mi mathvariant="normal" id="id1.m1.1.1" xref="id1.m1.1.1.cmml">Î¦</mi><annotation-xml encoding="MathML-Content" id="id1.m1.1c"><ci id="id1.m1.1.1.cmml" xref="id1.m1.1.1">Î¦</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.m1.1d">\mathrm{\Phi}</annotation></semantics></math>-lab, European Space Research Institute (ESRIN), European Space Agency (ESA), Frascati, Italy</span></span></span>
<h1 class="ltx_title ltx_title_document">How to find a good image-text embedding
<br class="ltx_break">for remote sensing visual question answering?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christel Chappuis
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-9622-5428" title="ORCID identifier" class="ltx_ref">0000-0002-9622-5428</a></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sylvain Lobry
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-4738-2416" title="ORCID identifier" class="ltx_ref">0000-0003-4738-2416</a></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Benjamin Kellenberger
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-2902-2014" title="ORCID identifier" class="ltx_ref">0000-0002-2902-2014</a></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bertrand Le Saux
</span><span class="ltx_author_notes">33
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-7162-6746" title="ORCID identifier" class="ltx_ref">0000-0001-7162-6746</a></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Devis Tuia
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-0374-2459" title="ORCID identifier" class="ltx_ref">0000-0003-0374-2459</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Visual question answering (VQA) has recently been introduced to remote sensing to make information extraction from overhead imagery more accessible to everyone. VQA considers a question (in natural language, therefore easy to formulate) about an image and aims at providing an answer through a model based on computer vision and natural language processing methods. As such, a VQA model needs to jointly consider visual and textual features, which is frequently done through a fusion step. In this work, we study three different fusion methodologies in the context of VQA for remote sensing and analyse the gains in accuracy with respect to the model complexity. Our findings indicate that more complex fusion mechanisms yield an improved performance, yet that seeking a trade-off between model complexity and performance is worthwhile in practice.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Earth observation Text-image Models Deep learning Data fusion
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the last decades, large quantities of information about our planet have been recorded, in particular through Earth observation imagery. With this comes the need to develop appropriate tools to extract useful knowledge, especially in regard to environmental monitoring. However, developing such tools requires a strong technical knowledge (in image processing, machine learning, <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">etc.</em>), which can limit the use of remote sensing imagery for applications with impact on daily life, such as urban planning, agriculture and environment.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recently, a new task to access image information in human language emerged, known as visual question answering (VQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>): here, a model aims at providing an answer to an open-ended question in natural language about an image. Considering the wide variety of images and potential inquiries, the task of VQA is as challenging as appealing. For remote sensing VQA in particular, research is motivated by the prospect of opening the use of remote sensing imagery, making it available as a tool for anyone to benefit from the abundant data generated in Earth observation campaigns. As such, VQA for remote sensing was identified as one of six promising directions in the agenda of AI for Earth Science Data AnalysisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The more often, VQA is based on two data processing streams, one pertaining to image analysis and another focusing on text mining. The feature extractors, generally based on deep learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, are usually separated and combine their outputs in a dedicated fusion step before predicting an answer category (Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ How to find a good image-text embedding for remote sensing visual question answering?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). This text-image embedding fusion step is central to the task as it not only combines the image and question features, but should also uncover interactions between the two modalities. In its simplest form, the fusion mechanism is an element-wise operation. While straightforward, this type of fusion is limiting in term of interactions, as it requires relevant and matching image and question contents to be aligned at the same index in the latent feature vectors prior to fusion.
If such ordering cannot be established, a simple fusion operation that restricts the interactions to the same index might not suffice. Therefore, more complex approaches have been proposedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. While promising in standard VQA benchmarks, these fusion strategies have never been evaluated on remote sensing imagery and their variability. In this paper, we evaluate the effectiveness of these more elaborate fusion strategies when exposed to the variability of remote sensing images. To this end, we build on the RSVQA model and datasets fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and compare three different fusion strategies.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2109.11848/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="123" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Simplified representation of a VQA model for remote sensing.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual question answering.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">The task of free-form and open-ended VQA has first been proposed byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The goal is to answer questions about images, where both questions and answers are formulated in natural language.
The original model ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is composed of two parallel feature extractors followed linearly by a fusion and classification. Follow-up works attempt to condition the model on relevant image and question parts, often by employing attention mechanismsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. A different directive focuses on the integration of external knowledgeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Finally, compositional models have been suggested to translate the question into a series of simpler processing tasks to be solved in a logical sequenceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA for remote sensing.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">The first application of VQA to remote sensing images was introduced byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The architecture proposed is in line withÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and specifically consists of an element-wise multiplication fusion to combine textual and visual features. The authors further published two datasets (â€œRSVQAâ€<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The dataset and the models used inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> are available at <a target="_blank" href="https://rsvqa.sylvainlobry.com" title="" class="ltx_ref ltx_href">rsvqa.sylvainlobry.com</a>.</span></span></span>), consisting of low- (Sentinel-2) and very-high-resolution (aerial) imagery, paired with questions and answers on various tasks (presence/absence classification, object counting, etc.) obtained from OpenStreetMap vector layers.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">Recently, three other remote sensing datasets involving a VQA component have been introduced: FloodNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> focuses on natural disasters and contains images acquired with an unmanned aerial vehicle (UAV) after the passage of hurricane Harvey. RSIVQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is composed of existing classification and object detection datasets. While a few images are annotated by humans for questions/answers, most of the samples had automatically been generated as in the RSVQA dataset. The authors also propose an architecture using a mutual attention and bilinear feature fusion. Finally, RSVQAxBENÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is a large-scale (14+ millions image/question/answer triplets) VQA-tailored derivation of the BigEarthNet classification datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Fusion methods.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">The fusion step in a conventional VQA model combines both image and question modalities and must therefore encode co-dependencies as efficiently as possible. In theory, an outer product between the feature vectors emerging from the image- and question-specific feature extractors would provide a maximum in terms of feature sharing. However, this quickly becomes intractable, as the number of learnable parameters increases drastically in the following fully-connected layers. Hence, a common compromise is to employ a straightforward element-wise operation (as implemented inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>), or a concatenation.
While computationally simple, these approaches rely on the feature extractors and classifier to establish feature correspondences in the latent space during end-to-end training of the model.
More complex methods have been proposed to enable a richer interaction between elements of both modalities while limiting the number of parameters to learn.
The first approach, multimodal compact bilinear pooling (MCBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>), uses a random projection on the image and question features before combining them. As such, it consider only a (random) subset of the complete combinatorial between the two modalities.
Later on,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> suggested a low-rank bilinear model (MLB) where the projection of the modalities is learned, fused with Hadamard product and its output projected to the prediction space.
A factorized bilinear pooling (MFB) was presented byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, building on MLB by adding a sum pooling to condense the output. An extension introduced byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is multimodal factorized high-order pooling (MFH) that applies several MFB to generalize to higher-order pooling (i.e., fusing more than two modalities).
Finally,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> introduced MUTAN, which employs a matrix decomposition (Tucker decomposition) to break down the full fusion into projections specific to each modality and a smaller learned fusion operation.
The idea of tensor decomposition was further researched and improved inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, where authors used a block-term decomposition.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.6" class="ltx_p">Our objective is to assess the required level of complexity at the fusion step to express the interplay between features extracted from the question <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">ğª</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathbf{q}</annotation></semantics></math> and the features extracted from the image <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{v}" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">ğ¯</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ğ¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathbf{v}</annotation></semantics></math> in VQA for remote sensing. Both vectors <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">ğª</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ğª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\mathbf{q}</annotation></semantics></math> and <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{v}" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">ğ¯</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ğ¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\mathbf{v}</annotation></semantics></math>, as well as the resulting vector of the fusion <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{f}" display="inline"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">ğŸ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">ğŸ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">\mathbf{f}</annotation></semantics></math>, are of dimension <math id="S3.p1.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p1.6.m6.1a"><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">n</annotation></semantics></math>.
In this work, we employ MCB and MUTAN as representatives for increasing fusion complexity and compare them with the element-wise multiplication ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.4" class="ltx_p">The element-wise multiplication (<math id="S3.p2.1.m1.2" class="ltx_Math" alttext="f_{i}=v_{i}\times q_{i},\forall i" display="inline"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml"><msub id="S3.p2.1.m1.2.2.4" xref="S3.p2.1.m1.2.2.4.cmml"><mi id="S3.p2.1.m1.2.2.4.2" xref="S3.p2.1.m1.2.2.4.2.cmml">f</mi><mi id="S3.p2.1.m1.2.2.4.3" xref="S3.p2.1.m1.2.2.4.3.cmml">i</mi></msub><mo id="S3.p2.1.m1.2.2.3" xref="S3.p2.1.m1.2.2.3.cmml">=</mo><mrow id="S3.p2.1.m1.2.2.2.2" xref="S3.p2.1.m1.2.2.2.3.cmml"><mrow id="S3.p2.1.m1.1.1.1.1.1" xref="S3.p2.1.m1.1.1.1.1.1.cmml"><msub id="S3.p2.1.m1.1.1.1.1.1.2" xref="S3.p2.1.m1.1.1.1.1.1.2.cmml"><mi id="S3.p2.1.m1.1.1.1.1.1.2.2" xref="S3.p2.1.m1.1.1.1.1.1.2.2.cmml">v</mi><mi id="S3.p2.1.m1.1.1.1.1.1.2.3" xref="S3.p2.1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.p2.1.m1.1.1.1.1.1.1" xref="S3.p2.1.m1.1.1.1.1.1.1.cmml">Ã—</mo><msub id="S3.p2.1.m1.1.1.1.1.1.3" xref="S3.p2.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.p2.1.m1.1.1.1.1.1.3.2" xref="S3.p2.1.m1.1.1.1.1.1.3.2.cmml">q</mi><mi id="S3.p2.1.m1.1.1.1.1.1.3.3" xref="S3.p2.1.m1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.p2.1.m1.2.2.2.2.3" xref="S3.p2.1.m1.2.2.2.3.cmml">,</mo><mrow id="S3.p2.1.m1.2.2.2.2.2" xref="S3.p2.1.m1.2.2.2.2.2.cmml"><mo rspace="0.167em" id="S3.p2.1.m1.2.2.2.2.2.1" xref="S3.p2.1.m1.2.2.2.2.2.1.cmml">âˆ€</mo><mi id="S3.p2.1.m1.2.2.2.2.2.2" xref="S3.p2.1.m1.2.2.2.2.2.2.cmml">i</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><apply id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2"><eq id="S3.p2.1.m1.2.2.3.cmml" xref="S3.p2.1.m1.2.2.3"></eq><apply id="S3.p2.1.m1.2.2.4.cmml" xref="S3.p2.1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.p2.1.m1.2.2.4.1.cmml" xref="S3.p2.1.m1.2.2.4">subscript</csymbol><ci id="S3.p2.1.m1.2.2.4.2.cmml" xref="S3.p2.1.m1.2.2.4.2">ğ‘“</ci><ci id="S3.p2.1.m1.2.2.4.3.cmml" xref="S3.p2.1.m1.2.2.4.3">ğ‘–</ci></apply><list id="S3.p2.1.m1.2.2.2.3.cmml" xref="S3.p2.1.m1.2.2.2.2"><apply id="S3.p2.1.m1.1.1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1.1.1"><times id="S3.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1.1.1.1"></times><apply id="S3.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.p2.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p2.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.p2.1.m1.1.1.1.1.1.2.2">ğ‘£</ci><ci id="S3.p2.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.p2.1.m1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.p2.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.p2.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.p2.1.m1.1.1.1.1.1.3.2">ğ‘</ci><ci id="S3.p2.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.p2.1.m1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><apply id="S3.p2.1.m1.2.2.2.2.2.cmml" xref="S3.p2.1.m1.2.2.2.2.2"><csymbol cd="latexml" id="S3.p2.1.m1.2.2.2.2.2.1.cmml" xref="S3.p2.1.m1.2.2.2.2.2.1">for-all</csymbol><ci id="S3.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.p2.1.m1.2.2.2.2.2.2">ğ‘–</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">f_{i}=v_{i}\times q_{i},\forall i</annotation></semantics></math>) is computationally efficient (<math id="S3.p2.2.m2.1" class="ltx_Math" alttext="\theta(n)" display="inline"><semantics id="S3.p2.2.m2.1a"><mrow id="S3.p2.2.m2.1.2" xref="S3.p2.2.m2.1.2.cmml"><mi id="S3.p2.2.m2.1.2.2" xref="S3.p2.2.m2.1.2.2.cmml">Î¸</mi><mo lspace="0em" rspace="0em" id="S3.p2.2.m2.1.2.1" xref="S3.p2.2.m2.1.2.1.cmml">â€‹</mo><mrow id="S3.p2.2.m2.1.2.3.2" xref="S3.p2.2.m2.1.2.cmml"><mo stretchy="false" id="S3.p2.2.m2.1.2.3.2.1" xref="S3.p2.2.m2.1.2.cmml">(</mo><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">n</mi><mo stretchy="false" id="S3.p2.2.m2.1.2.3.2.2" xref="S3.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.2.cmml" xref="S3.p2.2.m2.1.2"><times id="S3.p2.2.m2.1.2.1.cmml" xref="S3.p2.2.m2.1.2.1"></times><ci id="S3.p2.2.m2.1.2.2.cmml" xref="S3.p2.2.m2.1.2.2">ğœƒ</ci><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\theta(n)</annotation></semantics></math> complexity), but limits the interaction between elements of the textual and visual feature vector, as the <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">i</annotation></semantics></math>th element of the visual vector can only interact with the <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">i</annotation></semantics></math>th element of the textual vector. Moreover, the interaction is unweighted, since there are no additional learnable parameter in the fusion layer.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.23" class="ltx_p">The MCB fusion strategy uses the Count Sketch algorithmÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, a dimensionality reduction technique proposed to estimate the frequencies of items in data streams, to avoid computing the outer product of <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{v}" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">ğ¯</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ğ¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\mathbf{v}</annotation></semantics></math> and <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">ğª</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">ğª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">\mathbf{q}</annotation></semantics></math>. In the fusion step of VQA, a Count Sketch projection is applied to each modality independently. Practically, given a vector <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">\mathbf{x}</annotation></semantics></math> of dimension <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">n</annotation></semantics></math> and its projection <math id="S3.p3.5.m5.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.p3.5.m5.1a"><mi id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><ci id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">\mathbf{y}</annotation></semantics></math> of dimension <math id="S3.p3.6.m6.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.p3.6.m6.1a"><mi id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><ci id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">d</annotation></semantics></math> (<math id="S3.p3.7.m7.1" class="ltx_Math" alttext="d&gt;n" display="inline"><semantics id="S3.p3.7.m7.1a"><mrow id="S3.p3.7.m7.1.1" xref="S3.p3.7.m7.1.1.cmml"><mi id="S3.p3.7.m7.1.1.2" xref="S3.p3.7.m7.1.1.2.cmml">d</mi><mo id="S3.p3.7.m7.1.1.1" xref="S3.p3.7.m7.1.1.1.cmml">&gt;</mo><mi id="S3.p3.7.m7.1.1.3" xref="S3.p3.7.m7.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.7.m7.1b"><apply id="S3.p3.7.m7.1.1.cmml" xref="S3.p3.7.m7.1.1"><gt id="S3.p3.7.m7.1.1.1.cmml" xref="S3.p3.7.m7.1.1.1"></gt><ci id="S3.p3.7.m7.1.1.2.cmml" xref="S3.p3.7.m7.1.1.2">ğ‘‘</ci><ci id="S3.p3.7.m7.1.1.3.cmml" xref="S3.p3.7.m7.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.7.m7.1c">d&gt;n</annotation></semantics></math>), two new vectors of dimension <math id="S3.p3.8.m8.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p3.8.m8.1a"><mi id="S3.p3.8.m8.1.1" xref="S3.p3.8.m8.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p3.8.m8.1b"><ci id="S3.p3.8.m8.1.1.cmml" xref="S3.p3.8.m8.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.8.m8.1c">n</annotation></semantics></math> are created: <math id="S3.p3.9.m9.2" class="ltx_Math" alttext="\mathbf{s}~{}\in\{-1,1\}^{n}" display="inline"><semantics id="S3.p3.9.m9.2a"><mrow id="S3.p3.9.m9.2.2" xref="S3.p3.9.m9.2.2.cmml"><mi id="S3.p3.9.m9.2.2.3" xref="S3.p3.9.m9.2.2.3.cmml">ğ¬</mi><mo lspace="0.608em" id="S3.p3.9.m9.2.2.2" xref="S3.p3.9.m9.2.2.2.cmml">âˆˆ</mo><msup id="S3.p3.9.m9.2.2.1" xref="S3.p3.9.m9.2.2.1.cmml"><mrow id="S3.p3.9.m9.2.2.1.1.1" xref="S3.p3.9.m9.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.p3.9.m9.2.2.1.1.1.2" xref="S3.p3.9.m9.2.2.1.1.2.cmml">{</mo><mrow id="S3.p3.9.m9.2.2.1.1.1.1" xref="S3.p3.9.m9.2.2.1.1.1.1.cmml"><mo id="S3.p3.9.m9.2.2.1.1.1.1a" xref="S3.p3.9.m9.2.2.1.1.1.1.cmml">âˆ’</mo><mn id="S3.p3.9.m9.2.2.1.1.1.1.2" xref="S3.p3.9.m9.2.2.1.1.1.1.2.cmml">1</mn></mrow><mo id="S3.p3.9.m9.2.2.1.1.1.3" xref="S3.p3.9.m9.2.2.1.1.2.cmml">,</mo><mn id="S3.p3.9.m9.1.1" xref="S3.p3.9.m9.1.1.cmml">1</mn><mo stretchy="false" id="S3.p3.9.m9.2.2.1.1.1.4" xref="S3.p3.9.m9.2.2.1.1.2.cmml">}</mo></mrow><mi id="S3.p3.9.m9.2.2.1.3" xref="S3.p3.9.m9.2.2.1.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.9.m9.2b"><apply id="S3.p3.9.m9.2.2.cmml" xref="S3.p3.9.m9.2.2"><in id="S3.p3.9.m9.2.2.2.cmml" xref="S3.p3.9.m9.2.2.2"></in><ci id="S3.p3.9.m9.2.2.3.cmml" xref="S3.p3.9.m9.2.2.3">ğ¬</ci><apply id="S3.p3.9.m9.2.2.1.cmml" xref="S3.p3.9.m9.2.2.1"><csymbol cd="ambiguous" id="S3.p3.9.m9.2.2.1.2.cmml" xref="S3.p3.9.m9.2.2.1">superscript</csymbol><set id="S3.p3.9.m9.2.2.1.1.2.cmml" xref="S3.p3.9.m9.2.2.1.1.1"><apply id="S3.p3.9.m9.2.2.1.1.1.1.cmml" xref="S3.p3.9.m9.2.2.1.1.1.1"><minus id="S3.p3.9.m9.2.2.1.1.1.1.1.cmml" xref="S3.p3.9.m9.2.2.1.1.1.1"></minus><cn type="integer" id="S3.p3.9.m9.2.2.1.1.1.1.2.cmml" xref="S3.p3.9.m9.2.2.1.1.1.1.2">1</cn></apply><cn type="integer" id="S3.p3.9.m9.1.1.cmml" xref="S3.p3.9.m9.1.1">1</cn></set><ci id="S3.p3.9.m9.2.2.1.3.cmml" xref="S3.p3.9.m9.2.2.1.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.9.m9.2c">\mathbf{s}~{}\in\{-1,1\}^{n}</annotation></semantics></math> and <math id="S3.p3.10.m10.2" class="ltx_Math" alttext="\mathbf{h}~{}\in\llbracket 1,d\rrbracket^{n}" display="inline"><semantics id="S3.p3.10.m10.2a"><mrow id="S3.p3.10.m10.2.3" xref="S3.p3.10.m10.2.3.cmml"><mi id="S3.p3.10.m10.2.3.2" xref="S3.p3.10.m10.2.3.2.cmml">ğ¡</mi><mo lspace="0.608em" id="S3.p3.10.m10.2.3.1" xref="S3.p3.10.m10.2.3.1.cmml">âˆˆ</mo><msup id="S3.p3.10.m10.2.3.3" xref="S3.p3.10.m10.2.3.3.cmml"><mrow id="S3.p3.10.m10.2.3.3.2.2" xref="S3.p3.10.m10.2.3.3.2.1.cmml"><mo stretchy="false" id="S3.p3.10.m10.2.3.3.2.2.1" xref="S3.p3.10.m10.2.3.3.2.1.cmml">âŸ¦</mo><mn id="S3.p3.10.m10.1.1" xref="S3.p3.10.m10.1.1.cmml">1</mn><mo id="S3.p3.10.m10.2.3.3.2.2.2" xref="S3.p3.10.m10.2.3.3.2.1.cmml">,</mo><mi id="S3.p3.10.m10.2.2" xref="S3.p3.10.m10.2.2.cmml">d</mi><mo stretchy="false" id="S3.p3.10.m10.2.3.3.2.2.3" xref="S3.p3.10.m10.2.3.3.2.1.cmml">âŸ§</mo></mrow><mi id="S3.p3.10.m10.2.3.3.3" xref="S3.p3.10.m10.2.3.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.10.m10.2b"><apply id="S3.p3.10.m10.2.3.cmml" xref="S3.p3.10.m10.2.3"><in id="S3.p3.10.m10.2.3.1.cmml" xref="S3.p3.10.m10.2.3.1"></in><ci id="S3.p3.10.m10.2.3.2.cmml" xref="S3.p3.10.m10.2.3.2">ğ¡</ci><apply id="S3.p3.10.m10.2.3.3.cmml" xref="S3.p3.10.m10.2.3.3"><csymbol cd="ambiguous" id="S3.p3.10.m10.2.3.3.1.cmml" xref="S3.p3.10.m10.2.3.3">superscript</csymbol><list id="S3.p3.10.m10.2.3.3.2.1.cmml" xref="S3.p3.10.m10.2.3.3.2.2"><cn type="integer" id="S3.p3.10.m10.1.1.cmml" xref="S3.p3.10.m10.1.1">1</cn><ci id="S3.p3.10.m10.2.2.cmml" xref="S3.p3.10.m10.2.2">ğ‘‘</ci></list><ci id="S3.p3.10.m10.2.3.3.3.cmml" xref="S3.p3.10.m10.2.3.3.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.10.m10.2c">\mathbf{h}~{}\in\llbracket 1,d\rrbracket^{n}</annotation></semantics></math>. Each element of <math id="S3.p3.11.m11.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S3.p3.11.m11.1a"><mi id="S3.p3.11.m11.1.1" xref="S3.p3.11.m11.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.p3.11.m11.1b"><ci id="S3.p3.11.m11.1.1.cmml" xref="S3.p3.11.m11.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.11.m11.1c">\mathbf{x}</annotation></semantics></math> is assigned with a sign value from <math id="S3.p3.12.m12.1" class="ltx_Math" alttext="\mathbf{s}" display="inline"><semantics id="S3.p3.12.m12.1a"><mi id="S3.p3.12.m12.1.1" xref="S3.p3.12.m12.1.1.cmml">ğ¬</mi><annotation-xml encoding="MathML-Content" id="S3.p3.12.m12.1b"><ci id="S3.p3.12.m12.1.1.cmml" xref="S3.p3.12.m12.1.1">ğ¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.12.m12.1c">\mathbf{s}</annotation></semantics></math> and a position in the projected vector <math id="S3.p3.13.m13.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.p3.13.m13.1a"><mi id="S3.p3.13.m13.1.1" xref="S3.p3.13.m13.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.p3.13.m13.1b"><ci id="S3.p3.13.m13.1.1.cmml" xref="S3.p3.13.m13.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.13.m13.1c">\mathbf{y}</annotation></semantics></math> from <math id="S3.p3.14.m14.1" class="ltx_Math" alttext="\mathbf{h}" display="inline"><semantics id="S3.p3.14.m14.1a"><mi id="S3.p3.14.m14.1.1" xref="S3.p3.14.m14.1.1.cmml">ğ¡</mi><annotation-xml encoding="MathML-Content" id="S3.p3.14.m14.1b"><ci id="S3.p3.14.m14.1.1.cmml" xref="S3.p3.14.m14.1.1">ğ¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.14.m14.1c">\mathbf{h}</annotation></semantics></math> (and the other <math id="S3.p3.15.m15.1" class="ltx_Math" alttext="d-n" display="inline"><semantics id="S3.p3.15.m15.1a"><mrow id="S3.p3.15.m15.1.1" xref="S3.p3.15.m15.1.1.cmml"><mi id="S3.p3.15.m15.1.1.2" xref="S3.p3.15.m15.1.1.2.cmml">d</mi><mo id="S3.p3.15.m15.1.1.1" xref="S3.p3.15.m15.1.1.1.cmml">âˆ’</mo><mi id="S3.p3.15.m15.1.1.3" xref="S3.p3.15.m15.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.15.m15.1b"><apply id="S3.p3.15.m15.1.1.cmml" xref="S3.p3.15.m15.1.1"><minus id="S3.p3.15.m15.1.1.1.cmml" xref="S3.p3.15.m15.1.1.1"></minus><ci id="S3.p3.15.m15.1.1.2.cmml" xref="S3.p3.15.m15.1.1.2">ğ‘‘</ci><ci id="S3.p3.15.m15.1.1.3.cmml" xref="S3.p3.15.m15.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.15.m15.1c">d-n</annotation></semantics></math> elements of <math id="S3.p3.16.m16.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.p3.16.m16.1a"><mi id="S3.p3.16.m16.1.1" xref="S3.p3.16.m16.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.p3.16.m16.1b"><ci id="S3.p3.16.m16.1.1.cmml" xref="S3.p3.16.m16.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.16.m16.1c">\mathbf{y}</annotation></semantics></math> are set to 0). Randomly sampled from a uniform distribution, <math id="S3.p3.17.m17.1" class="ltx_Math" alttext="\mathbf{s}" display="inline"><semantics id="S3.p3.17.m17.1a"><mi id="S3.p3.17.m17.1.1" xref="S3.p3.17.m17.1.1.cmml">ğ¬</mi><annotation-xml encoding="MathML-Content" id="S3.p3.17.m17.1b"><ci id="S3.p3.17.m17.1.1.cmml" xref="S3.p3.17.m17.1.1">ğ¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.17.m17.1c">\mathbf{s}</annotation></semantics></math> and <math id="S3.p3.18.m18.1" class="ltx_Math" alttext="\mathbf{h}" display="inline"><semantics id="S3.p3.18.m18.1a"><mi id="S3.p3.18.m18.1.1" xref="S3.p3.18.m18.1.1.cmml">ğ¡</mi><annotation-xml encoding="MathML-Content" id="S3.p3.18.m18.1b"><ci id="S3.p3.18.m18.1.1.cmml" xref="S3.p3.18.m18.1.1">ğ¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.18.m18.1c">\mathbf{h}</annotation></semantics></math> are fixed at initialization and remain unchanged during the training. Once projected with Count Sketch, the two resulting vectors of <math id="S3.p3.19.m19.1" class="ltx_Math" alttext="\mathbf{v}" display="inline"><semantics id="S3.p3.19.m19.1a"><mi id="S3.p3.19.m19.1.1" xref="S3.p3.19.m19.1.1.cmml">ğ¯</mi><annotation-xml encoding="MathML-Content" id="S3.p3.19.m19.1b"><ci id="S3.p3.19.m19.1.1.cmml" xref="S3.p3.19.m19.1.1">ğ¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.19.m19.1c">\mathbf{v}</annotation></semantics></math> and <math id="S3.p3.20.m20.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.p3.20.m20.1a"><mi id="S3.p3.20.m20.1.1" xref="S3.p3.20.m20.1.1.cmml">ğª</mi><annotation-xml encoding="MathML-Content" id="S3.p3.20.m20.1b"><ci id="S3.p3.20.m20.1.1.cmml" xref="S3.p3.20.m20.1.1">ğª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.20.m20.1c">\mathbf{q}</annotation></semantics></math> are combined with a convolution (for efficiency, MCB performs an element-wise product in the frequency domain to this end). Although this approach enables more interactions between the feature vectors, they are random, fixed and determined by the Count Sketch projection and its vectors <math id="S3.p3.21.m21.1" class="ltx_Math" alttext="\mathbf{s}" display="inline"><semantics id="S3.p3.21.m21.1a"><mi id="S3.p3.21.m21.1.1" xref="S3.p3.21.m21.1.1.cmml">ğ¬</mi><annotation-xml encoding="MathML-Content" id="S3.p3.21.m21.1b"><ci id="S3.p3.21.m21.1.1.cmml" xref="S3.p3.21.m21.1.1">ğ¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.21.m21.1c">\mathbf{s}</annotation></semantics></math> and <math id="S3.p3.22.m22.1" class="ltx_Math" alttext="\mathbf{h}" display="inline"><semantics id="S3.p3.22.m22.1a"><mi id="S3.p3.22.m22.1.1" xref="S3.p3.22.m22.1.1.cmml">ğ¡</mi><annotation-xml encoding="MathML-Content" id="S3.p3.22.m22.1b"><ci id="S3.p3.22.m22.1.1.cmml" xref="S3.p3.22.m22.1.1">ğ¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.22.m22.1c">\mathbf{h}</annotation></semantics></math>. Thus, technically, the fusion itself is not learnt. Moreover, the optimal size of the output dimension <math id="S3.p3.23.m23.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.p3.23.m23.1a"><mi id="S3.p3.23.m23.1.1" xref="S3.p3.23.m23.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p3.23.m23.1b"><ci id="S3.p3.23.m23.1.1.cmml" xref="S3.p3.23.m23.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.23.m23.1c">d</annotation></semantics></math> is problem-specific and typically rather large, leading to a large input to the fully connected layer between fusion and classification parts.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.5" class="ltx_p">The last method, MUTAN, considers the fully-parameterized 3D tensor operator of a bilinear model and performs a Tucker decompositionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> (described inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> as a â€œhigher-order form of principal component analysisâ€) to reduce its size. The 3D tensor operator <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{T}" display="inline"><semantics id="S3.p4.1.m1.1a"><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">ğ“</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">ğ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">\mathbf{T}</annotation></semantics></math> is factorized into 3 matrices <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{W_{q}}" display="inline"><semantics id="S3.p4.2.m2.1a"><msub id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mi id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">ğ–</mi><mi id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml">ğª</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.1.cmml" xref="S3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2">ğ–</ci><ci id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3">ğª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">\mathbf{W_{q}}</annotation></semantics></math>, <math id="S3.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{W_{v}}" display="inline"><semantics id="S3.p4.3.m3.1a"><msub id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml"><mi id="S3.p4.3.m3.1.1.2" xref="S3.p4.3.m3.1.1.2.cmml">ğ–</mi><mi id="S3.p4.3.m3.1.1.3" xref="S3.p4.3.m3.1.1.3.cmml">ğ¯</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><apply id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p4.3.m3.1.1.1.cmml" xref="S3.p4.3.m3.1.1">subscript</csymbol><ci id="S3.p4.3.m3.1.1.2.cmml" xref="S3.p4.3.m3.1.1.2">ğ–</ci><ci id="S3.p4.3.m3.1.1.3.cmml" xref="S3.p4.3.m3.1.1.3">ğ¯</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">\mathbf{W_{v}}</annotation></semantics></math>, and <math id="S3.p4.4.m4.1" class="ltx_Math" alttext="\mathbf{W_{o}}" display="inline"><semantics id="S3.p4.4.m4.1a"><msub id="S3.p4.4.m4.1.1" xref="S3.p4.4.m4.1.1.cmml"><mi id="S3.p4.4.m4.1.1.2" xref="S3.p4.4.m4.1.1.2.cmml">ğ–</mi><mi id="S3.p4.4.m4.1.1.3" xref="S3.p4.4.m4.1.1.3.cmml">ğ¨</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.4.m4.1b"><apply id="S3.p4.4.m4.1.1.cmml" xref="S3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p4.4.m4.1.1.1.cmml" xref="S3.p4.4.m4.1.1">subscript</csymbol><ci id="S3.p4.4.m4.1.1.2.cmml" xref="S3.p4.4.m4.1.1.2">ğ–</ci><ci id="S3.p4.4.m4.1.1.3.cmml" xref="S3.p4.4.m4.1.1.3">ğ¨</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m4.1c">\mathbf{W_{o}}</annotation></semantics></math> and one smaller core tensor <math id="S3.p4.5.m5.1" class="ltx_Math" alttext="\mathbf{T_{c}}" display="inline"><semantics id="S3.p4.5.m5.1a"><msub id="S3.p4.5.m5.1.1" xref="S3.p4.5.m5.1.1.cmml"><mi id="S3.p4.5.m5.1.1.2" xref="S3.p4.5.m5.1.1.2.cmml">ğ“</mi><mi id="S3.p4.5.m5.1.1.3" xref="S3.p4.5.m5.1.1.3.cmml">ğœ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.5.m5.1b"><apply id="S3.p4.5.m5.1.1.cmml" xref="S3.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p4.5.m5.1.1.1.cmml" xref="S3.p4.5.m5.1.1">subscript</csymbol><ci id="S3.p4.5.m5.1.1.2.cmml" xref="S3.p4.5.m5.1.1.2">ğ“</ci><ci id="S3.p4.5.m5.1.1.3.cmml" xref="S3.p4.5.m5.1.1.3">ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.5.m5.1c">\mathbf{T_{c}}</annotation></semantics></math> (FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3 Methods â€£ How to find a good image-text embedding for remote sensing visual question answering?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). With this decomposition, each factor matrix is given a distinct role in the fusion and can regulate the complexity for its specific modality. The Tucker fusion can be written as:</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.1" class="ltx_Math" alttext="\mathbf{y}=((\mathbf{T_{c}}\times(\mathbf{q}^{T}\mathbf{W_{q}}))\times(\mathbf{v}^{T}\mathbf{W_{v}}))\times\mathbf{W_{o}}," display="block"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.3.cmml">ğ²</mi><mo id="S3.Ex1.m1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">ğ“</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">ğœ</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">Ã—</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msup id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğª</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">ğ–</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">ğª</mi></msub></mrow><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml">Ã—</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.cmml"><msup id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.cmml">ğ¯</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml">â€‹</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.2.cmml">ğ–</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.3.cmml">ğ¯</mi></msub></mrow><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.2.cmml">Ã—</mo><msub id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.3.2.cmml">ğ–</mi><mi id="S3.Ex1.m1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.3.3.cmml">ğ¨</mi></msub></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><eq id="S3.Ex1.m1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2"></eq><ci id="S3.Ex1.m1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3">ğ²</ci><apply id="S3.Ex1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2"></times><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3"></times><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ“</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">ğœ</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">ğª</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">ğ‘‡</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ–</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">ğª</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1"><times id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.1"></times><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2">superscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.2">ğ¯</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.2.3">ğ‘‡</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.2">ğ–</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.1.3.3">ğ¯</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.2">ğ–</ci><ci id="S3.Ex1.m1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.3">ğ¨</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\mathbf{y}=((\mathbf{T_{c}}\times(\mathbf{q}^{T}\mathbf{W_{q}}))\times(\mathbf{v}^{T}\mathbf{W_{v}}))\times\mathbf{W_{o}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.p4.13" class="ltx_p">where <math id="S3.p4.6.m1.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.p4.6.m1.1a"><mi id="S3.p4.6.m1.1.1" xref="S3.p4.6.m1.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.p4.6.m1.1b"><ci id="S3.p4.6.m1.1.1.cmml" xref="S3.p4.6.m1.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.6.m1.1c">\mathbf{y}</annotation></semantics></math> is the output vector and the core tensor <math id="S3.p4.7.m2.1" class="ltx_Math" alttext="\mathbf{T_{c}}" display="inline"><semantics id="S3.p4.7.m2.1a"><msub id="S3.p4.7.m2.1.1" xref="S3.p4.7.m2.1.1.cmml"><mi id="S3.p4.7.m2.1.1.2" xref="S3.p4.7.m2.1.1.2.cmml">ğ“</mi><mi id="S3.p4.7.m2.1.1.3" xref="S3.p4.7.m2.1.1.3.cmml">ğœ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.7.m2.1b"><apply id="S3.p4.7.m2.1.1.cmml" xref="S3.p4.7.m2.1.1"><csymbol cd="ambiguous" id="S3.p4.7.m2.1.1.1.cmml" xref="S3.p4.7.m2.1.1">subscript</csymbol><ci id="S3.p4.7.m2.1.1.2.cmml" xref="S3.p4.7.m2.1.1.2">ğ“</ci><ci id="S3.p4.7.m2.1.1.3.cmml" xref="S3.p4.7.m2.1.1.3">ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.7.m2.1c">\mathbf{T_{c}}</annotation></semantics></math> is further controlled with a structured sparsity constraint in MUTAN. Two operations are performed first: <math id="S3.p4.8.m3.2" class="ltx_Math" alttext="\tilde{\mathbf{q}}=\tanh(\mathbf{q}^{T}\mathbf{W_{q}})" display="inline"><semantics id="S3.p4.8.m3.2a"><mrow id="S3.p4.8.m3.2.2" xref="S3.p4.8.m3.2.2.cmml"><mover accent="true" id="S3.p4.8.m3.2.2.3" xref="S3.p4.8.m3.2.2.3.cmml"><mi id="S3.p4.8.m3.2.2.3.2" xref="S3.p4.8.m3.2.2.3.2.cmml">ğª</mi><mo id="S3.p4.8.m3.2.2.3.1" xref="S3.p4.8.m3.2.2.3.1.cmml">~</mo></mover><mo id="S3.p4.8.m3.2.2.2" xref="S3.p4.8.m3.2.2.2.cmml">=</mo><mrow id="S3.p4.8.m3.2.2.1.1" xref="S3.p4.8.m3.2.2.1.2.cmml"><mi id="S3.p4.8.m3.1.1" xref="S3.p4.8.m3.1.1.cmml">tanh</mi><mo id="S3.p4.8.m3.2.2.1.1a" xref="S3.p4.8.m3.2.2.1.2.cmml">â¡</mo><mrow id="S3.p4.8.m3.2.2.1.1.1" xref="S3.p4.8.m3.2.2.1.2.cmml"><mo stretchy="false" id="S3.p4.8.m3.2.2.1.1.1.2" xref="S3.p4.8.m3.2.2.1.2.cmml">(</mo><mrow id="S3.p4.8.m3.2.2.1.1.1.1" xref="S3.p4.8.m3.2.2.1.1.1.1.cmml"><msup id="S3.p4.8.m3.2.2.1.1.1.1.2" xref="S3.p4.8.m3.2.2.1.1.1.1.2.cmml"><mi id="S3.p4.8.m3.2.2.1.1.1.1.2.2" xref="S3.p4.8.m3.2.2.1.1.1.1.2.2.cmml">ğª</mi><mi id="S3.p4.8.m3.2.2.1.1.1.1.2.3" xref="S3.p4.8.m3.2.2.1.1.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.p4.8.m3.2.2.1.1.1.1.1" xref="S3.p4.8.m3.2.2.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.p4.8.m3.2.2.1.1.1.1.3" xref="S3.p4.8.m3.2.2.1.1.1.1.3.cmml"><mi id="S3.p4.8.m3.2.2.1.1.1.1.3.2" xref="S3.p4.8.m3.2.2.1.1.1.1.3.2.cmml">ğ–</mi><mi id="S3.p4.8.m3.2.2.1.1.1.1.3.3" xref="S3.p4.8.m3.2.2.1.1.1.1.3.3.cmml">ğª</mi></msub></mrow><mo stretchy="false" id="S3.p4.8.m3.2.2.1.1.1.3" xref="S3.p4.8.m3.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.8.m3.2b"><apply id="S3.p4.8.m3.2.2.cmml" xref="S3.p4.8.m3.2.2"><eq id="S3.p4.8.m3.2.2.2.cmml" xref="S3.p4.8.m3.2.2.2"></eq><apply id="S3.p4.8.m3.2.2.3.cmml" xref="S3.p4.8.m3.2.2.3"><ci id="S3.p4.8.m3.2.2.3.1.cmml" xref="S3.p4.8.m3.2.2.3.1">~</ci><ci id="S3.p4.8.m3.2.2.3.2.cmml" xref="S3.p4.8.m3.2.2.3.2">ğª</ci></apply><apply id="S3.p4.8.m3.2.2.1.2.cmml" xref="S3.p4.8.m3.2.2.1.1"><tanh id="S3.p4.8.m3.1.1.cmml" xref="S3.p4.8.m3.1.1"></tanh><apply id="S3.p4.8.m3.2.2.1.1.1.1.cmml" xref="S3.p4.8.m3.2.2.1.1.1.1"><times id="S3.p4.8.m3.2.2.1.1.1.1.1.cmml" xref="S3.p4.8.m3.2.2.1.1.1.1.1"></times><apply id="S3.p4.8.m3.2.2.1.1.1.1.2.cmml" xref="S3.p4.8.m3.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p4.8.m3.2.2.1.1.1.1.2.1.cmml" xref="S3.p4.8.m3.2.2.1.1.1.1.2">superscript</csymbol><ci id="S3.p4.8.m3.2.2.1.1.1.1.2.2.cmml" xref="S3.p4.8.m3.2.2.1.1.1.1.2.2">ğª</ci><ci id="S3.p4.8.m3.2.2.1.1.1.1.2.3.cmml" xref="S3.p4.8.m3.2.2.1.1.1.1.2.3">ğ‘‡</ci></apply><apply id="S3.p4.8.m3.2.2.1.1.1.1.3.cmml" xref="S3.p4.8.m3.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p4.8.m3.2.2.1.1.1.1.3.1.cmml" xref="S3.p4.8.m3.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.p4.8.m3.2.2.1.1.1.1.3.2.cmml" xref="S3.p4.8.m3.2.2.1.1.1.1.3.2">ğ–</ci><ci id="S3.p4.8.m3.2.2.1.1.1.1.3.3.cmml" xref="S3.p4.8.m3.2.2.1.1.1.1.3.3">ğª</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.8.m3.2c">\tilde{\mathbf{q}}=\tanh(\mathbf{q}^{T}\mathbf{W_{q}})</annotation></semantics></math> and <math id="S3.p4.9.m4.2" class="ltx_Math" alttext="\tilde{\mathbf{v}}=\tanh(\mathbf{v}^{T}\mathbf{W_{v}})" display="inline"><semantics id="S3.p4.9.m4.2a"><mrow id="S3.p4.9.m4.2.2" xref="S3.p4.9.m4.2.2.cmml"><mover accent="true" id="S3.p4.9.m4.2.2.3" xref="S3.p4.9.m4.2.2.3.cmml"><mi id="S3.p4.9.m4.2.2.3.2" xref="S3.p4.9.m4.2.2.3.2.cmml">ğ¯</mi><mo id="S3.p4.9.m4.2.2.3.1" xref="S3.p4.9.m4.2.2.3.1.cmml">~</mo></mover><mo id="S3.p4.9.m4.2.2.2" xref="S3.p4.9.m4.2.2.2.cmml">=</mo><mrow id="S3.p4.9.m4.2.2.1.1" xref="S3.p4.9.m4.2.2.1.2.cmml"><mi id="S3.p4.9.m4.1.1" xref="S3.p4.9.m4.1.1.cmml">tanh</mi><mo id="S3.p4.9.m4.2.2.1.1a" xref="S3.p4.9.m4.2.2.1.2.cmml">â¡</mo><mrow id="S3.p4.9.m4.2.2.1.1.1" xref="S3.p4.9.m4.2.2.1.2.cmml"><mo stretchy="false" id="S3.p4.9.m4.2.2.1.1.1.2" xref="S3.p4.9.m4.2.2.1.2.cmml">(</mo><mrow id="S3.p4.9.m4.2.2.1.1.1.1" xref="S3.p4.9.m4.2.2.1.1.1.1.cmml"><msup id="S3.p4.9.m4.2.2.1.1.1.1.2" xref="S3.p4.9.m4.2.2.1.1.1.1.2.cmml"><mi id="S3.p4.9.m4.2.2.1.1.1.1.2.2" xref="S3.p4.9.m4.2.2.1.1.1.1.2.2.cmml">ğ¯</mi><mi id="S3.p4.9.m4.2.2.1.1.1.1.2.3" xref="S3.p4.9.m4.2.2.1.1.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.p4.9.m4.2.2.1.1.1.1.1" xref="S3.p4.9.m4.2.2.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.p4.9.m4.2.2.1.1.1.1.3" xref="S3.p4.9.m4.2.2.1.1.1.1.3.cmml"><mi id="S3.p4.9.m4.2.2.1.1.1.1.3.2" xref="S3.p4.9.m4.2.2.1.1.1.1.3.2.cmml">ğ–</mi><mi id="S3.p4.9.m4.2.2.1.1.1.1.3.3" xref="S3.p4.9.m4.2.2.1.1.1.1.3.3.cmml">ğ¯</mi></msub></mrow><mo stretchy="false" id="S3.p4.9.m4.2.2.1.1.1.3" xref="S3.p4.9.m4.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.9.m4.2b"><apply id="S3.p4.9.m4.2.2.cmml" xref="S3.p4.9.m4.2.2"><eq id="S3.p4.9.m4.2.2.2.cmml" xref="S3.p4.9.m4.2.2.2"></eq><apply id="S3.p4.9.m4.2.2.3.cmml" xref="S3.p4.9.m4.2.2.3"><ci id="S3.p4.9.m4.2.2.3.1.cmml" xref="S3.p4.9.m4.2.2.3.1">~</ci><ci id="S3.p4.9.m4.2.2.3.2.cmml" xref="S3.p4.9.m4.2.2.3.2">ğ¯</ci></apply><apply id="S3.p4.9.m4.2.2.1.2.cmml" xref="S3.p4.9.m4.2.2.1.1"><tanh id="S3.p4.9.m4.1.1.cmml" xref="S3.p4.9.m4.1.1"></tanh><apply id="S3.p4.9.m4.2.2.1.1.1.1.cmml" xref="S3.p4.9.m4.2.2.1.1.1.1"><times id="S3.p4.9.m4.2.2.1.1.1.1.1.cmml" xref="S3.p4.9.m4.2.2.1.1.1.1.1"></times><apply id="S3.p4.9.m4.2.2.1.1.1.1.2.cmml" xref="S3.p4.9.m4.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p4.9.m4.2.2.1.1.1.1.2.1.cmml" xref="S3.p4.9.m4.2.2.1.1.1.1.2">superscript</csymbol><ci id="S3.p4.9.m4.2.2.1.1.1.1.2.2.cmml" xref="S3.p4.9.m4.2.2.1.1.1.1.2.2">ğ¯</ci><ci id="S3.p4.9.m4.2.2.1.1.1.1.2.3.cmml" xref="S3.p4.9.m4.2.2.1.1.1.1.2.3">ğ‘‡</ci></apply><apply id="S3.p4.9.m4.2.2.1.1.1.1.3.cmml" xref="S3.p4.9.m4.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p4.9.m4.2.2.1.1.1.1.3.1.cmml" xref="S3.p4.9.m4.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.p4.9.m4.2.2.1.1.1.1.3.2.cmml" xref="S3.p4.9.m4.2.2.1.1.1.1.3.2">ğ–</ci><ci id="S3.p4.9.m4.2.2.1.1.1.1.3.3.cmml" xref="S3.p4.9.m4.2.2.1.1.1.1.3.3">ğ¯</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.9.m4.2c">\tilde{\mathbf{v}}=\tanh(\mathbf{v}^{T}\mathbf{W_{v}})</annotation></semantics></math>. By combining them, the latent pair representation can be defined as <math id="S3.p4.10.m5.1" class="ltx_Math" alttext="\mathbf{z}=(\mathbf{T_{c}}\times\tilde{\mathbf{q}})\times\tilde{\mathbf{v}}" display="inline"><semantics id="S3.p4.10.m5.1a"><mrow id="S3.p4.10.m5.1.1" xref="S3.p4.10.m5.1.1.cmml"><mi id="S3.p4.10.m5.1.1.3" xref="S3.p4.10.m5.1.1.3.cmml">ğ³</mi><mo id="S3.p4.10.m5.1.1.2" xref="S3.p4.10.m5.1.1.2.cmml">=</mo><mrow id="S3.p4.10.m5.1.1.1" xref="S3.p4.10.m5.1.1.1.cmml"><mrow id="S3.p4.10.m5.1.1.1.1.1" xref="S3.p4.10.m5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p4.10.m5.1.1.1.1.1.2" xref="S3.p4.10.m5.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p4.10.m5.1.1.1.1.1.1" xref="S3.p4.10.m5.1.1.1.1.1.1.cmml"><msub id="S3.p4.10.m5.1.1.1.1.1.1.2" xref="S3.p4.10.m5.1.1.1.1.1.1.2.cmml"><mi id="S3.p4.10.m5.1.1.1.1.1.1.2.2" xref="S3.p4.10.m5.1.1.1.1.1.1.2.2.cmml">ğ“</mi><mi id="S3.p4.10.m5.1.1.1.1.1.1.2.3" xref="S3.p4.10.m5.1.1.1.1.1.1.2.3.cmml">ğœ</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.p4.10.m5.1.1.1.1.1.1.1" xref="S3.p4.10.m5.1.1.1.1.1.1.1.cmml">Ã—</mo><mover accent="true" id="S3.p4.10.m5.1.1.1.1.1.1.3" xref="S3.p4.10.m5.1.1.1.1.1.1.3.cmml"><mi id="S3.p4.10.m5.1.1.1.1.1.1.3.2" xref="S3.p4.10.m5.1.1.1.1.1.1.3.2.cmml">ğª</mi><mo id="S3.p4.10.m5.1.1.1.1.1.1.3.1" xref="S3.p4.10.m5.1.1.1.1.1.1.3.1.cmml">~</mo></mover></mrow><mo rspace="0.055em" stretchy="false" id="S3.p4.10.m5.1.1.1.1.1.3" xref="S3.p4.10.m5.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.p4.10.m5.1.1.1.2" xref="S3.p4.10.m5.1.1.1.2.cmml">Ã—</mo><mover accent="true" id="S3.p4.10.m5.1.1.1.3" xref="S3.p4.10.m5.1.1.1.3.cmml"><mi id="S3.p4.10.m5.1.1.1.3.2" xref="S3.p4.10.m5.1.1.1.3.2.cmml">ğ¯</mi><mo id="S3.p4.10.m5.1.1.1.3.1" xref="S3.p4.10.m5.1.1.1.3.1.cmml">~</mo></mover></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.10.m5.1b"><apply id="S3.p4.10.m5.1.1.cmml" xref="S3.p4.10.m5.1.1"><eq id="S3.p4.10.m5.1.1.2.cmml" xref="S3.p4.10.m5.1.1.2"></eq><ci id="S3.p4.10.m5.1.1.3.cmml" xref="S3.p4.10.m5.1.1.3">ğ³</ci><apply id="S3.p4.10.m5.1.1.1.cmml" xref="S3.p4.10.m5.1.1.1"><times id="S3.p4.10.m5.1.1.1.2.cmml" xref="S3.p4.10.m5.1.1.1.2"></times><apply id="S3.p4.10.m5.1.1.1.1.1.1.cmml" xref="S3.p4.10.m5.1.1.1.1.1"><times id="S3.p4.10.m5.1.1.1.1.1.1.1.cmml" xref="S3.p4.10.m5.1.1.1.1.1.1.1"></times><apply id="S3.p4.10.m5.1.1.1.1.1.1.2.cmml" xref="S3.p4.10.m5.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p4.10.m5.1.1.1.1.1.1.2.1.cmml" xref="S3.p4.10.m5.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p4.10.m5.1.1.1.1.1.1.2.2.cmml" xref="S3.p4.10.m5.1.1.1.1.1.1.2.2">ğ“</ci><ci id="S3.p4.10.m5.1.1.1.1.1.1.2.3.cmml" xref="S3.p4.10.m5.1.1.1.1.1.1.2.3">ğœ</ci></apply><apply id="S3.p4.10.m5.1.1.1.1.1.1.3.cmml" xref="S3.p4.10.m5.1.1.1.1.1.1.3"><ci id="S3.p4.10.m5.1.1.1.1.1.1.3.1.cmml" xref="S3.p4.10.m5.1.1.1.1.1.1.3.1">~</ci><ci id="S3.p4.10.m5.1.1.1.1.1.1.3.2.cmml" xref="S3.p4.10.m5.1.1.1.1.1.1.3.2">ğª</ci></apply></apply><apply id="S3.p4.10.m5.1.1.1.3.cmml" xref="S3.p4.10.m5.1.1.1.3"><ci id="S3.p4.10.m5.1.1.1.3.1.cmml" xref="S3.p4.10.m5.1.1.1.3.1">~</ci><ci id="S3.p4.10.m5.1.1.1.3.2.cmml" xref="S3.p4.10.m5.1.1.1.3.2">ğ¯</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.10.m5.1c">\mathbf{z}=(\mathbf{T_{c}}\times\tilde{\mathbf{q}})\times\tilde{\mathbf{v}}</annotation></semantics></math>, while the output projected in the prediction space is: <math id="S3.p4.11.m6.1" class="ltx_Math" alttext="\mathbf{y}=\mathbf{z}^{T}\mathbf{W_{o}}" display="inline"><semantics id="S3.p4.11.m6.1a"><mrow id="S3.p4.11.m6.1.1" xref="S3.p4.11.m6.1.1.cmml"><mi id="S3.p4.11.m6.1.1.2" xref="S3.p4.11.m6.1.1.2.cmml">ğ²</mi><mo id="S3.p4.11.m6.1.1.1" xref="S3.p4.11.m6.1.1.1.cmml">=</mo><mrow id="S3.p4.11.m6.1.1.3" xref="S3.p4.11.m6.1.1.3.cmml"><msup id="S3.p4.11.m6.1.1.3.2" xref="S3.p4.11.m6.1.1.3.2.cmml"><mi id="S3.p4.11.m6.1.1.3.2.2" xref="S3.p4.11.m6.1.1.3.2.2.cmml">ğ³</mi><mi id="S3.p4.11.m6.1.1.3.2.3" xref="S3.p4.11.m6.1.1.3.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.p4.11.m6.1.1.3.1" xref="S3.p4.11.m6.1.1.3.1.cmml">â€‹</mo><msub id="S3.p4.11.m6.1.1.3.3" xref="S3.p4.11.m6.1.1.3.3.cmml"><mi id="S3.p4.11.m6.1.1.3.3.2" xref="S3.p4.11.m6.1.1.3.3.2.cmml">ğ–</mi><mi id="S3.p4.11.m6.1.1.3.3.3" xref="S3.p4.11.m6.1.1.3.3.3.cmml">ğ¨</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.11.m6.1b"><apply id="S3.p4.11.m6.1.1.cmml" xref="S3.p4.11.m6.1.1"><eq id="S3.p4.11.m6.1.1.1.cmml" xref="S3.p4.11.m6.1.1.1"></eq><ci id="S3.p4.11.m6.1.1.2.cmml" xref="S3.p4.11.m6.1.1.2">ğ²</ci><apply id="S3.p4.11.m6.1.1.3.cmml" xref="S3.p4.11.m6.1.1.3"><times id="S3.p4.11.m6.1.1.3.1.cmml" xref="S3.p4.11.m6.1.1.3.1"></times><apply id="S3.p4.11.m6.1.1.3.2.cmml" xref="S3.p4.11.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.p4.11.m6.1.1.3.2.1.cmml" xref="S3.p4.11.m6.1.1.3.2">superscript</csymbol><ci id="S3.p4.11.m6.1.1.3.2.2.cmml" xref="S3.p4.11.m6.1.1.3.2.2">ğ³</ci><ci id="S3.p4.11.m6.1.1.3.2.3.cmml" xref="S3.p4.11.m6.1.1.3.2.3">ğ‘‡</ci></apply><apply id="S3.p4.11.m6.1.1.3.3.cmml" xref="S3.p4.11.m6.1.1.3.3"><csymbol cd="ambiguous" id="S3.p4.11.m6.1.1.3.3.1.cmml" xref="S3.p4.11.m6.1.1.3.3">subscript</csymbol><ci id="S3.p4.11.m6.1.1.3.3.2.cmml" xref="S3.p4.11.m6.1.1.3.3.2">ğ–</ci><ci id="S3.p4.11.m6.1.1.3.3.3.cmml" xref="S3.p4.11.m6.1.1.3.3.3">ğ¨</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.11.m6.1c">\mathbf{y}=\mathbf{z}^{T}\mathbf{W_{o}}</annotation></semantics></math>. With the structured sparsity constraint, <math id="S3.p4.12.m7.1" class="ltx_Math" alttext="\mathbf{z}" display="inline"><semantics id="S3.p4.12.m7.1a"><mi id="S3.p4.12.m7.1.1" xref="S3.p4.12.m7.1.1.cmml">ğ³</mi><annotation-xml encoding="MathML-Content" id="S3.p4.12.m7.1b"><ci id="S3.p4.12.m7.1.1.cmml" xref="S3.p4.12.m7.1.1">ğ³</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.12.m7.1c">\mathbf{z}</annotation></semantics></math> results from the sum of R multiplications between the question and visual parts: <math id="S3.p4.13.m8.2" class="ltx_Math" alttext="\mathbf{z}=\sum_{r=1}^{R}(\tilde{\mathbf{q}}^{T}\mathbf{M_{r}})*(\tilde{\mathbf{v}}^{T}\mathbf{N_{r}})" display="inline"><semantics id="S3.p4.13.m8.2a"><mrow id="S3.p4.13.m8.2.2" xref="S3.p4.13.m8.2.2.cmml"><mi id="S3.p4.13.m8.2.2.4" xref="S3.p4.13.m8.2.2.4.cmml">ğ³</mi><mo rspace="0.111em" id="S3.p4.13.m8.2.2.3" xref="S3.p4.13.m8.2.2.3.cmml">=</mo><mrow id="S3.p4.13.m8.2.2.2" xref="S3.p4.13.m8.2.2.2.cmml"><msubsup id="S3.p4.13.m8.2.2.2.3" xref="S3.p4.13.m8.2.2.2.3.cmml"><mo rspace="0em" id="S3.p4.13.m8.2.2.2.3.2.2" xref="S3.p4.13.m8.2.2.2.3.2.2.cmml">âˆ‘</mo><mrow id="S3.p4.13.m8.2.2.2.3.2.3" xref="S3.p4.13.m8.2.2.2.3.2.3.cmml"><mi id="S3.p4.13.m8.2.2.2.3.2.3.2" xref="S3.p4.13.m8.2.2.2.3.2.3.2.cmml">r</mi><mo id="S3.p4.13.m8.2.2.2.3.2.3.1" xref="S3.p4.13.m8.2.2.2.3.2.3.1.cmml">=</mo><mn id="S3.p4.13.m8.2.2.2.3.2.3.3" xref="S3.p4.13.m8.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.p4.13.m8.2.2.2.3.3" xref="S3.p4.13.m8.2.2.2.3.3.cmml">R</mi></msubsup><mrow id="S3.p4.13.m8.2.2.2.2" xref="S3.p4.13.m8.2.2.2.2.cmml"><mrow id="S3.p4.13.m8.1.1.1.1.1.1" xref="S3.p4.13.m8.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p4.13.m8.1.1.1.1.1.1.2" xref="S3.p4.13.m8.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p4.13.m8.1.1.1.1.1.1.1" xref="S3.p4.13.m8.1.1.1.1.1.1.1.cmml"><msup id="S3.p4.13.m8.1.1.1.1.1.1.1.2" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.p4.13.m8.1.1.1.1.1.1.1.2.2" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.p4.13.m8.1.1.1.1.1.1.1.2.2.2" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2.2.2.cmml">ğª</mi><mo id="S3.p4.13.m8.1.1.1.1.1.1.1.2.2.1" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2.2.1.cmml">~</mo></mover><mi id="S3.p4.13.m8.1.1.1.1.1.1.1.2.3" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.p4.13.m8.1.1.1.1.1.1.1.1" xref="S3.p4.13.m8.1.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.p4.13.m8.1.1.1.1.1.1.1.3" xref="S3.p4.13.m8.1.1.1.1.1.1.1.3.cmml"><mi id="S3.p4.13.m8.1.1.1.1.1.1.1.3.2" xref="S3.p4.13.m8.1.1.1.1.1.1.1.3.2.cmml">ğŒ</mi><mi id="S3.p4.13.m8.1.1.1.1.1.1.1.3.3" xref="S3.p4.13.m8.1.1.1.1.1.1.1.3.3.cmml">ğ«</mi></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.p4.13.m8.1.1.1.1.1.1.3" xref="S3.p4.13.m8.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.p4.13.m8.2.2.2.2.3" xref="S3.p4.13.m8.2.2.2.2.3.cmml">âˆ—</mo><mrow id="S3.p4.13.m8.2.2.2.2.2.1" xref="S3.p4.13.m8.2.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S3.p4.13.m8.2.2.2.2.2.1.2" xref="S3.p4.13.m8.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.p4.13.m8.2.2.2.2.2.1.1" xref="S3.p4.13.m8.2.2.2.2.2.1.1.cmml"><msup id="S3.p4.13.m8.2.2.2.2.2.1.1.2" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2.cmml"><mover accent="true" id="S3.p4.13.m8.2.2.2.2.2.1.1.2.2" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2.2.cmml"><mi id="S3.p4.13.m8.2.2.2.2.2.1.1.2.2.2" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2.2.2.cmml">ğ¯</mi><mo id="S3.p4.13.m8.2.2.2.2.2.1.1.2.2.1" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2.2.1.cmml">~</mo></mover><mi id="S3.p4.13.m8.2.2.2.2.2.1.1.2.3" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.p4.13.m8.2.2.2.2.2.1.1.1" xref="S3.p4.13.m8.2.2.2.2.2.1.1.1.cmml">â€‹</mo><msub id="S3.p4.13.m8.2.2.2.2.2.1.1.3" xref="S3.p4.13.m8.2.2.2.2.2.1.1.3.cmml"><mi id="S3.p4.13.m8.2.2.2.2.2.1.1.3.2" xref="S3.p4.13.m8.2.2.2.2.2.1.1.3.2.cmml">ğ</mi><mi id="S3.p4.13.m8.2.2.2.2.2.1.1.3.3" xref="S3.p4.13.m8.2.2.2.2.2.1.1.3.3.cmml">ğ«</mi></msub></mrow><mo stretchy="false" id="S3.p4.13.m8.2.2.2.2.2.1.3" xref="S3.p4.13.m8.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.13.m8.2b"><apply id="S3.p4.13.m8.2.2.cmml" xref="S3.p4.13.m8.2.2"><eq id="S3.p4.13.m8.2.2.3.cmml" xref="S3.p4.13.m8.2.2.3"></eq><ci id="S3.p4.13.m8.2.2.4.cmml" xref="S3.p4.13.m8.2.2.4">ğ³</ci><apply id="S3.p4.13.m8.2.2.2.cmml" xref="S3.p4.13.m8.2.2.2"><apply id="S3.p4.13.m8.2.2.2.3.cmml" xref="S3.p4.13.m8.2.2.2.3"><csymbol cd="ambiguous" id="S3.p4.13.m8.2.2.2.3.1.cmml" xref="S3.p4.13.m8.2.2.2.3">superscript</csymbol><apply id="S3.p4.13.m8.2.2.2.3.2.cmml" xref="S3.p4.13.m8.2.2.2.3"><csymbol cd="ambiguous" id="S3.p4.13.m8.2.2.2.3.2.1.cmml" xref="S3.p4.13.m8.2.2.2.3">subscript</csymbol><sum id="S3.p4.13.m8.2.2.2.3.2.2.cmml" xref="S3.p4.13.m8.2.2.2.3.2.2"></sum><apply id="S3.p4.13.m8.2.2.2.3.2.3.cmml" xref="S3.p4.13.m8.2.2.2.3.2.3"><eq id="S3.p4.13.m8.2.2.2.3.2.3.1.cmml" xref="S3.p4.13.m8.2.2.2.3.2.3.1"></eq><ci id="S3.p4.13.m8.2.2.2.3.2.3.2.cmml" xref="S3.p4.13.m8.2.2.2.3.2.3.2">ğ‘Ÿ</ci><cn type="integer" id="S3.p4.13.m8.2.2.2.3.2.3.3.cmml" xref="S3.p4.13.m8.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.p4.13.m8.2.2.2.3.3.cmml" xref="S3.p4.13.m8.2.2.2.3.3">ğ‘…</ci></apply><apply id="S3.p4.13.m8.2.2.2.2.cmml" xref="S3.p4.13.m8.2.2.2.2"><times id="S3.p4.13.m8.2.2.2.2.3.cmml" xref="S3.p4.13.m8.2.2.2.2.3"></times><apply id="S3.p4.13.m8.1.1.1.1.1.1.1.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1"><times id="S3.p4.13.m8.1.1.1.1.1.1.1.1.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.1"></times><apply id="S3.p4.13.m8.1.1.1.1.1.1.1.2.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p4.13.m8.1.1.1.1.1.1.1.2.1.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.p4.13.m8.1.1.1.1.1.1.1.2.2.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2.2"><ci id="S3.p4.13.m8.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2.2.1">~</ci><ci id="S3.p4.13.m8.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2.2.2">ğª</ci></apply><ci id="S3.p4.13.m8.1.1.1.1.1.1.1.2.3.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.2.3">ğ‘‡</ci></apply><apply id="S3.p4.13.m8.1.1.1.1.1.1.1.3.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p4.13.m8.1.1.1.1.1.1.1.3.1.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.p4.13.m8.1.1.1.1.1.1.1.3.2.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.3.2">ğŒ</ci><ci id="S3.p4.13.m8.1.1.1.1.1.1.1.3.3.cmml" xref="S3.p4.13.m8.1.1.1.1.1.1.1.3.3">ğ«</ci></apply></apply><apply id="S3.p4.13.m8.2.2.2.2.2.1.1.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1"><times id="S3.p4.13.m8.2.2.2.2.2.1.1.1.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.1"></times><apply id="S3.p4.13.m8.2.2.2.2.2.1.1.2.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.p4.13.m8.2.2.2.2.2.1.1.2.1.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2">superscript</csymbol><apply id="S3.p4.13.m8.2.2.2.2.2.1.1.2.2.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2.2"><ci id="S3.p4.13.m8.2.2.2.2.2.1.1.2.2.1.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2.2.1">~</ci><ci id="S3.p4.13.m8.2.2.2.2.2.1.1.2.2.2.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2.2.2">ğ¯</ci></apply><ci id="S3.p4.13.m8.2.2.2.2.2.1.1.2.3.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.2.3">ğ‘‡</ci></apply><apply id="S3.p4.13.m8.2.2.2.2.2.1.1.3.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.p4.13.m8.2.2.2.2.2.1.1.3.1.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.p4.13.m8.2.2.2.2.2.1.1.3.2.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.3.2">ğ</ci><ci id="S3.p4.13.m8.2.2.2.2.2.1.1.3.3.cmml" xref="S3.p4.13.m8.2.2.2.2.2.1.1.3.3">ğ«</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.13.m8.2c">\mathbf{z}=\sum_{r=1}^{R}(\tilde{\mathbf{q}}^{T}\mathbf{M_{r}})*(\tilde{\mathbf{v}}^{T}\mathbf{N_{r}})</annotation></semantics></math>, where R is a hyperparameter.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">These mechanisms cover different fusion strategies, including a straight-forward combination (point-wise multiplication), one ruled by randomness (MCB) and finally a process fully learnt during training and enabling more interactions as well (MUTAN). FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3 Methods â€£ How to find a good image-text embedding for remote sensing visual question answering?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the three types of fusion we experiment with.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2109.11848/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison of the three fusion strategies considered in the paper.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets.</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">We evaluate the three fusion strategies on the RSVQA datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>: a low resolution dataset with Sentinel-2 images over the Netherlands (77,232 questions-answers-images triplets) and a high resolution dataset with aerial imagery from the USGS collection (1,066,316 triplets). For the latter, locations cover different regions in the North East Coast of the USA. Training and validation sets contain images of New York City (NY), Long Island (NY) and Portland (ME), while two test sets are provided, one over New York City and Long Island, and a second one over Philadelphia (PA), respectively referred to as NYC and PHL in this paper. The resolutions of images in the low and high resolution datasets are 10m and 15cm, respectively.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model architecture.</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">As depicted in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ How to find a good image-text embedding for remote sensing visual question answering?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the fusion operation is preceded by two feature extractors and followed by a classification network. This structure remains unchanged while the fusion operation is investigated. To extract features from the image, we use a ResNet-152Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, pre-trained on ImageNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and keep all but the ultimate classification layer. For the question part, we use skip-thoughtsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, pre-trained on BookCorpusÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Both pathways result in a 2048- and 2400- dimensional latent feature vector respectively, reduced to 1200 with fully-connected layers prior to fusion. To predict the answer, the classification is done with one fully-connected layer and one output layer that contains as many classes as there are answers, as inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Experimental setup.</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.2" class="ltx_p">In total, we evaluate six models; a baseline with an element-wise multiplication fusion, a model with MCB fusion and a model with MUTAN fusion, one for the low and high resolution each. Each experiment is run three times to compute a mean performance with standard deviation. The performance matrix consists of an accuracy measure for each question type, as well as an average and overall accuracy. All models are trained with the Adam optimizerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, a learning rate of <math id="S4.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.1.m1.1a"><msup id="S4.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml"><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3a" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml">âˆ’</mo><mn id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2">10</cn><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3"><minus id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.m1.1c">10^{-5}</annotation></semantics></math>, and a batch size of 70. The models on low resolution are trained for 150 epochs while those on high resolution are trained for 35 epochs. Finally, we perform an ablation study on the model with MCB fusion on the low resolution dataset portion to test the sensibility of the output dimension of the fusion on the performances: we train and compare models with feature dimensions d = <math id="S4.SS0.SSS0.Px3.p1.2.m2.5" class="ltx_Math" alttext="\{1200,4000,8000,16000,32000\}" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.2.m2.5a"><mrow id="S4.SS0.SSS0.Px3.p1.2.m2.5.6.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.5.6.1.cmml"><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.2.m2.5.6.2.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.5.6.1.cmml">{</mo><mn id="S4.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml">1200</mn><mo id="S4.SS0.SSS0.Px3.p1.2.m2.5.6.2.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.5.6.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px3.p1.2.m2.2.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.2.2.cmml">4000</mn><mo id="S4.SS0.SSS0.Px3.p1.2.m2.5.6.2.3" xref="S4.SS0.SSS0.Px3.p1.2.m2.5.6.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px3.p1.2.m2.3.3" xref="S4.SS0.SSS0.Px3.p1.2.m2.3.3.cmml">8000</mn><mo id="S4.SS0.SSS0.Px3.p1.2.m2.5.6.2.4" xref="S4.SS0.SSS0.Px3.p1.2.m2.5.6.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px3.p1.2.m2.4.4" xref="S4.SS0.SSS0.Px3.p1.2.m2.4.4.cmml">16000</mn><mo id="S4.SS0.SSS0.Px3.p1.2.m2.5.6.2.5" xref="S4.SS0.SSS0.Px3.p1.2.m2.5.6.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px3.p1.2.m2.5.5" xref="S4.SS0.SSS0.Px3.p1.2.m2.5.5.cmml">32000</mn><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.2.m2.5.6.2.6" xref="S4.SS0.SSS0.Px3.p1.2.m2.5.6.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.m2.5b"><set id="S4.SS0.SSS0.Px3.p1.2.m2.5.6.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.5.6.2"><cn type="integer" id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1">1200</cn><cn type="integer" id="S4.SS0.SSS0.Px3.p1.2.m2.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.2.2">4000</cn><cn type="integer" id="S4.SS0.SSS0.Px3.p1.2.m2.3.3.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.3.3">8000</cn><cn type="integer" id="S4.SS0.SSS0.Px3.p1.2.m2.4.4.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.4.4">16000</cn><cn type="integer" id="S4.SS0.SSS0.Px3.p1.2.m2.5.5.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.5.5">32000</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.m2.5c">\{1200,4000,8000,16000,32000\}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and discussion</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Low resolution dataset.</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">The results for the models on low resolution are displayed in TableÂ <a href="#S5.T1" title="Table 1 â€£ Low resolution dataset. â€£ 5 Results and discussion â€£ How to find a good image-text embedding for remote sensing visual question answering?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. A more complex fusion helps the performances of the model in low resolution. However, the improvement varies depending on the question type, with the â€œcomparisonâ€ questions improving the most, followed by â€œcountingâ€ and â€œpresenceâ€, but to a lesser extent. We observe a strong variability for the accuracy on the question type â€œrural/urbanâ€. This is due to the low number of samples for this question type (only one question per image, 1% of the samples) as well as the subjective choice to differentiate rural and urban (a threshold on the number of buildings was used inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to define the image as rural or urban). Overall, MCB performs slightly better than MUTAN, although with a lower performance gap than between MCB and the baseline. However, the difference in the number of parameters to train is considerable. The output dimension of the fusion is 8,000 for MCB and 360 for MUTAN, and the question remains whether the better performance with MCB is the result of the more expressive fusion itself or of the larger capacity in the fully-connected layer used for classification.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Test results on low resolution images, average performance reported with standard deviation in brackets.</figcaption>
<table id="S5.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.3.4.1" class="ltx_tr">
<th id="S5.T1.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Question type</th>
<td id="S5.T1.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Baseline</td>
<td id="S5.T1.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MCB</td>
<td id="S5.T1.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MUTAN</td>
</tr>
<tr id="S5.T1.3.3" class="ltx_tr">
<th id="S5.T1.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"># of parameters learned</th>
<td id="S5.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T1.1.1.1.m1.1" class="ltx_Math" alttext="5.7\times 10^{6}" display="inline"><semantics id="S5.T1.1.1.1.m1.1a"><mrow id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.1.1.1.m1.1.1.2" xref="S5.T1.1.1.1.m1.1.1.2.cmml">5.7</mn><mo lspace="0.222em" rspace="0.222em" id="S5.T1.1.1.1.m1.1.1.1" xref="S5.T1.1.1.1.m1.1.1.1.cmml">Ã—</mo><msup id="S5.T1.1.1.1.m1.1.1.3" xref="S5.T1.1.1.1.m1.1.1.3.cmml"><mn id="S5.T1.1.1.1.m1.1.1.3.2" xref="S5.T1.1.1.1.m1.1.1.3.2.cmml">10</mn><mn id="S5.T1.1.1.1.m1.1.1.3.3" xref="S5.T1.1.1.1.m1.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><apply id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1"><times id="S5.T1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1.1"></times><cn type="float" id="S5.T1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.1.1.1.m1.1.1.2">5.7</cn><apply id="S5.T1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T1.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T1.1.1.1.m1.1.1.3.2">10</cn><cn type="integer" id="S5.T1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T1.1.1.1.m1.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">5.7\times 10^{6}</annotation></semantics></math></td>
<td id="S5.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T1.2.2.2.m1.1" class="ltx_Math" alttext="7.5\times 10^{6}" display="inline"><semantics id="S5.T1.2.2.2.m1.1a"><mrow id="S5.T1.2.2.2.m1.1.1" xref="S5.T1.2.2.2.m1.1.1.cmml"><mn id="S5.T1.2.2.2.m1.1.1.2" xref="S5.T1.2.2.2.m1.1.1.2.cmml">7.5</mn><mo lspace="0.222em" rspace="0.222em" id="S5.T1.2.2.2.m1.1.1.1" xref="S5.T1.2.2.2.m1.1.1.1.cmml">Ã—</mo><msup id="S5.T1.2.2.2.m1.1.1.3" xref="S5.T1.2.2.2.m1.1.1.3.cmml"><mn id="S5.T1.2.2.2.m1.1.1.3.2" xref="S5.T1.2.2.2.m1.1.1.3.2.cmml">10</mn><mn id="S5.T1.2.2.2.m1.1.1.3.3" xref="S5.T1.2.2.2.m1.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><apply id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1"><times id="S5.T1.2.2.2.m1.1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1.1"></times><cn type="float" id="S5.T1.2.2.2.m1.1.1.2.cmml" xref="S5.T1.2.2.2.m1.1.1.2">7.5</cn><apply id="S5.T1.2.2.2.m1.1.1.3.cmml" xref="S5.T1.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T1.2.2.2.m1.1.1.3.1.cmml" xref="S5.T1.2.2.2.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T1.2.2.2.m1.1.1.3.2.cmml" xref="S5.T1.2.2.2.m1.1.1.3.2">10</cn><cn type="integer" id="S5.T1.2.2.2.m1.1.1.3.3.cmml" xref="S5.T1.2.2.2.m1.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">7.5\times 10^{6}</annotation></semantics></math></td>
<td id="S5.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T1.3.3.3.m1.1" class="ltx_Math" alttext="4.4\times 10^{6}" display="inline"><semantics id="S5.T1.3.3.3.m1.1a"><mrow id="S5.T1.3.3.3.m1.1.1" xref="S5.T1.3.3.3.m1.1.1.cmml"><mn id="S5.T1.3.3.3.m1.1.1.2" xref="S5.T1.3.3.3.m1.1.1.2.cmml">4.4</mn><mo lspace="0.222em" rspace="0.222em" id="S5.T1.3.3.3.m1.1.1.1" xref="S5.T1.3.3.3.m1.1.1.1.cmml">Ã—</mo><msup id="S5.T1.3.3.3.m1.1.1.3" xref="S5.T1.3.3.3.m1.1.1.3.cmml"><mn id="S5.T1.3.3.3.m1.1.1.3.2" xref="S5.T1.3.3.3.m1.1.1.3.2.cmml">10</mn><mn id="S5.T1.3.3.3.m1.1.1.3.3" xref="S5.T1.3.3.3.m1.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.m1.1b"><apply id="S5.T1.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1"><times id="S5.T1.3.3.3.m1.1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1.1"></times><cn type="float" id="S5.T1.3.3.3.m1.1.1.2.cmml" xref="S5.T1.3.3.3.m1.1.1.2">4.4</cn><apply id="S5.T1.3.3.3.m1.1.1.3.cmml" xref="S5.T1.3.3.3.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T1.3.3.3.m1.1.1.3.1.cmml" xref="S5.T1.3.3.3.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T1.3.3.3.m1.1.1.3.2.cmml" xref="S5.T1.3.3.3.m1.1.1.3.2">10</cn><cn type="integer" id="S5.T1.3.3.3.m1.1.1.3.3.cmml" xref="S5.T1.3.3.3.m1.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.m1.1c">4.4\times 10^{6}</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.3.5.2" class="ltx_tr">
<th id="S5.T1.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Comparison</th>
<td id="S5.T1.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.44 (0.09)</td>
<td id="S5.T1.3.5.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.3.5.2.3.1" class="ltx_text ltx_font_bold">88.22 (0.14)</span></td>
<td id="S5.T1.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.52 (0.01)</td>
</tr>
<tr id="S5.T1.3.6.3" class="ltx_tr">
<th id="S5.T1.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Counting</th>
<td id="S5.T1.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r">68.00 (0.65)</td>
<td id="S5.T1.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.3.6.3.3.1" class="ltx_text ltx_font_bold">70.18 (0.46)</span></td>
<td id="S5.T1.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r">69.06 (0.45)</td>
</tr>
<tr id="S5.T1.3.7.4" class="ltx_tr">
<th id="S5.T1.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Presence</th>
<td id="S5.T1.3.7.4.2" class="ltx_td ltx_align_center ltx_border_r">88.49 (0.25)</td>
<td id="S5.T1.3.7.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.3.7.4.3.1" class="ltx_text ltx_font_bold">90.34 (0.47)</span></td>
<td id="S5.T1.3.7.4.4" class="ltx_td ltx_align_center ltx_border_r">90.07 (0.21)</td>
</tr>
<tr id="S5.T1.3.8.5" class="ltx_tr">
<th id="S5.T1.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Rural/urban</th>
<td id="S5.T1.3.8.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.3.8.5.2.1" class="ltx_text ltx_font_bold">90.67 (0.47)</span></td>
<td id="S5.T1.3.8.5.3" class="ltx_td ltx_align_center ltx_border_r">90.00 (0.82)</td>
<td id="S5.T1.3.8.5.4" class="ltx_td ltx_align_center ltx_border_r">87.67 (2.36)</td>
</tr>
<tr id="S5.T1.3.9.6" class="ltx_tr">
<th id="S5.T1.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Average accuracy</th>
<td id="S5.T1.3.9.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.90 (0.24)</td>
<td id="S5.T1.3.9.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.3.9.6.3.1" class="ltx_text ltx_font_bold">84.69 (0.33)</span></td>
<td id="S5.T1.3.9.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.58 (0.71)</td>
</tr>
<tr id="S5.T1.3.10.7" class="ltx_tr">
<th id="S5.T1.3.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Overall accuracy</th>
<td id="S5.T1.3.10.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">80.86 (0.24)</td>
<td id="S5.T1.3.10.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T1.3.10.7.3.1" class="ltx_text ltx_font_bold">83.55 (0.20)</span></td>
<td id="S5.T1.3.10.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">82.84 (0.21)</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p2.1" class="ltx_p">The confusion matrix for the baseline and its differences to MCB and MUTAN (Fig.Â <a href="#S5.F3" title="Figure 3 â€£ Low resolution dataset. â€£ 5 Results and discussion â€£ How to find a good image-text embedding for remote sensing visual question answering?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) highlight the capacity of the model to respond with the correct type of answer to each question. The answers â€œyes/noâ€ see most improvements, with the two more elaborated fusion improving the results. Yet, these answers are also the most frequent in the dataset, so it might be reasonable to consider techniques to account for less represented types of answers. Interestingly, MUTAN does slightly worse on the â€œrural/urbanâ€ questions, especially â€œurbanâ€, and MCB declines on â€œruralâ€ but improves on â€œurbanâ€. For â€œcountingâ€ questions, the pattern is similar between MCB and MUTAN but we can observe a few variations, the largest class â€œmore than 1000â€ for example is slightly better with MCB, and worse with MUTAN.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2109.11848/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Confusion matrix for the low resolution baseline and differences between the confusion matrix of MCB and MUTAN with the baseline. On the 2nd and 3rd figures, a positive value indicates more predictions with the model compared to the baseline, respectively negative indicates less.</figcaption>
</figure>
<div id="S5.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p3.1" class="ltx_p">As a final result for the low resolution dataset, TableÂ <a href="#S5.T2" title="Table 2 â€£ Low resolution dataset. â€£ 5 Results and discussion â€£ How to find a good image-text embedding for remote sensing visual question answering?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> lists the accuracies obtained with MCB and different output vector dimensions <math id="S5.SS0.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S5.SS0.SSS0.Px1.p3.1.m1.1a"><mi id="S5.SS0.SSS0.Px1.p3.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p3.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p3.1.m1.1b"><ci id="S5.SS0.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p3.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p3.1.m1.1c">d</annotation></semantics></math>. While the number of parameters increase substantially, the difference in both average and overall accuracy is narrow. The best results are obtained with an output dimension of 8,000 for both overall (OA) and average (AA) accuracies.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results (over the validation set of the low resolution dataset) for the ablation study of the fusion output dimension <math id="S5.T2.2.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S5.T2.2.m1.1b"><mi id="S5.T2.2.m1.1.1" xref="S5.T2.2.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.T2.2.m1.1c"><ci id="S5.T2.2.m1.1.1.cmml" xref="S5.T2.2.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.m1.1d">d</annotation></semantics></math> in MCB.</figcaption>
<table id="S5.T2.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.3.1" class="ltx_tr">
<th id="S5.T2.3.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S5.T2.3.1.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S5.T2.3.1.1.m1.1a"><mi id="S5.T2.3.1.1.m1.1.1" xref="S5.T2.3.1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.T2.3.1.1.m1.1b"><ci id="S5.T2.3.1.1.m1.1.1.cmml" xref="S5.T2.3.1.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.1.1.m1.1c">d</annotation></semantics></math></th>
<th id="S5.T2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">1,200</th>
<th id="S5.T2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4,000</th>
<th id="S5.T2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">8,000</th>
<th id="S5.T2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">16,000</th>
<th id="S5.T2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">32,000</th>
</tr>
<tr id="S5.T2.9.7" class="ltx_tr">
<th id="S5.T2.4.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"># of parameters learned (<math id="S5.T2.4.2.1.m1.1" class="ltx_Math" alttext="\times 10^{6}" display="inline"><semantics id="S5.T2.4.2.1.m1.1a"><mrow id="S5.T2.4.2.1.m1.1.1" xref="S5.T2.4.2.1.m1.1.1.cmml"><mi id="S5.T2.4.2.1.m1.1.1.2" xref="S5.T2.4.2.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S5.T2.4.2.1.m1.1.1.1" xref="S5.T2.4.2.1.m1.1.1.1.cmml">Ã—</mo><msup id="S5.T2.4.2.1.m1.1.1.3" xref="S5.T2.4.2.1.m1.1.1.3.cmml"><mn id="S5.T2.4.2.1.m1.1.1.3.2" xref="S5.T2.4.2.1.m1.1.1.3.2.cmml">10</mn><mn id="S5.T2.4.2.1.m1.1.1.3.3" xref="S5.T2.4.2.1.m1.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.4.2.1.m1.1b"><apply id="S5.T2.4.2.1.m1.1.1.cmml" xref="S5.T2.4.2.1.m1.1.1"><times id="S5.T2.4.2.1.m1.1.1.1.cmml" xref="S5.T2.4.2.1.m1.1.1.1"></times><csymbol cd="latexml" id="S5.T2.4.2.1.m1.1.1.2.cmml" xref="S5.T2.4.2.1.m1.1.1.2">absent</csymbol><apply id="S5.T2.4.2.1.m1.1.1.3.cmml" xref="S5.T2.4.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T2.4.2.1.m1.1.1.3.1.cmml" xref="S5.T2.4.2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T2.4.2.1.m1.1.1.3.2.cmml" xref="S5.T2.4.2.1.m1.1.1.3.2">10</cn><cn type="integer" id="S5.T2.4.2.1.m1.1.1.3.3.cmml" xref="S5.T2.4.2.1.m1.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.2.1.m1.1c">\times 10^{6}</annotation></semantics></math>)</th>
<th id="S5.T2.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S5.T2.5.3.2.m1.1" class="ltx_Math" alttext="5.7" display="inline"><semantics id="S5.T2.5.3.2.m1.1a"><mn id="S5.T2.5.3.2.m1.1.1" xref="S5.T2.5.3.2.m1.1.1.cmml">5.7</mn><annotation-xml encoding="MathML-Content" id="S5.T2.5.3.2.m1.1b"><cn type="float" id="S5.T2.5.3.2.m1.1.1.cmml" xref="S5.T2.5.3.2.m1.1.1">5.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.3.2.m1.1c">5.7</annotation></semantics></math></th>
<th id="S5.T2.6.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S5.T2.6.4.3.m1.1" class="ltx_Math" alttext="6.4" display="inline"><semantics id="S5.T2.6.4.3.m1.1a"><mn id="S5.T2.6.4.3.m1.1.1" xref="S5.T2.6.4.3.m1.1.1.cmml">6.4</mn><annotation-xml encoding="MathML-Content" id="S5.T2.6.4.3.m1.1b"><cn type="float" id="S5.T2.6.4.3.m1.1.1.cmml" xref="S5.T2.6.4.3.m1.1.1">6.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.4.3.m1.1c">6.4</annotation></semantics></math></th>
<th id="S5.T2.7.5.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S5.T2.7.5.4.m1.1" class="ltx_Math" alttext="7.5" display="inline"><semantics id="S5.T2.7.5.4.m1.1a"><mn id="S5.T2.7.5.4.m1.1.1" xref="S5.T2.7.5.4.m1.1.1.cmml">7.5</mn><annotation-xml encoding="MathML-Content" id="S5.T2.7.5.4.m1.1b"><cn type="float" id="S5.T2.7.5.4.m1.1.1.cmml" xref="S5.T2.7.5.4.m1.1.1">7.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.5.4.m1.1c">7.5</annotation></semantics></math></th>
<th id="S5.T2.8.6.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S5.T2.8.6.5.m1.1" class="ltx_Math" alttext="9.5" display="inline"><semantics id="S5.T2.8.6.5.m1.1a"><mn id="S5.T2.8.6.5.m1.1.1" xref="S5.T2.8.6.5.m1.1.1.cmml">9.5</mn><annotation-xml encoding="MathML-Content" id="S5.T2.8.6.5.m1.1b"><cn type="float" id="S5.T2.8.6.5.m1.1.1.cmml" xref="S5.T2.8.6.5.m1.1.1">9.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.6.5.m1.1c">9.5</annotation></semantics></math></th>
<th id="S5.T2.9.7.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S5.T2.9.7.6.m1.1" class="ltx_Math" alttext="13.6" display="inline"><semantics id="S5.T2.9.7.6.m1.1a"><mn id="S5.T2.9.7.6.m1.1.1" xref="S5.T2.9.7.6.m1.1.1.cmml">13.6</mn><annotation-xml encoding="MathML-Content" id="S5.T2.9.7.6.m1.1b"><cn type="float" id="S5.T2.9.7.6.m1.1.1.cmml" xref="S5.T2.9.7.6.m1.1.1">13.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.7.6.m1.1c">13.6</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.9.8.1" class="ltx_tr">
<th id="S5.T2.9.8.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">AA</th>
<td id="S5.T2.9.8.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">84.21</td>
<td id="S5.T2.9.8.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">84.57</td>
<td id="S5.T2.9.8.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T2.9.8.1.4.1" class="ltx_text ltx_font_bold">85.16</span></td>
<td id="S5.T2.9.8.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">85.11</td>
<td id="S5.T2.9.8.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">84.33</td>
</tr>
<tr id="S5.T2.9.9.2" class="ltx_tr">
<th id="S5.T2.9.9.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">OA</th>
<td id="S5.T2.9.9.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">85.13</td>
<td id="S5.T2.9.9.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">85.92</td>
<td id="S5.T2.9.9.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T2.9.9.2.4.1" class="ltx_text ltx_font_bold">86.36</span></td>
<td id="S5.T2.9.9.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">86.02</td>
<td id="S5.T2.9.9.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">86.28</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">High resolution dataset.</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">The results of the models on the two high resolution test sets, NYC and PHL, are displayed in TableÂ <a href="#S5.T3" title="Table 3 â€£ High resolution dataset. â€£ 5 Results and discussion â€£ How to find a good image-text embedding for remote sensing visual question answering?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. An improvement of performance is observed again between the baseline and MCB/MUTAN fusions, but with lower overall improvements compared against the baseline. Again, the largest gain is observed for comparison questions, while the other type of questions also improve, but less. Contrary to the low resolution dataset where MCB was performing better, the difference here is small and included in the standard deviation calculated over the three runs. In line withÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, a significant drop in performance can be observed in the PHL test set. This comes from the domain shift introduced in this test set, made of images over a city unseen during training. When trying to generalize with this additional PHL test set, the comparison of fusion strategies is consistent but the fusion is impacted as the performance differences are smaller and the variability higher. The domain shift primarily affects the image feature extractor and thus is not really compensated for in the fusion step. Hence, other modelling strategies or larger-scale datasets are needed for better generalization.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Test results on high resolution images, average performance reported with standard deviation in brackets.</figcaption>
<table id="S5.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.3.4.1" class="ltx_tr">
<th id="S5.T3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Test set</th>
<th id="S5.T3.3.4.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Question type</th>
<th id="S5.T3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Baseline</th>
<th id="S5.T3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">MCB</th>
<th id="S5.T3.3.4.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">MUTAN</th>
</tr>
<tr id="S5.T3.3.3" class="ltx_tr">
<th id="S5.T3.3.3.4" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S5.T3.3.3.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"># of parameters learned</th>
<th id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="5.7\times 10^{6}" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mrow id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml"><mn id="S5.T3.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.m1.1.1.2.cmml">5.7</mn><mo lspace="0.222em" rspace="0.222em" id="S5.T3.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.m1.1.1.1.cmml">Ã—</mo><msup id="S5.T3.1.1.1.m1.1.1.3" xref="S5.T3.1.1.1.m1.1.1.3.cmml"><mn id="S5.T3.1.1.1.m1.1.1.3.2" xref="S5.T3.1.1.1.m1.1.1.3.2.cmml">10</mn><mn id="S5.T3.1.1.1.m1.1.1.3.3" xref="S5.T3.1.1.1.m1.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1"><times id="S5.T3.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1.1"></times><cn type="float" id="S5.T3.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.m1.1.1.2">5.7</cn><apply id="S5.T3.1.1.1.m1.1.1.3.cmml" xref="S5.T3.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T3.1.1.1.m1.1.1.3.1.cmml" xref="S5.T3.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T3.1.1.1.m1.1.1.3.2.cmml" xref="S5.T3.1.1.1.m1.1.1.3.2">10</cn><cn type="integer" id="S5.T3.1.1.1.m1.1.1.3.3.cmml" xref="S5.T3.1.1.1.m1.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">5.7\times 10^{6}</annotation></semantics></math></th>
<th id="S5.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S5.T3.2.2.2.m1.1" class="ltx_Math" alttext="7.4\times 10^{6}" display="inline"><semantics id="S5.T3.2.2.2.m1.1a"><mrow id="S5.T3.2.2.2.m1.1.1" xref="S5.T3.2.2.2.m1.1.1.cmml"><mn id="S5.T3.2.2.2.m1.1.1.2" xref="S5.T3.2.2.2.m1.1.1.2.cmml">7.4</mn><mo lspace="0.222em" rspace="0.222em" id="S5.T3.2.2.2.m1.1.1.1" xref="S5.T3.2.2.2.m1.1.1.1.cmml">Ã—</mo><msup id="S5.T3.2.2.2.m1.1.1.3" xref="S5.T3.2.2.2.m1.1.1.3.cmml"><mn id="S5.T3.2.2.2.m1.1.1.3.2" xref="S5.T3.2.2.2.m1.1.1.3.2.cmml">10</mn><mn id="S5.T3.2.2.2.m1.1.1.3.3" xref="S5.T3.2.2.2.m1.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><apply id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1"><times id="S5.T3.2.2.2.m1.1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1.1"></times><cn type="float" id="S5.T3.2.2.2.m1.1.1.2.cmml" xref="S5.T3.2.2.2.m1.1.1.2">7.4</cn><apply id="S5.T3.2.2.2.m1.1.1.3.cmml" xref="S5.T3.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T3.2.2.2.m1.1.1.3.1.cmml" xref="S5.T3.2.2.2.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T3.2.2.2.m1.1.1.3.2.cmml" xref="S5.T3.2.2.2.m1.1.1.3.2">10</cn><cn type="integer" id="S5.T3.2.2.2.m1.1.1.3.3.cmml" xref="S5.T3.2.2.2.m1.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">7.4\times 10^{6}</annotation></semantics></math></th>
<th id="S5.T3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S5.T3.3.3.3.m1.1" class="ltx_Math" alttext="4.3\times 10^{6}" display="inline"><semantics id="S5.T3.3.3.3.m1.1a"><mrow id="S5.T3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.m1.1.1.cmml"><mn id="S5.T3.3.3.3.m1.1.1.2" xref="S5.T3.3.3.3.m1.1.1.2.cmml">4.3</mn><mo lspace="0.222em" rspace="0.222em" id="S5.T3.3.3.3.m1.1.1.1" xref="S5.T3.3.3.3.m1.1.1.1.cmml">Ã—</mo><msup id="S5.T3.3.3.3.m1.1.1.3" xref="S5.T3.3.3.3.m1.1.1.3.cmml"><mn id="S5.T3.3.3.3.m1.1.1.3.2" xref="S5.T3.3.3.3.m1.1.1.3.2.cmml">10</mn><mn id="S5.T3.3.3.3.m1.1.1.3.3" xref="S5.T3.3.3.3.m1.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.m1.1b"><apply id="S5.T3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1"><times id="S5.T3.3.3.3.m1.1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1.1"></times><cn type="float" id="S5.T3.3.3.3.m1.1.1.2.cmml" xref="S5.T3.3.3.3.m1.1.1.2">4.3</cn><apply id="S5.T3.3.3.3.m1.1.1.3.cmml" xref="S5.T3.3.3.3.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T3.3.3.3.m1.1.1.3.1.cmml" xref="S5.T3.3.3.3.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.T3.3.3.3.m1.1.1.3.2.cmml" xref="S5.T3.3.3.3.m1.1.1.3.2">10</cn><cn type="integer" id="S5.T3.3.3.3.m1.1.1.3.3.cmml" xref="S5.T3.3.3.3.m1.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.m1.1c">4.3\times 10^{6}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.3.5.1" class="ltx_tr">
<td id="S5.T3.3.5.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" rowspan="6"><span id="S5.T3.3.5.1.1.1" class="ltx_text">NYC</span></td>
<td id="S5.T3.3.5.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Area</td>
<td id="S5.T3.3.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.20 (0.14)</td>
<td id="S5.T3.3.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.77 (0.07)</td>
<td id="S5.T3.3.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.3.5.1.5.1" class="ltx_text ltx_font_bold">85.80 (0.07)</span></td>
</tr>
<tr id="S5.T3.3.6.2" class="ltx_tr">
<td id="S5.T3.3.6.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Comparison</td>
<td id="S5.T3.3.6.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.17 (0.06)</td>
<td id="S5.T3.3.6.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.3.6.2.3.1" class="ltx_text ltx_font_bold">90.03 (0.09)</span></td>
<td id="S5.T3.3.6.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.00 (0.05)</td>
</tr>
<tr id="S5.T3.3.7.3" class="ltx_tr">
<td id="S5.T3.3.7.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Counting</td>
<td id="S5.T3.3.7.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.63 (0.05)</td>
<td id="S5.T3.3.7.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.22 (0.04)</td>
<td id="S5.T3.3.7.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.3.7.3.4.1" class="ltx_text ltx_font_bold">69.27 (0.05)</span></td>
</tr>
<tr id="S5.T3.3.8.4" class="ltx_tr">
<td id="S5.T3.3.8.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Presence</td>
<td id="S5.T3.3.8.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.47 (0.03)</td>
<td id="S5.T3.3.8.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">91.31 (0.10)</td>
<td id="S5.T3.3.8.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.3.8.4.4.1" class="ltx_text ltx_font_bold">91.46 (0.02)</span></td>
</tr>
<tr id="S5.T3.3.9.5" class="ltx_tr">
<td id="S5.T3.3.9.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Average accuracy</td>
<td id="S5.T3.3.9.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">83.12 (0.01)</td>
<td id="S5.T3.3.9.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">84.08 (0.03)</td>
<td id="S5.T3.3.9.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T3.3.9.5.4.1" class="ltx_text ltx_font_bold">84.13 (0.02)</span></td>
</tr>
<tr id="S5.T3.3.10.6" class="ltx_tr">
<td id="S5.T3.3.10.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Overall accuracy</td>
<td id="S5.T3.3.10.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.23 (0.01)</td>
<td id="S5.T3.3.10.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.30 (0.02)</td>
<td id="S5.T3.3.10.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.3.10.6.4.1" class="ltx_text ltx_font_bold">84.35 (0.02)</span></td>
</tr>
<tr id="S5.T3.3.11.7" class="ltx_tr">
<td id="S5.T3.3.11.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_tt" rowspan="6"><span id="S5.T3.3.11.7.1.1" class="ltx_text">PHL</span></td>
<td id="S5.T3.3.11.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Area</td>
<td id="S5.T3.3.11.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">75.01 (1.46)</td>
<td id="S5.T3.3.11.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T3.3.11.7.4.1" class="ltx_text ltx_font_bold">75.17 (0.40)</span></td>
<td id="S5.T3.3.11.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">74.61 (0.88)</td>
</tr>
<tr id="S5.T3.3.12.8" class="ltx_tr">
<td id="S5.T3.3.12.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Comparison</td>
<td id="S5.T3.3.12.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.20 (0.32)</td>
<td id="S5.T3.3.12.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.38 (0.09)</td>
<td id="S5.T3.3.12.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.3.12.8.4.1" class="ltx_text ltx_font_bold">87.41 (0.38)</span></td>
</tr>
<tr id="S5.T3.3.13.9" class="ltx_tr">
<td id="S5.T3.3.13.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Counting</td>
<td id="S5.T3.3.13.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.47 (0.12)</td>
<td id="S5.T3.3.13.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.79 (0.13)</td>
<td id="S5.T3.3.13.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.3.13.9.4.1" class="ltx_text ltx_font_bold">61.99 (0.04)</span></td>
</tr>
<tr id="S5.T3.3.14.10" class="ltx_tr">
<td id="S5.T3.3.14.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Presence</td>
<td id="S5.T3.3.14.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.36 (0.56)</td>
<td id="S5.T3.3.14.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.3.14.10.3.1" class="ltx_text ltx_font_bold">86.82 (0.09)</span></td>
<td id="S5.T3.3.14.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.72 (0.20)</td>
</tr>
<tr id="S5.T3.3.15.11" class="ltx_tr">
<td id="S5.T3.3.15.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Average accuracy</td>
<td id="S5.T3.3.15.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.26 (0.60)</td>
<td id="S5.T3.3.15.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T3.3.15.11.3.1" class="ltx_text ltx_font_bold">77.79 (0.10)</span></td>
<td id="S5.T3.3.15.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.68 (0.29)</td>
</tr>
<tr id="S5.T3.3.16.12" class="ltx_tr">
<td id="S5.T3.3.16.12.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Overall accuracy</td>
<td id="S5.T3.3.16.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">78.14 (0.48)</td>
<td id="S5.T3.3.16.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T3.3.16.12.3.1" class="ltx_text ltx_font_bold">78.77 (0.06)</span></td>
<td id="S5.T3.3.16.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">78.72 (0.24)</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p2.1" class="ltx_p">The confusion matrices for the high resolution results are displayed in Fig.Â <a href="#S5.F4" title="Figure 4 â€£ High resolution dataset. â€£ 5 Results and discussion â€£ How to find a good image-text embedding for remote sensing visual question answering?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. For â€œyes/noâ€ answers, MUTAN shows a more consistent improvement compared to MCB. Although both MCB and MUTAN learn to predict wrong small counting values less often, the former does slightly better for counting questions with a few more positive values on the diagonal. The difficulty for the models to answer counting questions is clearly illustrated in these figures.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2109.11848/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="115" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Confusion matrix for the high resolution baseline, along with the differences with MCB and MUTAN (test set NYC, and only the first main classes are represented)</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We considered a model for visual question answering (VQA) in remote sensing imagery where high-level representations acquired from an image and a question are fused together before predicting an answer. We particularly focused on the fusion component of deep VQA models, which is expected to create meaningful interactions between both modalities, uncovering the key relation to derive a correct answer while maintaining a tractable model. From experiments on the RSVQA dataset, we conclude that a richer fusion is beneficial to the task. We found performances to improve with more elaborate fusion strategies. While MCB shows better accuracy for the low resolution models, the difference is very small for the high resolution models. MUTAN achieves competing results with less than half the number of parameters and therefore constitutes a valuable choice over simpler strategies without compromising computational cost.</p>
</div>
<section id="S6.SS0.SSS1" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.0.1 </span>Acknowledgements.</h3>

<div id="S6.SS0.SSS1.p1" class="ltx_para">
<p id="S6.SS0.SSS1.p1.1" class="ltx_p">This work is supported by the European Space Agency through the Open Space Innovation Program, and is part of the project â€œAn AI assistant to interact with remote sensing imagesâ€ led in partnership between the ECEO lab (EPFL) and the <math id="S6.SS0.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\mathrm{\Phi}" display="inline"><semantics id="S6.SS0.SSS1.p1.1.m1.1a"><mi mathvariant="normal" id="S6.SS0.SSS1.p1.1.m1.1.1" xref="S6.SS0.SSS1.p1.1.m1.1.1.cmml">Î¦</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS1.p1.1.m1.1b"><ci id="S6.SS0.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS1.p1.1.m1.1.1">Î¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS1.p1.1.m1.1c">\mathrm{\Phi}</annotation></semantics></math>-lab (ESA).</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Andreas, J., etÂ al.: Learning to Compose Neural Networks for Question
Answering. In: NAACL 2016. pp. 1545â€“1554. Association for Computational
Linguistics, San Diego, California (2016)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Andreas, J., etÂ al.: Neural Module Networks. In: CVPR 2016. pp. 39â€“48.
IEEE, Las Vegas, NV, USA (2016)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Antol, S., etÂ al.: VQA: Visual Question Answering. In: ICCV 2015. pp.
2425â€“2433 (2015)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ben-younes, H., etÂ al.: MUTAN: Multimodal Tucker Fusion for Visual
Question Answering. In: 2017 ICCV. pp. 2631â€“2639. IEEE, Venice (2017)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ben-younes, H., etÂ al.: BLOCK: Bilinear Superdiagonal Fusion for
Visual Question Answering and Visual Relationship Detection.
arXiv:1902.00038 [cs] (2019)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Charikar, M., etÂ al.: Finding frequent items in data streams. Theoretical
Computer Science <span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">312</span>(1), 3â€“15 (2004)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chen, K., etÂ al.: ABC-CNN: An Attention Based Convolutional
Neural Network for Visual Question Answering. arXiv:1511.05960 [cs]
(2016)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Deng, J., etÂ al.: ImageNet: A large-scale hierarchical image database. In:
CVPR 2009. pp. 248â€“255. IEEE, Miami, FL (2009)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Fukui, A., etÂ al.: Multimodal Compact Bilinear Pooling for Visual
Question Answering and Visual Grounding. In: EMNLP 2016. pp.
457â€“468. Association for Computational Linguistics, Austin, USA (2016)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
He, K., etÂ al.: Deep residual learning for image recognition. In: CVPR 2016.
pp. 770â€“778 (2016)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kim, J.H., etÂ al.: Hadamard Product for Low-rank Bilinear Pooling.
arXiv:1610.04325 [cs] (2017)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization. In:
ICLR 2015. San Diego, California (2015)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kiros, R., etÂ al.: Skip-Thought Vectors. In: NIPS. pp. 3294â€“3302 (2015)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Kolda, T.G., Bader, B.W.: Tensor Decompositions and Applications. SIAM
Review <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">51</span>(3), 455â€“500 (2009)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
LeCun, Y., etÂ al.: Deep learning. nature <span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">521</span>(7553), 436â€“444 (2015)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Lobry, S., etÂ al.: RSVQA: Visual Question Answering for Remote
Sensing Data. IEEE Transactions on Geoscience and Remote Sensing
<span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">58</span>(12), 8555â€“8566 (2020)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Lobry, S., etÂ al.: RSVQA meets BigEarthNet: a new, large-scale, visual
question answering dataset for remote sensing. In: IGARSS 2021. IEEE
(2021)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Rahnemoonfar, M., etÂ al.: FloodNet: A High Resolution Aerial
Imagery Dataset for Post Flood Scene Understanding.
arXiv:2012.02951 [cs] (2020)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Sumbul, G., etÂ al.: BigEarthNet: A Large-Scale Benchmark Archive For Remote
Sensing Image Understanding. In: IGARSS. pp. 5901â€“5904 (2019)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Tucker, L.R.: Some mathematical notes on three-mode factor analysis.
Psychometrika <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">31</span>(3), 279â€“311 (1966)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Tuia, D., etÂ al.: Toward a Collective Agenda on AI for Earth Science
Data Analysis. IEEE Geoscience and Remote Sensing Magazine
<span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">9</span>(2), 88â€“104 (2021)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Wang, P., etÂ al.: Explicit Knowledge-based Reasoning for Visual
Question Answering. In: IJCAI 2017. pp. 1290â€“1296. IJCAI Organization,
Melbourne, Australia (2017)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Wu, Q., etÂ al.: Ask Me Anything: Free-Form Visual Question
Answering Based on Knowledge from External Sources. In: 2016 CVPR.
pp. 4622â€“4630. IEEE, Las Vegas, USA (2016)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Yang, Z., etÂ al.: Stacked Attention Networks for Image Question
Answering. In: 2016 CVPR. pp. 21â€“29. IEEE, Las Vegas, USA (2016)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Yu, Z., etÂ al.: Multi-modal Factorized Bilinear Pooling with
Co-attention Learning for Visual Question Answering. In: ICCV 2017.
pp. 1839â€“1848. IEEE, Venice (2017)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Yu, Z., etÂ al.: Beyond Bilinear: Generalized Multimodal Factorized
High-order Pooling for Visual Question Answering. IEEE Transactions
on Neural Networks and Learning Systems <span id="bib.bib26.1.1" class="ltx_text ltx_font_bold">29</span>(12), 5947â€“5959 (2018)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zhang, C., etÂ al.: Multimodal Intelligence: Representation Learning,
Information Fusion, and Applications. IEEE Journal of Selected Topics
in Signal Processing <span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">14</span>(3), 478â€“493 (2020)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Zheng, X., etÂ al.: Mutual Attention Inception Network for Remote
Sensing Visual Question Answering. IEEE Transactions on Geoscience
and Remote Sensing pp. 1â€“14 (2021)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Zhu, Y., etÂ al.: Aligning Books and Movies: Towards Story-Like
Visual Explanations by Watching Movies and Reading Books. In:
ICCV 2015. pp. 19â€“27. IEEE, Santiago, Chile (2015)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.11847" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.11848" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.11848">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.11848" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.11849" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 03:36:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
