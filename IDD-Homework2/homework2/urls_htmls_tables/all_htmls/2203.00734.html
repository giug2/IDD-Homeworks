<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2203.00734] Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data</title><meta property="og:description" content="Automatic player identification is an essential and complex task in sports video analysis. Different strategies have
been devised over the years, but identification based on jersey numbers is one of the most common appâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2203.00734">

<!--Generated on Mon Mar 11 10:55:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_ERROR undefined">\name</span>Divya Bhargavi <span id="id2.2.id2" class="ltx_ERROR undefined">\email</span>dbharga@amazon.com 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_ERROR undefined">\name</span>Erika Pelaez Coyotl <span id="id4.4.id4" class="ltx_ERROR undefined">\email</span>erpelaez@amazon.com 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_ERROR undefined">\name</span>Sia Gholami <span id="id6.6.id6" class="ltx_ERROR undefined">\email</span>gholami@amazon.com 
<br class="ltx_break"><span id="id7.7.id7" class="ltx_ERROR undefined">\addr</span>Amazon Web Services, CA USA
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Automatic player identification is an essential and complex task in sports video analysis. Different strategies have
been devised over the years, but identification based on jersey numbers is one of the most common approaches given its
versatility and relative simplicity. However, automatic detection of jersey numbers is still challenging due to
changing camera angles, low video resolution, small object size in wide-range shots and transient changes in the
playerâ€™s posture and movement. In this paper we present a novel approach for jersey number identification in a small,
highly imbalanced dataset from the Seattle Seahawks practice videos. We use a multi-step strategy that enforces
attention to a particular region of interest (playerâ€™s torso), to identify jersey numbers. We generate in-house
synthetic datasets of different complexities to supplement the data imbalance and scarcity in the samples. Our
multi-step pipeline first identifies and crops players in a frame using a pretrained person detection model.
We then utilize a pretrained human pose estimation model to localize jersey numbers (using torso key-points) in
the detected players, obviating the need for annotating bounding boxes for number detection. This results in images
that are on average 20x25px in size. We trained two light-weight Convolutional Neural Networks (CNNs) with different
learning objectives: multi-class for two-digit number identification and multi-label for digit-wise detection to
compare performance. Both models went through a pre-training round with the synthetic datasets and were finetuned
with the real-world dataset to achieve a final best accuracy of 89%. Our results indicate that simple models can
achieve an acceptable performance on the jersey number detection task and that synthetic data can improve the
performance dramatically (accuracy increase of Â 9% overall, Â 18% on low frequency numbers) making our approach
achieve state of the art results.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, interest in analyzing team sport videos has increased significantly in academia and
industryÂ <cite class="ltx_cite ltx_citemacro_citep">(Ye etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2005</a>; Å ari etÂ al., <a href="#bib.bib22" title="" class="ltx_ref">2008</a>; Lu etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2013</a>; Gerke etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2015</a>; Li etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>; Liu and Bhanu, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Vats etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>.
This is important for sports broadcasters and teams to understand key events in the game and
extract useful information from the videos. Use cases include identifying participating players, tracking player movement
for game statistics, measuring health and safety indicators, and automatically placing graphic overlays.
For broadcasters and teams that donâ€™t have the leeway or the capital to install hardware sensors in player wearables,
a Computer Vision (CV) based solution is the only viable option to automatically understand and generate insights
from games or practice videos. One important task in all sports CV applications is identifying players, specifically
identifying players with their jersey numbers. This task is challenging due to distortion and deformation of player
jerseys based on the player posture, movement and camera angle, rarity of labelled datasets, low-quality videos,
small image size in zoomed out videos, and warped display caused by the player movement.
(see FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Â <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>)</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Current approaches for jersey number identification consist of two steps: collecting and annotating large
datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>; Vats etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>, and training large and complex modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>; Liu and Bhanu, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Vats etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>. These approaches include either sequential training of
multiple computer vision models or training one large model, solving for 2 objectives: identifying the jersey number
location (through custom object detection models or training a custom human pose estimation model) and classifying
the jersey numberÂ <cite class="ltx_cite ltx_citemacro_citep">(Gerke etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2015</a>; Li etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>; Liu and Bhanu, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Vats etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>. These approaches are tedious, time-consuming, and cost-prohibitive thus making it
intractable for all sports organizations.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper we present a novel approach to detect jersey numbers in a small dataset consisting of practice video
footage from the Seattle Seahawks team . We use a three-step approach to number detection that leverages pretrained
models and novel synthetic datasets. We first identify and crop players in a video frame using a person detection model.
We then utilize a human pose estimation model for localizing jerseys on the detected players using the torso key-points,
obviating the need for annotating bounding boxes for number locations. This results in images that are less than
20x25 px with a high imbalance in jersey numbers (see FigureÂ <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Finally, we test two different learning approaches
for model training - multi-class and multi-label each yielding an accuracy of 88%, with an ensemble accuracy of
89% to identify jersey numbers from cropped player torsos.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Additionally, to compensate for the low number of examples in some of the jersey numbers, we propose two novel
synthetic dataset generators â€” Simple2D and Complex2D. The Simple2D generator creates two-digit number images from
different combinations of fonts and background colors to mimic those of the Seattle Seahawks jerseys. The Complex2D
generator superimposes the Simple2D numbers on random COCO datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2014</a>)</cite> images to add more complexity to the background
and make the model training robust. By pretraining our two CNNs on these synthetic datasets, we observe a 9% increase
in accuracy on the ensemble models pre-trained with synthetic data compared to the baseline models trained with the
only the Seattle Seahawks numbers. Furthermore, we observe better generalization with low data.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2203.00734/assets/figures/wideshot.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="339" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example frames from the practice videos demonstrating the challenges to identify jersey numbers in zoomed out videos.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2203.00734/assets/figures/playerposture.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="342" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Cropped players examples showing the player posture, movement and camera angle challenges to
identify jersey numbers.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Synthetic Data Generation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">CNN algorithms, that are commonly used in most CV tasks, require large datasets to learn patterns in images.
Collecting and annotating large datasets is a manual, costly and time-consuming task. Several new approaches
including Active LearningÂ <cite class="ltx_cite ltx_citemacro_citep">(Settles, <a href="#bib.bib23" title="" class="ltx_ref">2009</a>)</cite>, Zero or Few-shot learningÂ <cite class="ltx_cite ltx_citemacro_citep">(Larochelle etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2008</a>)</cite> and Synthetic data generationÂ <cite class="ltx_cite ltx_citemacro_citep">(DeÂ Campos etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2009</a>)</cite> have emerged in
recent years to tackle complexities in obtaining a large annotated dataset. Our work focuses primarily on the use
of synthetically generated data. This idea dates back to the 1990â€™sÂ <cite class="ltx_cite ltx_citemacro_citep">(Nikolenko etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> and is an active field of research that
alleviates the cost and efforts needed to obtain and manually label real-world data. Nowadays, models (pre)trained
on synthetic datasets have a broad range of utility including feature matchingÂ <cite class="ltx_cite ltx_citemacro_citep">(DeTone etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> autonomous drivingÂ <cite class="ltx_cite ltx_citemacro_citep">(Siam etÂ al., <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>, robotics
indoor and aerial navigationÂ <cite class="ltx_cite ltx_citemacro_citep">(Nikolenko, <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, scene segmentationÂ <cite class="ltx_cite ltx_citemacro_citep">(Roberts etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite> and anonymized image generation in healthcareÂ <cite class="ltx_cite ltx_citemacro_citep">(Piacentino etÂ al., <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>.
The approaches broadly adopt the following process: pre-train with synthetic data before training on real-world
scenesÂ <cite class="ltx_cite ltx_citemacro_citep">(DeTone etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Hinterstoisser etÂ al., <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>, generate composites of synthetic data and real images to create a new one that contains the desired
representationÂ <cite class="ltx_cite ltx_citemacro_citep">(Hinterstoisser etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> or generate realistic datasets using simulation engines like UnityÂ <cite class="ltx_cite ltx_citemacro_citep">(Borkman etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite> or generative models
like GANsÂ <cite class="ltx_cite ltx_citemacro_citep">(Jeon etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2021</a>; Mustikovela etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. There are limitations to each of these regimes but one of the most common pitfalls is
performance deterioration in real-world datasets. Models trained only synthetic datasets donâ€™t generalize to
real-world data; this phenomenon is called â€domain shiftâ€Â <cite class="ltx_cite ltx_citemacro_citep">(Jeon etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In order to reduce the need for annotating large dataset as well as account for the size and imbalance of the
real-world data, we generated two double-digit synthetic datasets - Simple2D and Complex2D with different levels
of complexity as described in SectionÂ <a href="#S3.SS2.SSS2" title="3.2.2 Synthetic Data Generation â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a> This helps to circumvent the domain shift when only synthetic data is
used and improves generalization on real-world data for fine-tuning.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Number Identification</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Automatic number identification in sports video has evolved from classical computer vision techniques including
feature extraction using contrast adjustment, edge detection of numbersÂ <cite class="ltx_cite ltx_citemacro_citep">(Ye etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2005</a>; Å ari etÂ al., <a href="#bib.bib22" title="" class="ltx_ref">2008</a>; Lu etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2013</a>)</cite> to deep learning-based architectures
that use CNNs for classificationÂ <cite class="ltx_cite ltx_citemacro_citep">(Gerke etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2015</a>; Li etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>; Liu and Bhanu, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Vats etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>. A fundamental problem in number identification in sports is the
jersey number distortion due to erratic and continuous player movement. The spatial transformer-based approach
introduced inÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> tries to localize and better position the number, so that the classifier has a better chance of
an accurate prediction. The faster-RCNN with pose estimation guidance mechanismÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu and Bhanu, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite> combines the detection,
classification and key-point estimation tasks in one large network to correct region proposals, reducing the
number of false negative predictions. This approach needed careful labeling of the player bounding-boxes and four
human body key-points, shoulder (right, left), hip (right, left), in addition to the numbers. It also made use of
high-resolution number images (512 px). This approach yields 92% accuracy for jersey number recognition as a whole
and 94% on the digit-wise number recognition task. However, getting the right conditions for it i.e., label the
dataset for the three tasks, acquiring high resolution images and training a large model might be challenging for
real-world cases. Furthermore, a lack of standardization and availability of public (commercial use) datasets,
makes it difficult to obtain a benchmark for the number identification task.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Task Definition</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We define a jersey number as the one or two-digit number printed on the back of a playerâ€™s shirt. The jersey number is
used to identify and distinguish players and one number is associated with exactly one player. Our solution takes
cropped images of playerâ€™s torsos as input and attempts to classify the jersey number into 101 classes
(0-99 for actual numbers and 100 for unrecognizable images/ jerseys with no numbers).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>American Football Dataset</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The data used for this work consisted of a collection of 6 practice videos from different angles for training and
additional 4 for testing from the Seattle Seahawks archives. Half of the videos were from the endzone perspective,
that is, the scoring zone between the end line and the goal line. The other half were from the sideline perspective,
the boundary line that separates the play area from the sides. Both cameras were placed on a high altitude to get a
panoramic view for the play and capture the majority of the actions taken by the players. A pitfall for collecting
data using this camera angle is that the size of a player is less than 10% of the image size when the players are
far away from the camera. In addition, the sideline view has restricted visibility of jersey numbers compared to
end-zone (see FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The videos were recorded in 1280x720 resolution and we sampled frames
from each video at 1, 5 and 10 frames per second (fps) rates. We noticed that images sampled at 5 fps sufficiently
captured all the jersey numbers in a play and we decided to use the same sampling rate throughout our solution.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2203.00734/assets/figures/perspectives.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of frames obtained from the two different angles from the training videos. Left, is the endzone
view of the players. Right is the sideline view which offers better visibility into jersey numbers. Within a play,
we can find players, observers with/without football jerseys.</figcaption>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Jersey number localization</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">To mitigate the need for annotating player location, jersey number bounding boxes and consequently training person and
jersey number detection models, we utilized pretrained models for person detection and pose estimation to localize the
jersey number region. This approach prevents the model to generate correlations with wrong features like player
background, helmets or clothing items and confining the learning to the region of interest.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">For the number localization we first use a pretrained person detector, CenternetÂ <cite class="ltx_cite ltx_citemacro_citep">(Duan etÂ al., <a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite> model (ResNet50 backbone), to
detect and crop players from an image. Instead of training a custom human key-point estimation headÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu and Bhanu, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>, we use a
pretrained, pose estimation model, AlphaPose (https://gitee.com/marcy/AlphaPose, with ResNet101 backbone), to identify
four torso key-points
(left and right - hips and shoulders) on the cropped player images from the person detection step (see FigureÂ <a href="#S3.F7" title="Figure 7 â€£ 3.2.3 Jersey number detection â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).
We use the four key-points to create a bounding box around jersey numbers. To accommodate inaccuracies in key-point
prediction and localization due to complex human poses, we increased the size of torso keypoint area by expanding the
coordinates 60% outward to better capture jersey numbers. The torso area is then cropped and used as the input for
the number prediction models discussed in SectionÂ <a href="#S3.SS2.SSS2" title="3.2.2 Synthetic Data Generation â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a> In previous works, the use of high-resolution images of
players and jersey numbers is very common. However, the American football dataset we used was captured from a birdâ€™s
eye view, where jersey numbers were smaller than 32x32 px. In fact, the average size of the torso crops is 20x25 with
the actual jersey number being even a smaller portion of this area (see FigureÂ <a href="#S3.F4" title="Figure 4 â€£ 3.2.1 Jersey number localization â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2203.00734/assets/figures/datasize.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="447" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Distribution of the sizes from person and torso bounding boxes. Note how the great majority of torso sizes is less than 32x32 px.</figcaption>
</figure>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">After player detection and jersey number localization, we generated 9,000 candidate images for number detection.
We labelled the images with Amazon SageMaker GroundTruth and noticed that 6,000 images contained non-players
(trainers, referees, watchers); the pose estimation model for jersey number localization simply identifies human
body key-points and doesnâ€™t differentiate between players and non-players. 3,000 labelled images with severe
imbalance (see FigureÂ <a href="#S3.F5" title="Figure 5 â€£ 3.2.1 Jersey number localization â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) were usable for the training.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2203.00734/assets/figures/datadistro.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Distribution of the jersey number labels in training set. Number 3 has 500+ images while numbers 43, 63, 69 and 93 have 10 images or less.</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Synthetic Data Generation</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Typically, a licensed (SVHNÂ <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow etÂ al., <a href="#bib.bib6" title="" class="ltx_ref">2013</a>)</cite>) or a large custom dataset is used for (pre)training number recognition models.
Since there are no standardized public datasets with permissive licenses, we created two 2-digit synthetic datasets
to pretrain our models. We investigated 2-digit MNISTÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun, <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>, however it did not have pixel color and font variations
needed for jersey detection and performed poorly in our tests. Hence, we generated two different synthetic datasets;
a simple two-digit (Simple2D) numbers with font and background similar to the football dataset and other with 2-digit
synthetic numbers superimposed on COCOÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2014</a>)</cite> dataset images (Complex2D) to account for variations in numbers background.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">The Simple2D dataset was generated by randomly selecting a number from a uniform distribution of 0 to 9 and randomly
scaling it. Color backgrounds (Red, Navy Blue, Green, Red, Yellow, White) and special font (Freshman ) that resembled
the team jerseys were used to generate these numbers (see FigureÂ <a href="#S3.F4" title="Figure 4 â€£ 3.2.1 Jersey number localization â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). One Light, five Medium and five Hard augmentations
(see TableÂ <a href="#S3.T1" title="Table 1 â€£ 3.2.2 Synthetic Data Generation â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) were used on each digit to be later permuted and concatenated to obtain 4000 images (100 x 100 px) of
each 2-digit number, from 00 to 99. At the end this dataset consisted of a total of 400,000 images.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">Since the real-world images had more complicated background, textures and lighting conditions, we decided to
synthetically generate another dataset (see FigureÂ <a href="#S3.F6" title="Figure 6 â€£ 3.2.2 Synthetic Data Generation â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) to increase the robustness and generalization of our pretrained
model. The complex2D dataset was designed to increase background noise by superimposing numbers from Sample2D on
random real-world images from the COCO datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2014</a>)</cite>. We generated a total of 400,000 images (4000 per class) with
noisy backgrounds.
Our algorithm is explained in more details in AlgorithmsÂ <a href="#algorithm1" title="In 3.2.2 Synthetic Data Generation â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <a href="#algorithm2" title="In 3.2.2 Synthetic Data Generation â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#algorithm3" title="In 3.2.2 Synthetic Data Generation â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>data augmentations</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1" class="ltx_p" style="width:43.4pt;">Name</span>
</span>
</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p" style="width:368.6pt;">Augmentations</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.1.1.1" class="ltx_p" style="width:43.4pt;">Light</span>
</span>
</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.2.1.1" class="ltx_p" style="width:368.6pt;">Gaussian Noise, Optical distortion</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1.1" class="ltx_p" style="width:43.4pt;">Medium</span>
</span>
</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.2.1.1" class="ltx_p" style="width:368.6pt;">Light + Grid distortion</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.1.1.1" class="ltx_p" style="width:43.4pt;">Hard</span>
</span>
</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.2.1.1" class="ltx_p" style="width:368.6pt;">Medium + Shuffling RGB channels, Random Shift-Scale-Rotation</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2203.00734/assets/figures/synthetic.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="289" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Synthetic data generation with Simple2D and Complex2D. Simple2D dataset was generated by creating numbers
in football dataset jersey colors and fonts. Several augmentations (TableÂ <a href="#S3.T1" title="Table 1 â€£ 3.2.2 Synthetic Data Generation â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) were applied on these numbers to get
Simple2D dataset. The numbers from this dataset were randomly sampled and randomly placed on COCO dataset images
to form Complex2D dataset</figcaption>
</figure>
<figure id="algorithm1" class="ltx_float ltx_algorithm">
<div id="algorithm1.2" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm1.2.1" class="ltx_listingline">

<span id="algorithm1.2.1.1" class="ltx_text ltx_font_bold">forall</span>Â <em id="algorithm1.2.1.2" class="ltx_emph ltx_font_italic">n in 0-9</em>Â <span id="algorithm1.2.1.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm1.2.2" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
select a jersey background and font color with a probability of U(1,n) = number of combinations;
</div>
<div id="algorithm1.2.3" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
choose a font size with a probability of U(a,b) if a, b are scaled factors of image size ;
</div>
<div id="algorithm1.2.4" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
paste single number with chosen font and background color and size ;
</div>
<div id="algorithm1.2.5" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 

</div>
<div id="algorithm1.2.6" class="ltx_listingline"> end forall
</div>
<div id="algorithm1.2.7" class="ltx_listingline">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="algorithm1.3.1.1" class="ltx_text ltx_font_bold">AlgorithmÂ 1</span> </span>Number generation</figcaption>
</figure>
<figure id="algorithm2" class="ltx_float ltx_algorithm">
<div id="algorithm2.2" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm2.2.1" class="ltx_listingline">

<span id="algorithm2.2.1.1" class="ltx_text ltx_font_bold">forall</span>Â <em id="algorithm2.2.1.2" class="ltx_emph ltx_font_italic">n in 0-99</em>Â <span id="algorithm2.2.1.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm2.2.2" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span id="algorithm2.2.2.1" class="ltx_text ltx_font_bold">forall</span>Â <em id="algorithm2.2.2.2" class="ltx_emph ltx_font_italic">background colors</em>Â <span id="algorithm2.2.2.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm2.2.3" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
generate 1000 images;
</div>
<div id="algorithm2.2.4" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span id="algorithm2.2.4.1" class="ltx_text ltx_font_bold">if</span>Â <em id="algorithm2.2.4.2" class="ltx_emph ltx_font_italic">single digit</em>Â <span id="algorithm2.2.4.3" class="ltx_text ltx_font_bold">then</span>
</div>
<div id="algorithm2.2.5" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
perform light, medium and hard augmentations;
</div>
<div id="algorithm2.2.6" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
scale image to 100x100 px;
</div>
<div id="algorithm2.2.7" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 

</div>
<div id="algorithm2.2.8" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â <span id="algorithm2.2.8.1" class="ltx_text ltx_font_bold">else</span> 
</div>
<div id="algorithm2.2.9" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
perform light, medium and hard augmentations on each digit;
</div>
<div id="algorithm2.2.10" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
concatinate digits ;
</div>
<div id="algorithm2.2.11" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
scale image to 100x100 px;
</div>
<div id="algorithm2.2.12" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 

</div>
<div id="algorithm2.2.13" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â  end if
</div>
<div id="algorithm2.2.14" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
</div>
<div id="algorithm2.2.15" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â  end forall
</div>
<div id="algorithm2.2.16" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
</div>
<div id="algorithm2.2.17" class="ltx_listingline"> end forall
</div>
<div id="algorithm2.2.18" class="ltx_listingline">randomly sample 4000 images per number across all color combinations ;
</div>
<div id="algorithm2.2.19" class="ltx_listingline">

</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="algorithm2.3.1.1" class="ltx_text ltx_font_bold">AlgorithmÂ 2</span> </span>Simple2D</figcaption>
</figure>
<figure id="algorithm3" class="ltx_float ltx_algorithm">
<div id="algorithm3.2" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm3.2.1" class="ltx_listingline">

<span id="algorithm3.2.1.1" class="ltx_text ltx_font_bold">forall</span>Â <em id="algorithm3.2.1.2" class="ltx_emph ltx_font_italic">n in 0-99</em>Â <span id="algorithm3.2.1.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm3.2.2" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
select a random image from COCO dataset;
</div>
<div id="algorithm3.2.3" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
select a random jersey number image;
</div>
<div id="algorithm3.2.4" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
super-impose jersey number at a random position in the COCO image;
</div>
<div id="algorithm3.2.5" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
rescale image to 100x100 px;
</div>
<div id="algorithm3.2.6" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
continue until 4000 images per number are obtained;
</div>
<div id="algorithm3.2.7" class="ltx_listingline">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 

</div>
<div id="algorithm3.2.8" class="ltx_listingline"> end forall
</div>
<div id="algorithm3.2.9" class="ltx_listingline">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="algorithm3.3.1.1" class="ltx_text ltx_font_bold">AlgorithmÂ 3</span> </span>Complex2D</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Jersey number detection</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">After the number localization step above, two models were sequentially pretrained with the synthetic datasets
(Simple2D to Complex2D) and fine-tuned with the real-world football dataset (see FigureÂ <a href="#S3.F7" title="Figure 7 â€£ 3.2.3 Jersey number detection â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). The idea of training a
model with increasingly difficult samples is called curriculum learning. This technique has empirically shown accuracy
increase and faster convergenceÂ <cite class="ltx_cite ltx_citemacro_citep">(Weinshall etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2018</a>; Hacohen and Weinshall, <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>. One of the challenges of implementing curriculum learning is manually ranking
difficulty in the training setÂ <cite class="ltx_cite ltx_citemacro_citep">(Weinshall etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2018</a>)</cite>. In our case, the synthetic data was generated explicitly in this manner
(simple to complex) and our training regime adopted this order, thus, bypassing this challenge.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">Both models used a ResNet50Â <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite> architecture with deep residual connections, as backbone and a final layer predicting
classes (jersey numbers). The first model was a multi-class image classifier to detect two-digit number with a total
of 101 different classes (numbers from 0 - 99 plus an unrecognizable class). The second model was a multi-class
multi-label classifier with 21 classes to detect single digits (10 digits for each side- right, left numbers, plus an
unrecognizable class).</p>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p id="S3.SS2.SSS3.p3.2" class="ltx_p">We define the i-th input feature <math id="S3.SS2.SSS3.p3.1.m1.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S3.SS2.SSS3.p3.1.m1.1a"><msub id="S3.SS2.SSS3.p3.1.m1.1.1" xref="S3.SS2.SSS3.p3.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p3.1.m1.1.1.2" xref="S3.SS2.SSS3.p3.1.m1.1.1.2.cmml">X</mi><mi id="S3.SS2.SSS3.p3.1.m1.1.1.3" xref="S3.SS2.SSS3.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.1.m1.1b"><apply id="S3.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1.2">ğ‘‹</ci><ci id="S3.SS2.SSS3.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.1.m1.1c">X_{i}</annotation></semantics></math> (cropped image of a player) with the label <math id="S3.SS2.SSS3.p3.2.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S3.SS2.SSS3.p3.2.m2.1a"><msub id="S3.SS2.SSS3.p3.2.m2.1.1" xref="S3.SS2.SSS3.p3.2.m2.1.1.cmml"><mi id="S3.SS2.SSS3.p3.2.m2.1.1.2" xref="S3.SS2.SSS3.p3.2.m2.1.1.2.cmml">y</mi><mi id="S3.SS2.SSS3.p3.2.m2.1.1.3" xref="S3.SS2.SSS3.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.2.m2.1b"><apply id="S3.SS2.SSS3.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p3.2.m2.1.1.2">ğ‘¦</ci><ci id="S3.SS2.SSS3.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p3.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.2.m2.1c">y_{i}</annotation></semantics></math> (0-99 for actual numbers and 100 for
unrecognizable). Our multi-class model was optimized with the following loss function:</p>
</div>
<div id="S3.SS2.SSS3.p4" class="ltx_para">
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.1" class="ltx_Math" alttext="L_{mc}=\sum_{i}{L_{i}}=-\sum_{i}{y_{i}\log\hat{y}_{mc}(X_{i})}" display="block"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><msub id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.3.2" xref="S3.Ex1.m1.1.1.3.2.cmml">L</mi><mrow id="S3.Ex1.m1.1.1.3.3" xref="S3.Ex1.m1.1.1.3.3.cmml"><mi id="S3.Ex1.m1.1.1.3.3.2" xref="S3.Ex1.m1.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.3.3.1" xref="S3.Ex1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.1.3.3.3" xref="S3.Ex1.m1.1.1.3.3.3.cmml">c</mi></mrow></msub><mo rspace="0.111em" id="S3.Ex1.m1.1.1.4" xref="S3.Ex1.m1.1.1.4.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.5" xref="S3.Ex1.m1.1.1.5.cmml"><munder id="S3.Ex1.m1.1.1.5.1" xref="S3.Ex1.m1.1.1.5.1.cmml"><mo movablelimits="false" id="S3.Ex1.m1.1.1.5.1.2" xref="S3.Ex1.m1.1.1.5.1.2.cmml">âˆ‘</mo><mi id="S3.Ex1.m1.1.1.5.1.3" xref="S3.Ex1.m1.1.1.5.1.3.cmml">i</mi></munder><msub id="S3.Ex1.m1.1.1.5.2" xref="S3.Ex1.m1.1.1.5.2.cmml"><mi id="S3.Ex1.m1.1.1.5.2.2" xref="S3.Ex1.m1.1.1.5.2.2.cmml">L</mi><mi id="S3.Ex1.m1.1.1.5.2.3" xref="S3.Ex1.m1.1.1.5.2.3.cmml">i</mi></msub></mrow><mo id="S3.Ex1.m1.1.1.6" xref="S3.Ex1.m1.1.1.6.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1a" xref="S3.Ex1.m1.1.1.1.cmml">âˆ’</mo><mrow id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><munder id="S3.Ex1.m1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.Ex1.m1.1.1.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.2.2.cmml">âˆ‘</mo><mi id="S3.Ex1.m1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.2.3.cmml">i</mi></munder><mrow id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml"><msub id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S3.Ex1.m1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0.167em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.4" xref="S3.Ex1.m1.1.1.1.1.1.4.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.4.1" xref="S3.Ex1.m1.1.1.1.1.1.4.1.cmml">log</mi><mo lspace="0.167em" id="S3.Ex1.m1.1.1.1.1.1.4a" xref="S3.Ex1.m1.1.1.1.1.1.4.cmml">â¡</mo><msub id="S3.Ex1.m1.1.1.1.1.1.4.2" xref="S3.Ex1.m1.1.1.1.1.1.4.2.cmml"><mover accent="true" id="S3.Ex1.m1.1.1.1.1.1.4.2.2" xref="S3.Ex1.m1.1.1.1.1.1.4.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.4.2.2.2" xref="S3.Ex1.m1.1.1.1.1.1.4.2.2.2.cmml">y</mi><mo id="S3.Ex1.m1.1.1.1.1.1.4.2.2.1" xref="S3.Ex1.m1.1.1.1.1.1.4.2.2.1.cmml">^</mo></mover><mrow id="S3.Ex1.m1.1.1.1.1.1.4.2.3" xref="S3.Ex1.m1.1.1.1.1.1.4.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.4.2.3.2" xref="S3.Ex1.m1.1.1.1.1.1.4.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.4.2.3.1" xref="S3.Ex1.m1.1.1.1.1.1.4.2.3.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.1.1.1.1.4.2.3.3" xref="S3.Ex1.m1.1.1.1.1.1.4.2.3.3.cmml">c</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.2a" xref="S3.Ex1.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><and id="S3.Ex1.m1.1.1a.cmml" xref="S3.Ex1.m1.1.1"></and><apply id="S3.Ex1.m1.1.1b.cmml" xref="S3.Ex1.m1.1.1"><eq id="S3.Ex1.m1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.4"></eq><apply id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2">ğ¿</ci><apply id="S3.Ex1.m1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3"><times id="S3.Ex1.m1.1.1.3.3.1.cmml" xref="S3.Ex1.m1.1.1.3.3.1"></times><ci id="S3.Ex1.m1.1.1.3.3.2.cmml" xref="S3.Ex1.m1.1.1.3.3.2">ğ‘š</ci><ci id="S3.Ex1.m1.1.1.3.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3.3">ğ‘</ci></apply></apply><apply id="S3.Ex1.m1.1.1.5.cmml" xref="S3.Ex1.m1.1.1.5"><apply id="S3.Ex1.m1.1.1.5.1.cmml" xref="S3.Ex1.m1.1.1.5.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.5.1.1.cmml" xref="S3.Ex1.m1.1.1.5.1">subscript</csymbol><sum id="S3.Ex1.m1.1.1.5.1.2.cmml" xref="S3.Ex1.m1.1.1.5.1.2"></sum><ci id="S3.Ex1.m1.1.1.5.1.3.cmml" xref="S3.Ex1.m1.1.1.5.1.3">ğ‘–</ci></apply><apply id="S3.Ex1.m1.1.1.5.2.cmml" xref="S3.Ex1.m1.1.1.5.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.5.2.1.cmml" xref="S3.Ex1.m1.1.1.5.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.5.2.2.cmml" xref="S3.Ex1.m1.1.1.5.2.2">ğ¿</ci><ci id="S3.Ex1.m1.1.1.5.2.3.cmml" xref="S3.Ex1.m1.1.1.5.2.3">ğ‘–</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1c.cmml" xref="S3.Ex1.m1.1.1"><eq id="S3.Ex1.m1.1.1.6.cmml" xref="S3.Ex1.m1.1.1.6"></eq><share href="#S3.Ex1.m1.1.1.5.cmml" id="S3.Ex1.m1.1.1d.cmml" xref="S3.Ex1.m1.1.1"></share><apply id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><minus id="S3.Ex1.m1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1"></minus><apply id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1"><apply id="S3.Ex1.m1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.Ex1.m1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2"></sum><ci id="S3.Ex1.m1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2"></times><apply id="S3.Ex1.m1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.2">ğ‘¦</ci><ci id="S3.Ex1.m1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4"><log id="S3.Ex1.m1.1.1.1.1.1.4.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.1"></log><apply id="S3.Ex1.m1.1.1.1.1.1.4.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.4.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.2">subscript</csymbol><apply id="S3.Ex1.m1.1.1.1.1.1.4.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.2.2"><ci id="S3.Ex1.m1.1.1.1.1.1.4.2.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.2.2.1">^</ci><ci id="S3.Ex1.m1.1.1.1.1.1.4.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.2.2.2">ğ‘¦</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.4.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.2.3"><times id="S3.Ex1.m1.1.1.1.1.1.4.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.2.3.1"></times><ci id="S3.Ex1.m1.1.1.1.1.1.4.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.2.3.2">ğ‘š</ci><ci id="S3.Ex1.m1.1.1.1.1.1.4.2.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.2.3.3">ğ‘</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">L_{mc}=\sum_{i}{L_{i}}=-\sum_{i}{y_{i}\log\hat{y}_{mc}(X_{i})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS3.p5" class="ltx_para">
<p id="S3.SS2.SSS3.p5.2" class="ltx_p">where <math id="S3.SS2.SSS3.p5.1.m1.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S3.SS2.SSS3.p5.1.m1.1a"><msub id="S3.SS2.SSS3.p5.1.m1.1.1" xref="S3.SS2.SSS3.p5.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p5.1.m1.1.1.2" xref="S3.SS2.SSS3.p5.1.m1.1.1.2.cmml">y</mi><mi id="S3.SS2.SSS3.p5.1.m1.1.1.3" xref="S3.SS2.SSS3.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p5.1.m1.1b"><apply id="S3.SS2.SSS3.p5.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p5.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p5.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p5.1.m1.1.1.2">ğ‘¦</ci><ci id="S3.SS2.SSS3.p5.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p5.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p5.1.m1.1c">y_{i}</annotation></semantics></math> is the true label and <math id="S3.SS2.SSS3.p5.2.m2.1" class="ltx_Math" alttext="\hat{y}_{mc}" display="inline"><semantics id="S3.SS2.SSS3.p5.2.m2.1a"><msub id="S3.SS2.SSS3.p5.2.m2.1.1" xref="S3.SS2.SSS3.p5.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.SSS3.p5.2.m2.1.1.2" xref="S3.SS2.SSS3.p5.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS3.p5.2.m2.1.1.2.2" xref="S3.SS2.SSS3.p5.2.m2.1.1.2.2.cmml">y</mi><mo id="S3.SS2.SSS3.p5.2.m2.1.1.2.1" xref="S3.SS2.SSS3.p5.2.m2.1.1.2.1.cmml">^</mo></mover><mrow id="S3.SS2.SSS3.p5.2.m2.1.1.3" xref="S3.SS2.SSS3.p5.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS3.p5.2.m2.1.1.3.2" xref="S3.SS2.SSS3.p5.2.m2.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p5.2.m2.1.1.3.1" xref="S3.SS2.SSS3.p5.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p5.2.m2.1.1.3.3" xref="S3.SS2.SSS3.p5.2.m2.1.1.3.3.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p5.2.m2.1b"><apply id="S3.SS2.SSS3.p5.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p5.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p5.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.SSS3.p5.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p5.2.m2.1.1.2"><ci id="S3.SS2.SSS3.p5.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS3.p5.2.m2.1.1.2.1">^</ci><ci id="S3.SS2.SSS3.p5.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS3.p5.2.m2.1.1.2.2">ğ‘¦</ci></apply><apply id="S3.SS2.SSS3.p5.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p5.2.m2.1.1.3"><times id="S3.SS2.SSS3.p5.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS3.p5.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS3.p5.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS3.p5.2.m2.1.1.3.2">ğ‘š</ci><ci id="S3.SS2.SSS3.p5.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS3.p5.2.m2.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p5.2.m2.1c">\hat{y}_{mc}</annotation></semantics></math> is calculated as a softmax over scores computed by the multi-class
model as follows:</p>
</div>
<div id="S3.SS2.SSS3.p6" class="ltx_para">
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.2" class="ltx_Math" alttext="\hat{y}_{mc}(X_{i})=\sigma(\vec{Z})" display="block"><semantics id="S3.Ex2.m1.2a"><mrow id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml"><mrow id="S3.Ex2.m1.2.2.1" xref="S3.Ex2.m1.2.2.1.cmml"><msub id="S3.Ex2.m1.2.2.1.3" xref="S3.Ex2.m1.2.2.1.3.cmml"><mover accent="true" id="S3.Ex2.m1.2.2.1.3.2" xref="S3.Ex2.m1.2.2.1.3.2.cmml"><mi id="S3.Ex2.m1.2.2.1.3.2.2" xref="S3.Ex2.m1.2.2.1.3.2.2.cmml">y</mi><mo id="S3.Ex2.m1.2.2.1.3.2.1" xref="S3.Ex2.m1.2.2.1.3.2.1.cmml">^</mo></mover><mrow id="S3.Ex2.m1.2.2.1.3.3" xref="S3.Ex2.m1.2.2.1.3.3.cmml"><mi id="S3.Ex2.m1.2.2.1.3.3.2" xref="S3.Ex2.m1.2.2.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.2.1.3.3.1" xref="S3.Ex2.m1.2.2.1.3.3.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.2.2.1.3.3.3" xref="S3.Ex2.m1.2.2.1.3.3.3.cmml">c</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.2.1.2" xref="S3.Ex2.m1.2.2.1.2.cmml">â€‹</mo><mrow id="S3.Ex2.m1.2.2.1.1.1" xref="S3.Ex2.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.2.2.1.1.1.2" xref="S3.Ex2.m1.2.2.1.1.1.1.cmml">(</mo><msub id="S3.Ex2.m1.2.2.1.1.1.1" xref="S3.Ex2.m1.2.2.1.1.1.1.cmml"><mi id="S3.Ex2.m1.2.2.1.1.1.1.2" xref="S3.Ex2.m1.2.2.1.1.1.1.2.cmml">X</mi><mi id="S3.Ex2.m1.2.2.1.1.1.1.3" xref="S3.Ex2.m1.2.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.Ex2.m1.2.2.1.1.1.3" xref="S3.Ex2.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.2.2.2" xref="S3.Ex2.m1.2.2.2.cmml">=</mo><mrow id="S3.Ex2.m1.2.2.3" xref="S3.Ex2.m1.2.2.3.cmml"><mi id="S3.Ex2.m1.2.2.3.2" xref="S3.Ex2.m1.2.2.3.2.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.2.3.1" xref="S3.Ex2.m1.2.2.3.1.cmml">â€‹</mo><mrow id="S3.Ex2.m1.2.2.3.3.2" xref="S3.Ex2.m1.1.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.2.2.3.3.2.1" xref="S3.Ex2.m1.1.1.cmml">(</mo><mover accent="true" id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.2" xref="S3.Ex2.m1.1.1.2.cmml">Z</mi><mo stretchy="false" id="S3.Ex2.m1.1.1.1" xref="S3.Ex2.m1.1.1.1.cmml">â†’</mo></mover><mo stretchy="false" id="S3.Ex2.m1.2.2.3.3.2.2" xref="S3.Ex2.m1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.2b"><apply id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2"><eq id="S3.Ex2.m1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2"></eq><apply id="S3.Ex2.m1.2.2.1.cmml" xref="S3.Ex2.m1.2.2.1"><times id="S3.Ex2.m1.2.2.1.2.cmml" xref="S3.Ex2.m1.2.2.1.2"></times><apply id="S3.Ex2.m1.2.2.1.3.cmml" xref="S3.Ex2.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.3.1.cmml" xref="S3.Ex2.m1.2.2.1.3">subscript</csymbol><apply id="S3.Ex2.m1.2.2.1.3.2.cmml" xref="S3.Ex2.m1.2.2.1.3.2"><ci id="S3.Ex2.m1.2.2.1.3.2.1.cmml" xref="S3.Ex2.m1.2.2.1.3.2.1">^</ci><ci id="S3.Ex2.m1.2.2.1.3.2.2.cmml" xref="S3.Ex2.m1.2.2.1.3.2.2">ğ‘¦</ci></apply><apply id="S3.Ex2.m1.2.2.1.3.3.cmml" xref="S3.Ex2.m1.2.2.1.3.3"><times id="S3.Ex2.m1.2.2.1.3.3.1.cmml" xref="S3.Ex2.m1.2.2.1.3.3.1"></times><ci id="S3.Ex2.m1.2.2.1.3.3.2.cmml" xref="S3.Ex2.m1.2.2.1.3.3.2">ğ‘š</ci><ci id="S3.Ex2.m1.2.2.1.3.3.3.cmml" xref="S3.Ex2.m1.2.2.1.3.3.3">ğ‘</ci></apply></apply><apply id="S3.Ex2.m1.2.2.1.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.1.1.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.2">ğ‘‹</ci><ci id="S3.Ex2.m1.2.2.1.1.1.1.3.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S3.Ex2.m1.2.2.3.cmml" xref="S3.Ex2.m1.2.2.3"><times id="S3.Ex2.m1.2.2.3.1.cmml" xref="S3.Ex2.m1.2.2.3.1"></times><ci id="S3.Ex2.m1.2.2.3.2.cmml" xref="S3.Ex2.m1.2.2.3.2">ğœ</ci><apply id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.2.2.3.3.2"><ci id="S3.Ex2.m1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1">â†’</ci><ci id="S3.Ex2.m1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.2">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.2c">\hat{y}_{mc}(X_{i})=\sigma(\vec{Z})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table id="S3.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex3.m1.1" class="ltx_Math" alttext="\sigma(\vec{Z})_{k}=\frac{e^{Z_{k}}}{\sum_{j=0}^{100}e^{Z_{j}}}" display="block"><semantics id="S3.Ex3.m1.1a"><mrow id="S3.Ex3.m1.1.2" xref="S3.Ex3.m1.1.2.cmml"><mrow id="S3.Ex3.m1.1.2.2" xref="S3.Ex3.m1.1.2.2.cmml"><mi id="S3.Ex3.m1.1.2.2.2" xref="S3.Ex3.m1.1.2.2.2.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.1.2.2.1" xref="S3.Ex3.m1.1.2.2.1.cmml">â€‹</mo><msub id="S3.Ex3.m1.1.2.2.3" xref="S3.Ex3.m1.1.2.2.3.cmml"><mrow id="S3.Ex3.m1.1.2.2.3.2.2" xref="S3.Ex3.m1.1.1.cmml"><mo stretchy="false" id="S3.Ex3.m1.1.2.2.3.2.2.1" xref="S3.Ex3.m1.1.1.cmml">(</mo><mover accent="true" id="S3.Ex3.m1.1.1" xref="S3.Ex3.m1.1.1.cmml"><mi id="S3.Ex3.m1.1.1.2" xref="S3.Ex3.m1.1.1.2.cmml">Z</mi><mo stretchy="false" id="S3.Ex3.m1.1.1.1" xref="S3.Ex3.m1.1.1.1.cmml">â†’</mo></mover><mo stretchy="false" id="S3.Ex3.m1.1.2.2.3.2.2.2" xref="S3.Ex3.m1.1.1.cmml">)</mo></mrow><mi id="S3.Ex3.m1.1.2.2.3.3" xref="S3.Ex3.m1.1.2.2.3.3.cmml">k</mi></msub></mrow><mo id="S3.Ex3.m1.1.2.1" xref="S3.Ex3.m1.1.2.1.cmml">=</mo><mfrac id="S3.Ex3.m1.1.2.3" xref="S3.Ex3.m1.1.2.3.cmml"><msup id="S3.Ex3.m1.1.2.3.2" xref="S3.Ex3.m1.1.2.3.2.cmml"><mi id="S3.Ex3.m1.1.2.3.2.2" xref="S3.Ex3.m1.1.2.3.2.2.cmml">e</mi><msub id="S3.Ex3.m1.1.2.3.2.3" xref="S3.Ex3.m1.1.2.3.2.3.cmml"><mi id="S3.Ex3.m1.1.2.3.2.3.2" xref="S3.Ex3.m1.1.2.3.2.3.2.cmml">Z</mi><mi id="S3.Ex3.m1.1.2.3.2.3.3" xref="S3.Ex3.m1.1.2.3.2.3.3.cmml">k</mi></msub></msup><mrow id="S3.Ex3.m1.1.2.3.3" xref="S3.Ex3.m1.1.2.3.3.cmml"><msubsup id="S3.Ex3.m1.1.2.3.3.1" xref="S3.Ex3.m1.1.2.3.3.1.cmml"><mo id="S3.Ex3.m1.1.2.3.3.1.2.2" xref="S3.Ex3.m1.1.2.3.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex3.m1.1.2.3.3.1.2.3" xref="S3.Ex3.m1.1.2.3.3.1.2.3.cmml"><mi id="S3.Ex3.m1.1.2.3.3.1.2.3.2" xref="S3.Ex3.m1.1.2.3.3.1.2.3.2.cmml">j</mi><mo id="S3.Ex3.m1.1.2.3.3.1.2.3.1" xref="S3.Ex3.m1.1.2.3.3.1.2.3.1.cmml">=</mo><mn id="S3.Ex3.m1.1.2.3.3.1.2.3.3" xref="S3.Ex3.m1.1.2.3.3.1.2.3.3.cmml">0</mn></mrow><mn id="S3.Ex3.m1.1.2.3.3.1.3" xref="S3.Ex3.m1.1.2.3.3.1.3.cmml">100</mn></msubsup><msup id="S3.Ex3.m1.1.2.3.3.2" xref="S3.Ex3.m1.1.2.3.3.2.cmml"><mi id="S3.Ex3.m1.1.2.3.3.2.2" xref="S3.Ex3.m1.1.2.3.3.2.2.cmml">e</mi><msub id="S3.Ex3.m1.1.2.3.3.2.3" xref="S3.Ex3.m1.1.2.3.3.2.3.cmml"><mi id="S3.Ex3.m1.1.2.3.3.2.3.2" xref="S3.Ex3.m1.1.2.3.3.2.3.2.cmml">Z</mi><mi id="S3.Ex3.m1.1.2.3.3.2.3.3" xref="S3.Ex3.m1.1.2.3.3.2.3.3.cmml">j</mi></msub></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.1b"><apply id="S3.Ex3.m1.1.2.cmml" xref="S3.Ex3.m1.1.2"><eq id="S3.Ex3.m1.1.2.1.cmml" xref="S3.Ex3.m1.1.2.1"></eq><apply id="S3.Ex3.m1.1.2.2.cmml" xref="S3.Ex3.m1.1.2.2"><times id="S3.Ex3.m1.1.2.2.1.cmml" xref="S3.Ex3.m1.1.2.2.1"></times><ci id="S3.Ex3.m1.1.2.2.2.cmml" xref="S3.Ex3.m1.1.2.2.2">ğœ</ci><apply id="S3.Ex3.m1.1.2.2.3.cmml" xref="S3.Ex3.m1.1.2.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.2.2.3.1.cmml" xref="S3.Ex3.m1.1.2.2.3">subscript</csymbol><apply id="S3.Ex3.m1.1.1.cmml" xref="S3.Ex3.m1.1.2.2.3.2.2"><ci id="S3.Ex3.m1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1">â†’</ci><ci id="S3.Ex3.m1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.2">ğ‘</ci></apply><ci id="S3.Ex3.m1.1.2.2.3.3.cmml" xref="S3.Ex3.m1.1.2.2.3.3">ğ‘˜</ci></apply></apply><apply id="S3.Ex3.m1.1.2.3.cmml" xref="S3.Ex3.m1.1.2.3"><divide id="S3.Ex3.m1.1.2.3.1.cmml" xref="S3.Ex3.m1.1.2.3"></divide><apply id="S3.Ex3.m1.1.2.3.2.cmml" xref="S3.Ex3.m1.1.2.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.2.3.2.1.cmml" xref="S3.Ex3.m1.1.2.3.2">superscript</csymbol><ci id="S3.Ex3.m1.1.2.3.2.2.cmml" xref="S3.Ex3.m1.1.2.3.2.2">ğ‘’</ci><apply id="S3.Ex3.m1.1.2.3.2.3.cmml" xref="S3.Ex3.m1.1.2.3.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.2.3.2.3.1.cmml" xref="S3.Ex3.m1.1.2.3.2.3">subscript</csymbol><ci id="S3.Ex3.m1.1.2.3.2.3.2.cmml" xref="S3.Ex3.m1.1.2.3.2.3.2">ğ‘</ci><ci id="S3.Ex3.m1.1.2.3.2.3.3.cmml" xref="S3.Ex3.m1.1.2.3.2.3.3">ğ‘˜</ci></apply></apply><apply id="S3.Ex3.m1.1.2.3.3.cmml" xref="S3.Ex3.m1.1.2.3.3"><apply id="S3.Ex3.m1.1.2.3.3.1.cmml" xref="S3.Ex3.m1.1.2.3.3.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.2.3.3.1.1.cmml" xref="S3.Ex3.m1.1.2.3.3.1">superscript</csymbol><apply id="S3.Ex3.m1.1.2.3.3.1.2.cmml" xref="S3.Ex3.m1.1.2.3.3.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.2.3.3.1.2.1.cmml" xref="S3.Ex3.m1.1.2.3.3.1">subscript</csymbol><sum id="S3.Ex3.m1.1.2.3.3.1.2.2.cmml" xref="S3.Ex3.m1.1.2.3.3.1.2.2"></sum><apply id="S3.Ex3.m1.1.2.3.3.1.2.3.cmml" xref="S3.Ex3.m1.1.2.3.3.1.2.3"><eq id="S3.Ex3.m1.1.2.3.3.1.2.3.1.cmml" xref="S3.Ex3.m1.1.2.3.3.1.2.3.1"></eq><ci id="S3.Ex3.m1.1.2.3.3.1.2.3.2.cmml" xref="S3.Ex3.m1.1.2.3.3.1.2.3.2">ğ‘—</ci><cn type="integer" id="S3.Ex3.m1.1.2.3.3.1.2.3.3.cmml" xref="S3.Ex3.m1.1.2.3.3.1.2.3.3">0</cn></apply></apply><cn type="integer" id="S3.Ex3.m1.1.2.3.3.1.3.cmml" xref="S3.Ex3.m1.1.2.3.3.1.3">100</cn></apply><apply id="S3.Ex3.m1.1.2.3.3.2.cmml" xref="S3.Ex3.m1.1.2.3.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.2.3.3.2.1.cmml" xref="S3.Ex3.m1.1.2.3.3.2">superscript</csymbol><ci id="S3.Ex3.m1.1.2.3.3.2.2.cmml" xref="S3.Ex3.m1.1.2.3.3.2.2">ğ‘’</ci><apply id="S3.Ex3.m1.1.2.3.3.2.3.cmml" xref="S3.Ex3.m1.1.2.3.3.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.2.3.3.2.3.1.cmml" xref="S3.Ex3.m1.1.2.3.3.2.3">subscript</csymbol><ci id="S3.Ex3.m1.1.2.3.3.2.3.2.cmml" xref="S3.Ex3.m1.1.2.3.3.2.3.2">ğ‘</ci><ci id="S3.Ex3.m1.1.2.3.3.2.3.3.cmml" xref="S3.Ex3.m1.1.2.3.3.2.3.3">ğ‘—</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.1c">\sigma(\vec{Z})_{k}=\frac{e^{Z_{k}}}{\sum_{j=0}^{100}e^{Z_{j}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS3.p7" class="ltx_para">
<p id="S3.SS2.SSS3.p7.3" class="ltx_p">Where <math id="S3.SS2.SSS3.p7.1.m1.1" class="ltx_Math" alttext="\vec{Z}" display="inline"><semantics id="S3.SS2.SSS3.p7.1.m1.1a"><mover accent="true" id="S3.SS2.SSS3.p7.1.m1.1.1" xref="S3.SS2.SSS3.p7.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p7.1.m1.1.1.2" xref="S3.SS2.SSS3.p7.1.m1.1.1.2.cmml">Z</mi><mo stretchy="false" id="S3.SS2.SSS3.p7.1.m1.1.1.1" xref="S3.SS2.SSS3.p7.1.m1.1.1.1.cmml">â†’</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p7.1.m1.1b"><apply id="S3.SS2.SSS3.p7.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p7.1.m1.1.1"><ci id="S3.SS2.SSS3.p7.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p7.1.m1.1.1.1">â†’</ci><ci id="S3.SS2.SSS3.p7.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p7.1.m1.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p7.1.m1.1c">\vec{Z}</annotation></semantics></math> is the outputs from the last layer of the multiclass model consists of <math id="S3.SS2.SSS3.p7.2.m2.3" class="ltx_Math" alttext="(z_{0},...,z_{1}00)" display="inline"><semantics id="S3.SS2.SSS3.p7.2.m2.3a"><mrow id="S3.SS2.SSS3.p7.2.m2.3.3.2" xref="S3.SS2.SSS3.p7.2.m2.3.3.3.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p7.2.m2.3.3.2.3" xref="S3.SS2.SSS3.p7.2.m2.3.3.3.cmml">(</mo><msub id="S3.SS2.SSS3.p7.2.m2.2.2.1.1" xref="S3.SS2.SSS3.p7.2.m2.2.2.1.1.cmml"><mi id="S3.SS2.SSS3.p7.2.m2.2.2.1.1.2" xref="S3.SS2.SSS3.p7.2.m2.2.2.1.1.2.cmml">z</mi><mn id="S3.SS2.SSS3.p7.2.m2.2.2.1.1.3" xref="S3.SS2.SSS3.p7.2.m2.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.SS2.SSS3.p7.2.m2.3.3.2.4" xref="S3.SS2.SSS3.p7.2.m2.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS3.p7.2.m2.1.1" xref="S3.SS2.SSS3.p7.2.m2.1.1.cmml">â€¦</mi><mo id="S3.SS2.SSS3.p7.2.m2.3.3.2.5" xref="S3.SS2.SSS3.p7.2.m2.3.3.3.cmml">,</mo><mrow id="S3.SS2.SSS3.p7.2.m2.3.3.2.2" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.cmml"><msub id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.2" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.2.cmml">z</mi><mn id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.3" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.1" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.1.cmml">â€‹</mo><mn id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.3" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.3.cmml">00</mn></mrow><mo stretchy="false" id="S3.SS2.SSS3.p7.2.m2.3.3.2.6" xref="S3.SS2.SSS3.p7.2.m2.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p7.2.m2.3b"><vector id="S3.SS2.SSS3.p7.2.m2.3.3.3.cmml" xref="S3.SS2.SSS3.p7.2.m2.3.3.2"><apply id="S3.SS2.SSS3.p7.2.m2.2.2.1.1.cmml" xref="S3.SS2.SSS3.p7.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p7.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.SSS3.p7.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p7.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.SSS3.p7.2.m2.2.2.1.1.2">ğ‘§</ci><cn type="integer" id="S3.SS2.SSS3.p7.2.m2.2.2.1.1.3.cmml" xref="S3.SS2.SSS3.p7.2.m2.2.2.1.1.3">0</cn></apply><ci id="S3.SS2.SSS3.p7.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p7.2.m2.1.1">â€¦</ci><apply id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.cmml" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2"><times id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.1.cmml" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.1"></times><apply id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.2">ğ‘§</ci><cn type="integer" id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.3.cmml" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.2.3">1</cn></apply><cn type="integer" id="S3.SS2.SSS3.p7.2.m2.3.3.2.2.3.cmml" xref="S3.SS2.SSS3.p7.2.m2.3.3.2.2.3">00</cn></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p7.2.m2.3c">(z_{0},...,z_{1}00)</annotation></semantics></math> given <math id="S3.SS2.SSS3.p7.3.m3.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S3.SS2.SSS3.p7.3.m3.1a"><msub id="S3.SS2.SSS3.p7.3.m3.1.1" xref="S3.SS2.SSS3.p7.3.m3.1.1.cmml"><mi id="S3.SS2.SSS3.p7.3.m3.1.1.2" xref="S3.SS2.SSS3.p7.3.m3.1.1.2.cmml">X</mi><mi id="S3.SS2.SSS3.p7.3.m3.1.1.3" xref="S3.SS2.SSS3.p7.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p7.3.m3.1b"><apply id="S3.SS2.SSS3.p7.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p7.3.m3.1.1.1.cmml" xref="S3.SS2.SSS3.p7.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p7.3.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p7.3.m3.1.1.2">ğ‘‹</ci><ci id="S3.SS2.SSS3.p7.3.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p7.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p7.3.m3.1c">X_{i}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.SSS3.p8" class="ltx_para">
<p id="S3.SS2.SSS3.p8.1" class="ltx_p">For the multi-label model, the loss function is defined as:</p>
</div>
<div id="S3.SS2.SSS3.p9" class="ltx_para">
<table id="S3.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex4.m1.1" class="ltx_Math" alttext="L_{ml}=\sum_{i}{L_{i}}=-\sum_{i}{y_{i}\log\hat{y}_{ml}(X_{i})}" display="block"><semantics id="S3.Ex4.m1.1a"><mrow id="S3.Ex4.m1.1.1" xref="S3.Ex4.m1.1.1.cmml"><msub id="S3.Ex4.m1.1.1.3" xref="S3.Ex4.m1.1.1.3.cmml"><mi id="S3.Ex4.m1.1.1.3.2" xref="S3.Ex4.m1.1.1.3.2.cmml">L</mi><mrow id="S3.Ex4.m1.1.1.3.3" xref="S3.Ex4.m1.1.1.3.3.cmml"><mi id="S3.Ex4.m1.1.1.3.3.2" xref="S3.Ex4.m1.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.3.3.1" xref="S3.Ex4.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.1.1.3.3.3" xref="S3.Ex4.m1.1.1.3.3.3.cmml">l</mi></mrow></msub><mo rspace="0.111em" id="S3.Ex4.m1.1.1.4" xref="S3.Ex4.m1.1.1.4.cmml">=</mo><mrow id="S3.Ex4.m1.1.1.5" xref="S3.Ex4.m1.1.1.5.cmml"><munder id="S3.Ex4.m1.1.1.5.1" xref="S3.Ex4.m1.1.1.5.1.cmml"><mo movablelimits="false" id="S3.Ex4.m1.1.1.5.1.2" xref="S3.Ex4.m1.1.1.5.1.2.cmml">âˆ‘</mo><mi id="S3.Ex4.m1.1.1.5.1.3" xref="S3.Ex4.m1.1.1.5.1.3.cmml">i</mi></munder><msub id="S3.Ex4.m1.1.1.5.2" xref="S3.Ex4.m1.1.1.5.2.cmml"><mi id="S3.Ex4.m1.1.1.5.2.2" xref="S3.Ex4.m1.1.1.5.2.2.cmml">L</mi><mi id="S3.Ex4.m1.1.1.5.2.3" xref="S3.Ex4.m1.1.1.5.2.3.cmml">i</mi></msub></mrow><mo id="S3.Ex4.m1.1.1.6" xref="S3.Ex4.m1.1.1.6.cmml">=</mo><mrow id="S3.Ex4.m1.1.1.1" xref="S3.Ex4.m1.1.1.1.cmml"><mo id="S3.Ex4.m1.1.1.1a" xref="S3.Ex4.m1.1.1.1.cmml">âˆ’</mo><mrow id="S3.Ex4.m1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.cmml"><munder id="S3.Ex4.m1.1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.Ex4.m1.1.1.1.1.2.2" xref="S3.Ex4.m1.1.1.1.1.2.2.cmml">âˆ‘</mo><mi id="S3.Ex4.m1.1.1.1.1.2.3" xref="S3.Ex4.m1.1.1.1.1.2.3.cmml">i</mi></munder><mrow id="S3.Ex4.m1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.cmml"><msub id="S3.Ex4.m1.1.1.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.1.3.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.3.2" xref="S3.Ex4.m1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S3.Ex4.m1.1.1.1.1.1.3.3" xref="S3.Ex4.m1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0.167em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex4.m1.1.1.1.1.1.4" xref="S3.Ex4.m1.1.1.1.1.1.4.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.4.1" xref="S3.Ex4.m1.1.1.1.1.1.4.1.cmml">log</mi><mo lspace="0.167em" id="S3.Ex4.m1.1.1.1.1.1.4a" xref="S3.Ex4.m1.1.1.1.1.1.4.cmml">â¡</mo><msub id="S3.Ex4.m1.1.1.1.1.1.4.2" xref="S3.Ex4.m1.1.1.1.1.1.4.2.cmml"><mover accent="true" id="S3.Ex4.m1.1.1.1.1.1.4.2.2" xref="S3.Ex4.m1.1.1.1.1.1.4.2.2.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.4.2.2.2" xref="S3.Ex4.m1.1.1.1.1.1.4.2.2.2.cmml">y</mi><mo id="S3.Ex4.m1.1.1.1.1.1.4.2.2.1" xref="S3.Ex4.m1.1.1.1.1.1.4.2.2.1.cmml">^</mo></mover><mrow id="S3.Ex4.m1.1.1.1.1.1.4.2.3" xref="S3.Ex4.m1.1.1.1.1.1.4.2.3.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.4.2.3.2" xref="S3.Ex4.m1.1.1.1.1.1.4.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.1.4.2.3.1" xref="S3.Ex4.m1.1.1.1.1.1.4.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.1.1.1.1.1.4.2.3.3" xref="S3.Ex4.m1.1.1.1.1.1.4.2.3.3.cmml">l</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.1.2a" xref="S3.Ex4.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex4.m1.1.1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex4.m1.1.1.1.1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex4.m1.1.1.1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.Ex4.m1.1.1.1.1.1.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.Ex4.m1.1.1.1.1.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex4.m1.1b"><apply id="S3.Ex4.m1.1.1.cmml" xref="S3.Ex4.m1.1.1"><and id="S3.Ex4.m1.1.1a.cmml" xref="S3.Ex4.m1.1.1"></and><apply id="S3.Ex4.m1.1.1b.cmml" xref="S3.Ex4.m1.1.1"><eq id="S3.Ex4.m1.1.1.4.cmml" xref="S3.Ex4.m1.1.1.4"></eq><apply id="S3.Ex4.m1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.3.1.cmml" xref="S3.Ex4.m1.1.1.3">subscript</csymbol><ci id="S3.Ex4.m1.1.1.3.2.cmml" xref="S3.Ex4.m1.1.1.3.2">ğ¿</ci><apply id="S3.Ex4.m1.1.1.3.3.cmml" xref="S3.Ex4.m1.1.1.3.3"><times id="S3.Ex4.m1.1.1.3.3.1.cmml" xref="S3.Ex4.m1.1.1.3.3.1"></times><ci id="S3.Ex4.m1.1.1.3.3.2.cmml" xref="S3.Ex4.m1.1.1.3.3.2">ğ‘š</ci><ci id="S3.Ex4.m1.1.1.3.3.3.cmml" xref="S3.Ex4.m1.1.1.3.3.3">ğ‘™</ci></apply></apply><apply id="S3.Ex4.m1.1.1.5.cmml" xref="S3.Ex4.m1.1.1.5"><apply id="S3.Ex4.m1.1.1.5.1.cmml" xref="S3.Ex4.m1.1.1.5.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.5.1.1.cmml" xref="S3.Ex4.m1.1.1.5.1">subscript</csymbol><sum id="S3.Ex4.m1.1.1.5.1.2.cmml" xref="S3.Ex4.m1.1.1.5.1.2"></sum><ci id="S3.Ex4.m1.1.1.5.1.3.cmml" xref="S3.Ex4.m1.1.1.5.1.3">ğ‘–</ci></apply><apply id="S3.Ex4.m1.1.1.5.2.cmml" xref="S3.Ex4.m1.1.1.5.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.5.2.1.cmml" xref="S3.Ex4.m1.1.1.5.2">subscript</csymbol><ci id="S3.Ex4.m1.1.1.5.2.2.cmml" xref="S3.Ex4.m1.1.1.5.2.2">ğ¿</ci><ci id="S3.Ex4.m1.1.1.5.2.3.cmml" xref="S3.Ex4.m1.1.1.5.2.3">ğ‘–</ci></apply></apply></apply><apply id="S3.Ex4.m1.1.1c.cmml" xref="S3.Ex4.m1.1.1"><eq id="S3.Ex4.m1.1.1.6.cmml" xref="S3.Ex4.m1.1.1.6"></eq><share href="#S3.Ex4.m1.1.1.5.cmml" id="S3.Ex4.m1.1.1d.cmml" xref="S3.Ex4.m1.1.1"></share><apply id="S3.Ex4.m1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1"><minus id="S3.Ex4.m1.1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1"></minus><apply id="S3.Ex4.m1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1"><apply id="S3.Ex4.m1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.Ex4.m1.1.1.1.1.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2"></sum><ci id="S3.Ex4.m1.1.1.1.1.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.Ex4.m1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1"><times id="S3.Ex4.m1.1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.2"></times><apply id="S3.Ex4.m1.1.1.1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.1.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.1.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3.2">ğ‘¦</ci><ci id="S3.Ex4.m1.1.1.1.1.1.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S3.Ex4.m1.1.1.1.1.1.4.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4"><log id="S3.Ex4.m1.1.1.1.1.1.4.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4.1"></log><apply id="S3.Ex4.m1.1.1.1.1.1.4.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.1.4.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4.2">subscript</csymbol><apply id="S3.Ex4.m1.1.1.1.1.1.4.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4.2.2"><ci id="S3.Ex4.m1.1.1.1.1.1.4.2.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4.2.2.1">^</ci><ci id="S3.Ex4.m1.1.1.1.1.1.4.2.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4.2.2.2">ğ‘¦</ci></apply><apply id="S3.Ex4.m1.1.1.1.1.1.4.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4.2.3"><times id="S3.Ex4.m1.1.1.1.1.1.4.2.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4.2.3.1"></times><ci id="S3.Ex4.m1.1.1.1.1.1.4.2.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4.2.3.2">ğ‘š</ci><ci id="S3.Ex4.m1.1.1.1.1.1.4.2.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.4.2.3.3">ğ‘™</ci></apply></apply></apply><apply id="S3.Ex4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S3.Ex4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex4.m1.1c">L_{ml}=\sum_{i}{L_{i}}=-\sum_{i}{y_{i}\log\hat{y}_{ml}(X_{i})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS3.p10" class="ltx_para">
<p id="S3.SS2.SSS3.p10.2" class="ltx_p">where <math id="S3.SS2.SSS3.p10.1.m1.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S3.SS2.SSS3.p10.1.m1.1a"><msub id="S3.SS2.SSS3.p10.1.m1.1.1" xref="S3.SS2.SSS3.p10.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p10.1.m1.1.1.2" xref="S3.SS2.SSS3.p10.1.m1.1.1.2.cmml">y</mi><mi id="S3.SS2.SSS3.p10.1.m1.1.1.3" xref="S3.SS2.SSS3.p10.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p10.1.m1.1b"><apply id="S3.SS2.SSS3.p10.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p10.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p10.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p10.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p10.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p10.1.m1.1.1.2">ğ‘¦</ci><ci id="S3.SS2.SSS3.p10.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p10.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p10.1.m1.1c">y_{i}</annotation></semantics></math> is the true label and <math id="S3.SS2.SSS3.p10.2.m2.1" class="ltx_Math" alttext="\hat{y}_{ml}" display="inline"><semantics id="S3.SS2.SSS3.p10.2.m2.1a"><msub id="S3.SS2.SSS3.p10.2.m2.1.1" xref="S3.SS2.SSS3.p10.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.SSS3.p10.2.m2.1.1.2" xref="S3.SS2.SSS3.p10.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS3.p10.2.m2.1.1.2.2" xref="S3.SS2.SSS3.p10.2.m2.1.1.2.2.cmml">y</mi><mo id="S3.SS2.SSS3.p10.2.m2.1.1.2.1" xref="S3.SS2.SSS3.p10.2.m2.1.1.2.1.cmml">^</mo></mover><mrow id="S3.SS2.SSS3.p10.2.m2.1.1.3" xref="S3.SS2.SSS3.p10.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS3.p10.2.m2.1.1.3.2" xref="S3.SS2.SSS3.p10.2.m2.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p10.2.m2.1.1.3.1" xref="S3.SS2.SSS3.p10.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p10.2.m2.1.1.3.3" xref="S3.SS2.SSS3.p10.2.m2.1.1.3.3.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p10.2.m2.1b"><apply id="S3.SS2.SSS3.p10.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p10.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p10.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p10.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.SSS3.p10.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p10.2.m2.1.1.2"><ci id="S3.SS2.SSS3.p10.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS3.p10.2.m2.1.1.2.1">^</ci><ci id="S3.SS2.SSS3.p10.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS3.p10.2.m2.1.1.2.2">ğ‘¦</ci></apply><apply id="S3.SS2.SSS3.p10.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p10.2.m2.1.1.3"><times id="S3.SS2.SSS3.p10.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS3.p10.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS3.p10.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS3.p10.2.m2.1.1.3.2">ğ‘š</ci><ci id="S3.SS2.SSS3.p10.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS3.p10.2.m2.1.1.3.3">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p10.2.m2.1c">\hat{y}_{ml}</annotation></semantics></math> is calculated as a sigmoid over scores computed by the multi-label model as follows:</p>
<table id="S3.Ex5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex5.m1.1" class="ltx_Math" alttext="\hat{y}_{ml}(X_{i})=\frac{1}{1+e^{\vec{Z}}}" display="block"><semantics id="S3.Ex5.m1.1a"><mrow id="S3.Ex5.m1.1.1" xref="S3.Ex5.m1.1.1.cmml"><mrow id="S3.Ex5.m1.1.1.1" xref="S3.Ex5.m1.1.1.1.cmml"><msub id="S3.Ex5.m1.1.1.1.3" xref="S3.Ex5.m1.1.1.1.3.cmml"><mover accent="true" id="S3.Ex5.m1.1.1.1.3.2" xref="S3.Ex5.m1.1.1.1.3.2.cmml"><mi id="S3.Ex5.m1.1.1.1.3.2.2" xref="S3.Ex5.m1.1.1.1.3.2.2.cmml">y</mi><mo id="S3.Ex5.m1.1.1.1.3.2.1" xref="S3.Ex5.m1.1.1.1.3.2.1.cmml">^</mo></mover><mrow id="S3.Ex5.m1.1.1.1.3.3" xref="S3.Ex5.m1.1.1.1.3.3.cmml"><mi id="S3.Ex5.m1.1.1.1.3.3.2" xref="S3.Ex5.m1.1.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.1.1.1.3.3.1" xref="S3.Ex5.m1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.Ex5.m1.1.1.1.3.3.3" xref="S3.Ex5.m1.1.1.1.3.3.3.cmml">l</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.1.1.1.2" xref="S3.Ex5.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex5.m1.1.1.1.1.1" xref="S3.Ex5.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex5.m1.1.1.1.1.1.2" xref="S3.Ex5.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex5.m1.1.1.1.1.1.1" xref="S3.Ex5.m1.1.1.1.1.1.1.cmml"><mi id="S3.Ex5.m1.1.1.1.1.1.1.2" xref="S3.Ex5.m1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.Ex5.m1.1.1.1.1.1.1.3" xref="S3.Ex5.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.Ex5.m1.1.1.1.1.1.3" xref="S3.Ex5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex5.m1.1.1.2" xref="S3.Ex5.m1.1.1.2.cmml">=</mo><mfrac id="S3.Ex5.m1.1.1.3" xref="S3.Ex5.m1.1.1.3.cmml"><mn id="S3.Ex5.m1.1.1.3.2" xref="S3.Ex5.m1.1.1.3.2.cmml">1</mn><mrow id="S3.Ex5.m1.1.1.3.3" xref="S3.Ex5.m1.1.1.3.3.cmml"><mn id="S3.Ex5.m1.1.1.3.3.2" xref="S3.Ex5.m1.1.1.3.3.2.cmml">1</mn><mo id="S3.Ex5.m1.1.1.3.3.1" xref="S3.Ex5.m1.1.1.3.3.1.cmml">+</mo><msup id="S3.Ex5.m1.1.1.3.3.3" xref="S3.Ex5.m1.1.1.3.3.3.cmml"><mi id="S3.Ex5.m1.1.1.3.3.3.2" xref="S3.Ex5.m1.1.1.3.3.3.2.cmml">e</mi><mover accent="true" id="S3.Ex5.m1.1.1.3.3.3.3" xref="S3.Ex5.m1.1.1.3.3.3.3.cmml"><mi id="S3.Ex5.m1.1.1.3.3.3.3.2" xref="S3.Ex5.m1.1.1.3.3.3.3.2.cmml">Z</mi><mo stretchy="false" id="S3.Ex5.m1.1.1.3.3.3.3.1" xref="S3.Ex5.m1.1.1.3.3.3.3.1.cmml">â†’</mo></mover></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex5.m1.1b"><apply id="S3.Ex5.m1.1.1.cmml" xref="S3.Ex5.m1.1.1"><eq id="S3.Ex5.m1.1.1.2.cmml" xref="S3.Ex5.m1.1.1.2"></eq><apply id="S3.Ex5.m1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1"><times id="S3.Ex5.m1.1.1.1.2.cmml" xref="S3.Ex5.m1.1.1.1.2"></times><apply id="S3.Ex5.m1.1.1.1.3.cmml" xref="S3.Ex5.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.1.1.1.3.1.cmml" xref="S3.Ex5.m1.1.1.1.3">subscript</csymbol><apply id="S3.Ex5.m1.1.1.1.3.2.cmml" xref="S3.Ex5.m1.1.1.1.3.2"><ci id="S3.Ex5.m1.1.1.1.3.2.1.cmml" xref="S3.Ex5.m1.1.1.1.3.2.1">^</ci><ci id="S3.Ex5.m1.1.1.1.3.2.2.cmml" xref="S3.Ex5.m1.1.1.1.3.2.2">ğ‘¦</ci></apply><apply id="S3.Ex5.m1.1.1.1.3.3.cmml" xref="S3.Ex5.m1.1.1.1.3.3"><times id="S3.Ex5.m1.1.1.1.3.3.1.cmml" xref="S3.Ex5.m1.1.1.1.3.3.1"></times><ci id="S3.Ex5.m1.1.1.1.3.3.2.cmml" xref="S3.Ex5.m1.1.1.1.3.3.2">ğ‘š</ci><ci id="S3.Ex5.m1.1.1.1.3.3.3.cmml" xref="S3.Ex5.m1.1.1.1.3.3.3">ğ‘™</ci></apply></apply><apply id="S3.Ex5.m1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex5.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex5.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S3.Ex5.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S3.Ex5.m1.1.1.3.cmml" xref="S3.Ex5.m1.1.1.3"><divide id="S3.Ex5.m1.1.1.3.1.cmml" xref="S3.Ex5.m1.1.1.3"></divide><cn type="integer" id="S3.Ex5.m1.1.1.3.2.cmml" xref="S3.Ex5.m1.1.1.3.2">1</cn><apply id="S3.Ex5.m1.1.1.3.3.cmml" xref="S3.Ex5.m1.1.1.3.3"><plus id="S3.Ex5.m1.1.1.3.3.1.cmml" xref="S3.Ex5.m1.1.1.3.3.1"></plus><cn type="integer" id="S3.Ex5.m1.1.1.3.3.2.cmml" xref="S3.Ex5.m1.1.1.3.3.2">1</cn><apply id="S3.Ex5.m1.1.1.3.3.3.cmml" xref="S3.Ex5.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.1.1.3.3.3.1.cmml" xref="S3.Ex5.m1.1.1.3.3.3">superscript</csymbol><ci id="S3.Ex5.m1.1.1.3.3.3.2.cmml" xref="S3.Ex5.m1.1.1.3.3.3.2">ğ‘’</ci><apply id="S3.Ex5.m1.1.1.3.3.3.3.cmml" xref="S3.Ex5.m1.1.1.3.3.3.3"><ci id="S3.Ex5.m1.1.1.3.3.3.3.1.cmml" xref="S3.Ex5.m1.1.1.3.3.3.3.1">â†’</ci><ci id="S3.Ex5.m1.1.1.3.3.3.3.2.cmml" xref="S3.Ex5.m1.1.1.3.3.3.3.2">ğ‘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m1.1c">\hat{y}_{ml}(X_{i})=\frac{1}{1+e^{\vec{Z}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS3.p11" class="ltx_para">
<p id="S3.SS2.SSS3.p11.2" class="ltx_p">Where <math id="S3.SS2.SSS3.p11.1.m1.1" class="ltx_Math" alttext="\vec{Z}" display="inline"><semantics id="S3.SS2.SSS3.p11.1.m1.1a"><mover accent="true" id="S3.SS2.SSS3.p11.1.m1.1.1" xref="S3.SS2.SSS3.p11.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p11.1.m1.1.1.2" xref="S3.SS2.SSS3.p11.1.m1.1.1.2.cmml">Z</mi><mo stretchy="false" id="S3.SS2.SSS3.p11.1.m1.1.1.1" xref="S3.SS2.SSS3.p11.1.m1.1.1.1.cmml">â†’</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p11.1.m1.1b"><apply id="S3.SS2.SSS3.p11.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p11.1.m1.1.1"><ci id="S3.SS2.SSS3.p11.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p11.1.m1.1.1.1">â†’</ci><ci id="S3.SS2.SSS3.p11.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p11.1.m1.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p11.1.m1.1c">\vec{Z}</annotation></semantics></math> is the outputs from the last layer of the multilabel model given <math id="S3.SS2.SSS3.p11.2.m2.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S3.SS2.SSS3.p11.2.m2.1a"><msub id="S3.SS2.SSS3.p11.2.m2.1.1" xref="S3.SS2.SSS3.p11.2.m2.1.1.cmml"><mi id="S3.SS2.SSS3.p11.2.m2.1.1.2" xref="S3.SS2.SSS3.p11.2.m2.1.1.2.cmml">X</mi><mi id="S3.SS2.SSS3.p11.2.m2.1.1.3" xref="S3.SS2.SSS3.p11.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p11.2.m2.1b"><apply id="S3.SS2.SSS3.p11.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p11.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p11.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p11.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p11.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p11.2.m2.1.1.2">ğ‘‹</ci><ci id="S3.SS2.SSS3.p11.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p11.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p11.2.m2.1c">X_{i}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.SSS3.p12" class="ltx_para">
<p id="S3.SS2.SSS3.p12.1" class="ltx_p">Both models were trained until convergence and the model from the epoch with the best performance was selected. We
explored the combination of the two models to provide the final decision and we explain our results in sectionÂ <a href="#S4" title="4 Experimental Results â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>
Our original idea was that the multi-label model would augment performance of the multi-class model and address
generalization issues with unseen/ low data availability for certain numbers. For example, if 83, 74 were present in
the training set but not 73, the right and left side of prediction nodes for 3 and 7 would have been activated in the
train set for all numbers starting and ending with 7 or 3 and hence the multi-label model would have enough samples
to predict 73.</p>
</div>
<div id="S3.SS2.SSS3.p13" class="ltx_para">
<p id="S3.SS2.SSS3.p13.1" class="ltx_p">We considered training a custom object detection model to identify single-digit numbers. However, due to additional
cost and time associated with labeling bounding boxes, image quality and small size of localized jersey numbers
(approximately 20 x 25 px), we chose the image classification approach.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2203.00734/assets/figures/models.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="270" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Overview of the approach for extracting data, training and generating jersey number predictions.
a) describes the high-level football dataset processing pipeline - identify person in video, pass each person image
through pose estimation model to identify torso region and crop them. b) shows the sequential pretraining of
multi-class/label models with synthetic number datasets - Simple2D and Complex2D as well as fine-tuning on football
dataset. c) represents the inference pipeline that uses data pipeline from a) to crop jersey numbers and perform
prediction using multi-class/label models Figure b)</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We trained the ResNet50 multi-class(number-detection) and multi-label(digit-detection) jersey number classifiers on
the football dataset to establish baseline performance without the synthetic data. For the multi-class model, we
took the number with highest softmax score as the prediction. For the multi-label model, we applied a threshold of
0.5 to both right and left predicted classes to get the output. Eventually we computed the final prediction from the
output of the two models.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The baseline model accuracy was 80% for both models. We experimented with various input image sizes and found optimal
accuracy at 224x224 px for the multi-class and 100x100 px for the multi-label model. Our dataset presented a high
imbalance across several numbers where 24% of the numbers have less than 100 samples and only 5% reach the 400-sample
mark (See FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Hence, we duplicated data points for each number to have 400 images in the training set when
needed. Our training pipeline dynamically applies image augmentation so that no image is seen twice by the models,
even when the base image is the same. We also up sample our test-set images to maintain 20 images per number.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">After having our baselines, we investigated the effects of pre-training with the generated synthetic data on our model
performance. Pre-training on the Simple2D dataset and fine-tuning on the football dataset, resulted in a performance
improvement of 2% over the baseline (82%), for both, multi-class and multi-label models. However, pre-training on
the Complex2D dataset and fine-tuning on the football dataset, resulted in 3% improvement on the multi-class model
and 8% on the multi-label model. By pre-training on both Simple2D and Complex2D, we achieved 8.8% and 6% improvement
above the baseline in multi-class and multi-label models respectively.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">The best multi-label model (Complex2D + Football dataset) had positive accuracy improvements on 74 classes, no change
in accuracy in 19 classes, negative change in accuracy in 8 classes (drop by 10%). The best multi-class model
(Simple2D + Complex2D + Football dataset) had positive accuracy improvements on 63 classes, no change in accuracy in
21 classes, negative change in accuracy in 17 classes (drop by 7%). In order to validate the hypothesis
(SectionÂ <a href="#S3.SS2.SSS3" title="3.2.3 Jersey number detection â€£ 3.2 American Football Dataset â€£ 3 Approach â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>) that multi-label model could have better performance on numbers with
less images, we compare its
results with best multi-class model on numbers with less than 50 images in training set. We notice an average increase
in accuracy of 18.5% for multi-class model and 20% for multi-label model before and after training on synthetic data,
for these numbers. Despite larger gains in accuracy shown by multi-label model, the absolute accuracy scores for these
numbers were better for multi-class model, 81% compared to 78% for multi-label model.</p>
</div>
<figure id="S4.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2203.00734/assets/figures/player1.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="210" height="192" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2203.00734/assets/figures/player2.png" id="S4.F8.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="180" height="193" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Images where multi-label predicted class 100. The multi-label model is not sure of the number class when
the input image has very low resolution.</figcaption>
</figure>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">By analyzing the confusion matrix of the model predictions , we learnt that the best multi-label
model produces false predictions in 2 major scenarios (see FigureÂ <a href="#S4.F8" title="Figure 8 â€£ 4 Experimental Results â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>): predicting one digit rather than both digits,
and predicting class 100 for low-resolution and hard-to-recognize digits. In other words, the multi-label model is
more likely to predict one digit number and non-number classes when challenged with new data. The multi-class model,
however, has relatively spread-out false predictions (see FigureÂ <a href="#S4.F9" title="Figure 9 â€£ 4 Experimental Results â€£ Knock, knock. Whoâ€™s there?â€“Identifying football player jersey numbers with synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Major areas of error for this model are:
predicting one digit rather than both digits, and mistaking single digits for two digits or unrecognizable class.</p>
</div>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2203.00734/assets/figures/player3.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="180" height="189" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2203.00734/assets/figures/player4.png" id="S4.F9.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="180" height="188" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Image where multi-class predicted class 100. Confusion for the multi-class model arise when the
numbers are rotated or occluded.</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Examining the performance of the two models independently we noticed that predictions agree in 84.4% of the test
cases, suggesting that despite the different objectives (multi-class vs multi-label) there is a robust learning of
the number representations. Furthermore, we notice an additional improvement of 0.4% by two-model ensemble.
Table 2 presents our results.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>A comparison of model performance under different conditions with confidence threshold of 0.5</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.1.1.1" class="ltx_p" style="width:216.8pt;">Experiment</span>
</span>
</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.2.1.1" class="ltx_p" style="width:43.4pt;">Multi-class</span>
</span>
</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.3.1.1" class="ltx_p" style="width:43.4pt;">Multi-label</span>
</span>
</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T2.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.4.1.1" class="ltx_p" style="width:43.4pt;">Ensemble</span>
</span>
</th>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_t" colspan="4">Without synthetic data</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.3.1" class="ltx_tr">
<td id="S4.T2.1.3.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.1.1.1.1" class="ltx_p" style="width:216.8pt;">Football dataset</span>
</span>
</td>
<td id="S4.T2.1.3.1.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.1.2.1.1" class="ltx_p" style="width:43.4pt;">0.8064</span>
</span>
</td>
<td id="S4.T2.1.3.1.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.1.3.1.1" class="ltx_p" style="width:43.4pt;">0.8</span>
</span>
</td>
<td id="S4.T2.1.3.1.4" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S4.T2.1.4.2" class="ltx_tr">
<td id="S4.T2.1.4.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.2.1.1.1" class="ltx_p" style="width:216.8pt;">Best (Multi-class + Multi-label)</span>
</span>
</td>
<td id="S4.T2.1.4.2.2" class="ltx_td ltx_align_top"></td>
<td id="S4.T2.1.4.2.3" class="ltx_td ltx_align_top"></td>
<td id="S4.T2.1.4.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.4.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.2.4.1.1" class="ltx_p" style="width:43.4pt;">0.8028</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.5.3" class="ltx_tr">
<td id="S4.T2.1.5.3.1" class="ltx_td ltx_align_center ltx_align_top" colspan="4">With synthetic data pre-training</td>
</tr>
<tr id="S4.T2.1.6.4" class="ltx_tr">
<td id="S4.T2.1.6.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.6.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.4.1.1.1" class="ltx_p" style="width:216.8pt;">Simple2D + Football dataset</span>
</span>
</td>
<td id="S4.T2.1.6.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.6.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.4.2.1.1" class="ltx_p" style="width:43.4pt;">0.8282</span>
</span>
</td>
<td id="S4.T2.1.6.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.6.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.4.3.1.1" class="ltx_p" style="width:43.4pt;">0.82</span>
</span>
</td>
<td id="S4.T2.1.6.4.4" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S4.T2.1.7.5" class="ltx_tr">
<td id="S4.T2.1.7.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.7.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.7.5.1.1.1" class="ltx_p" style="width:216.8pt;">Complex2D + Football dataset</span>
</span>
</td>
<td id="S4.T2.1.7.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.7.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.7.5.2.1.1" class="ltx_p" style="width:43.4pt;">0.8306</span>
</span>
</td>
<td id="S4.T2.1.7.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.7.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.7.5.3.1.1" class="ltx_p" style="width:43.4pt;">0.88</span>
</span>
</td>
<td id="S4.T2.1.7.5.4" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S4.T2.1.8.6" class="ltx_tr">
<td id="S4.T2.1.8.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.8.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.8.6.1.1.1" class="ltx_p" style="width:216.8pt;">Simple2D + Complex2D + Football dataset</span>
</span>
</td>
<td id="S4.T2.1.8.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.8.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.8.6.2.1.1" class="ltx_p" style="width:43.4pt;">0.8886</span>
</span>
</td>
<td id="S4.T2.1.8.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.8.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.8.6.3.1.1" class="ltx_p" style="width:43.4pt;">0.86</span>
</span>
</td>
<td id="S4.T2.1.8.6.4" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S4.T2.1.9.7" class="ltx_tr">
<td id="S4.T2.1.9.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T2.1.9.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.9.7.1.1.1" class="ltx_p" style="width:216.8pt;">Best (Multi-class + Multi-label)</span>
</span>
</td>
<td id="S4.T2.1.9.7.2" class="ltx_td ltx_align_top ltx_border_bb"></td>
<td id="S4.T2.1.9.7.3" class="ltx_td ltx_align_top ltx_border_bb"></td>
<td id="S4.T2.1.9.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T2.1.9.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.9.7.4.1.1" class="ltx_p" style="width:43.4pt;">0.8931</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The work presented in this paper shows that the number identification task can be simplified by leveraging synthetic
datasets. We were able to obtain a good performance that is comparable with previous worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Ye etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2005</a>; Å ari etÂ al., <a href="#bib.bib22" title="" class="ltx_ref">2008</a>; Gerke etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2015</a>)</cite> requiring no
change in the data collection pipeline. Despite these findings, we recognize this approach has some limitations which
we describe in this section.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We were able to achieve 89% accuracy for our test dataset regardless of the challenging nature of jersey number
identification in a low-data regime. This performance is on par with some of the most recent worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Vats etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>. However,
the lack of a benchmark dataset for this task and unavailability of already implemented tools, is a big barrier
for comparing performance across all methods. The only solution is to label large amounts of high-quality data
and retrain the available solutions in-house. This requires a lot of computational resources and man-hours put
into work, which is not always an option for all institutions.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In our jersey detection models, we used ResNet50 as a base model, because it proved to be effective for this task.
Bigger and more sophisticated models might provide better accuracy and recall but an exhaustive search is necessary
for each of the components of the solutions to determine an optimal cost-benefit tradeoff. We recognize that more
investigation is needed here to determine such optimal.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">In our solution we chose a three-model pipeline approach versus a one-pass prediction model. Our approach comes with
a few limitations including cascading inaccuracies from one model to the next and increase in latency. However, our
choice was justified by ease of implementation, maintenance and portability to other domains. Even with this cascading
effect, our solution proves to have a good performance in our highly imbalanced, limited dataset.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our approach to increase performance can be broadly classified into two categories: improving data quality and quantity
or experimenting with different models.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Data quality and quantity</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">We observed no improvement in model accuracy by increasing the number of duplicated samples or the number of image
augmentations. The confidence of the predictions directly correlated with the quality and resolution of the jersey
number crop (input image). In future work, we plan to experiment with various image quality enhancement methods in
classical CV and deep learning domains to observe if it improves performance. Another path that can be considered is
to refine our synthetic data generation pipeline to produce images that are closer to the real-world dataset.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Different model strategies</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Our current method has minimal labeling effort. However, by collecting more images of reasonable quality and quantity
we plan to test object detection-based models. One way to improve frame level accuracy would be to track detected
jersey numbers across both side-line and end-zone views so that in situations where numbers are partially visible
or player pose is complex, we would be able to obtain predictions with continuity. Tracking players in team sports
like football is still a major challenge in the sports CV domain and we will evaluate its utility in our future work.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This paper presented a new solution for low-data regime jersey detection with two-stage novel synthetic data generation
techniques, pose estimation for jersey number localization and CNN ensemble learning to detect jersey numbers.
Data augmentations during training and the use of large synthetic dataset provided enough variations for the model
to generalize well and learn numbers. Our solution is easy to implement, requires minimal labeling, curation,
supervision, and can be customized for various sports jersey fonts, colors and backgrounds. Our framework improves
the accuracy of number detection task by 9% and can be easily extended to similar tasks across various Sports
communities as well as industries with similar use cases. Furthermore, our solution did not require the modification
of the data capturing or processing pipeline that is already in place, making it convenient and flexible.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Additionally, it introduces a novel data synthesis technique that can boost custom solution performance in a wide
array of sports. We hope this solution enables the Sport Analytics community to rapidly automate video understanding solutions.</p>
</div>
<div id="S7.p3" class="ltx_para">
<br class="ltx_break">
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borkman etÂ al. (2021)</span>
<span class="ltx_bibblock">
S.Â Borkman, A.Â Crespi, S.Â Dhakad, S.Â Ganguly, J.Â Hogins, Y.-C. Jhang,
M.Â Kamalzadeh, B.Â Li, S.Â Leal, P.Â Parisi, etÂ al.

</span>
<span class="ltx_bibblock">Unity perception: Generate synthetic data for computer vision.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.04259</em>, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeÂ Campos etÂ al. (2009)</span>
<span class="ltx_bibblock">
T.Â E. DeÂ Campos, B.Â R. Babu, M.Â Varma, etÂ al.

</span>
<span class="ltx_bibblock">Character recognition in natural images.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">VISAPP (2)</em>, 7:2, 2009.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeTone etÂ al. (2018)</span>
<span class="ltx_bibblock">
D.Â DeTone, T.Â Malisiewicz, and A.Â Rabinovich.

</span>
<span class="ltx_bibblock">Superpoint: Self-supervised interest point detection and description.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition workshops</em>, pages 224â€“236, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan etÂ al. (2019)</span>
<span class="ltx_bibblock">
K.Â Duan, S.Â Bai, L.Â Xie, H.Â Qi, Q.Â Huang, and Q.Â Tian.

</span>
<span class="ltx_bibblock">Centernet: Keypoint triplets for object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision</em>, pages 6569â€“6578, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerke etÂ al. (2015)</span>
<span class="ltx_bibblock">
S.Â Gerke, K.Â Muller, and R.Â Schafer.

</span>
<span class="ltx_bibblock">Soccer jersey number recognition using convolutional neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision Workshops</em>, pages 17â€“24, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow etÂ al. (2013)</span>
<span class="ltx_bibblock">
I.Â J. Goodfellow, Y.Â Bulatov, J.Â Ibarz, S.Â Arnoud, and V.Â Shet.

</span>
<span class="ltx_bibblock">Multi-digit number recognition from street view imagery using deep
convolutional neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1312.6082</em>, 2013.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hacohen and Weinshall (2019)</span>
<span class="ltx_bibblock">
G.Â Hacohen and D.Â Weinshall.

</span>
<span class="ltx_bibblock">On the power of curriculum learning in training deep networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
2535â€“2544. PMLR, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2016)</span>
<span class="ltx_bibblock">
K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 770â€“778, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinterstoisser etÂ al. (2018)</span>
<span class="ltx_bibblock">
S.Â Hinterstoisser, V.Â Lepetit, P.Â Wohlhart, and K.Â Konolige.

</span>
<span class="ltx_bibblock">On pre-trained image features and synthetic images for deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV) Workshops</em>, pages 0â€“0, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinterstoisser etÂ al. (2019)</span>
<span class="ltx_bibblock">
S.Â Hinterstoisser, O.Â Pauly, H.Â Heibel, M.Â Martina, and M.Â Bokeloh.

</span>
<span class="ltx_bibblock">An annotation saved is an annotation earned: Using fully synthetic
training for object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision workshops</em>, pages 0â€“0, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeon etÂ al. (2021)</span>
<span class="ltx_bibblock">
E.Â Jeon, K.Â Kim, and D.Â Kim.

</span>
<span class="ltx_bibblock">Fa-gan: Feature-aware gan for text to image synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Image Processing
(ICIP)</em>, pages 2443â€“2447. IEEE, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Larochelle etÂ al. (2008)</span>
<span class="ltx_bibblock">
H.Â Larochelle, D.Â Erhan, and Y.Â Bengio.

</span>
<span class="ltx_bibblock">Zero-data learning of new tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, volumeÂ 1, pageÂ 3, 2008.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2018)</span>
<span class="ltx_bibblock">
G.Â Li, S.Â Xu, X.Â Liu, L.Â Li, and C.Â Wang.

</span>
<span class="ltx_bibblock">Jersey number recognition with semi-supervised spatial transformer
network.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</em>, pages 1783â€“1790, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2014)</span>
<span class="ltx_bibblock">
T.-Y. Lin, M.Â Maire, S.Â Belongie, J.Â Hays, P.Â Perona, D.Â Ramanan,
P.Â DollÃ¡r, and C.Â L. Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 740â€“755.
Springer, 2014.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Bhanu (2019)</span>
<span class="ltx_bibblock">
H.Â Liu and B.Â Bhanu.

</span>
<span class="ltx_bibblock">Pose-guided r-cnn for jersey number recognition in sports.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition Workshops</em>, pages 0â€“0, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2013)</span>
<span class="ltx_bibblock">
C.-W. Lu, C.-Y. Lin, C.-Y. Hsu, M.-F. Weng, L.-W. Kang, and H.-Y.Â M. Liao.

</span>
<span class="ltx_bibblock">Identification and tracking of players in sport videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifth International Conference on
Internet Multimedia Computing and Service</em>, pages 113â€“116, 2013.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mustikovela etÂ al. (2021)</span>
<span class="ltx_bibblock">
S.Â K. Mustikovela, S.Â DeÂ Mello, A.Â Prakash, U.Â Iqbal, S.Â Liu, T.Â Nguyen-Phuoc,
C.Â Rother, and J.Â Kautz.

</span>
<span class="ltx_bibblock">Self-supervised object detection via generative image synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 8609â€“8618, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nikolenko (2021)</span>
<span class="ltx_bibblock">
S.Â I. Nikolenko.

</span>
<span class="ltx_bibblock">Synthetic simulated environments.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Synthetic Data for Deep Learning</em>, pages 195â€“215. Springer,
2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nikolenko etÂ al. (2021)</span>
<span class="ltx_bibblock">
S.Â I. Nikolenko etÂ al.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Synthetic data for deep learning</em>.

</span>
<span class="ltx_bibblock">Springer, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piacentino etÂ al. (2021)</span>
<span class="ltx_bibblock">
E.Â Piacentino, A.Â Guarner, and C.Â Angulo.

</span>
<span class="ltx_bibblock">Generating synthetic ecgs using gans for anonymizing healthcare data.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Electronics</em>, 10(4):389, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts etÂ al. (2021)</span>
<span class="ltx_bibblock">
M.Â Roberts, J.Â Ramapuram, A.Â Ranjan, A.Â Kumar, M.Â A. Bautista, N.Â Paczan,
R.Â Webb, and J.Â M. Susskind.

</span>
<span class="ltx_bibblock">Hypersim: A photorealistic synthetic dataset for holistic indoor
scene understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 10912â€“10922, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Å ari etÂ al. (2008)</span>
<span class="ltx_bibblock">
M.Â Å ari, H.Â Dujmi, V.Â Papi, and N.Â RoÅ¾i.

</span>
<span class="ltx_bibblock">Player number localization and recognition in soccer video using hsv
color space and internal contours.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">The International Conference on Signal and Image Processing
(ICSIP 2008)</em>. Citeseer, 2008.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Settles (2009)</span>
<span class="ltx_bibblock">
B.Â Settles.

</span>
<span class="ltx_bibblock">Active learning literature survey.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siam etÂ al. (2021)</span>
<span class="ltx_bibblock">
M.Â Siam, A.Â Kendall, and M.Â Jagersand.

</span>
<span class="ltx_bibblock">Video class agnostic segmentation benchmark for autonomous driving.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 2825â€“2834, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun (2019)</span>
<span class="ltx_bibblock">
S.Â Sun.

</span>
<span class="ltx_bibblock">Multi-digit mnist for few-shot learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">URL: https://github. com/shaohua0116/MultiDigitMNIST</em>, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vats etÂ al. (2021)</span>
<span class="ltx_bibblock">
K.Â Vats, M.Â Fani, D.Â A. Clausi, and J.Â Zelek.

</span>
<span class="ltx_bibblock">Multi-task learning for jersey number recognition in ice hockey.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 4th International Workshop on Multimedia
Content Analysis in Sports</em>, pages 11â€“15, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weinshall etÂ al. (2018)</span>
<span class="ltx_bibblock">
D.Â Weinshall, G.Â Cohen, and D.Â Amir.

</span>
<span class="ltx_bibblock">Curriculum learning by transfer learning: Theory and experiments with
deep networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
5238â€“5246. PMLR, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. (2005)</span>
<span class="ltx_bibblock">
Q.Â Ye, Q.Â Huang, S.Â Jiang, Y.Â Liu, and W.Â Gao.

</span>
<span class="ltx_bibblock">Jersey number detection in sports video for athlete identification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Visual Communications and Image Processing 2005</em>, volume
5960, pages 1599â€“1606. SPIE, 2005.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2203.00733" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2203.00734" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2203.00734">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2203.00734" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2203.00735" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 10:55:35 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
