<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.00996] VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data</title><meta property="og:description" content="In the blind single image super-resolution (SISR) task, existing works have been successful in restoring image-level unknown degradations. However, when a single video frame becomes the input, these works usually fail ‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.00996">

<!--Generated on Tue Feb 27 20:43:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VCISR: Blind Single Image Super-Resolution 
<br class="ltx_break">with Video Compression Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Boyang Wang<math id="id1.1.m1.1" class="ltx_math_unparsed" alttext="{}^{*}\dagger" display="inline"><semantics id="id1.1.m1.1a"><mmultiscripts id="id1.1.m1.1.1"><mo id="id1.1.m1.1.1.2">‚Ä†</mo><mprescripts id="id1.1.m1.1.1a"></mprescripts><mrow id="id1.1.m1.1.1b"></mrow><mo id="id1.1.m1.1.1.3">‚àó</mo></mmultiscripts><annotation encoding="application/x-tex" id="id1.1.m1.1b">{}^{*}\dagger</annotation></semantics></math>, Bowen Liu<sup id="id4.4.id1" class="ltx_sup">‚àó</sup>, Shiyu Liu<sup id="id5.5.id2" class="ltx_sup">‚àó</sup>, Fengyu Yang
<br class="ltx_break">University of Michigan, Ann Arbor
<br class="ltx_break"><span id="id6.6.id3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{boyangwa, bowenliu, shiyuliu, fredyang}@umich.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">In the blind single image super-resolution (SISR) task, existing works have been successful in restoring image-level unknown degradations. However, when a single video frame becomes the input, these works usually fail to address degradations caused by video compression, such as mosquito noise, ringing, blockiness, and staircase noise. In this work, we for the first time, present a video compression-based degradation model to synthesize low-resolution image data in the blind SISR task. Our proposed image synthesizing method is widely applicable to existing image datasets, so that a single degraded image can contain distortions caused by the lossy video compression algorithms. This overcomes the leak of feature diversity in video data and thus retains the training efficiency. By introducing video coding artifacts to SISR degradation models, neural networks can super-resolve images with the ability to restore video compression degradations, and achieve better results on restoring generic distortions caused by image compression as well. Our proposed approach achieves superior performance in SOTA no-reference Image Quality Assessment, and shows better visual quality on various datasets. In addition, we evaluate the SISR neural network trained with our degradation model on video super-resolution (VSR) datasets. Compared to architectures specifically designed for the VSR purpose, our method exhibits similar or better performance, evidencing that the presented strategy on infusing video-based degradation is generalizable to address more complicated compression artifacts even without temporal cues. The code is available at <a target="_blank" href="https://github.com/Kiteretsu77/VCISR-official" title="" class="ltx_ref ltx_href ltx_font_italic">https://github.com/Kiteretsu77/VCISR-official</a>.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>Authors contributed equally to this work.</span></span></span><span id="footnotex2" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">$\dagger$</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">$\dagger$</sup><span class="ltx_note_type">footnotetext: </span>Corresponding author.</span></span></span>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2311.00996/assets/figures/TEASER_RE.png" id="S0.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="574" height="250" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.5.2" class="ltx_text" style="font-size:90%;">Qualitative comparisons of the bicubic-upsampled baseline, and RealSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, BSRGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, Real-ESRGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, SwinIR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, RealBasicVSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and our proposed VCISR super-resolved real-world images. The SR network trained with our proposed data synthesis and degradation block can produce finer details and more visually appealing results. <span id="S0.F1.5.2.1" class="ltx_text ltx_font_bold">(Zoom in for best view.)</span></span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Single image super-resolution (SISR) aims at reconstructing a low-resolution (LR) image into a high-resolution (HR) one. Traditional image upsampling methods include interpolation techniques such as bicubic, nearest neighbor, and bilinear, which calculate sub-pixel values from surrounding pixel values. Since SRCNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, super-resolution (SR) focuses on using convolution neural network techniques to generate HR images<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. They first collect the HR datasets and then create paired LR datasets through bicubic downsampling. Nevertheless, the bicubic degradation method deviates from real-world degradations, making it difficult for neural networks to restore real-world LR images.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Real-world images often contain various complex degradations, such as camera out-of-focus blurring, exposure sensor noise, read noise, analog-to-digital converters noise, and lossy data compression artifacts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. This phenomenon raises the field of blind SISR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, where input LR images may contain any real-world degradation, and neural networks need to learn how to restore these artifacts while upscaling the resolution.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Blind SISR works synthesize LR images from HR images using a degradation model. The closer the synthesized LR images are to the real-world degradations, the more effective the network can learn to generate better visual-quality HR images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>. Previous works<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> aim at adopting blurring, noise, and image compression in their degradation synthesis.
However, they overlook some scenarios in the real world. For example, part of the LR input content could be a video frame, and some SR implementations split a video into frames and super-resolve each frame individually as SISR. In these scenarios, it is crucial to consider the impact of video compression artifacts, temporal and spectral distortions (<em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p3.1.2" class="ltx_text"></span>, mosquito noise and blockiness) due to lossy data compression, on images.
Yet simply using frames from compressed video clips for image network training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> can be prone to significant training costs. This is due to the fact that there are much fewer distinct features the network can learn from video datasets, where objects and scenes share great similarity between frames, compared to image datasets (<em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p3.1.4" class="ltx_text"></span>, DIV2K<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>).
To address the aforementioned challenges, we are motivated to design a video compression degradation model to synthesize LR images with video compression artifacts for image SR networks under image training datasets.
Meanwhile, introducing our model benefits image compression restoration in SR applications, primarily because we are not likely to have prior knowledge of the image compression algorithm used on input. It may not be the most widely-used JPEG algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. Some image compression algorithms, like WebP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, may involve intra-prediction techniques that are widely used in video compression but not in JPEG.
Thus, we argue that video compression artifacts are capable of approximating a wider variety of distortions caused by image compression algorithms in the real world.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">After reviewing video codecs and compression settings adopted in existing video SR (VSR) works<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, we find that the video compression based degradation models they use are too scattered to serve as a proximity to real-world video coding artifacts. Consequently, networks trained on these scattered video compression degradations are hard to restore complicated distortions (<em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.2" class="ltx_text"></span>, blockiness with basis patterns), even with the aid of temporal domain propagation paths across the frames to exploit more correlations. This observation advocates us to evaluate the same network trained for blind SISR on low-quality video datasets.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our experiments confirm that temporal compression artifacts can be simulated with spatial-only information. This allows us to synthesize video artifacts on common image SR datasets (<em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p5.1.2" class="ltx_text"></span>, DIV2K<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>), as a manner to facilitate image SR network training. To intensify these compression artifacts in images, we present a comprehensive degradation model, which promotes the qualitative and quantitative performance of the trained SR network to a new level.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Furthermore, we propose an image dataset that contains versatile compression artifacts, which broadly exist in real-world images. This dataset is targeted to become a guideline for future researchers on how compression distortions may appear in the real world.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our contributions can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">This work uses image super-resolution network and image training datasets to restore video and broader compression quality loss by introducing video artifacts in the degradation model. As a result, our proposed method is competitive with SR networks on real-world image and video restoration.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We introduce VC-RealLQ, a real-world image dataset consisting of versatile temporal and spatial compression artifacts from various contents, resolutions, and compression algorithms. Our dataset could serve as a common benchmark for future methods and will be released for ease of future research.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The proposed video compression-based degradation block can be directly adopted by widely used blind SISR degradation models with minimal effort.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2311.00996/assets/figures/WACV_final.jpg" id="S1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="167" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.4.2" class="ltx_text" style="font-size:90%;">Overview of the proposed video compression degradation model. We use the degradation model introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> as a backbone of our method, and our approach comprises a data preprocessing step and a video-based degradation block.</span></figcaption>
</figure>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2311.00996/assets/figures/CRF_SMALL.jpg" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="574" height="324" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F3.5.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S1.F3.6.2" class="ltx_text" style="font-size:90%;">Encode Speed vs. Quantization Control. Under the same Quantization level (<span id="S1.F3.6.2.1" class="ltx_text ltx_font_italic">CRF</span>), a faster encode speed leads to more high-frequency information (margins and details of objects) lost. This phenomenon becomes more severe for a higher <span id="S1.F3.6.2.2" class="ltx_text ltx_font_italic">CRF</span> value. No other noise is introduced in this comparison. The frame comes from UVG<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> <span id="S1.F3.6.2.3" class="ltx_text ltx_font_bold">(Zoom in for best view.)</span></span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Deep Blind Image SR Networks</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Blind SISR aims to upscale and restore LR images with unknown degradations. The study in this field has achieved substantial progress in recent years due to the advancement of deep neural networks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. A mainstream of the existing methods adopts CNN-based¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> building blocks to their network architectures (<em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p1.1.2" class="ltx_text"></span> RealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, BSRGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, Real-ESRGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>). In particular, multiple levels of residual and/or dense blocks are widely involved to improve the network depth and the restoration quality. With the Transformer gaining promising performance in a variety of vision tasks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, some latest SISR frameworks (<em id="S2.SS1.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p1.1.4" class="ltx_text"></span> SwinIR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, VRT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, GRL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>) start to include Transformer-based blocks, aiming to better capture the long-range dependencies and enhance the capacity of representation learning to facilitate image restoration and super-resolution.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Degradation Models</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.4" class="ltx_p"><span id="S2.SS2.p1.4.1" class="ltx_text ltx_font_bold">Image degradation models.</span> Recent deep blind SISR networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> are mostly trained with LR images generated from the HR ones by explicit degradation model. Their models use similar degradation elements and follow a fundamental pattern:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\mathbf{LR}=[\!(\mathbf{HR}\otimes\mathbf{k})\downarrow_{s}+\mathbf{n}]_{\mathtt{JPEG}.}" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mi id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">ùêãùêë</mi><mo id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml">=</mo><msub id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.cmml"><mrow id="S2.E1.m1.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.2.cmml"><mpadded width="0.247em"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.2" xref="S2.E1.m1.2.2.1.1.2.1.cmml">[</mo></mpadded><mrow id="S2.E1.m1.2.2.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">ùêáùêë</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">‚äó</mo><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">ùê§</mi></mrow><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow><msub id="S2.E1.m1.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.2.2" xref="S2.E1.m1.2.2.1.1.1.1.2.2.cmml">‚Üì</mo><mi id="S2.E1.m1.2.2.1.1.1.1.2.3" xref="S2.E1.m1.2.2.1.1.1.1.2.3.cmml">s</mi></msub><mrow id="S2.E1.m1.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.3.cmml"><mo id="S2.E1.m1.2.2.1.1.1.1.3a" xref="S2.E1.m1.2.2.1.1.1.1.3.cmml">+</mo><mi id="S2.E1.m1.2.2.1.1.1.1.3.2" xref="S2.E1.m1.2.2.1.1.1.1.3.2.cmml">ùêß</mi></mrow></mrow><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.3" xref="S2.E1.m1.2.2.1.1.2.1.cmml">]</mo></mrow><mrow id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">ùôπùôøùô¥ùô∂</mi><mo lspace="0em" id="S2.E1.m1.1.1.1.3.1" xref="S2.E1.m1.1.1.1.2.cmml">.</mo></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></eq><ci id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3">ùêãùêë</ci><apply id="S2.E1.m1.2.2.1.cmml" xref="S2.E1.m1.2.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.2.cmml" xref="S2.E1.m1.2.2.1">subscript</csymbol><apply id="S2.E1.m1.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.2.2.1.1.2.1.cmml" xref="S2.E1.m1.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S2.E1.m1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1"><apply id="S2.E1.m1.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.2.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2.2">‚Üì</ci><ci id="S2.E1.m1.2.2.1.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2.3">ùë†</ci></apply><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1">tensor-product</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2">ùêáùêë</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3">ùê§</ci></apply><apply id="S2.E1.m1.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3"><plus id="S2.E1.m1.2.2.1.1.1.1.3.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3"></plus><ci id="S2.E1.m1.2.2.1.1.1.1.3.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3.2">ùêß</ci></apply></apply></apply><list id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.3"><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">ùôπùôøùô¥ùô∂</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\mathbf{LR}=[\!(\mathbf{HR}\otimes\mathbf{k})\downarrow_{s}+\mathbf{n}]_{\mathtt{JPEG}.}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.3" class="ltx_p">First, HR images are convolved with kernel <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{k}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">ùê§</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ùê§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\mathbf{k}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> to simulate blurring from out-of-focus camera capture. Then, it is followed by a downsampling operation with scale factor <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">s</annotation></semantics></math>. Noises <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{n}" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">ùêß</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">ùêß</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\mathbf{n}</annotation></semantics></math> are then injected into the LR images.
Finally, images will be compressed by the JPEG<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> to introduce compression artifacts.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">From the aforementioned set of degradation elements (Eq. <a href="#S2.E1" title="Equation 1 ‚Ä£ 2.2 Degradation Models ‚Ä£ 2 Related Works ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), Wang¬†<em id="S2.SS2.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> propose a high-order degradation model which repeats the steps of blurring, resizing, adding noise, and JPEG compression a second turn. To better simulate compression artifacts, <span id="S2.SS2.p2.1.3" class="ltx_text ltx_font_italic">sinc</span> filters are adopted to create pseudo-ringing distortions.
Other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> employ a randomly shuffled degradation model to select degradation modules from a pool containing blur, resize, noise, and JPEG compression modules.
After that, there are also works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> that combine both randomly shuffled and high-order degradation together with a skip mechanism to increase the performance.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Overall, we observe that the existing LR image synthesis flows are still insufficient to address the intricacy of real-world images, which limits the generality and practical usage of SISR networks. Specifically, previous works only consider image-level compression artifacts, but some images may contain temporally correlated artifacts, like contents from video. To mitigate this domain gap, we propose the video compression degradation element, in the image degradation pipeline.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Video degradation models.</span>
Previous works in video SR tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> mostly use H.264 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> and H.265 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> in their degradation model. COMSIR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> proposes a compression-informed model for super-resolving LR videos. They only adopt H.264 in their experiment with a <span id="S2.SS2.p4.1.2" class="ltx_text ltx_font_italic">CRF</span> value (encoder parameter for QP control) between 15 and 25 for training degradation. Khani¬†<em id="S2.SS2.p4.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p4.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> adopt a lightweight SR model to augment H.265, They use a fixed slow mode preset to consolidate their ideas.
On the contrary, our work uses video compression artifacts to serve as a generic degradation model for SR, not exclusively for images or videos by adopting broader <span id="S2.SS2.p4.1.5" class="ltx_text ltx_font_italic">preset</span> modes and combining <span id="S2.SS2.p4.1.6" class="ltx_text ltx_font_italic">preset</span> with <span id="S2.SS2.p4.1.7" class="ltx_text ltx_font_italic">QP</span> control to advance toward better-represented distortions. Quantization control in RealBasicVSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is enforced by selecting bitrate from a predefined range. However, bitrate only constrains the data size of the compressed video per second, and the influence of FPS (frames per second) is not considered as a result. With a higher FPS, the data budget distributed to each frame is decreased and the image frame quality may be dramatically degraded. To better simulate real-world video scenarios, we consider both bitrate and FPS influence as a combination.
To the best of our knowledge, we are the first work that enriches the blind SISR degradation model with video coding-based artifacts, which are simulated on image datasets without the loss of generality. With this technique, we can produce more realistic compression artifacts in degraded LR images to facilitate SR network training, which previous SR works have yet to investigate.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Base Degradation Elements</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The blind SISR degradation approaches for LR image synthesis in existing works share a collection of modules in common as follows.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Blur.</span> Blurring artifact is introduced by convolving the high-resolution with isotropic or anisotropic Gaussian blur kernels under the regular-shaped, generalized-shaped, or plateau-shaped distributions.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Resize.</span> Resizing includes bicubic, bilinear, or area interpolation operations. Both downsampling and upsampling are considered to cover broader resize scenarios used in the real world.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Noise.</span> The resized LR images will be added with additive synthetic noise: Gaussian noise and Poisson noise. For Gaussian noise, we perform both speckle gray noise (same synthetic noise for all RGB channels) and color noise (different synthetic noise for each RGB channel). For Poisson noise, the intensity of each pixel is independent and is proportional to its pixel intensity.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.3" class="ltx_p"><span id="S3.SS1.p5.3.1" class="ltx_text ltx_font_bold">JPEG compression.</span> By the end of a degradation pass, JPEG compression is introduced. It first converts RGB images into luma and chroma components (YCbCr color space). Then, each independent 8 <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mo id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><times id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\times</annotation></semantics></math> 8 block is self-encoded by discrete cosine transform (DCT). To synthesize compression artifacts, the DCT-transformed blocks are quantized by a quality factor <math id="S3.SS1.p5.2.m2.2" class="ltx_Math" alttext="q\in[0,100]" display="inline"><semantics id="S3.SS1.p5.2.m2.2a"><mrow id="S3.SS1.p5.2.m2.2.3" xref="S3.SS1.p5.2.m2.2.3.cmml"><mi id="S3.SS1.p5.2.m2.2.3.2" xref="S3.SS1.p5.2.m2.2.3.2.cmml">q</mi><mo id="S3.SS1.p5.2.m2.2.3.1" xref="S3.SS1.p5.2.m2.2.3.1.cmml">‚àà</mo><mrow id="S3.SS1.p5.2.m2.2.3.3.2" xref="S3.SS1.p5.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p5.2.m2.2.3.3.2.1" xref="S3.SS1.p5.2.m2.2.3.3.1.cmml">[</mo><mn id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">0</mn><mo id="S3.SS1.p5.2.m2.2.3.3.2.2" xref="S3.SS1.p5.2.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS1.p5.2.m2.2.2" xref="S3.SS1.p5.2.m2.2.2.cmml">100</mn><mo stretchy="false" id="S3.SS1.p5.2.m2.2.3.3.2.3" xref="S3.SS1.p5.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.2b"><apply id="S3.SS1.p5.2.m2.2.3.cmml" xref="S3.SS1.p5.2.m2.2.3"><in id="S3.SS1.p5.2.m2.2.3.1.cmml" xref="S3.SS1.p5.2.m2.2.3.1"></in><ci id="S3.SS1.p5.2.m2.2.3.2.cmml" xref="S3.SS1.p5.2.m2.2.3.2">ùëû</ci><interval closure="closed" id="S3.SS1.p5.2.m2.2.3.3.1.cmml" xref="S3.SS1.p5.2.m2.2.3.3.2"><cn type="integer" id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">0</cn><cn type="integer" id="S3.SS1.p5.2.m2.2.2.cmml" xref="S3.SS1.p5.2.m2.2.2">100</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.2c">q\in[0,100]</annotation></semantics></math>, where a lower <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><mi id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><ci id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">ùëû</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">q</annotation></semantics></math> indicates a lower quality. Lastly, the quantized blocks are transformed back by inverse DCT (IDCT) and converted from YCbCr color space to RGB color space.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2311.00996/assets/figures/WACV_arch_final.jpg" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.4.2" class="ltx_text" style="font-size:90%;">The super-resolution network architecture. Our method adopts a smaller model of GRL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> for x4 super-resolution. </span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Modeling Lossy Video Compression Artifacts</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">All conventional video compression standards follow the same hybrid coding algorithm. In these schemes, there are a very limited number of macroblocks
(a compression processing unit usually with sizes of 4x4, 8x8, 16x16, or even larger for some compression standards)
being self-encoded, and searching algorithms aid most macroblocks to correlate with a reference that has the closest pixel content similarity.
The reference may come from the current frame (intra-prediction) or frames before or after it (inter-prediction).
Moreover, the search process is constrained by the limited computation resources to compare with every block in the scope. Hence, the resulting reference blocks may only be sub-optimal in most cases.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">This mechanism is not used by JPEG, which only incorporates self-encoding within fixed-size macroblocks without finding any reference. Our proposed degradation block is therefore inclusive of a wider variety of real-world codecs, including JPEG.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">With a reference in hand, a pixel-level residual between the target and reference macroblock is calculated. This residual will be converted into the frequency domain by Discrete Cosine Transform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> or Wavelet Transform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>. Each value of the transformed matrix will be divided by a common quantization parameter (<span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_italic">QP</span>) to increase the compression rate. However, the higher the <span id="S3.SS2.p3.1.2" class="ltx_text ltx_font_italic">QP</span>, the more irreversible high-frequency information on the sources will be lost. Perceptually, the borders of the object become vague, and more blocking and noise artifacts may occur. Note that the purpose of macroblock reference prediction is to minimize the residual information as small as possible, such that it will be less influenced by the <span id="S3.SS2.p3.1.3" class="ltx_text ltx_font_italic">QP</span>. Thus, under the same <span id="S3.SS2.p3.1.4" class="ltx_text ltx_font_italic">QP</span>, macroblocks with better prediction schemes will present better visual quality as demonstrated in Figure <a href="#S1.F3" title="Figure 3 ‚Ä£ 1 Introduction ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">In summary, the quality of the compressed video is affected by
<span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">1)</span> quantization intensity and
<span id="S3.SS2.p4.1.2" class="ltx_text ltx_font_bold">2)</span> how well the searching algorithm can predict references for macroblocks in a limited preset time.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Video Compression Degradation Model</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The video compression degradation model includes a dataset preparation stage and video compression module as shown in Figure <a href="#S1.F2" title="Figure 2 ‚Ä£ 1 Introduction ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Dataset preprocessing.</span>
In the dataset preparation stage, images are cropped to non-overlap patches and aligned from left to right then top to bottom. We desire to synthesize temporal compression artifacts from spatially-correlated patches. Our insight comes from the understanding of compression algorithms. In compression, lossy contents (artifacts) come from the quantization of residual information between the ground truth source and the most similar reference selected by the algorithm.
In regular video compression, the codec algorithm needs to find either a spatially-correlated reference from the same frame or a temporally-correlated reference from the nearby frames. In this context, when spatially-correlated patches become temporally-correlated patches, the spatial reference searching functionality of the codec algorithm is now turned to temporal searching. This change does not affect the fundamental methodology of compression but helps us to create more generic and complex compression artifacts to heuristically simulate distortions in the real world.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">To best utilize the gap between the selected cropped patches and the full image height and width, we apply random padding on the left or the top of the image to contain more image zones as a form of augmentation.
The degradation batch size is selected to be 128, which is an empirical value that can provide a large enough window for patches to run video compression with the effectiveness of the video encoder parameters (<em id="S3.SS3.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS3.p3.1.2" class="ltx_text"></span>, <span id="S3.SS3.p3.1.3" class="ltx_text ltx_font_italic">preset</span>, <span id="S3.SS3.p3.1.4" class="ltx_text ltx_font_italic">CRF</span>). To avoid the keyframe selection bias, we set the first degradation batch size in the range <math id="S3.SS3.p3.1.m1.2" class="ltx_Math" alttext="[32,128]" display="inline"><semantics id="S3.SS3.p3.1.m1.2a"><mrow id="S3.SS3.p3.1.m1.2.3.2" xref="S3.SS3.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p3.1.m1.2.3.2.1" xref="S3.SS3.p3.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">32</mn><mo id="S3.SS3.p3.1.m1.2.3.2.2" xref="S3.SS3.p3.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS3.p3.1.m1.2.2" xref="S3.SS3.p3.1.m1.2.2.cmml">128</mn><mo stretchy="false" id="S3.SS3.p3.1.m1.2.3.2.3" xref="S3.SS3.p3.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.2b"><interval closure="closed" id="S3.SS3.p3.1.m1.2.3.1.cmml" xref="S3.SS3.p3.1.m1.2.3.2"><cn type="integer" id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">32</cn><cn type="integer" id="S3.SS3.p3.1.m1.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2">128</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.2c">[32,128]</annotation></semantics></math>. In this way, videos in each degradation batch will have inter-prediction in a different pool compared to the previous epoch.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">With the frame patches being encoded into a video, they will immediately be decoded back with some extent of quality loss being introduced by the video codec. Note that, in network training, patches are shuffled as a single identity with no correlation to other patches.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Diverse video compression codec standards.</span>
Though, based on Section <a href="#S3.SS2" title="3.2 Modeling Lossy Video Compression Artifacts ‚Ä£ 3 Proposed Methods ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, all video compression standards shared a common pattern, the encoder algorithm designs on how they search for the reference of macroblocks are massively different. To simulate the encoder from a historical perspective, considering both the modern and earlier standards helps the model to learn versatile compression artifacts due to the difference of the video encoder reference searching and architecture designs. This helps us to simulate more real-world video scenarios.
As a result, we choose the most representative and widely used video compression standards: MPEG-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, MPEG-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, H.264 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, and H.265 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS3.p6.1" class="ltx_p"><span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">Quantization parameter control.</span>
Quantization Parameter (<span id="S3.SS3.p6.1.2" class="ltx_text ltx_font_italic">QP</span>) is the direct cause of quality loss in compression. In the real world, people will not explicitly set the <span id="S3.SS3.p6.1.3" class="ltx_text ltx_font_italic">QP</span> of a video, but it is affected by other settings. For instance, in H.264 and H.265, <span id="S3.SS3.p6.1.4" class="ltx_text ltx_font_italic">QP</span> is controlled by a Constant Rate Factor (<span id="S3.SS3.p6.1.5" class="ltx_text ltx_font_italic">CRF</span>) that can be programmed in <span id="S3.SS3.p6.1.6" class="ltx_text ltx_font_italic">ffmpeg</span>. <span id="S3.SS3.p6.1.7" class="ltx_text ltx_font_italic">CRF</span> provides an engineered compression rate control for the entire video instead of single macroblocks. The lower the <span id="S3.SS3.p6.1.8" class="ltx_text ltx_font_italic">CRF</span> is, the less information it would be lost. For MPEG-2 and MPEG-4, we find that controlling bitrate is a better way to manipulate <span id="S3.SS3.p6.1.9" class="ltx_text ltx_font_italic">QP</span> based on their codec design: to restrict the code size under a certain bitrate, codecs have to increase the <span id="S3.SS3.p6.1.10" class="ltx_text ltx_font_italic">QP</span> of each macroblock. Hence, we considered <span id="S3.SS3.p6.1.11" class="ltx_text ltx_font_italic">CRF</span> for H.264 and H.265, and bitrate for MPEG-2 and MPEG-4 in our degradation block implementation.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para ltx_noindent">
<p id="S3.SS3.p7.5" class="ltx_p"><span id="S3.SS3.p7.5.1" class="ltx_text ltx_font_bold">Encoder speed control.</span>
Though quantization is widely regarded as the direct source of distortion in compression, and previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> mainly focus on tuning <span id="S3.SS3.p7.5.2" class="ltx_text ltx_font_italic">QP</span>-related encoder parameters. In Figure <a href="#S1.F3" title="Figure 3 ‚Ä£ 1 Introduction ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our study on video compression standards promotes the finding that encoder speed is a hidden factor that influences <span id="S3.SS3.p7.5.3" class="ltx_text ltx_font_italic">QP</span> to create video compression artifacts to frames.
To accelerate the video processing speed, video encoders provide various encode speeds (<span id="S3.SS3.p7.5.4" class="ltx_text ltx_font_italic">preset</span>) from <math id="S3.SS3.p7.1.m1.1" class="ltx_Math" alttext="medium" display="inline"><semantics id="S3.SS3.p7.1.m1.1a"><mrow id="S3.SS3.p7.1.m1.1.1" xref="S3.SS3.p7.1.m1.1.1.cmml"><mi id="S3.SS3.p7.1.m1.1.1.2" xref="S3.SS3.p7.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.1.m1.1.1.1" xref="S3.SS3.p7.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.1.m1.1.1.3" xref="S3.SS3.p7.1.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.1.m1.1.1.1a" xref="S3.SS3.p7.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.1.m1.1.1.4" xref="S3.SS3.p7.1.m1.1.1.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.1.m1.1.1.1b" xref="S3.SS3.p7.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.1.m1.1.1.5" xref="S3.SS3.p7.1.m1.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.1.m1.1.1.1c" xref="S3.SS3.p7.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.1.m1.1.1.6" xref="S3.SS3.p7.1.m1.1.1.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.1.m1.1.1.1d" xref="S3.SS3.p7.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.1.m1.1.1.7" xref="S3.SS3.p7.1.m1.1.1.7.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.1.m1.1b"><apply id="S3.SS3.p7.1.m1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1"><times id="S3.SS3.p7.1.m1.1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1.1"></times><ci id="S3.SS3.p7.1.m1.1.1.2.cmml" xref="S3.SS3.p7.1.m1.1.1.2">ùëö</ci><ci id="S3.SS3.p7.1.m1.1.1.3.cmml" xref="S3.SS3.p7.1.m1.1.1.3">ùëí</ci><ci id="S3.SS3.p7.1.m1.1.1.4.cmml" xref="S3.SS3.p7.1.m1.1.1.4">ùëë</ci><ci id="S3.SS3.p7.1.m1.1.1.5.cmml" xref="S3.SS3.p7.1.m1.1.1.5">ùëñ</ci><ci id="S3.SS3.p7.1.m1.1.1.6.cmml" xref="S3.SS3.p7.1.m1.1.1.6">ùë¢</ci><ci id="S3.SS3.p7.1.m1.1.1.7.cmml" xref="S3.SS3.p7.1.m1.1.1.7">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.1.m1.1c">medium</annotation></semantics></math> (default mode) to <math id="S3.SS3.p7.2.m2.1" class="ltx_Math" alttext="fast" display="inline"><semantics id="S3.SS3.p7.2.m2.1a"><mrow id="S3.SS3.p7.2.m2.1.1" xref="S3.SS3.p7.2.m2.1.1.cmml"><mi id="S3.SS3.p7.2.m2.1.1.2" xref="S3.SS3.p7.2.m2.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.2.m2.1.1.1" xref="S3.SS3.p7.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.2.m2.1.1.3" xref="S3.SS3.p7.2.m2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.2.m2.1.1.1a" xref="S3.SS3.p7.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.2.m2.1.1.4" xref="S3.SS3.p7.2.m2.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.2.m2.1.1.1b" xref="S3.SS3.p7.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.2.m2.1.1.5" xref="S3.SS3.p7.2.m2.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.2.m2.1b"><apply id="S3.SS3.p7.2.m2.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1"><times id="S3.SS3.p7.2.m2.1.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1.1"></times><ci id="S3.SS3.p7.2.m2.1.1.2.cmml" xref="S3.SS3.p7.2.m2.1.1.2">ùëì</ci><ci id="S3.SS3.p7.2.m2.1.1.3.cmml" xref="S3.SS3.p7.2.m2.1.1.3">ùëé</ci><ci id="S3.SS3.p7.2.m2.1.1.4.cmml" xref="S3.SS3.p7.2.m2.1.1.4">ùë†</ci><ci id="S3.SS3.p7.2.m2.1.1.5.cmml" xref="S3.SS3.p7.2.m2.1.1.5">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.2.m2.1c">fast</annotation></semantics></math>, <math id="S3.SS3.p7.3.m3.1" class="ltx_Math" alttext="faster" display="inline"><semantics id="S3.SS3.p7.3.m3.1a"><mrow id="S3.SS3.p7.3.m3.1.1" xref="S3.SS3.p7.3.m3.1.1.cmml"><mi id="S3.SS3.p7.3.m3.1.1.2" xref="S3.SS3.p7.3.m3.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.3.m3.1.1.1" xref="S3.SS3.p7.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.3.m3.1.1.3" xref="S3.SS3.p7.3.m3.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.3.m3.1.1.1a" xref="S3.SS3.p7.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.3.m3.1.1.4" xref="S3.SS3.p7.3.m3.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.3.m3.1.1.1b" xref="S3.SS3.p7.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.3.m3.1.1.5" xref="S3.SS3.p7.3.m3.1.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.3.m3.1.1.1c" xref="S3.SS3.p7.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.3.m3.1.1.6" xref="S3.SS3.p7.3.m3.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.3.m3.1.1.1d" xref="S3.SS3.p7.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.3.m3.1.1.7" xref="S3.SS3.p7.3.m3.1.1.7.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.3.m3.1b"><apply id="S3.SS3.p7.3.m3.1.1.cmml" xref="S3.SS3.p7.3.m3.1.1"><times id="S3.SS3.p7.3.m3.1.1.1.cmml" xref="S3.SS3.p7.3.m3.1.1.1"></times><ci id="S3.SS3.p7.3.m3.1.1.2.cmml" xref="S3.SS3.p7.3.m3.1.1.2">ùëì</ci><ci id="S3.SS3.p7.3.m3.1.1.3.cmml" xref="S3.SS3.p7.3.m3.1.1.3">ùëé</ci><ci id="S3.SS3.p7.3.m3.1.1.4.cmml" xref="S3.SS3.p7.3.m3.1.1.4">ùë†</ci><ci id="S3.SS3.p7.3.m3.1.1.5.cmml" xref="S3.SS3.p7.3.m3.1.1.5">ùë°</ci><ci id="S3.SS3.p7.3.m3.1.1.6.cmml" xref="S3.SS3.p7.3.m3.1.1.6">ùëí</ci><ci id="S3.SS3.p7.3.m3.1.1.7.cmml" xref="S3.SS3.p7.3.m3.1.1.7">ùëü</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.3.m3.1c">faster</annotation></semantics></math>, <math id="S3.SS3.p7.4.m4.1" class="ltx_Math" alttext="veryfast" display="inline"><semantics id="S3.SS3.p7.4.m4.1a"><mrow id="S3.SS3.p7.4.m4.1.1" xref="S3.SS3.p7.4.m4.1.1.cmml"><mi id="S3.SS3.p7.4.m4.1.1.2" xref="S3.SS3.p7.4.m4.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.4.m4.1.1.1" xref="S3.SS3.p7.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.4.m4.1.1.3" xref="S3.SS3.p7.4.m4.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.4.m4.1.1.1a" xref="S3.SS3.p7.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.4.m4.1.1.4" xref="S3.SS3.p7.4.m4.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.4.m4.1.1.1b" xref="S3.SS3.p7.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.4.m4.1.1.5" xref="S3.SS3.p7.4.m4.1.1.5.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.4.m4.1.1.1c" xref="S3.SS3.p7.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.4.m4.1.1.6" xref="S3.SS3.p7.4.m4.1.1.6.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.4.m4.1.1.1d" xref="S3.SS3.p7.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.4.m4.1.1.7" xref="S3.SS3.p7.4.m4.1.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.4.m4.1.1.1e" xref="S3.SS3.p7.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.4.m4.1.1.8" xref="S3.SS3.p7.4.m4.1.1.8.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.4.m4.1.1.1f" xref="S3.SS3.p7.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.4.m4.1.1.9" xref="S3.SS3.p7.4.m4.1.1.9.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.4.m4.1b"><apply id="S3.SS3.p7.4.m4.1.1.cmml" xref="S3.SS3.p7.4.m4.1.1"><times id="S3.SS3.p7.4.m4.1.1.1.cmml" xref="S3.SS3.p7.4.m4.1.1.1"></times><ci id="S3.SS3.p7.4.m4.1.1.2.cmml" xref="S3.SS3.p7.4.m4.1.1.2">ùë£</ci><ci id="S3.SS3.p7.4.m4.1.1.3.cmml" xref="S3.SS3.p7.4.m4.1.1.3">ùëí</ci><ci id="S3.SS3.p7.4.m4.1.1.4.cmml" xref="S3.SS3.p7.4.m4.1.1.4">ùëü</ci><ci id="S3.SS3.p7.4.m4.1.1.5.cmml" xref="S3.SS3.p7.4.m4.1.1.5">ùë¶</ci><ci id="S3.SS3.p7.4.m4.1.1.6.cmml" xref="S3.SS3.p7.4.m4.1.1.6">ùëì</ci><ci id="S3.SS3.p7.4.m4.1.1.7.cmml" xref="S3.SS3.p7.4.m4.1.1.7">ùëé</ci><ci id="S3.SS3.p7.4.m4.1.1.8.cmml" xref="S3.SS3.p7.4.m4.1.1.8">ùë†</ci><ci id="S3.SS3.p7.4.m4.1.1.9.cmml" xref="S3.SS3.p7.4.m4.1.1.9">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.4.m4.1c">veryfast</annotation></semantics></math>, and even <math id="S3.SS3.p7.5.m5.1" class="ltx_Math" alttext="ultrafast" display="inline"><semantics id="S3.SS3.p7.5.m5.1a"><mrow id="S3.SS3.p7.5.m5.1.1" xref="S3.SS3.p7.5.m5.1.1.cmml"><mi id="S3.SS3.p7.5.m5.1.1.2" xref="S3.SS3.p7.5.m5.1.1.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.5.m5.1.1.1" xref="S3.SS3.p7.5.m5.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.5.m5.1.1.3" xref="S3.SS3.p7.5.m5.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.5.m5.1.1.1a" xref="S3.SS3.p7.5.m5.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.5.m5.1.1.4" xref="S3.SS3.p7.5.m5.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.5.m5.1.1.1b" xref="S3.SS3.p7.5.m5.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.5.m5.1.1.5" xref="S3.SS3.p7.5.m5.1.1.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.5.m5.1.1.1c" xref="S3.SS3.p7.5.m5.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.5.m5.1.1.6" xref="S3.SS3.p7.5.m5.1.1.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.5.m5.1.1.1d" xref="S3.SS3.p7.5.m5.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.5.m5.1.1.7" xref="S3.SS3.p7.5.m5.1.1.7.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.5.m5.1.1.1e" xref="S3.SS3.p7.5.m5.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.5.m5.1.1.8" xref="S3.SS3.p7.5.m5.1.1.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.5.m5.1.1.1f" xref="S3.SS3.p7.5.m5.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.5.m5.1.1.9" xref="S3.SS3.p7.5.m5.1.1.9.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.5.m5.1.1.1g" xref="S3.SS3.p7.5.m5.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p7.5.m5.1.1.10" xref="S3.SS3.p7.5.m5.1.1.10.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.5.m5.1b"><apply id="S3.SS3.p7.5.m5.1.1.cmml" xref="S3.SS3.p7.5.m5.1.1"><times id="S3.SS3.p7.5.m5.1.1.1.cmml" xref="S3.SS3.p7.5.m5.1.1.1"></times><ci id="S3.SS3.p7.5.m5.1.1.2.cmml" xref="S3.SS3.p7.5.m5.1.1.2">ùë¢</ci><ci id="S3.SS3.p7.5.m5.1.1.3.cmml" xref="S3.SS3.p7.5.m5.1.1.3">ùëô</ci><ci id="S3.SS3.p7.5.m5.1.1.4.cmml" xref="S3.SS3.p7.5.m5.1.1.4">ùë°</ci><ci id="S3.SS3.p7.5.m5.1.1.5.cmml" xref="S3.SS3.p7.5.m5.1.1.5">ùëü</ci><ci id="S3.SS3.p7.5.m5.1.1.6.cmml" xref="S3.SS3.p7.5.m5.1.1.6">ùëé</ci><ci id="S3.SS3.p7.5.m5.1.1.7.cmml" xref="S3.SS3.p7.5.m5.1.1.7">ùëì</ci><ci id="S3.SS3.p7.5.m5.1.1.8.cmml" xref="S3.SS3.p7.5.m5.1.1.8">ùëé</ci><ci id="S3.SS3.p7.5.m5.1.1.9.cmml" xref="S3.SS3.p7.5.m5.1.1.9">ùë†</ci><ci id="S3.SS3.p7.5.m5.1.1.10.cmml" xref="S3.SS3.p7.5.m5.1.1.10">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.5.m5.1c">ultrafast</annotation></semantics></math> speed mode. The faster the encoder speed is, the less time the searching algorithm will have for intra-prediction and inter-prediction.
Consequently, the predicted reference will be hard to match target macroblocks by pixel comparison, which increases the residual magnitude.</p>
</div>
<div id="S3.SS3.p8" class="ltx_para">
<p id="S3.SS3.p8.1" class="ltx_p">The <span id="S3.SS3.p8.1.1" class="ltx_text ltx_font_italic">QP</span> factor by itself, therefore, is not enough to encompass real-world video distortions, and the addition of encoder speed control is needed.
In addition, we introduce Frame rate (FPS) control and aspect ratio scaling to augment real-world compression artifact synthesis.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Network Architecture and Training</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Given the prevalent success of Transformer-based networks in diverse vision tasks, this work leverages the GRL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> architecture as the foundational model. We enhance this baseline architecture by introducing the video artifact degradation pipeline, thereby enabling the network to learn an extended range of degradation patterns, including those associated with video coding artifacts.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">GRL exploits correlations across multiple levels of image hierarchies via Transformer-based architecture, which achieves state-of-the-art performance in generic super-resolution and restoration tasks (<span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_italic">e.g.</span>, deblurring, JPEG restoration, demosaic, and classical SR). To forge efficient training and to reach a reasonable balance between computation resources and performance, we choose the <span id="S3.SS4.p2.1.2" class="ltx_text ltx_font_italic">small</span> variation of GRL (Figure <a href="#S3.F4" title="Figure 4 ‚Ä£ 3.1 Base Degradation Elements ‚Ä£ 3 Proposed Methods ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) with 3.49M parameters as opposed to the base GRL model that totals 20.2M parameters.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Following Real-ESRGAN, BSRGAN, SwinIR, and GRL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, we first train the network with a PSNR-oriented L1 loss. Then we use the trained network parameters to initialize the generator and train a GAN model to boost perceptual quality. The loss function during this phase combines L1 loss, perceptual loss<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and GAN loss<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2311.00996/assets/figures/VIS_SMALL_RE.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="536" height="840" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.5.2.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.2.1" class="ltx_text" style="font-size:90%;">Qualitative comparisons of different methods on <math id="S3.F5.2.1.m1.1" class="ltx_Math" alttext="\times 4" display="inline"><semantics id="S3.F5.2.1.m1.1b"><mrow id="S3.F5.2.1.m1.1.1" xref="S3.F5.2.1.m1.1.1.cmml"><mi id="S3.F5.2.1.m1.1.1.2" xref="S3.F5.2.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.F5.2.1.m1.1.1.1" xref="S3.F5.2.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.F5.2.1.m1.1.1.3" xref="S3.F5.2.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F5.2.1.m1.1c"><apply id="S3.F5.2.1.m1.1.1.cmml" xref="S3.F5.2.1.m1.1.1"><times id="S3.F5.2.1.m1.1.1.1.cmml" xref="S3.F5.2.1.m1.1.1.1"></times><csymbol cd="latexml" id="S3.F5.2.1.m1.1.1.2.cmml" xref="S3.F5.2.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.F5.2.1.m1.1.1.3.cmml" xref="S3.F5.2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.2.1.m1.1d">\times 4</annotation></semantics></math> super-resolved images in the DRealSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, VideoLQ<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, VideoLQ<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and AVC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> datasets from top to bottom respectively. <span id="S3.F5.2.1.1" class="ltx_text ltx_font_bold">(Zoom in for best view.)</span></span></figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.14.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.15.2" class="ltx_text" style="font-size:90%;">The NIQE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, BRISQUE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, NRQM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, and CLIP-IQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> results of different blind SISR methods on the proposed VC-RealLQ dataset, together with the RealSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and DRealSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> dataset. The best and the second best results are remarked in bold font and underlined respectively. We use pyiqa<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> library to test all datasets.</span></figcaption>
<div id="S3.T1.12" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:65.9pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-260.4pt,39.3pt) scale(0.454302712563065,0.454302712563065) ;">
<table id="S3.T1.12.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.12.12.13.1" class="ltx_tr">
<th id="S3.T1.12.12.13.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T1.12.12.13.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S3.T1.12.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T1.12.12.13.1.2.1" class="ltx_text ltx_font_bold"># Params (M)</span></th>
<th id="S3.T1.12.12.13.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4"><span id="S3.T1.12.12.13.1.3.1" class="ltx_text ltx_font_bold">VC-RealLQ</span></th>
<th id="S3.T1.12.12.13.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4"><span id="S3.T1.12.12.13.1.4.1" class="ltx_text ltx_font_bold">RealSR-Nikon</span></th>
<th id="S3.T1.12.12.13.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S3.T1.12.12.13.1.5.1" class="ltx_text ltx_font_bold">DRealSR</span></th>
</tr>
<tr id="S3.T1.12.12.12" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NIQE <math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BRISQUE <math id="S3.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.2.2.2.2.m1.1a"><mo stretchy="false" id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NRQM <math id="S3.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.3.3.3.3.m1.1a"><mo stretchy="false" id="S3.T1.3.3.3.3.m1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CLIPIQA <math id="S3.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.4.4.4.4.m1.1a"><mo stretchy="false" id="S3.T1.4.4.4.4.m1.1.1" xref="S3.T1.4.4.4.4.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.m1.1b"><ci id="S3.T1.4.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.4.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NIQE <math id="S3.T1.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.5.5.5.5.m1.1a"><mo stretchy="false" id="S3.T1.5.5.5.5.m1.1.1" xref="S3.T1.5.5.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.m1.1b"><ci id="S3.T1.5.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BRISQUE <math id="S3.T1.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.6.6.6.6.m1.1a"><mo stretchy="false" id="S3.T1.6.6.6.6.m1.1.1" xref="S3.T1.6.6.6.6.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.6.m1.1b"><ci id="S3.T1.6.6.6.6.m1.1.1.cmml" xref="S3.T1.6.6.6.6.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.7.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NRQM <math id="S3.T1.7.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.7.7.7.7.m1.1a"><mo stretchy="false" id="S3.T1.7.7.7.7.m1.1.1" xref="S3.T1.7.7.7.7.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.7.m1.1b"><ci id="S3.T1.7.7.7.7.m1.1.1.cmml" xref="S3.T1.7.7.7.7.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.7.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T1.8.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CLIPIQA <math id="S3.T1.8.8.8.8.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.8.8.8.8.m1.1a"><mo stretchy="false" id="S3.T1.8.8.8.8.m1.1.1" xref="S3.T1.8.8.8.8.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.8.m1.1b"><ci id="S3.T1.8.8.8.8.m1.1.1.cmml" xref="S3.T1.8.8.8.8.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.8.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T1.9.9.9.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NIQE <math id="S3.T1.9.9.9.9.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.9.9.9.9.m1.1a"><mo stretchy="false" id="S3.T1.9.9.9.9.m1.1.1" xref="S3.T1.9.9.9.9.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.9.m1.1b"><ci id="S3.T1.9.9.9.9.m1.1.1.cmml" xref="S3.T1.9.9.9.9.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.9.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.10.10.10.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BRISQUE <math id="S3.T1.10.10.10.10.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.10.10.10.10.m1.1a"><mo stretchy="false" id="S3.T1.10.10.10.10.m1.1.1" xref="S3.T1.10.10.10.10.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.10.10.m1.1b"><ci id="S3.T1.10.10.10.10.m1.1.1.cmml" xref="S3.T1.10.10.10.10.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.10.10.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.11.11.11.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NRQM <math id="S3.T1.11.11.11.11.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.11.11.11.11.m1.1a"><mo stretchy="false" id="S3.T1.11.11.11.11.m1.1.1" xref="S3.T1.11.11.11.11.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.11.11.m1.1b"><ci id="S3.T1.11.11.11.11.m1.1.1.cmml" xref="S3.T1.11.11.11.11.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.11.11.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T1.12.12.12.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CLIPIQA <math id="S3.T1.12.12.12.12.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.12.12.12.12.m1.1a"><mo stretchy="false" id="S3.T1.12.12.12.12.m1.1.1" xref="S3.T1.12.12.12.12.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.12.12.m1.1b"><ci id="S3.T1.12.12.12.12.m1.1.1.cmml" xref="S3.T1.12.12.12.12.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.12.12.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.12.12.14.1" class="ltx_tr">
<th id="S3.T1.12.12.14.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">RealSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</th>
<th id="S3.T1.12.12.14.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">16.7</th>
<td id="S3.T1.12.12.14.1.3" class="ltx_td ltx_align_center ltx_border_t">5.738</td>
<td id="S3.T1.12.12.14.1.4" class="ltx_td ltx_align_center ltx_border_t">37.04</td>
<td id="S3.T1.12.12.14.1.5" class="ltx_td ltx_align_center ltx_border_t">5.147</td>
<td id="S3.T1.12.12.14.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.334</td>
<td id="S3.T1.12.12.14.1.7" class="ltx_td ltx_align_center ltx_border_t">7.435</td>
<td id="S3.T1.12.12.14.1.8" class="ltx_td ltx_align_center ltx_border_t">57.029</td>
<td id="S3.T1.12.12.14.1.9" class="ltx_td ltx_align_center ltx_border_t">3.21</td>
<td id="S3.T1.12.12.14.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.275</td>
<td id="S3.T1.12.12.14.1.11" class="ltx_td ltx_align_center ltx_border_t">8.457</td>
<td id="S3.T1.12.12.14.1.12" class="ltx_td ltx_align_center ltx_border_t">56.877</td>
<td id="S3.T1.12.12.14.1.13" class="ltx_td ltx_align_center ltx_border_t">3.507</td>
<td id="S3.T1.12.12.14.1.14" class="ltx_td ltx_align_center ltx_border_t">0.238</td>
</tr>
<tr id="S3.T1.12.12.15.2" class="ltx_tr">
<th id="S3.T1.12.12.15.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Real-ESRGAN+<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</th>
<th id="S3.T1.12.12.15.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">16.7</th>
<td id="S3.T1.12.12.15.2.3" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.15.2.3.1" class="ltx_text ltx_framed ltx_framed_underline">4.967</span></td>
<td id="S3.T1.12.12.15.2.4" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.15.2.4.1" class="ltx_text ltx_framed ltx_framed_underline">29.29</span></td>
<td id="S3.T1.12.12.15.2.5" class="ltx_td ltx_align_center">5.231</td>
<td id="S3.T1.12.12.15.2.6" class="ltx_td ltx_align_center ltx_border_r">0.438</td>
<td id="S3.T1.12.12.15.2.7" class="ltx_td ltx_align_center">4.901</td>
<td id="S3.T1.12.12.15.2.8" class="ltx_td ltx_align_center">31.911</td>
<td id="S3.T1.12.12.15.2.9" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.15.2.9.1" class="ltx_text ltx_framed ltx_framed_underline">5.668</span></td>
<td id="S3.T1.12.12.15.2.10" class="ltx_td ltx_align_center ltx_border_r">0.499</td>
<td id="S3.T1.12.12.15.2.11" class="ltx_td ltx_align_center">4.718</td>
<td id="S3.T1.12.12.15.2.12" class="ltx_td ltx_align_center">29.872</td>
<td id="S3.T1.12.12.15.2.13" class="ltx_td ltx_align_center">5.428</td>
<td id="S3.T1.12.12.15.2.14" class="ltx_td ltx_align_center">0.518</td>
</tr>
<tr id="S3.T1.12.12.16.3" class="ltx_tr">
<th id="S3.T1.12.12.16.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BSRGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</th>
<th id="S3.T1.12.12.16.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">16.7</th>
<td id="S3.T1.12.12.16.3.3" class="ltx_td ltx_align_center">5.164</td>
<td id="S3.T1.12.12.16.3.4" class="ltx_td ltx_align_center">29.394</td>
<td id="S3.T1.12.12.16.3.5" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.16.3.5.1" class="ltx_text ltx_framed ltx_framed_underline">5.242</span></td>
<td id="S3.T1.12.12.16.3.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.12.12.16.3.6.1" class="ltx_text ltx_framed ltx_framed_underline">0.498</span></td>
<td id="S3.T1.12.12.16.3.7" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.16.3.7.1" class="ltx_text ltx_font_bold">4.772</span></td>
<td id="S3.T1.12.12.16.3.8" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.16.3.8.1" class="ltx_text ltx_font_bold">25.382</span></td>
<td id="S3.T1.12.12.16.3.9" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.16.3.9.1" class="ltx_text ltx_font_bold">5.938</span></td>
<td id="S3.T1.12.12.16.3.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.12.12.16.3.10.1" class="ltx_text ltx_framed ltx_framed_underline">0.564</span></td>
<td id="S3.T1.12.12.16.3.11" class="ltx_td ltx_align_center">4.681</td>
<td id="S3.T1.12.12.16.3.12" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.16.3.12.1" class="ltx_text ltx_framed ltx_framed_underline">27.858</span></td>
<td id="S3.T1.12.12.16.3.13" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.16.3.13.1" class="ltx_text ltx_framed ltx_framed_underline">5.461</span></td>
<td id="S3.T1.12.12.16.3.14" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.16.3.14.1" class="ltx_text ltx_framed ltx_framed_underline">0.57</span></td>
</tr>
<tr id="S3.T1.12.12.17.4" class="ltx_tr">
<th id="S3.T1.12.12.17.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SwinIR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</th>
<th id="S3.T1.12.12.17.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">11.9</th>
<td id="S3.T1.12.12.17.4.3" class="ltx_td ltx_align_center">5.095</td>
<td id="S3.T1.12.12.17.4.4" class="ltx_td ltx_align_center">33.097</td>
<td id="S3.T1.12.12.17.4.5" class="ltx_td ltx_align_center">4.922</td>
<td id="S3.T1.12.12.17.4.6" class="ltx_td ltx_align_center ltx_border_r">0.438</td>
<td id="S3.T1.12.12.17.4.7" class="ltx_td ltx_align_center">4.877</td>
<td id="S3.T1.12.12.17.4.8" class="ltx_td ltx_align_center">34.964</td>
<td id="S3.T1.12.12.17.4.9" class="ltx_td ltx_align_center">5.408</td>
<td id="S3.T1.12.12.17.4.10" class="ltx_td ltx_align_center ltx_border_r">0.47</td>
<td id="S3.T1.12.12.17.4.11" class="ltx_td ltx_align_center">6.259</td>
<td id="S3.T1.12.12.17.4.12" class="ltx_td ltx_align_center">49.546</td>
<td id="S3.T1.12.12.17.4.13" class="ltx_td ltx_align_center">5.183</td>
<td id="S3.T1.12.12.17.4.14" class="ltx_td ltx_align_center">0.465</td>
</tr>
<tr id="S3.T1.12.12.18.5" class="ltx_tr">
<th id="S3.T1.12.12.18.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">GRL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</th>
<th id="S3.T1.12.12.18.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">20.2</th>
<td id="S3.T1.12.12.18.5.3" class="ltx_td ltx_align_center">5.338</td>
<td id="S3.T1.12.12.18.5.4" class="ltx_td ltx_align_center">33.769</td>
<td id="S3.T1.12.12.18.5.5" class="ltx_td ltx_align_center">5.043</td>
<td id="S3.T1.12.12.18.5.6" class="ltx_td ltx_align_center ltx_border_r">0.451</td>
<td id="S3.T1.12.12.18.5.7" class="ltx_td ltx_align_center">4.981</td>
<td id="S3.T1.12.12.18.5.8" class="ltx_td ltx_align_center">34.937</td>
<td id="S3.T1.12.12.18.5.9" class="ltx_td ltx_align_center">5.37</td>
<td id="S3.T1.12.12.18.5.10" class="ltx_td ltx_align_center ltx_border_r">0.456</td>
<td id="S3.T1.12.12.18.5.11" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.18.5.11.1" class="ltx_text ltx_framed ltx_framed_underline">4.633</span></td>
<td id="S3.T1.12.12.18.5.12" class="ltx_td ltx_align_center">29.323</td>
<td id="S3.T1.12.12.18.5.13" class="ltx_td ltx_align_center">5.389</td>
<td id="S3.T1.12.12.18.5.14" class="ltx_td ltx_align_center">0.545</td>
</tr>
<tr id="S3.T1.12.12.19.6" class="ltx_tr">
<th id="S3.T1.12.12.19.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">VCISR (ours)</th>
<th id="S3.T1.12.12.19.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S3.T1.12.12.19.6.2.1" class="ltx_text ltx_font_bold">3.49</span></th>
<td id="S3.T1.12.12.19.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.12.12.19.6.3.1" class="ltx_text ltx_font_bold">4.542</span></td>
<td id="S3.T1.12.12.19.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.12.12.19.6.4.1" class="ltx_text ltx_font_bold">16.975</span></td>
<td id="S3.T1.12.12.19.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.12.12.19.6.5.1" class="ltx_text ltx_font_bold">5.479</span></td>
<td id="S3.T1.12.12.19.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T1.12.12.19.6.6.1" class="ltx_text ltx_font_bold">0.58</span></td>
<td id="S3.T1.12.12.19.6.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.12.12.19.6.7.1" class="ltx_text ltx_framed ltx_framed_underline">4.823</span></td>
<td id="S3.T1.12.12.19.6.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.12.12.19.6.8.1" class="ltx_text ltx_framed ltx_framed_underline">29.203</span></td>
<td id="S3.T1.12.12.19.6.9" class="ltx_td ltx_align_center ltx_border_bb">5.445</td>
<td id="S3.T1.12.12.19.6.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T1.12.12.19.6.10.1" class="ltx_text ltx_font_bold">0.603</span></td>
<td id="S3.T1.12.12.19.6.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.12.12.19.6.11.1" class="ltx_text ltx_font_bold">3.983</span></td>
<td id="S3.T1.12.12.19.6.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.12.12.19.6.12.1" class="ltx_text ltx_font_bold">15.303</span></td>
<td id="S3.T1.12.12.19.6.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.12.12.19.6.13.1" class="ltx_text ltx_font_bold">5.778</span></td>
<td id="S3.T1.12.12.19.6.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.12.12.19.6.14.1" class="ltx_text ltx_font_bold">0.646</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.14.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.15.2" class="ltx_text" style="font-size:90%;">The NIQE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, BRISQUE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, NRQM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, and CLIP-IQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> results of different ISR, VSR methods on the REDS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, AVC-RealLQ<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, and ViedoLQ<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> dataset. The best and the second best results are remarked in bold font and underlined respectively. Due to high computation intensity, following RealBasicVSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, NRQM is computed on the first, middle, and last frames in each sequence. We use pyiqa<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> library to test all datasets.</span></figcaption>
<div id="S3.T2.12" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:74.1pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-260.1pt,44.2pt) scale(0.454580501148508,0.454580501148508) ;">
<table id="S3.T2.12.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.12.12.13.1" class="ltx_tr">
<th id="S3.T2.12.12.13.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T2.12.12.13.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S3.T2.12.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T2.12.12.13.1.2.1" class="ltx_text ltx_font_bold"># Params (M)</span></th>
<th id="S3.T2.12.12.13.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4"><span id="S3.T2.12.12.13.1.3.1" class="ltx_text ltx_font_bold">REDS+Blur+MPEG</span></th>
<th id="S3.T2.12.12.13.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4"><span id="S3.T2.12.12.13.1.4.1" class="ltx_text ltx_font_bold">AVC-RealLQ</span></th>
<th id="S3.T2.12.12.13.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S3.T2.12.12.13.1.5.1" class="ltx_text ltx_font_bold">VideoLQ</span></th>
</tr>
<tr id="S3.T2.12.12.12" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NIQE <math id="S3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BRISQUE <math id="S3.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.2.2.2.2.m1.1a"><mo stretchy="false" id="S3.T2.2.2.2.2.m1.1.1" xref="S3.T2.2.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.2.m1.1b"><ci id="S3.T2.2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NRQM <math id="S3.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.3.3.3.3.m1.1a"><mo stretchy="false" id="S3.T2.3.3.3.3.m1.1.1" xref="S3.T2.3.3.3.3.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.3.m1.1b"><ci id="S3.T2.3.3.3.3.m1.1.1.cmml" xref="S3.T2.3.3.3.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CLIPIQA <math id="S3.T2.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.4.4.4.4.m1.1a"><mo stretchy="false" id="S3.T2.4.4.4.4.m1.1.1" xref="S3.T2.4.4.4.4.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.4.4.m1.1b"><ci id="S3.T2.4.4.4.4.m1.1.1.cmml" xref="S3.T2.4.4.4.4.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T2.5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NIQE <math id="S3.T2.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.5.5.5.5.m1.1a"><mo stretchy="false" id="S3.T2.5.5.5.5.m1.1.1" xref="S3.T2.5.5.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.5.5.m1.1b"><ci id="S3.T2.5.5.5.5.m1.1.1.cmml" xref="S3.T2.5.5.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T2.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BRISQUE <math id="S3.T2.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.6.6.6.6.m1.1a"><mo stretchy="false" id="S3.T2.6.6.6.6.m1.1.1" xref="S3.T2.6.6.6.6.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.6.6.m1.1b"><ci id="S3.T2.6.6.6.6.m1.1.1.cmml" xref="S3.T2.6.6.6.6.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T2.7.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NRQM <math id="S3.T2.7.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.7.7.7.7.m1.1a"><mo stretchy="false" id="S3.T2.7.7.7.7.m1.1.1" xref="S3.T2.7.7.7.7.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.7.7.7.7.m1.1b"><ci id="S3.T2.7.7.7.7.m1.1.1.cmml" xref="S3.T2.7.7.7.7.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.7.7.7.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T2.8.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CLIPIQA <math id="S3.T2.8.8.8.8.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.8.8.8.8.m1.1a"><mo stretchy="false" id="S3.T2.8.8.8.8.m1.1.1" xref="S3.T2.8.8.8.8.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.8.8.8.8.m1.1b"><ci id="S3.T2.8.8.8.8.m1.1.1.cmml" xref="S3.T2.8.8.8.8.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.8.8.8.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T2.9.9.9.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NIQE <math id="S3.T2.9.9.9.9.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.9.9.9.9.m1.1a"><mo stretchy="false" id="S3.T2.9.9.9.9.m1.1.1" xref="S3.T2.9.9.9.9.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.9.9.9.9.m1.1b"><ci id="S3.T2.9.9.9.9.m1.1.1.cmml" xref="S3.T2.9.9.9.9.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.9.9.9.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T2.10.10.10.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BRISQUE <math id="S3.T2.10.10.10.10.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.10.10.10.10.m1.1a"><mo stretchy="false" id="S3.T2.10.10.10.10.m1.1.1" xref="S3.T2.10.10.10.10.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.10.10.10.10.m1.1b"><ci id="S3.T2.10.10.10.10.m1.1.1.cmml" xref="S3.T2.10.10.10.10.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.10.10.10.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T2.11.11.11.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NRQM <math id="S3.T2.11.11.11.11.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.11.11.11.11.m1.1a"><mo stretchy="false" id="S3.T2.11.11.11.11.m1.1.1" xref="S3.T2.11.11.11.11.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.11.11.11.11.m1.1b"><ci id="S3.T2.11.11.11.11.m1.1.1.cmml" xref="S3.T2.11.11.11.11.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.11.11.11.11.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T2.12.12.12.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CLIPIQA <math id="S3.T2.12.12.12.12.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.12.12.12.12.m1.1a"><mo stretchy="false" id="S3.T2.12.12.12.12.m1.1.1" xref="S3.T2.12.12.12.12.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T2.12.12.12.12.m1.1b"><ci id="S3.T2.12.12.12.12.m1.1.1.cmml" xref="S3.T2.12.12.12.12.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.12.12.12.12.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.12.12.14.1" class="ltx_tr">
<th id="S3.T2.12.12.14.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Real-ESRGAN+<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</th>
<th id="S3.T2.12.12.14.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">16.7</th>
<td id="S3.T2.12.12.14.1.3" class="ltx_td ltx_align_center ltx_border_t">5.192</td>
<td id="S3.T2.12.12.14.1.4" class="ltx_td ltx_align_center ltx_border_t">32.261</td>
<td id="S3.T2.12.12.14.1.5" class="ltx_td ltx_align_center ltx_border_t">5.038</td>
<td id="S3.T2.12.12.14.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.331</td>
<td id="S3.T2.12.12.14.1.7" class="ltx_td ltx_align_center ltx_border_t">8.632</td>
<td id="S3.T2.12.12.14.1.8" class="ltx_td ltx_align_center ltx_border_t">36.087</td>
<td id="S3.T2.12.12.14.1.9" class="ltx_td ltx_align_center ltx_border_t">5.054</td>
<td id="S3.T2.12.12.14.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.534</td>
<td id="S3.T2.12.12.14.1.11" class="ltx_td ltx_align_center ltx_border_t">4.204</td>
<td id="S3.T2.12.12.14.1.12" class="ltx_td ltx_align_center ltx_border_t">29.844</td>
<td id="S3.T2.12.12.14.1.13" class="ltx_td ltx_align_center ltx_border_t">5.815</td>
<td id="S3.T2.12.12.14.1.14" class="ltx_td ltx_align_center ltx_border_t">0.362</td>
</tr>
<tr id="S3.T2.12.12.15.2" class="ltx_tr">
<th id="S3.T2.12.12.15.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BSRGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</th>
<th id="S3.T2.12.12.15.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">16.7</th>
<td id="S3.T2.12.12.15.2.3" class="ltx_td ltx_align_center">5.303</td>
<td id="S3.T2.12.12.15.2.4" class="ltx_td ltx_align_center">28.498</td>
<td id="S3.T2.12.12.15.2.5" class="ltx_td ltx_align_center">4.906</td>
<td id="S3.T2.12.12.15.2.6" class="ltx_td ltx_align_center ltx_border_r">0.402</td>
<td id="S3.T2.12.12.15.2.7" class="ltx_td ltx_align_center">8.281</td>
<td id="S3.T2.12.12.15.2.8" class="ltx_td ltx_align_center">34.987</td>
<td id="S3.T2.12.12.15.2.9" class="ltx_td ltx_align_center">5.004</td>
<td id="S3.T2.12.12.15.2.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.12.12.15.2.10.1" class="ltx_text ltx_framed ltx_framed_underline">0.61</span></td>
<td id="S3.T2.12.12.15.2.11" class="ltx_td ltx_align_center">4.211</td>
<td id="S3.T2.12.12.15.2.12" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.15.2.12.1" class="ltx_text ltx_framed ltx_framed_underline">25.24</span></td>
<td id="S3.T2.12.12.15.2.13" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.15.2.13.1" class="ltx_text ltx_framed ltx_framed_underline">5.825</span></td>
<td id="S3.T2.12.12.15.2.14" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.15.2.14.1" class="ltx_text ltx_framed ltx_framed_underline">0.422</span></td>
</tr>
<tr id="S3.T2.12.12.16.3" class="ltx_tr">
<th id="S3.T2.12.12.16.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SwinIR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</th>
<th id="S3.T2.12.12.16.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">11.9</th>
<td id="S3.T2.12.12.16.3.3" class="ltx_td ltx_align_center">5.214</td>
<td id="S3.T2.12.12.16.3.4" class="ltx_td ltx_align_center">32.476</td>
<td id="S3.T2.12.12.16.3.5" class="ltx_td ltx_align_center">4.984</td>
<td id="S3.T2.12.12.16.3.6" class="ltx_td ltx_align_center ltx_border_r">0.34</td>
<td id="S3.T2.12.12.16.3.7" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.16.3.7.1" class="ltx_text ltx_framed ltx_framed_underline">6.351</span></td>
<td id="S3.T2.12.12.16.3.8" class="ltx_td ltx_align_center">41.285</td>
<td id="S3.T2.12.12.16.3.9" class="ltx_td ltx_align_center">4.739</td>
<td id="S3.T2.12.12.16.3.10" class="ltx_td ltx_align_center ltx_border_r">0.506</td>
<td id="S3.T2.12.12.16.3.11" class="ltx_td ltx_align_center">4.198</td>
<td id="S3.T2.12.12.16.3.12" class="ltx_td ltx_align_center">31.492</td>
<td id="S3.T2.12.12.16.3.13" class="ltx_td ltx_align_center">5.667</td>
<td id="S3.T2.12.12.16.3.14" class="ltx_td ltx_align_center">0.38</td>
</tr>
<tr id="S3.T2.12.12.17.4" class="ltx_tr">
<th id="S3.T2.12.12.17.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">GRL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</th>
<th id="S3.T2.12.12.17.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">20.2</th>
<td id="S3.T2.12.12.17.4.3" class="ltx_td ltx_align_center">5.611</td>
<td id="S3.T2.12.12.17.4.4" class="ltx_td ltx_align_center">35.256</td>
<td id="S3.T2.12.12.17.4.5" class="ltx_td ltx_align_center">4.767</td>
<td id="S3.T2.12.12.17.4.6" class="ltx_td ltx_align_center ltx_border_r">0.368</td>
<td id="S3.T2.12.12.17.4.7" class="ltx_td ltx_align_center">6.457</td>
<td id="S3.T2.12.12.17.4.8" class="ltx_td ltx_align_center">43.439</td>
<td id="S3.T2.12.12.17.4.9" class="ltx_td ltx_align_center">5.034</td>
<td id="S3.T2.12.12.17.4.10" class="ltx_td ltx_align_center ltx_border_r">0.575</td>
<td id="S3.T2.12.12.17.4.11" class="ltx_td ltx_align_center">4.476</td>
<td id="S3.T2.12.12.17.4.12" class="ltx_td ltx_align_center">32.349</td>
<td id="S3.T2.12.12.17.4.13" class="ltx_td ltx_align_center">5.523</td>
<td id="S3.T2.12.12.17.4.14" class="ltx_td ltx_align_center">0.389</td>
</tr>
<tr id="S3.T2.12.12.18.5" class="ltx_tr">
<th id="S3.T2.12.12.18.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">RealBasicVSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<th id="S3.T2.12.12.18.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">6.3</th>
<td id="S3.T2.12.12.18.5.3" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.18.5.3.1" class="ltx_text ltx_font_bold">3.765</span></td>
<td id="S3.T2.12.12.18.5.4" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.18.5.4.1" class="ltx_text ltx_framed ltx_framed_underline">14.6</span></td>
<td id="S3.T2.12.12.18.5.5" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.18.5.5.1" class="ltx_text ltx_font_bold">5.43</span></td>
<td id="S3.T2.12.12.18.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.12.12.18.5.6.1" class="ltx_text ltx_framed ltx_framed_underline">0.41</span></td>
<td id="S3.T2.12.12.18.5.7" class="ltx_td ltx_align_center">8.639</td>
<td id="S3.T2.12.12.18.5.8" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.18.5.8.1" class="ltx_text ltx_framed ltx_framed_underline">26.013</span></td>
<td id="S3.T2.12.12.18.5.9" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.18.5.9.1" class="ltx_text ltx_framed ltx_framed_underline">5.07</span></td>
<td id="S3.T2.12.12.18.5.10" class="ltx_td ltx_align_center ltx_border_r">0.583</td>
<td id="S3.T2.12.12.18.5.11" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.18.5.11.1" class="ltx_text ltx_framed ltx_framed_underline">3.766</span></td>
<td id="S3.T2.12.12.18.5.12" class="ltx_td ltx_align_center">29.03</td>
<td id="S3.T2.12.12.18.5.13" class="ltx_td ltx_align_center"><span id="S3.T2.12.12.18.5.13.1" class="ltx_text ltx_font_bold">6.0477</span></td>
<td id="S3.T2.12.12.18.5.14" class="ltx_td ltx_align_center">0.376</td>
</tr>
<tr id="S3.T2.12.12.19.6" class="ltx_tr">
<th id="S3.T2.12.12.19.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">DBVSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<th id="S3.T2.12.12.19.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">25.5</th>
<td id="S3.T2.12.12.19.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.12.12.19.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.12.12.19.6.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.12.12.19.6.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.12.12.19.6.7" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.12.12.19.6.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.12.12.19.6.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.12.12.19.6.10" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.12.12.19.6.11" class="ltx_td ltx_align_center">6.7866</td>
<td id="S3.T2.12.12.19.6.12" class="ltx_td ltx_align_center">50.936</td>
<td id="S3.T2.12.12.19.6.13" class="ltx_td ltx_align_center">3.4097</td>
<td id="S3.T2.12.12.19.6.14" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.12.12.20.7" class="ltx_tr">
<th id="S3.T2.12.12.20.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">VCISR (ours)</th>
<th id="S3.T2.12.12.20.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S3.T2.12.12.20.7.2.1" class="ltx_text ltx_font_bold">3.49</span></th>
<td id="S3.T2.12.12.20.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.12.12.20.7.3.1" class="ltx_text ltx_framed ltx_framed_underline">4.427</span></td>
<td id="S3.T2.12.12.20.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.12.12.20.7.4.1" class="ltx_text ltx_font_bold">12.709</span></td>
<td id="S3.T2.12.12.20.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.12.12.20.7.5.1" class="ltx_text ltx_framed ltx_framed_underline">5.207</span></td>
<td id="S3.T2.12.12.20.7.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.12.12.20.7.6.1" class="ltx_text ltx_font_bold">0.508</span></td>
<td id="S3.T2.12.12.20.7.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.12.12.20.7.7.1" class="ltx_text ltx_font_bold">5.515</span></td>
<td id="S3.T2.12.12.20.7.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.12.12.20.7.8.1" class="ltx_text ltx_font_bold">19.529</span></td>
<td id="S3.T2.12.12.20.7.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.12.12.20.7.9.1" class="ltx_text ltx_font_bold">5.236</span></td>
<td id="S3.T2.12.12.20.7.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.12.12.20.7.10.1" class="ltx_text ltx_font_bold">0.689</span></td>
<td id="S3.T2.12.12.20.7.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.12.12.20.7.11.1" class="ltx_text ltx_font_bold">3.725</span></td>
<td id="S3.T2.12.12.20.7.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.12.12.20.7.12.1" class="ltx_text ltx_font_bold">19.372</span></td>
<td id="S3.T2.12.12.20.7.13" class="ltx_td ltx_align_center ltx_border_bb">5.735</td>
<td id="S3.T2.12.12.20.7.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.12.12.20.7.14.1" class="ltx_text ltx_font_bold">0.488</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Datasets.</span>
We train the proposed model with the DIV2K dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which has gained prominence in the field of image super-resolution. To validate the efficacy of the presented approach, we perform comprehensive evaluations on both image super-resolution and video super-resolution tasks. Specifically, for image super-resolution, we employ the RealSR-Nikon<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and the DRealSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> dataset, capitalizing on their distinct characteristics. Additionally, to enhance the diversity of degradation patterns in our test set, we bring in a specialized dataset simulating video coding-introduced degradations. More details are elaborated in Section <a href="#S4.SS3" title="4.3 VC-RealLQ Dataset ‚Ä£ 4 Experiments ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
Furthermore, to assess the versatility of our proposed method, we subjected it to rigorous testing on widely used Video Super-Resolution (VSR) datasets, including the REDS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, AVC-RealLQ<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, and VideoLQ<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. These datasets are chosen for their relevance and ability to provide insights into the method‚Äôs performance across varying video content and quality levels. Through this meticulous evaluation process, we aim to showcase the robustness and adaptability of our approach across both image and video super-resolution domains.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.4" class="ltx_p"><span id="S4.SS1.p2.4.1" class="ltx_text ltx_font_bold">Training details.</span>
The neural network training is performed on one Nvidia RTX 4090 GPU. For the first stage, we train the network with L1 loss for 700<span id="S4.SS1.p2.4.2" class="ltx_text ltx_font_italic">K</span> iterations, employing a batch size of 12. The Adam optimizer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is adopted with a learning rate of <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="2\times 10^{-4}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">√ó</mo><msup id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><mn id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p2.1.m1.1.1.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml"><mo id="S4.SS1.p2.1.m1.1.1.3.3a" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml">‚àí</mo><mn id="S4.SS1.p2.1.m1.1.1.3.3.2" xref="S4.SS1.p2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><times id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">2</cn><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2">10</cn><apply id="S4.SS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3"><minus id="S4.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">2\times 10^{-4}</annotation></semantics></math>, which is decayed by half every 50<span id="S4.SS1.p2.4.3" class="ltx_text ltx_font_italic">K</span> iterations.
In the subsequent adversarial training stage, the model is trained for 280<span id="S4.SS1.p2.4.4" class="ltx_text ltx_font_italic">K</span> iterations, employing a batch size of 6. The learning rate for this stage is set at <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mn id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">√ó</mo><msup id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml"><mn id="S4.SS1.p2.2.m2.1.1.3.2" xref="S4.SS1.p2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p2.2.m2.1.1.3.3" xref="S4.SS1.p2.2.m2.1.1.3.3.cmml"><mo id="S4.SS1.p2.2.m2.1.1.3.3a" xref="S4.SS1.p2.2.m2.1.1.3.3.cmml">‚àí</mo><mn id="S4.SS1.p2.2.m2.1.1.3.3.2" xref="S4.SS1.p2.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><times id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">1</cn><apply id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2">10</cn><apply id="S4.SS1.p2.2.m2.1.1.3.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3"><minus id="S4.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">1\times 10^{-4}</annotation></semantics></math> and decayed to half every 20<span id="S4.SS1.p2.4.5" class="ltx_text ltx_font_italic">K</span> iterations. The adversarial training of our model adhered to a weighted perceptual loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and employed a U-Net discriminator with spectral normalization<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
The perceptual loss utilized the same pre-trained VGG-19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, with weight coefficients <math id="S4.SS1.p2.3.m3.5" class="ltx_Math" alttext="\{0.1,0.1,1,1,1\}" display="inline"><semantics id="S4.SS1.p2.3.m3.5a"><mrow id="S4.SS1.p2.3.m3.5.6.2" xref="S4.SS1.p2.3.m3.5.6.1.cmml"><mo stretchy="false" id="S4.SS1.p2.3.m3.5.6.2.1" xref="S4.SS1.p2.3.m3.5.6.1.cmml">{</mo><mn id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">0.1</mn><mo id="S4.SS1.p2.3.m3.5.6.2.2" xref="S4.SS1.p2.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS1.p2.3.m3.2.2" xref="S4.SS1.p2.3.m3.2.2.cmml">0.1</mn><mo id="S4.SS1.p2.3.m3.5.6.2.3" xref="S4.SS1.p2.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS1.p2.3.m3.3.3" xref="S4.SS1.p2.3.m3.3.3.cmml">1</mn><mo id="S4.SS1.p2.3.m3.5.6.2.4" xref="S4.SS1.p2.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS1.p2.3.m3.4.4" xref="S4.SS1.p2.3.m3.4.4.cmml">1</mn><mo id="S4.SS1.p2.3.m3.5.6.2.5" xref="S4.SS1.p2.3.m3.5.6.1.cmml">,</mo><mn id="S4.SS1.p2.3.m3.5.5" xref="S4.SS1.p2.3.m3.5.5.cmml">1</mn><mo stretchy="false" id="S4.SS1.p2.3.m3.5.6.2.6" xref="S4.SS1.p2.3.m3.5.6.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.5b"><set id="S4.SS1.p2.3.m3.5.6.1.cmml" xref="S4.SS1.p2.3.m3.5.6.2"><cn type="float" id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">0.1</cn><cn type="float" id="S4.SS1.p2.3.m3.2.2.cmml" xref="S4.SS1.p2.3.m3.2.2">0.1</cn><cn type="integer" id="S4.SS1.p2.3.m3.3.3.cmml" xref="S4.SS1.p2.3.m3.3.3">1</cn><cn type="integer" id="S4.SS1.p2.3.m3.4.4.cmml" xref="S4.SS1.p2.3.m3.4.4">1</cn><cn type="integer" id="S4.SS1.p2.3.m3.5.5.cmml" xref="S4.SS1.p2.3.m3.5.5">1</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.5c">\{0.1,0.1,1,1,1\}</annotation></semantics></math> corresponding to feature maps <math id="S4.SS1.p2.4.m4.2" class="ltx_Math" alttext="\{\mathtt{conv1},...\mathtt{conv5}\}" display="inline"><semantics id="S4.SS1.p2.4.m4.2a"><mrow id="S4.SS1.p2.4.m4.2.2.1" xref="S4.SS1.p2.4.m4.2.2.2.cmml"><mo stretchy="false" id="S4.SS1.p2.4.m4.2.2.1.2" xref="S4.SS1.p2.4.m4.2.2.2.cmml">{</mo><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">ùöåùöòùöóùöüùü∑</mi><mo id="S4.SS1.p2.4.m4.2.2.1.3" xref="S4.SS1.p2.4.m4.2.2.2.cmml">,</mo><mrow id="S4.SS1.p2.4.m4.2.2.1.1" xref="S4.SS1.p2.4.m4.2.2.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.p2.4.m4.2.2.1.1.2" xref="S4.SS1.p2.4.m4.2.2.1.1.2.cmml">‚Ä¶</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.4.m4.2.2.1.1.1" xref="S4.SS1.p2.4.m4.2.2.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p2.4.m4.2.2.1.1.3" xref="S4.SS1.p2.4.m4.2.2.1.1.3.cmml">ùöåùöòùöóùöüùüª</mi></mrow><mo stretchy="false" id="S4.SS1.p2.4.m4.2.2.1.4" xref="S4.SS1.p2.4.m4.2.2.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.2b"><set id="S4.SS1.p2.4.m4.2.2.2.cmml" xref="S4.SS1.p2.4.m4.2.2.1"><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">ùöåùöòùöóùöüùü∑</ci><apply id="S4.SS1.p2.4.m4.2.2.1.1.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1"><times id="S4.SS1.p2.4.m4.2.2.1.1.1.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1.1"></times><ci id="S4.SS1.p2.4.m4.2.2.1.1.2.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1.2">‚Ä¶</ci><ci id="S4.SS1.p2.4.m4.2.2.1.1.3.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1.3">ùöåùöòùöóùöüùüª</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.2c">\{\mathtt{conv1},...\mathtt{conv5}\}</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.4" class="ltx_p">In accordance with the details outlined in the previous Section <a href="#S3.SS3" title="3.3 Video Compression Degradation Model ‚Ä£ 3 Proposed Methods ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, a preprocessing step is performed before commencing training. Large images are initially cropped into non-overlapping high-resolution (HR) patches with a resolution of <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="360\times 360" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mn id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">360</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">360</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><times id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">360</cn><cn type="integer" id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">360</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">360\times 360</annotation></semantics></math>. Consequently, for our <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\times 4" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mi id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p3.2.m2.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.cmml">√ó</mo><mn id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><times id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1"></times><csymbol cd="latexml" id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S4.SS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\times 4</annotation></semantics></math> scaling task, the corresponding low-resolution (LR) patch size is established at <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="90\times 90" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mn id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">90</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p3.3.m3.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.cmml">√ó</mo><mn id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml">90</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><times id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1"></times><cn type="integer" id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">90</cn><cn type="integer" id="S4.SS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">90\times 90</annotation></semantics></math>, constituting the output size for all degradation models. This choice of patch size is informed by the characteristics of H.265 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, where the basic processing unit reaches dimensions of <math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mrow id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml"><mn id="S4.SS1.p3.4.m4.1.1.2" xref="S4.SS1.p3.4.m4.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p3.4.m4.1.1.1" xref="S4.SS1.p3.4.m4.1.1.1.cmml">√ó</mo><mn id="S4.SS1.p3.4.m4.1.1.3" xref="S4.SS1.p3.4.m4.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><apply id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><times id="S4.SS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1.1"></times><cn type="integer" id="S4.SS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1.2">64</cn><cn type="integer" id="S4.SS1.p3.4.m4.1.1.3.cmml" xref="S4.SS1.p3.4.m4.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">64\times 64</annotation></semantics></math>. This selection is based on the belief that a relatively larger image size is imperative for optimizing the effectiveness of the video encoder during the intra-prediction and inter-prediction stages. In total, our training dataset is expended to 12,630 HR patches as input during the training phase, all sourced from 800 DIV2K training images.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.11" class="ltx_p"><span id="S4.SS1.p4.11.1" class="ltx_text ltx_font_bold">Degradation details.</span>
The proposed degradation model runs in every epoch to prepare the LR, HR pair used for training. The degradation batch size is set to 128. We referred to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> for degradation settings in noise, blur, resize, and JPEG compression blocks.
Our video compression degradation block uses standard codecs from MPEG-2, MPEG-4, H.264, and H.265 with probability <math id="S4.SS1.p4.1.m1.4" class="ltx_Math" alttext="[0.2,0.2,0.4,0.2]" display="inline"><semantics id="S4.SS1.p4.1.m1.4a"><mrow id="S4.SS1.p4.1.m1.4.5.2" xref="S4.SS1.p4.1.m1.4.5.1.cmml"><mo stretchy="false" id="S4.SS1.p4.1.m1.4.5.2.1" xref="S4.SS1.p4.1.m1.4.5.1.cmml">[</mo><mn id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">0.2</mn><mo id="S4.SS1.p4.1.m1.4.5.2.2" xref="S4.SS1.p4.1.m1.4.5.1.cmml">,</mo><mn id="S4.SS1.p4.1.m1.2.2" xref="S4.SS1.p4.1.m1.2.2.cmml">0.2</mn><mo id="S4.SS1.p4.1.m1.4.5.2.3" xref="S4.SS1.p4.1.m1.4.5.1.cmml">,</mo><mn id="S4.SS1.p4.1.m1.3.3" xref="S4.SS1.p4.1.m1.3.3.cmml">0.4</mn><mo id="S4.SS1.p4.1.m1.4.5.2.4" xref="S4.SS1.p4.1.m1.4.5.1.cmml">,</mo><mn id="S4.SS1.p4.1.m1.4.4" xref="S4.SS1.p4.1.m1.4.4.cmml">0.2</mn><mo stretchy="false" id="S4.SS1.p4.1.m1.4.5.2.5" xref="S4.SS1.p4.1.m1.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.4b"><list id="S4.SS1.p4.1.m1.4.5.1.cmml" xref="S4.SS1.p4.1.m1.4.5.2"><cn type="float" id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">0.2</cn><cn type="float" id="S4.SS1.p4.1.m1.2.2.cmml" xref="S4.SS1.p4.1.m1.2.2">0.2</cn><cn type="float" id="S4.SS1.p4.1.m1.3.3.cmml" xref="S4.SS1.p4.1.m1.3.3">0.4</cn><cn type="float" id="S4.SS1.p4.1.m1.4.4.cmml" xref="S4.SS1.p4.1.m1.4.4">0.2</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.4c">[0.2,0.2,0.4,0.2]</annotation></semantics></math>. H.264 and H.265 control quantization through CRF with range <math id="S4.SS1.p4.2.m2.2" class="ltx_Math" alttext="[20,32]" display="inline"><semantics id="S4.SS1.p4.2.m2.2a"><mrow id="S4.SS1.p4.2.m2.2.3.2" xref="S4.SS1.p4.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p4.2.m2.2.3.2.1" xref="S4.SS1.p4.2.m2.2.3.1.cmml">[</mo><mn id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">20</mn><mo id="S4.SS1.p4.2.m2.2.3.2.2" xref="S4.SS1.p4.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS1.p4.2.m2.2.2" xref="S4.SS1.p4.2.m2.2.2.cmml">32</mn><mo stretchy="false" id="S4.SS1.p4.2.m2.2.3.2.3" xref="S4.SS1.p4.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.2b"><interval closure="closed" id="S4.SS1.p4.2.m2.2.3.1.cmml" xref="S4.SS1.p4.2.m2.2.3.2"><cn type="integer" id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">20</cn><cn type="integer" id="S4.SS1.p4.2.m2.2.2.cmml" xref="S4.SS1.p4.2.m2.2.2">32</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.2c">[20,32]</annotation></semantics></math> and <math id="S4.SS1.p4.3.m3.2" class="ltx_Math" alttext="[25,37]" display="inline"><semantics id="S4.SS1.p4.3.m3.2a"><mrow id="S4.SS1.p4.3.m3.2.3.2" xref="S4.SS1.p4.3.m3.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p4.3.m3.2.3.2.1" xref="S4.SS1.p4.3.m3.2.3.1.cmml">[</mo><mn id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml">25</mn><mo id="S4.SS1.p4.3.m3.2.3.2.2" xref="S4.SS1.p4.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS1.p4.3.m3.2.2" xref="S4.SS1.p4.3.m3.2.2.cmml">37</mn><mo stretchy="false" id="S4.SS1.p4.3.m3.2.3.2.3" xref="S4.SS1.p4.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.2b"><interval closure="closed" id="S4.SS1.p4.3.m3.2.3.1.cmml" xref="S4.SS1.p4.3.m3.2.3.2"><cn type="integer" id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">25</cn><cn type="integer" id="S4.SS1.p4.3.m3.2.2.cmml" xref="S4.SS1.p4.3.m3.2.2">37</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.2c">[25,37]</annotation></semantics></math> respectively.
MPEG-2 and MPEG-4 control quantization through bitrate restriction in the range <math id="S4.SS1.p4.4.m4.2" class="ltx_Math" alttext="[4000,6000]" display="inline"><semantics id="S4.SS1.p4.4.m4.2a"><mrow id="S4.SS1.p4.4.m4.2.3.2" xref="S4.SS1.p4.4.m4.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p4.4.m4.2.3.2.1" xref="S4.SS1.p4.4.m4.2.3.1.cmml">[</mo><mn id="S4.SS1.p4.4.m4.1.1" xref="S4.SS1.p4.4.m4.1.1.cmml">4000</mn><mo id="S4.SS1.p4.4.m4.2.3.2.2" xref="S4.SS1.p4.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS1.p4.4.m4.2.2" xref="S4.SS1.p4.4.m4.2.2.cmml">6000</mn><mo stretchy="false" id="S4.SS1.p4.4.m4.2.3.2.3" xref="S4.SS1.p4.4.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.2b"><interval closure="closed" id="S4.SS1.p4.4.m4.2.3.1.cmml" xref="S4.SS1.p4.4.m4.2.3.2"><cn type="integer" id="S4.SS1.p4.4.m4.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1">4000</cn><cn type="integer" id="S4.SS1.p4.4.m4.2.2.cmml" xref="S4.SS1.p4.4.m4.2.2">6000</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.2c">[4000,6000]</annotation></semantics></math> Kbit/s.
For all standard codecs, encoder speed setting <span id="S4.SS1.p4.11.2" class="ltx_text ltx_font_italic">preset</span> is chosen from <math id="S4.SS1.p4.5.m5.5" class="ltx_Math" alttext="\{slow,medium,fast,faster,superfast\}" display="inline"><semantics id="S4.SS1.p4.5.m5.5a"><mrow id="S4.SS1.p4.5.m5.5.5.5" xref="S4.SS1.p4.5.m5.5.5.6.cmml"><mo stretchy="false" id="S4.SS1.p4.5.m5.5.5.5.6" xref="S4.SS1.p4.5.m5.5.5.6.cmml">{</mo><mrow id="S4.SS1.p4.5.m5.1.1.1.1" xref="S4.SS1.p4.5.m5.1.1.1.1.cmml"><mi id="S4.SS1.p4.5.m5.1.1.1.1.2" xref="S4.SS1.p4.5.m5.1.1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.1.1.1.1.1" xref="S4.SS1.p4.5.m5.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.1.1.1.1.3" xref="S4.SS1.p4.5.m5.1.1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.1.1.1.1.1a" xref="S4.SS1.p4.5.m5.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.1.1.1.1.4" xref="S4.SS1.p4.5.m5.1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.1.1.1.1.1b" xref="S4.SS1.p4.5.m5.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.1.1.1.1.5" xref="S4.SS1.p4.5.m5.1.1.1.1.5.cmml">w</mi></mrow><mo id="S4.SS1.p4.5.m5.5.5.5.7" xref="S4.SS1.p4.5.m5.5.5.6.cmml">,</mo><mrow id="S4.SS1.p4.5.m5.2.2.2.2" xref="S4.SS1.p4.5.m5.2.2.2.2.cmml"><mi id="S4.SS1.p4.5.m5.2.2.2.2.2" xref="S4.SS1.p4.5.m5.2.2.2.2.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.2.2.2.2.1" xref="S4.SS1.p4.5.m5.2.2.2.2.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.2.2.2.2.3" xref="S4.SS1.p4.5.m5.2.2.2.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.2.2.2.2.1a" xref="S4.SS1.p4.5.m5.2.2.2.2.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.2.2.2.2.4" xref="S4.SS1.p4.5.m5.2.2.2.2.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.2.2.2.2.1b" xref="S4.SS1.p4.5.m5.2.2.2.2.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.2.2.2.2.5" xref="S4.SS1.p4.5.m5.2.2.2.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.2.2.2.2.1c" xref="S4.SS1.p4.5.m5.2.2.2.2.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.2.2.2.2.6" xref="S4.SS1.p4.5.m5.2.2.2.2.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.2.2.2.2.1d" xref="S4.SS1.p4.5.m5.2.2.2.2.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.2.2.2.2.7" xref="S4.SS1.p4.5.m5.2.2.2.2.7.cmml">m</mi></mrow><mo id="S4.SS1.p4.5.m5.5.5.5.8" xref="S4.SS1.p4.5.m5.5.5.6.cmml">,</mo><mrow id="S4.SS1.p4.5.m5.3.3.3.3" xref="S4.SS1.p4.5.m5.3.3.3.3.cmml"><mi id="S4.SS1.p4.5.m5.3.3.3.3.2" xref="S4.SS1.p4.5.m5.3.3.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.3.3.3.3.1" xref="S4.SS1.p4.5.m5.3.3.3.3.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.3.3.3.3.3" xref="S4.SS1.p4.5.m5.3.3.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.3.3.3.3.1a" xref="S4.SS1.p4.5.m5.3.3.3.3.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.3.3.3.3.4" xref="S4.SS1.p4.5.m5.3.3.3.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.3.3.3.3.1b" xref="S4.SS1.p4.5.m5.3.3.3.3.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.3.3.3.3.5" xref="S4.SS1.p4.5.m5.3.3.3.3.5.cmml">t</mi></mrow><mo id="S4.SS1.p4.5.m5.5.5.5.9" xref="S4.SS1.p4.5.m5.5.5.6.cmml">,</mo><mrow id="S4.SS1.p4.5.m5.4.4.4.4" xref="S4.SS1.p4.5.m5.4.4.4.4.cmml"><mi id="S4.SS1.p4.5.m5.4.4.4.4.2" xref="S4.SS1.p4.5.m5.4.4.4.4.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.4.4.4.4.1" xref="S4.SS1.p4.5.m5.4.4.4.4.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.4.4.4.4.3" xref="S4.SS1.p4.5.m5.4.4.4.4.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.4.4.4.4.1a" xref="S4.SS1.p4.5.m5.4.4.4.4.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.4.4.4.4.4" xref="S4.SS1.p4.5.m5.4.4.4.4.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.4.4.4.4.1b" xref="S4.SS1.p4.5.m5.4.4.4.4.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.4.4.4.4.5" xref="S4.SS1.p4.5.m5.4.4.4.4.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.4.4.4.4.1c" xref="S4.SS1.p4.5.m5.4.4.4.4.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.4.4.4.4.6" xref="S4.SS1.p4.5.m5.4.4.4.4.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.4.4.4.4.1d" xref="S4.SS1.p4.5.m5.4.4.4.4.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.4.4.4.4.7" xref="S4.SS1.p4.5.m5.4.4.4.4.7.cmml">r</mi></mrow><mo id="S4.SS1.p4.5.m5.5.5.5.10" xref="S4.SS1.p4.5.m5.5.5.6.cmml">,</mo><mrow id="S4.SS1.p4.5.m5.5.5.5.5" xref="S4.SS1.p4.5.m5.5.5.5.5.cmml"><mi id="S4.SS1.p4.5.m5.5.5.5.5.2" xref="S4.SS1.p4.5.m5.5.5.5.5.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.5.5.5.5.1" xref="S4.SS1.p4.5.m5.5.5.5.5.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.5.5.5.5.3" xref="S4.SS1.p4.5.m5.5.5.5.5.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.5.5.5.5.1a" xref="S4.SS1.p4.5.m5.5.5.5.5.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.5.5.5.5.4" xref="S4.SS1.p4.5.m5.5.5.5.5.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.5.5.5.5.1b" xref="S4.SS1.p4.5.m5.5.5.5.5.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.5.5.5.5.5" xref="S4.SS1.p4.5.m5.5.5.5.5.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.5.5.5.5.1c" xref="S4.SS1.p4.5.m5.5.5.5.5.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.5.5.5.5.6" xref="S4.SS1.p4.5.m5.5.5.5.5.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.5.5.5.5.1d" xref="S4.SS1.p4.5.m5.5.5.5.5.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.5.5.5.5.7" xref="S4.SS1.p4.5.m5.5.5.5.5.7.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.5.5.5.5.1e" xref="S4.SS1.p4.5.m5.5.5.5.5.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.5.5.5.5.8" xref="S4.SS1.p4.5.m5.5.5.5.5.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.5.5.5.5.1f" xref="S4.SS1.p4.5.m5.5.5.5.5.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.5.5.5.5.9" xref="S4.SS1.p4.5.m5.5.5.5.5.9.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.5.m5.5.5.5.5.1g" xref="S4.SS1.p4.5.m5.5.5.5.5.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.5.m5.5.5.5.5.10" xref="S4.SS1.p4.5.m5.5.5.5.5.10.cmml">t</mi></mrow><mo stretchy="false" id="S4.SS1.p4.5.m5.5.5.5.11" xref="S4.SS1.p4.5.m5.5.5.6.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m5.5b"><set id="S4.SS1.p4.5.m5.5.5.6.cmml" xref="S4.SS1.p4.5.m5.5.5.5"><apply id="S4.SS1.p4.5.m5.1.1.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1.1.1"><times id="S4.SS1.p4.5.m5.1.1.1.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1.1.1.1"></times><ci id="S4.SS1.p4.5.m5.1.1.1.1.2.cmml" xref="S4.SS1.p4.5.m5.1.1.1.1.2">ùë†</ci><ci id="S4.SS1.p4.5.m5.1.1.1.1.3.cmml" xref="S4.SS1.p4.5.m5.1.1.1.1.3">ùëô</ci><ci id="S4.SS1.p4.5.m5.1.1.1.1.4.cmml" xref="S4.SS1.p4.5.m5.1.1.1.1.4">ùëú</ci><ci id="S4.SS1.p4.5.m5.1.1.1.1.5.cmml" xref="S4.SS1.p4.5.m5.1.1.1.1.5">ùë§</ci></apply><apply id="S4.SS1.p4.5.m5.2.2.2.2.cmml" xref="S4.SS1.p4.5.m5.2.2.2.2"><times id="S4.SS1.p4.5.m5.2.2.2.2.1.cmml" xref="S4.SS1.p4.5.m5.2.2.2.2.1"></times><ci id="S4.SS1.p4.5.m5.2.2.2.2.2.cmml" xref="S4.SS1.p4.5.m5.2.2.2.2.2">ùëö</ci><ci id="S4.SS1.p4.5.m5.2.2.2.2.3.cmml" xref="S4.SS1.p4.5.m5.2.2.2.2.3">ùëí</ci><ci id="S4.SS1.p4.5.m5.2.2.2.2.4.cmml" xref="S4.SS1.p4.5.m5.2.2.2.2.4">ùëë</ci><ci id="S4.SS1.p4.5.m5.2.2.2.2.5.cmml" xref="S4.SS1.p4.5.m5.2.2.2.2.5">ùëñ</ci><ci id="S4.SS1.p4.5.m5.2.2.2.2.6.cmml" xref="S4.SS1.p4.5.m5.2.2.2.2.6">ùë¢</ci><ci id="S4.SS1.p4.5.m5.2.2.2.2.7.cmml" xref="S4.SS1.p4.5.m5.2.2.2.2.7">ùëö</ci></apply><apply id="S4.SS1.p4.5.m5.3.3.3.3.cmml" xref="S4.SS1.p4.5.m5.3.3.3.3"><times id="S4.SS1.p4.5.m5.3.3.3.3.1.cmml" xref="S4.SS1.p4.5.m5.3.3.3.3.1"></times><ci id="S4.SS1.p4.5.m5.3.3.3.3.2.cmml" xref="S4.SS1.p4.5.m5.3.3.3.3.2">ùëì</ci><ci id="S4.SS1.p4.5.m5.3.3.3.3.3.cmml" xref="S4.SS1.p4.5.m5.3.3.3.3.3">ùëé</ci><ci id="S4.SS1.p4.5.m5.3.3.3.3.4.cmml" xref="S4.SS1.p4.5.m5.3.3.3.3.4">ùë†</ci><ci id="S4.SS1.p4.5.m5.3.3.3.3.5.cmml" xref="S4.SS1.p4.5.m5.3.3.3.3.5">ùë°</ci></apply><apply id="S4.SS1.p4.5.m5.4.4.4.4.cmml" xref="S4.SS1.p4.5.m5.4.4.4.4"><times id="S4.SS1.p4.5.m5.4.4.4.4.1.cmml" xref="S4.SS1.p4.5.m5.4.4.4.4.1"></times><ci id="S4.SS1.p4.5.m5.4.4.4.4.2.cmml" xref="S4.SS1.p4.5.m5.4.4.4.4.2">ùëì</ci><ci id="S4.SS1.p4.5.m5.4.4.4.4.3.cmml" xref="S4.SS1.p4.5.m5.4.4.4.4.3">ùëé</ci><ci id="S4.SS1.p4.5.m5.4.4.4.4.4.cmml" xref="S4.SS1.p4.5.m5.4.4.4.4.4">ùë†</ci><ci id="S4.SS1.p4.5.m5.4.4.4.4.5.cmml" xref="S4.SS1.p4.5.m5.4.4.4.4.5">ùë°</ci><ci id="S4.SS1.p4.5.m5.4.4.4.4.6.cmml" xref="S4.SS1.p4.5.m5.4.4.4.4.6">ùëí</ci><ci id="S4.SS1.p4.5.m5.4.4.4.4.7.cmml" xref="S4.SS1.p4.5.m5.4.4.4.4.7">ùëü</ci></apply><apply id="S4.SS1.p4.5.m5.5.5.5.5.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5"><times id="S4.SS1.p4.5.m5.5.5.5.5.1.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5.1"></times><ci id="S4.SS1.p4.5.m5.5.5.5.5.2.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5.2">ùë†</ci><ci id="S4.SS1.p4.5.m5.5.5.5.5.3.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5.3">ùë¢</ci><ci id="S4.SS1.p4.5.m5.5.5.5.5.4.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5.4">ùëù</ci><ci id="S4.SS1.p4.5.m5.5.5.5.5.5.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5.5">ùëí</ci><ci id="S4.SS1.p4.5.m5.5.5.5.5.6.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5.6">ùëü</ci><ci id="S4.SS1.p4.5.m5.5.5.5.5.7.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5.7">ùëì</ci><ci id="S4.SS1.p4.5.m5.5.5.5.5.8.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5.8">ùëé</ci><ci id="S4.SS1.p4.5.m5.5.5.5.5.9.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5.9">ùë†</ci><ci id="S4.SS1.p4.5.m5.5.5.5.5.10.cmml" xref="S4.SS1.p4.5.m5.5.5.5.5.10">ùë°</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m5.5c">\{slow,medium,fast,faster,superfast\}</annotation></semantics></math> with probability <math id="S4.SS1.p4.6.m6.5" class="ltx_Math" alttext="\{0.1,0.5,0.25,0.12,0.03\}" display="inline"><semantics id="S4.SS1.p4.6.m6.5a"><mrow id="S4.SS1.p4.6.m6.5.6.2" xref="S4.SS1.p4.6.m6.5.6.1.cmml"><mo stretchy="false" id="S4.SS1.p4.6.m6.5.6.2.1" xref="S4.SS1.p4.6.m6.5.6.1.cmml">{</mo><mn id="S4.SS1.p4.6.m6.1.1" xref="S4.SS1.p4.6.m6.1.1.cmml">0.1</mn><mo id="S4.SS1.p4.6.m6.5.6.2.2" xref="S4.SS1.p4.6.m6.5.6.1.cmml">,</mo><mn id="S4.SS1.p4.6.m6.2.2" xref="S4.SS1.p4.6.m6.2.2.cmml">0.5</mn><mo id="S4.SS1.p4.6.m6.5.6.2.3" xref="S4.SS1.p4.6.m6.5.6.1.cmml">,</mo><mn id="S4.SS1.p4.6.m6.3.3" xref="S4.SS1.p4.6.m6.3.3.cmml">0.25</mn><mo id="S4.SS1.p4.6.m6.5.6.2.4" xref="S4.SS1.p4.6.m6.5.6.1.cmml">,</mo><mn id="S4.SS1.p4.6.m6.4.4" xref="S4.SS1.p4.6.m6.4.4.cmml">0.12</mn><mo id="S4.SS1.p4.6.m6.5.6.2.5" xref="S4.SS1.p4.6.m6.5.6.1.cmml">,</mo><mn id="S4.SS1.p4.6.m6.5.5" xref="S4.SS1.p4.6.m6.5.5.cmml">0.03</mn><mo stretchy="false" id="S4.SS1.p4.6.m6.5.6.2.6" xref="S4.SS1.p4.6.m6.5.6.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.6.m6.5b"><set id="S4.SS1.p4.6.m6.5.6.1.cmml" xref="S4.SS1.p4.6.m6.5.6.2"><cn type="float" id="S4.SS1.p4.6.m6.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1">0.1</cn><cn type="float" id="S4.SS1.p4.6.m6.2.2.cmml" xref="S4.SS1.p4.6.m6.2.2">0.5</cn><cn type="float" id="S4.SS1.p4.6.m6.3.3.cmml" xref="S4.SS1.p4.6.m6.3.3">0.25</cn><cn type="float" id="S4.SS1.p4.6.m6.4.4.cmml" xref="S4.SS1.p4.6.m6.4.4">0.12</cn><cn type="float" id="S4.SS1.p4.6.m6.5.5.cmml" xref="S4.SS1.p4.6.m6.5.5">0.03</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.6.m6.5c">\{0.1,0.5,0.25,0.12,0.03\}</annotation></semantics></math> respectively. For aspect ratio scaling, the width scales in <math id="S4.SS1.p4.7.m7.2" class="ltx_Math" alttext="[0.85,1.35]" display="inline"><semantics id="S4.SS1.p4.7.m7.2a"><mrow id="S4.SS1.p4.7.m7.2.3.2" xref="S4.SS1.p4.7.m7.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p4.7.m7.2.3.2.1" xref="S4.SS1.p4.7.m7.2.3.1.cmml">[</mo><mn id="S4.SS1.p4.7.m7.1.1" xref="S4.SS1.p4.7.m7.1.1.cmml">0.85</mn><mo id="S4.SS1.p4.7.m7.2.3.2.2" xref="S4.SS1.p4.7.m7.2.3.1.cmml">,</mo><mn id="S4.SS1.p4.7.m7.2.2" xref="S4.SS1.p4.7.m7.2.2.cmml">1.35</mn><mo stretchy="false" id="S4.SS1.p4.7.m7.2.3.2.3" xref="S4.SS1.p4.7.m7.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.7.m7.2b"><interval closure="closed" id="S4.SS1.p4.7.m7.2.3.1.cmml" xref="S4.SS1.p4.7.m7.2.3.2"><cn type="float" id="S4.SS1.p4.7.m7.1.1.cmml" xref="S4.SS1.p4.7.m7.1.1">0.85</cn><cn type="float" id="S4.SS1.p4.7.m7.2.2.cmml" xref="S4.SS1.p4.7.m7.2.2">1.35</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.7.m7.2c">[0.85,1.35]</annotation></semantics></math> with a probability of <math id="S4.SS1.p4.8.m8.3" class="ltx_Math" alttext="\{0.2,0.4,0.4\}" display="inline"><semantics id="S4.SS1.p4.8.m8.3a"><mrow id="S4.SS1.p4.8.m8.3.4.2" xref="S4.SS1.p4.8.m8.3.4.1.cmml"><mo stretchy="false" id="S4.SS1.p4.8.m8.3.4.2.1" xref="S4.SS1.p4.8.m8.3.4.1.cmml">{</mo><mn id="S4.SS1.p4.8.m8.1.1" xref="S4.SS1.p4.8.m8.1.1.cmml">0.2</mn><mo id="S4.SS1.p4.8.m8.3.4.2.2" xref="S4.SS1.p4.8.m8.3.4.1.cmml">,</mo><mn id="S4.SS1.p4.8.m8.2.2" xref="S4.SS1.p4.8.m8.2.2.cmml">0.4</mn><mo id="S4.SS1.p4.8.m8.3.4.2.3" xref="S4.SS1.p4.8.m8.3.4.1.cmml">,</mo><mn id="S4.SS1.p4.8.m8.3.3" xref="S4.SS1.p4.8.m8.3.3.cmml">0.4</mn><mo stretchy="false" id="S4.SS1.p4.8.m8.3.4.2.4" xref="S4.SS1.p4.8.m8.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.8.m8.3b"><set id="S4.SS1.p4.8.m8.3.4.1.cmml" xref="S4.SS1.p4.8.m8.3.4.2"><cn type="float" id="S4.SS1.p4.8.m8.1.1.cmml" xref="S4.SS1.p4.8.m8.1.1">0.2</cn><cn type="float" id="S4.SS1.p4.8.m8.2.2.cmml" xref="S4.SS1.p4.8.m8.2.2">0.4</cn><cn type="float" id="S4.SS1.p4.8.m8.3.3.cmml" xref="S4.SS1.p4.8.m8.3.3">0.4</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.8.m8.3c">\{0.2,0.4,0.4\}</annotation></semantics></math> to shrink, expand, or keep the same. FPS is chosen from <math id="S4.SS1.p4.9.m9.2" class="ltx_Math" alttext="[16,30]" display="inline"><semantics id="S4.SS1.p4.9.m9.2a"><mrow id="S4.SS1.p4.9.m9.2.3.2" xref="S4.SS1.p4.9.m9.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p4.9.m9.2.3.2.1" xref="S4.SS1.p4.9.m9.2.3.1.cmml">[</mo><mn id="S4.SS1.p4.9.m9.1.1" xref="S4.SS1.p4.9.m9.1.1.cmml">16</mn><mo id="S4.SS1.p4.9.m9.2.3.2.2" xref="S4.SS1.p4.9.m9.2.3.1.cmml">,</mo><mn id="S4.SS1.p4.9.m9.2.2" xref="S4.SS1.p4.9.m9.2.2.cmml">30</mn><mo stretchy="false" id="S4.SS1.p4.9.m9.2.3.2.3" xref="S4.SS1.p4.9.m9.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.9.m9.2b"><interval closure="closed" id="S4.SS1.p4.9.m9.2.3.1.cmml" xref="S4.SS1.p4.9.m9.2.3.2"><cn type="integer" id="S4.SS1.p4.9.m9.1.1.cmml" xref="S4.SS1.p4.9.m9.1.1">16</cn><cn type="integer" id="S4.SS1.p4.9.m9.2.2.cmml" xref="S4.SS1.p4.9.m9.2.2">30</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.9.m9.2c">[16,30]</annotation></semantics></math>. Since the parameter range that MPEG-2 can support is limited, we set it to a fixed 25 FPS without any aspect ratio scaling.
Other video compression parameters for all standards are fixed. For example, pixel encode format is uniformly set to be <span id="S4.SS1.p4.11.3" class="ltx_text ltx_font_italic">YUV420p</span>. We leave the rest compression parameters to be decided by codec, like <math id="S4.SS1.p4.10.m10.1" class="ltx_Math" alttext="Profile" display="inline"><semantics id="S4.SS1.p4.10.m10.1a"><mrow id="S4.SS1.p4.10.m10.1.1" xref="S4.SS1.p4.10.m10.1.1.cmml"><mi id="S4.SS1.p4.10.m10.1.1.2" xref="S4.SS1.p4.10.m10.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.10.m10.1.1.1" xref="S4.SS1.p4.10.m10.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.10.m10.1.1.3" xref="S4.SS1.p4.10.m10.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.10.m10.1.1.1a" xref="S4.SS1.p4.10.m10.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.10.m10.1.1.4" xref="S4.SS1.p4.10.m10.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.10.m10.1.1.1b" xref="S4.SS1.p4.10.m10.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.10.m10.1.1.5" xref="S4.SS1.p4.10.m10.1.1.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.10.m10.1.1.1c" xref="S4.SS1.p4.10.m10.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.10.m10.1.1.6" xref="S4.SS1.p4.10.m10.1.1.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.10.m10.1.1.1d" xref="S4.SS1.p4.10.m10.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.10.m10.1.1.7" xref="S4.SS1.p4.10.m10.1.1.7.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.10.m10.1.1.1e" xref="S4.SS1.p4.10.m10.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.10.m10.1.1.8" xref="S4.SS1.p4.10.m10.1.1.8.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.10.m10.1b"><apply id="S4.SS1.p4.10.m10.1.1.cmml" xref="S4.SS1.p4.10.m10.1.1"><times id="S4.SS1.p4.10.m10.1.1.1.cmml" xref="S4.SS1.p4.10.m10.1.1.1"></times><ci id="S4.SS1.p4.10.m10.1.1.2.cmml" xref="S4.SS1.p4.10.m10.1.1.2">ùëÉ</ci><ci id="S4.SS1.p4.10.m10.1.1.3.cmml" xref="S4.SS1.p4.10.m10.1.1.3">ùëü</ci><ci id="S4.SS1.p4.10.m10.1.1.4.cmml" xref="S4.SS1.p4.10.m10.1.1.4">ùëú</ci><ci id="S4.SS1.p4.10.m10.1.1.5.cmml" xref="S4.SS1.p4.10.m10.1.1.5">ùëì</ci><ci id="S4.SS1.p4.10.m10.1.1.6.cmml" xref="S4.SS1.p4.10.m10.1.1.6">ùëñ</ci><ci id="S4.SS1.p4.10.m10.1.1.7.cmml" xref="S4.SS1.p4.10.m10.1.1.7">ùëô</ci><ci id="S4.SS1.p4.10.m10.1.1.8.cmml" xref="S4.SS1.p4.10.m10.1.1.8">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.10.m10.1c">Profile</annotation></semantics></math> and <math id="S4.SS1.p4.11.m11.1" class="ltx_Math" alttext="Levels" display="inline"><semantics id="S4.SS1.p4.11.m11.1a"><mrow id="S4.SS1.p4.11.m11.1.1" xref="S4.SS1.p4.11.m11.1.1.cmml"><mi id="S4.SS1.p4.11.m11.1.1.2" xref="S4.SS1.p4.11.m11.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.11.m11.1.1.1" xref="S4.SS1.p4.11.m11.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.11.m11.1.1.3" xref="S4.SS1.p4.11.m11.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.11.m11.1.1.1a" xref="S4.SS1.p4.11.m11.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.11.m11.1.1.4" xref="S4.SS1.p4.11.m11.1.1.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.11.m11.1.1.1b" xref="S4.SS1.p4.11.m11.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.11.m11.1.1.5" xref="S4.SS1.p4.11.m11.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.11.m11.1.1.1c" xref="S4.SS1.p4.11.m11.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.11.m11.1.1.6" xref="S4.SS1.p4.11.m11.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.11.m11.1.1.1d" xref="S4.SS1.p4.11.m11.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p4.11.m11.1.1.7" xref="S4.SS1.p4.11.m11.1.1.7.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.11.m11.1b"><apply id="S4.SS1.p4.11.m11.1.1.cmml" xref="S4.SS1.p4.11.m11.1.1"><times id="S4.SS1.p4.11.m11.1.1.1.cmml" xref="S4.SS1.p4.11.m11.1.1.1"></times><ci id="S4.SS1.p4.11.m11.1.1.2.cmml" xref="S4.SS1.p4.11.m11.1.1.2">ùêø</ci><ci id="S4.SS1.p4.11.m11.1.1.3.cmml" xref="S4.SS1.p4.11.m11.1.1.3">ùëí</ci><ci id="S4.SS1.p4.11.m11.1.1.4.cmml" xref="S4.SS1.p4.11.m11.1.1.4">ùë£</ci><ci id="S4.SS1.p4.11.m11.1.1.5.cmml" xref="S4.SS1.p4.11.m11.1.1.5">ùëí</ci><ci id="S4.SS1.p4.11.m11.1.1.6.cmml" xref="S4.SS1.p4.11.m11.1.1.6">ùëô</ci><ci id="S4.SS1.p4.11.m11.1.1.7.cmml" xref="S4.SS1.p4.11.m11.1.1.7">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.11.m11.1c">Levels</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results and Analysis</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluate our proposed VCISR qualitatively and quantitatively in comparison with several state-of-the-art approaches, including RealSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, Real-ESRGAN+<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, BSRGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, SwinIR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and GRL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. For the VSR task, results from RealBasicVSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and DBVSR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> are involved additionally.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Qualitative analysis.</span> Figure <a href="#S3.F5" title="Figure 5 ‚Ä£ 3.4 Network Architecture and Training ‚Ä£ 3 Proposed Methods ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> captures a couple of illustrative samples from a collection of image and video test datasets with diverse scenes and contents. Our work is capable of restoring textual information without introducing ringing artifacts and unfaithful reconstructions, which owes to a more complex degradation model as we proposed in VCISR. For scenes captured under high motion, our method demonstrates an even better ability to resolve blurry artifacts compared to the VSR method. As for the Anime content, the HR image super-resolved by VCISR does not include block artifacts and fringes. Both the edge and color details are better restored by our scheme.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Quantitative analysis</span>. For quantitative comparisons, since not all datasets have paired HR ground truth, following<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, we adopt no-reference quality assessments, instead of PSNR or SSIM, to evaluate the resulting HR images. Table <a href="#S3.T1" title="Table 1 ‚Ä£ 3.4 Network Architecture and Training ‚Ä£ 3 Proposed Methods ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S3.T2" title="Table 2 ‚Ä£ 3.4 Network Architecture and Training ‚Ä£ 3 Proposed Methods ‚Ä£ VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents our quantitative measurements based on the no-reference metrics that are widely used in previous real-world image and video SR works<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>: NIQE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, BRISQUE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, and NRQM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. In addition, we employ a SOTA learning-based CLIP-IQA metric<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, which gives a score closer to human perceptual assessment.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">The proposed method exhibits similar or better quantitative results on the tested data for blind SISR compared to recently published works with much fewer network parameters. For the VSR task, we showcase that involving a video-based degradation module can produce effective restorations even without leveraging the temporal correlations by referring to previous frames, which typically require more memory and compute complexity during inference.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>VC-RealLQ Dataset</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Most of the existing image datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> only consider photography as the input source. This makes them dismiss compression artifacts in the real world. For the convenience of future researchers, we propose an image-based dataset that contains 35 images each with versatile compression artifacts (<em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS3.p1.1.2" class="ltx_text"></span> ringing artifacts, mosquito noise, blockiness, and color bleeding). They come from video sequence screenshots after H.264 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, H.265 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, or WebP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> compression on video datasets<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, and unknown compression degradation from online resources. We select these images with different resolutions, contents, and different levels of degradation.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work presents a novel and versatile video-codec-based degradation module to enrich the existing image SR degradation pipelines and improve its proximity to practical exercises. The method we propose for LR image synthesis can simulate distortions from a variety of video codecs without the presence of video data, which leads to <span id="S5.p1.1.1" class="ltx_text ltx_font_bold">1)</span> a generic element that could be added to existing SR degradation pipelines; <span id="S5.p1.1.2" class="ltx_text ltx_font_bold">2)</span> better-approximated artifacts to cover more complicated quality loss in real-world images and videos. We demonstrate the viability of this approach by evaluating a unified network trained with the proposed degradation flow on real-world ISR and VSR datasets. In both tasks, our work exhibits similar or better performance compared to other state-of-the-art methods with fewer network parameters and compute complexity.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We thank Xiaotong Chen, Huijie Zhang, and Jimmy Tobin for feedback on drafts and early discussions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Eirikur Agustsson and Radu Timofte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Ntire 2017 challenge on single image super-resolution: Dataset and study.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 126‚Äì135, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Nasir Ahmed, T_ Natarajan, and Kamisetty¬†R Rao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Discrete cosine transform.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on Computers</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 100(1):90‚Äì93, 1974.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Olivier Avaro, Alexandros Eleftheriadis, Carsten Herpel, Ganesh Rajan, and Liam Ward.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Mpeg-4 systems: overview.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Signal Processing: Image Communication</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 15(4-5):281‚Äì298, 2000.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Yochai Blau and Tomer Michaeli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">The perception-distortion tradeoff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 6228‚Äì6237, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Jose Caballero, Christian Ledig, Andrew Aitken, Alejandro Acosta, Johannes Totz, Zehan Wang, and Wenzhe Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Real-time video super-resolution with spatio-temporal networks and motion compensation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 4778‚Äì4787, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Toward real-world single image super-resolution: A new benchmark and a new model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 3086‚Äì3095, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Kelvin¬†CK Chan, Shangchen Zhou, Xiangyu Xu, and Chen¬†Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Investigating tradeoffs in real-world video super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 5962‚Äì5971, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Chaofeng Chen and Jiadi Mo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">IQA-PyTorch: Pytorch toolbox for image quality assessment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">[Online]. Available: </span><a target="_blank" href="https://github.com/chaofengc/IQA-PyTorch" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/chaofengc/IQA-PyTorch</a><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Yu Chen, Mingyu Yang, and Hun-Seok Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Search for efficient deep visual-inertial odometry through neural architecture search.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 1‚Äì5. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Chao Dong, Chen¬†Change Loy, Kaiming He, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Image super-resolution using deep convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 38(2):295‚Äì307, 2015.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.11929</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz Nadler, and Anat Levin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Accurate blur models vs. image priors in single image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer Vision</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 2832‚Äì2839, 2013.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Jiahong Fu, Hong Wang, Qi Xie, Qian Zhao, Deyu Meng, and Zongben Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Kxnet: A model-driven deep neural network for blind super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XIX</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 235‚Äì253. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Communications of the ACM</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 63(11):139‚Äì144, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, pages 770‚Äì778, 2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc Van¬†Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Dslr-quality photos on mobile devices with deep convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 3277‚Äì3285, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Real-world super-resolution via kernel estimation and noise injection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 466‚Äì467, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Real-world super-resolution via kernel estimation and noise injection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Alexandre Alahi, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Perceptual losses for real-time style transfer and super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision‚ÄìECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 694‚Äì711. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Mehrdad Khani, Vibhaalakshmi Sivaraman, and Mohammad Alizadeh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Efficient video compression via content-adaptive super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 4521‚Äì4530, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Diederik¬†P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Christian Ledig, Lucas Theis, Ferenc Husz√°r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Photo-realistic single image super-resolution using a generative adversarial network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 4681‚Äì4690, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Gaotang Li, Marlena Duda, Xiang Zhang, Danai Koutra, and Yujun Yan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Interpretable sparsification of brain graphs: Better practices and effective designs for graph neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.14375</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Yawei Li, Yuchen Fan, Xiaoyu Xiang, Denis Demandolx, Rakesh Ranjan, Radu Timofte, and Luc Van¬†Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Efficient and explicit modelling of image hierarchies for image restoration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 18278‚Äì18289, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Yinxiao Li, Pengchong Jin, Feng Yang, Ce Liu, Ming-Hsuan Yang, and Peyman Milanfar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Comisr: Compression-informed video super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 2543‚Äì2552, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van¬†Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Vrt: A video restoration transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.12288</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van¬†Gool, and Radu Timofte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Swinir: Image restoration using swin transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 1833‚Äì1844, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Jingyun Liang, Guolei Sun, Kai Zhang, Luc Van¬†Gool, and Radu Timofte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Mutual affine network for spatially variant kernel estimation in blind image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 4096‚Äì4105, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu¬†Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Enhanced deep residual networks for single image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 136‚Äì144, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Blind image super-resolution: A survey and beyond.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Bowen Liu, Ang Cao, and Hun-Seok Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Unified signal compression using generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 3177‚Äì3181. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Bowen Liu, Yu Chen, Shiyu Liu, and Hun-Seok Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Deep learning in latent space for video prediction and compression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 701‚Äì710, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Bowen Liu, Yu Chen, Rakesh¬†Chowdary Machineni, Shiyu Liu, and Hun-Seok Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Mmvc: Learned multi-mode video compression with block-based prediction mode selection and density-adaptive entropy coding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 18487‚Äì18496, 2023.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Ce Liu and Deqing Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">On bayesian adaptive video super resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 36(2):346‚Äì360, 2013.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Yu-Qi Liu, Xin Du, Hui-Liang Shen, and Shu-Jie Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Estimating generalized gaussian blur kernels for out-of-focus image deblurring.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on circuits and systems for video technology</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 31(3):829‚Äì843, 2020.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Ziwei Luo, Haibin Huang, Lei Yu, Youwei Li, Haoqiang Fan, and Shuaicheng Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Deep constrained least squares for blind image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 17642‚Äì17652, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, and Tieniu Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Learning the degradation distribution for blind image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 6063‚Äì6072, 2022.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Chenyang Ma, Xinchi Qiu, Daniel¬†J. Beutel, and Nicholas¬†D. Lane.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Gradient-less federated gradient boosting tree with learnable learning rates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 3rd Workshop on Machine Learning and Systems, EuroMLSys 2023, Rome, Italy, 8 May 2023</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 56‚Äì63. ACM, 2023.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and Ming-Hsuan Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Learning a no-reference quality metric for single-image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 158:1‚Äì16, 2017.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Alexandre Mercat, Marko Viitanen, and Jarno Vanne.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Uvg dataset: 50/120fps 4k sequences for video codec analysis and development.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 11th ACM Multimedia Systems Conference</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 297‚Äì302, 2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Joan¬†L Mitchell, William¬†B Pennebaker, Chad¬†E Fogg, Didier¬†J LeGall, Joan¬†L Mitchell, William¬†B Pennebaker, Chad¬†E Fogg, and Didier¬†J LeGall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Mpeg-2 overview.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">MPEG Video Compression Standard</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, pages 171‚Äì186, 1996.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Anish Mittal, Anush¬†K Moorthy, and Alan¬†C Bovik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Blind/referenceless image spatial quality evaluator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2011 conference record of the forty fifth asilomar conference on signals, systems and computers (ASILOMAR)</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 723‚Äì727. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Anish Mittal, Rajiv Soundararajan, and Alan¬†C Bovik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Making a ‚Äúcompletely blind‚Äù image quality analyzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Signal processing letters</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 20(3):209‚Äì212, 2012.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Spectral normalization for generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.05957</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung¬†Mu Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR Workshops</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Jinshan Pan, Haoran Bai, Jiangxin Dong, Jiawei Zhang, and Jinhui Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Deep blind video super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, pages 4811‚Äì4820, 2021.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Heiko Schwarz, Detlev Marpe, and Thomas Wiegand.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Overview of the scalable video coding extension of the h. 264/avc standard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on circuits and systems for video technology</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 17(9):1103‚Äì1120, 2007.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Wenzhe Shi, Jose Caballero, Ferenc Husz√°r, Johannes Totz, Andrew¬†P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 1874‚Äì1883, 2016.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Zhanjun Si and Ke Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Research on the webp image format.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advanced graphic communications, packaging technology and materials</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 271‚Äì277. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Gary¬†J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Overview of the high efficiency video coding (hevc) standard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on circuits and systems for video technology</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 22(12):1649‚Äì1668, 2012.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Gregory¬†K Wallace.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">The jpeg still picture compression standard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on consumer electronics</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 38(1):xviii‚Äìxxxiv, 1992.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Jianyi Wang, Kelvin¬†CK Chan, and Chen¬†Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Exploring clip for assessing the look and feel of images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, volume¬†37, pages 2555‚Äì2563, 2023.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin¬†CK Chan, and Chen¬†Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Exploiting diffusion prior for real-world image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.07015</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Real-esrgan: Training real-world blind super-resolution with pure synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, pages 1905‚Äì1914, 2021.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change¬†Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Esrgan: Enhanced super-resolution generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision (ECCV) workshops</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, pages 0‚Äì0, 2018.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng Zuo, and Liang Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Component divide-and-conquer for real-world image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part VIII 16</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages 101‚Äì117. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Shaokai Wu and Fengyu Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Boosting detection in crowd analysis via underutilized output features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, pages 15609‚Äì15618, June 2023.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Xintian Wu, Hanbin Zhao, Liangli Zheng, Shouhong Ding, and Xi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Adma-gan: Attribute-driven memory augmented gans for text-to-image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 30th ACM International Conference on Multimedia</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, pages 1593‚Äì1602, 2022.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Yanze Wu, Xintao Wang, Gen Li, and Ying Shan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Animesr: Learning real-world super-resolution models for animation videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.07038</span><span id="bib.bib59.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Xiaoyu Xiang, Qian Lin, and Jan¬†P Allebach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Boosting high-level vision with joint compression artifacts reduction and super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 25th International Conference on Pattern Recognition (ICPR)</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, pages 2390‚Äì2397. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Fengyu Yang and Chenyan Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Sparse and complete latent organization for geospatial semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib61.4.2" class="ltx_text" style="font-size:90%;">, pages 1799‚Äì1808, 2022.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu, Wenzhen Yuan, and Andrew Owens.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Touch and go: Learning from human-collected vision and touch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Information Processing Systems (NeurIPS) - Datasets and Benchmarks Track</span><span id="bib.bib62.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Fengyu Yang, Jiacheng Zhang, and Andrew Owens.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Generating visual scenes from touch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision (ICCV)</span><span id="bib.bib63.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Jiayu Yang, Chunhui Yang, Fei Xiong, Feng Wang, and Ronggang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Learned low bitrate video compression with space-time super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, pages 1786‚Äì1790, 2022.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Ren Yang, Radu Timofte, Meisong Zheng, Qunliang Xing, Minglang Qiao, Mai Xu, Lai Jiang, Huaida Liu, Ying Chen, Youcheng Ben, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Ntire 2022 challenge on super-resolution and quality enhancement of compressed video: Dataset, methods and results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, pages 1221‚Äì1238, 2022.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Zongsheng Yue, Qian Zhao, Jianwen Xie, Lei Zhang, Deyu Meng, and Kwan-Yee¬†K Wong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Blind image super-resolution with elaborate degradation modeling on noise and kernel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, pages 2128‚Äì2138, 2022.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Liyue Shen, and Qing Qu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">The emergence of reproducibility and consistency in diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.05264</span><span id="bib.bib67.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Kai Zhang, Jingyun Liang, Luc Van¬†Gool, and Radu Timofte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Designing a practical degradation model for deep blind image super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib68.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib68.5.3" class="ltx_text" style="font-size:90%;">, pages 4791‚Äì4800, 2021.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Kaihao Zhang, Wenqi Ren, Wenhan Luo, Wei-Sheng Lai, Bj√∂rn Stenger, Ming-Hsuan Yang, and Hongdong Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Deep image deblurring: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib69.4.2" class="ltx_text" style="font-size:90%;">, 130(9):2103‚Äì2130, 2022.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Wenlong Zhang, Guangyuan Shi, Yihao Liu, Chao Dong, and Xiao-Ming Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">A closer look at blind super-resolution: Degradation models, baselines, and performance upper bounds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib70.5.3" class="ltx_text" style="font-size:90%;">, pages 527‚Äì536, 2022.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Hanbin Zhao, Fengyu Yang, Xinghe Fu, and Xi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">Rbc: Rectifying the biased context in continual semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib71.4.2" class="ltx_text" style="font-size:90%;">, abs/2203.08404, 2022.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Kai Zhao, Kun Yuan, Ming Sun, Mading Li, and Xing Wen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Quality-aware pre-trained models for blind image quality assessment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib72.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib72.5.3" class="ltx_text" style="font-size:90%;">, pages 22302‚Äì22313, 2023.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.00995" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.00996" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.00996">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.00996" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.00998" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 20:43:37 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
