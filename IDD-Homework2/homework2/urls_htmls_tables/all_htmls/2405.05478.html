<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Using Machine Translation to Augment Multilingual Classification</title>
<!--Generated on Tue May 14 18:06:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on May 6, 2024.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.05478v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S1" title="In Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S2" title="In Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S3" title="In Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Image captioning and Image-Text Contrastive Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S4" title="In Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S4.SS1" title="In 4 Experiments â€£ Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S4.SS2" title="In 4 Experiments â€£ Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experiment design</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S5" title="In Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S6" title="In Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion and future directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S7" title="In Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Acknowledgements</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Using Machine Translation to Augment Multilingual Classification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adam King
<br class="ltx_break"/>GumGum
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">aking@gumgum.com</span>
</span></span>
</div>
<div class="ltx_dates">(May 6, 2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">An all-too-present bottleneck for text classification model development is the need to annotate training data and this need is multiplied for multilingual classifiers. Fortunately, contemporary machine translation models are both easily accessible and have dependable translation quality, making it possible to translate labeled training data from one language into another. Here, we explore the effects of using machine translation to fine-tune a multilingual model for a classification task across multiple languages. We also investigate the benefits of using a novel technique, originally proposed in the field of image captioning, to account for potential negative effects of tuning models on translated data. We show that translated data are of sufficient quality to tune multilingual classifiers and that this novel loss technique is able to offer some improvement over models tuned without it.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">One of the most common uses of machine learning for natural language processing (NLP) is the classification of text into one of multiple mutually-inclusive or mutually-exclusive labels. Recently, generative LLMs, such as PaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx5" title="">Chung et al., 2022</a>]</cite> and ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx17" title="">Ouyang et al., 2022</a>]</cite> have shown exciting and impressive capabilities to do zero- or few-shot prompting, classify text given only a few examples for the task across a variety of languages. Nevertheless, it is still the case that the highest performing and most efficient means to classify text is the use of a bespoke classifier trained with hundreds or thousands labeled examples <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx19" title="">Pires et al., 2019</a>]</cite>, particularly when the task requires a level of human-like subjectivity or general reasoning ability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx13" title="">KocoÅ„ et al., 2023</a>, see discussion]</cite>. To this end, finding or creating a corpus of labeled examples is a necessary step in the creation of any classifier.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">For high-resource languages like English, which have many existing labeled corpora available and large populations of annotators on crowd-sourced workers such as Amazon Mechanical Turk, the challenge of creating or finding training and evaluation data can be costly, but not prohibitively so. Yet, for lower-resourced languages which lack existing annotated corpora and have smaller or even non-existent populations on these large annotation platforms, acquiring the required training data can prove to be much more difficult. Moreover, if the model is intended to be able to perform the same classification across multiple languages, the time and effort required to annotate training data becomes multiplicative. Fortunately, classification is not alone in the applications of machine learning in NLP. Machine translation (MT) has seen major improvements in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx22" title="">Stahlberg, 2020</a>]</cite>, accelerated by the adoption of the transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx25" title="">Vaswani et al., 2017</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To date, several options for high quality machine translation currently exist, between API services and open-source models. MT API services, such as Google translate, have become nearly ubiquitous, provide high quality translations, while still being relatively inexpensive. In fact, in one experiment, translating data using Google translate into English and using existing English-trained classifier models outperformed certain models trained on the original language directly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx1" title="">Araujo et al., 2016</a>]</cite>. In addition to MT API services, several open-source translation models are easily available, such as the multilingual M2M100 model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx8" title="">Fan et al., 2020</a>]</cite>, NLLB200 model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx23" title="">Team et al., 2022</a>]</cite> or the over 1400 models trained by the University of Helsinki <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx24" title="">Tiedemann and Thottingal, 2020</a>]</cite>, with many of these models have performance that approaches or exceeds that of MT APIs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx22" title="">Stahlberg, 2020</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">With this in mind, it may be the case that translating an existing, labeled dataset with one of the aforementioned MT options is a feasible alternative to creating a novel dataset directly in that language. This has several benefits. Firstly, it avoids the problem of existing corpora or annotation options not existing for the language in question. Secondly, it minimizes the data needed for multilingual models and allows annotations for one language to serve another. Here, we ask if it is possible to use MT to train a multilingual model, given only original, annotated data for a single language.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Of course, the potential benefits of using MT to train a multilingual model are still affected by the old machine learning adage: garbage in, garbage out. Even the best translations, either human or machine, will lose some of the information of the original language, which will inevitably lead to dropped performance for a model trained on the translated examples. Fortunately, the problem of training models using semantically similar but imperfect pairs of data is not unique to the task at hand and there is a growing body of research which may provide some benefit. In particular, image captioning is a task to generate the ideal natural language text caption for an image and these captioning models must learn to represent semantically related data from very different modalities similarly, i.e., text and images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx14" title="">Li et al., 2021</a>]</cite>. In this way, image captioning is somewhat analogous to the task of training on translated data, where we want to have semantically identical text from different languages predicted to have the same labels. As a result, we ask in addition whether some of the model training techniques used in image captioning models can lead to improved performance for multilingual models trained using MT data.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This work is by no means the first to suggest the usage of machine translation to create or augment datasets for lower resourced languages. Wei and Pal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx26" title="">Wei and Pal, 2010</a>]</cite> and Pan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx18" title="">Pan et al., 2011</a>]</cite> augmented Chinese language corpora with annotated data translated from English to improve the performance of a Chinese-language sentiment analysis model. On the other hand, Barriere and Balahur <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx3" title="">Barriere and Balahur, 2020</a>]</cite> and Ghafoor et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx9" title="">Ghafoor et al., 2021</a>]</cite> used existing API translation services to translate annotated data from English into lower-resourced languages and trained classifiers solely on these translated data, finding that classifiers trained on translated data were fairly accurate but did see drops in performance, likely due to the effects of imperfect translations of the training data.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">It should be noted that training a model from scratch is not the only means to create an accurate classifier, particularly for lower-resourced languages. Large multilingual transformer models such as <span class="ltx_text ltx_font_smallcaps" id="S2.p2.1.1">m-BERT</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx7" title="">Devlin et al., 2018</a>]</cite>, <span class="ltx_text ltx_font_smallcaps" id="S2.p2.1.2">xlm-RoBERTa</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx6" title="">Conneau et al., 2019</a>]</cite> or <span class="ltx_text ltx_font_smallcaps" id="S2.p2.1.3">gpt-3</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx4" title="">Brown et al., 2020</a>]</cite> have been shown to have the ability to generalize from one language to the other, i.e., train in one language and improve test performance in another language, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx19" title="">Pires et al., 2019</a>]</cite>, but benefits of this vary on the languages in question, with languages that share closer genealogical origin or structural similarities benefiting more from inter-language transfer. Regardless, training a model with examples of a particular language dependably yields the best classifier for new data in that language.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Nevertheless, to date there has been no investigation of how fine-tuning large multilingual transformer models on translated data affects final performance compared to simple interlanguage transfer. Moreover, previous work to train models using translated data employed a naive approach, treating translated data as if it were no different than original, untranslated data which annotated itself. In this work, we investigate both how multilingual transformer models trained on translated data perform compared to interlanguage transfer and explore a means to mitigate imperfect translation quality when creating these training datasets.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Image captioning and Image-Text Contrastive Loss</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Image-text Contrastive (ITC) loss is a technique used when training multimodal models to caption images with natural language descriptions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx14" title="">Li et al., 2021</a>]</cite>. For example, <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">BLIP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx15" title="">Li et al., 2022</a>]</cite> is a image-captioning model that was trained with a mix of human- and artificially-annotated images where ITC loss was integral to the models ability to learn from noisy, artificially-annotated data. ITC loss, then, has been shown to mitigate negative effects of both noise and different modalities for multimodal models.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">At an intuitional level, these captioning models decompose text and images into a shared embedding space and ITC loss seeks to penalize cases where related image-text pairs are dissimilar in this shared embedding space. In other words, ITC looks seeks to bring semantically related items from disparate modalities closer in a shared embedded space and has empirically improved image-captioning models, with little impact on training time or resources.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Training multilingual classification models with translated data bears a similarity to captioning, though rather than have semantically related examples from different modalities, there are semantically parallel data in different languages. That being the case, we will be a slightly modified form of ITC loss, namely original-translated contrastive (OTC) loss, to enforce similarity within a batch between data from the original language and its translated counterpart. Like ITC loss, OTC loss penalizes a transformer model for dissimilar embedding representations for translated pairs. One way to think of it is that this loss encourages the model to embed sentences with the same meaning identically, regardless of language.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.2">In detail, we implement OTC loss as follows. We begin by deriving a probability of each original/translated pairing in a training minibatch, <math alttext="p^{o2t}" class="ltx_Math" display="inline" id="S3.p4.1.m1.1"><semantics id="S3.p4.1.m1.1a"><msup id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml"><mi id="S3.p4.1.m1.1.1.2" xref="S3.p4.1.m1.1.1.2.cmml">p</mi><mrow id="S3.p4.1.m1.1.1.3" xref="S3.p4.1.m1.1.1.3.cmml"><mi id="S3.p4.1.m1.1.1.3.2" xref="S3.p4.1.m1.1.1.3.2.cmml">o</mi><mo id="S3.p4.1.m1.1.1.3.1" xref="S3.p4.1.m1.1.1.3.1.cmml">â¢</mo><mn id="S3.p4.1.m1.1.1.3.3" xref="S3.p4.1.m1.1.1.3.3.cmml">2</mn><mo id="S3.p4.1.m1.1.1.3.1a" xref="S3.p4.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.p4.1.m1.1.1.3.4" xref="S3.p4.1.m1.1.1.3.4.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p4.1.m1.1.1.1.cmml" xref="S3.p4.1.m1.1.1">superscript</csymbol><ci id="S3.p4.1.m1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.2">ğ‘</ci><apply id="S3.p4.1.m1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.3"><times id="S3.p4.1.m1.1.1.3.1.cmml" xref="S3.p4.1.m1.1.1.3.1"></times><ci id="S3.p4.1.m1.1.1.3.2.cmml" xref="S3.p4.1.m1.1.1.3.2">ğ‘œ</ci><cn id="S3.p4.1.m1.1.1.3.3.cmml" type="integer" xref="S3.p4.1.m1.1.1.3.3">2</cn><ci id="S3.p4.1.m1.1.1.3.4.cmml" xref="S3.p4.1.m1.1.1.3.4">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">p^{o2t}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.1.m1.1d">italic_p start_POSTSUPERSCRIPT italic_o 2 italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="p^{t2o}" class="ltx_Math" display="inline" id="S3.p4.2.m2.1"><semantics id="S3.p4.2.m2.1a"><msup id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mi id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">p</mi><mrow id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml"><mi id="S3.p4.2.m2.1.1.3.2" xref="S3.p4.2.m2.1.1.3.2.cmml">t</mi><mo id="S3.p4.2.m2.1.1.3.1" xref="S3.p4.2.m2.1.1.3.1.cmml">â¢</mo><mn id="S3.p4.2.m2.1.1.3.3" xref="S3.p4.2.m2.1.1.3.3.cmml">2</mn><mo id="S3.p4.2.m2.1.1.3.1a" xref="S3.p4.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.p4.2.m2.1.1.3.4" xref="S3.p4.2.m2.1.1.3.4.cmml">o</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.1.cmml" xref="S3.p4.2.m2.1.1">superscript</csymbol><ci id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2">ğ‘</ci><apply id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3"><times id="S3.p4.2.m2.1.1.3.1.cmml" xref="S3.p4.2.m2.1.1.3.1"></times><ci id="S3.p4.2.m2.1.1.3.2.cmml" xref="S3.p4.2.m2.1.1.3.2">ğ‘¡</ci><cn id="S3.p4.2.m2.1.1.3.3.cmml" type="integer" xref="S3.p4.2.m2.1.1.3.3">2</cn><ci id="S3.p4.2.m2.1.1.3.4.cmml" xref="S3.p4.2.m2.1.1.3.4">ğ‘œ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">p^{t2o}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.2.m2.1d">italic_p start_POSTSUPERSCRIPT italic_t 2 italic_o end_POSTSUPERSCRIPT</annotation></semantics></math>, that is, which original examples pairs with which translated example and vice versa.</p>
</div>
<div class="ltx_para" id="S3.p5">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p^{o2t}_{m}=\frac{exp(s(O,T_{m})/\tau)}{\Sigma_{m}^{M}exp(s(O,T_{m})/\tau)}" class="ltx_Math" display="block" id="S3.E1.m1.4"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.5" xref="S3.E1.m1.4.5.cmml"><msubsup id="S3.E1.m1.4.5.2" xref="S3.E1.m1.4.5.2.cmml"><mi id="S3.E1.m1.4.5.2.2.2" xref="S3.E1.m1.4.5.2.2.2.cmml">p</mi><mi id="S3.E1.m1.4.5.2.3" xref="S3.E1.m1.4.5.2.3.cmml">m</mi><mrow id="S3.E1.m1.4.5.2.2.3" xref="S3.E1.m1.4.5.2.2.3.cmml"><mi id="S3.E1.m1.4.5.2.2.3.2" xref="S3.E1.m1.4.5.2.2.3.2.cmml">o</mi><mo id="S3.E1.m1.4.5.2.2.3.1" xref="S3.E1.m1.4.5.2.2.3.1.cmml">â¢</mo><mn id="S3.E1.m1.4.5.2.2.3.3" xref="S3.E1.m1.4.5.2.2.3.3.cmml">2</mn><mo id="S3.E1.m1.4.5.2.2.3.1a" xref="S3.E1.m1.4.5.2.2.3.1.cmml">â¢</mo><mi id="S3.E1.m1.4.5.2.2.3.4" xref="S3.E1.m1.4.5.2.2.3.4.cmml">t</mi></mrow></msubsup><mo id="S3.E1.m1.4.5.1" xref="S3.E1.m1.4.5.1.cmml">=</mo><mfrac id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml">e</mi><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">â¢</mo><mi id="S3.E1.m1.2.2.2.5" xref="S3.E1.m1.2.2.2.5.cmml">x</mi><mo id="S3.E1.m1.2.2.2.3a" xref="S3.E1.m1.2.2.2.3.cmml">â¢</mo><mi id="S3.E1.m1.2.2.2.6" xref="S3.E1.m1.2.2.2.6.cmml">p</mi><mo id="S3.E1.m1.2.2.2.3b" xref="S3.E1.m1.2.2.2.3.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><mo id="S3.E1.m1.2.2.2.2.1.2" stretchy="false" xref="S3.E1.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.3.cmml">s</mi><mo id="S3.E1.m1.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.2.cmml"><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.2.2.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">O</mi><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.1.2.cmml">,</mo><msub id="S3.E1.m1.2.2.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.3.cmml">m</mi></msub><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.1.4" stretchy="false" xref="S3.E1.m1.2.2.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2.cmml">/</mo><mi id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml">Ï„</mi></mrow><mo id="S3.E1.m1.2.2.2.2.1.3" stretchy="false" xref="S3.E1.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.4.4.4" xref="S3.E1.m1.4.4.4.cmml"><msubsup id="S3.E1.m1.4.4.4.4" xref="S3.E1.m1.4.4.4.4.cmml"><mi id="S3.E1.m1.4.4.4.4.2.2" mathvariant="normal" xref="S3.E1.m1.4.4.4.4.2.2.cmml">Î£</mi><mi id="S3.E1.m1.4.4.4.4.2.3" xref="S3.E1.m1.4.4.4.4.2.3.cmml">m</mi><mi id="S3.E1.m1.4.4.4.4.3" xref="S3.E1.m1.4.4.4.4.3.cmml">M</mi></msubsup><mo id="S3.E1.m1.4.4.4.3" xref="S3.E1.m1.4.4.4.3.cmml">â¢</mo><mi id="S3.E1.m1.4.4.4.5" xref="S3.E1.m1.4.4.4.5.cmml">e</mi><mo id="S3.E1.m1.4.4.4.3a" xref="S3.E1.m1.4.4.4.3.cmml">â¢</mo><mi id="S3.E1.m1.4.4.4.6" xref="S3.E1.m1.4.4.4.6.cmml">x</mi><mo id="S3.E1.m1.4.4.4.3b" xref="S3.E1.m1.4.4.4.3.cmml">â¢</mo><mi id="S3.E1.m1.4.4.4.7" xref="S3.E1.m1.4.4.4.7.cmml">p</mi><mo id="S3.E1.m1.4.4.4.3c" xref="S3.E1.m1.4.4.4.3.cmml">â¢</mo><mrow id="S3.E1.m1.4.4.4.2.1" xref="S3.E1.m1.4.4.4.2.1.1.cmml"><mo id="S3.E1.m1.4.4.4.2.1.2" stretchy="false" xref="S3.E1.m1.4.4.4.2.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.4.2.1.1" xref="S3.E1.m1.4.4.4.2.1.1.cmml"><mrow id="S3.E1.m1.4.4.4.2.1.1.1" xref="S3.E1.m1.4.4.4.2.1.1.1.cmml"><mi id="S3.E1.m1.4.4.4.2.1.1.1.3" xref="S3.E1.m1.4.4.4.2.1.1.1.3.cmml">s</mi><mo id="S3.E1.m1.4.4.4.2.1.1.1.2" xref="S3.E1.m1.4.4.4.2.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.4.4.4.2.1.1.1.1.1" xref="S3.E1.m1.4.4.4.2.1.1.1.1.2.cmml"><mo id="S3.E1.m1.4.4.4.2.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.4.4.4.2.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.3.1.cmml">O</mi><mo id="S3.E1.m1.4.4.4.2.1.1.1.1.1.3" xref="S3.E1.m1.4.4.4.2.1.1.1.1.2.cmml">,</mo><msub id="S3.E1.m1.4.4.4.2.1.1.1.1.1.1" xref="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.3" xref="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.3.cmml">m</mi></msub><mo id="S3.E1.m1.4.4.4.2.1.1.1.1.1.4" stretchy="false" xref="S3.E1.m1.4.4.4.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.4.2.1.1.2" xref="S3.E1.m1.4.4.4.2.1.1.2.cmml">/</mo><mi id="S3.E1.m1.4.4.4.2.1.1.3" xref="S3.E1.m1.4.4.4.2.1.1.3.cmml">Ï„</mi></mrow><mo id="S3.E1.m1.4.4.4.2.1.3" stretchy="false" xref="S3.E1.m1.4.4.4.2.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.5.cmml" xref="S3.E1.m1.4.5"><eq id="S3.E1.m1.4.5.1.cmml" xref="S3.E1.m1.4.5.1"></eq><apply id="S3.E1.m1.4.5.2.cmml" xref="S3.E1.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.2.1.cmml" xref="S3.E1.m1.4.5.2">subscript</csymbol><apply id="S3.E1.m1.4.5.2.2.cmml" xref="S3.E1.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.2.2.1.cmml" xref="S3.E1.m1.4.5.2">superscript</csymbol><ci id="S3.E1.m1.4.5.2.2.2.cmml" xref="S3.E1.m1.4.5.2.2.2">ğ‘</ci><apply id="S3.E1.m1.4.5.2.2.3.cmml" xref="S3.E1.m1.4.5.2.2.3"><times id="S3.E1.m1.4.5.2.2.3.1.cmml" xref="S3.E1.m1.4.5.2.2.3.1"></times><ci id="S3.E1.m1.4.5.2.2.3.2.cmml" xref="S3.E1.m1.4.5.2.2.3.2">ğ‘œ</ci><cn id="S3.E1.m1.4.5.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.4.5.2.2.3.3">2</cn><ci id="S3.E1.m1.4.5.2.2.3.4.cmml" xref="S3.E1.m1.4.5.2.2.3.4">ğ‘¡</ci></apply></apply><ci id="S3.E1.m1.4.5.2.3.cmml" xref="S3.E1.m1.4.5.2.3">ğ‘š</ci></apply><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><divide id="S3.E1.m1.4.4.5.cmml" xref="S3.E1.m1.4.4"></divide><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><ci id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4">ğ‘’</ci><ci id="S3.E1.m1.2.2.2.5.cmml" xref="S3.E1.m1.2.2.2.5">ğ‘¥</ci><ci id="S3.E1.m1.2.2.2.6.cmml" xref="S3.E1.m1.2.2.2.6">ğ‘</ci><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1"><divide id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2"></divide><apply id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1"><times id="S3.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.2"></times><ci id="S3.E1.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3">ğ‘ </ci><interval closure="open" id="S3.E1.m1.2.2.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ğ‘‚</ci><apply id="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.2">ğ‘‡</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.3">ğ‘š</ci></apply></interval></apply><ci id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">ğœ</ci></apply></apply><apply id="S3.E1.m1.4.4.4.cmml" xref="S3.E1.m1.4.4.4"><times id="S3.E1.m1.4.4.4.3.cmml" xref="S3.E1.m1.4.4.4.3"></times><apply id="S3.E1.m1.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.4.1.cmml" xref="S3.E1.m1.4.4.4.4">superscript</csymbol><apply id="S3.E1.m1.4.4.4.4.2.cmml" xref="S3.E1.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.4.2.1.cmml" xref="S3.E1.m1.4.4.4.4">subscript</csymbol><ci id="S3.E1.m1.4.4.4.4.2.2.cmml" xref="S3.E1.m1.4.4.4.4.2.2">Î£</ci><ci id="S3.E1.m1.4.4.4.4.2.3.cmml" xref="S3.E1.m1.4.4.4.4.2.3">ğ‘š</ci></apply><ci id="S3.E1.m1.4.4.4.4.3.cmml" xref="S3.E1.m1.4.4.4.4.3">ğ‘€</ci></apply><ci id="S3.E1.m1.4.4.4.5.cmml" xref="S3.E1.m1.4.4.4.5">ğ‘’</ci><ci id="S3.E1.m1.4.4.4.6.cmml" xref="S3.E1.m1.4.4.4.6">ğ‘¥</ci><ci id="S3.E1.m1.4.4.4.7.cmml" xref="S3.E1.m1.4.4.4.7">ğ‘</ci><apply id="S3.E1.m1.4.4.4.2.1.1.cmml" xref="S3.E1.m1.4.4.4.2.1"><divide id="S3.E1.m1.4.4.4.2.1.1.2.cmml" xref="S3.E1.m1.4.4.4.2.1.1.2"></divide><apply id="S3.E1.m1.4.4.4.2.1.1.1.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1"><times id="S3.E1.m1.4.4.4.2.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.2"></times><ci id="S3.E1.m1.4.4.4.2.1.1.1.3.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.3">ğ‘ </ci><interval closure="open" id="S3.E1.m1.4.4.4.2.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.1"><ci id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.1">ğ‘‚</ci><apply id="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.2">ğ‘‡</ci><ci id="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.1.1.3">ğ‘š</ci></apply></interval></apply><ci id="S3.E1.m1.4.4.4.2.1.1.3.cmml" xref="S3.E1.m1.4.4.4.2.1.1.3">ğœ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">p^{o2t}_{m}=\frac{exp(s(O,T_{m})/\tau)}{\Sigma_{m}^{M}exp(s(O,T_{m})/\tau)}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">italic_p start_POSTSUPERSCRIPT italic_o 2 italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = divide start_ARG italic_e italic_x italic_p ( italic_s ( italic_O , italic_T start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) / italic_Ï„ ) end_ARG start_ARG roman_Î£ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_e italic_x italic_p ( italic_s ( italic_O , italic_T start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) / italic_Ï„ ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p^{t2o}_{m}=\frac{exp(s(T,O_{m})/\tau)}{\Sigma_{m}^{M}exp(s(T,O_{m})/\tau)}" class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.5" xref="S3.E2.m1.4.5.cmml"><msubsup id="S3.E2.m1.4.5.2" xref="S3.E2.m1.4.5.2.cmml"><mi id="S3.E2.m1.4.5.2.2.2" xref="S3.E2.m1.4.5.2.2.2.cmml">p</mi><mi id="S3.E2.m1.4.5.2.3" xref="S3.E2.m1.4.5.2.3.cmml">m</mi><mrow id="S3.E2.m1.4.5.2.2.3" xref="S3.E2.m1.4.5.2.2.3.cmml"><mi id="S3.E2.m1.4.5.2.2.3.2" xref="S3.E2.m1.4.5.2.2.3.2.cmml">t</mi><mo id="S3.E2.m1.4.5.2.2.3.1" xref="S3.E2.m1.4.5.2.2.3.1.cmml">â¢</mo><mn id="S3.E2.m1.4.5.2.2.3.3" xref="S3.E2.m1.4.5.2.2.3.3.cmml">2</mn><mo id="S3.E2.m1.4.5.2.2.3.1a" xref="S3.E2.m1.4.5.2.2.3.1.cmml">â¢</mo><mi id="S3.E2.m1.4.5.2.2.3.4" xref="S3.E2.m1.4.5.2.2.3.4.cmml">o</mi></mrow></msubsup><mo id="S3.E2.m1.4.5.1" xref="S3.E2.m1.4.5.1.cmml">=</mo><mfrac id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.4.cmml">e</mi><mo id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml">â¢</mo><mi id="S3.E2.m1.2.2.2.5" xref="S3.E2.m1.2.2.2.5.cmml">x</mi><mo id="S3.E2.m1.2.2.2.3a" xref="S3.E2.m1.2.2.2.3.cmml">â¢</mo><mi id="S3.E2.m1.2.2.2.6" xref="S3.E2.m1.2.2.2.6.cmml">p</mi><mo id="S3.E2.m1.2.2.2.3b" xref="S3.E2.m1.2.2.2.3.cmml">â¢</mo><mrow id="S3.E2.m1.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.1.1.cmml"><mo id="S3.E2.m1.2.2.2.2.1.2" stretchy="false" xref="S3.E2.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.1.3.cmml">s</mi><mo id="S3.E2.m1.2.2.2.2.1.1.1.2" xref="S3.E2.m1.2.2.2.2.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.2.2.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.1.2.cmml"><mo id="S3.E2.m1.2.2.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.2.2.1.1.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">T</mi><mo id="S3.E2.m1.2.2.2.2.1.1.1.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.1.1.2.cmml">,</mo><msub id="S3.E2.m1.2.2.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.2.cmml">O</mi><mi id="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.3.cmml">m</mi></msub><mo id="S3.E2.m1.2.2.2.2.1.1.1.1.1.4" stretchy="false" xref="S3.E2.m1.2.2.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.2.2.1.1.2" xref="S3.E2.m1.2.2.2.2.1.1.2.cmml">/</mo><mi id="S3.E2.m1.2.2.2.2.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.3.cmml">Ï„</mi></mrow><mo id="S3.E2.m1.2.2.2.2.1.3" stretchy="false" xref="S3.E2.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml"><msubsup id="S3.E2.m1.4.4.4.4" xref="S3.E2.m1.4.4.4.4.cmml"><mi id="S3.E2.m1.4.4.4.4.2.2" mathvariant="normal" xref="S3.E2.m1.4.4.4.4.2.2.cmml">Î£</mi><mi id="S3.E2.m1.4.4.4.4.2.3" xref="S3.E2.m1.4.4.4.4.2.3.cmml">m</mi><mi id="S3.E2.m1.4.4.4.4.3" xref="S3.E2.m1.4.4.4.4.3.cmml">M</mi></msubsup><mo id="S3.E2.m1.4.4.4.3" xref="S3.E2.m1.4.4.4.3.cmml">â¢</mo><mi id="S3.E2.m1.4.4.4.5" xref="S3.E2.m1.4.4.4.5.cmml">e</mi><mo id="S3.E2.m1.4.4.4.3a" xref="S3.E2.m1.4.4.4.3.cmml">â¢</mo><mi id="S3.E2.m1.4.4.4.6" xref="S3.E2.m1.4.4.4.6.cmml">x</mi><mo id="S3.E2.m1.4.4.4.3b" xref="S3.E2.m1.4.4.4.3.cmml">â¢</mo><mi id="S3.E2.m1.4.4.4.7" xref="S3.E2.m1.4.4.4.7.cmml">p</mi><mo id="S3.E2.m1.4.4.4.3c" xref="S3.E2.m1.4.4.4.3.cmml">â¢</mo><mrow id="S3.E2.m1.4.4.4.2.1" xref="S3.E2.m1.4.4.4.2.1.1.cmml"><mo id="S3.E2.m1.4.4.4.2.1.2" stretchy="false" xref="S3.E2.m1.4.4.4.2.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.4.2.1.1" xref="S3.E2.m1.4.4.4.2.1.1.cmml"><mrow id="S3.E2.m1.4.4.4.2.1.1.1" xref="S3.E2.m1.4.4.4.2.1.1.1.cmml"><mi id="S3.E2.m1.4.4.4.2.1.1.1.3" xref="S3.E2.m1.4.4.4.2.1.1.1.3.cmml">s</mi><mo id="S3.E2.m1.4.4.4.2.1.1.1.2" xref="S3.E2.m1.4.4.4.2.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.4.4.4.2.1.1.1.1.1" xref="S3.E2.m1.4.4.4.2.1.1.1.1.2.cmml"><mo id="S3.E2.m1.4.4.4.2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.4.2.1.1.1.1.2.cmml">(</mo><mi id="S3.E2.m1.3.3.3.1" xref="S3.E2.m1.3.3.3.1.cmml">T</mi><mo id="S3.E2.m1.4.4.4.2.1.1.1.1.1.3" xref="S3.E2.m1.4.4.4.2.1.1.1.1.2.cmml">,</mo><msub id="S3.E2.m1.4.4.4.2.1.1.1.1.1.1" xref="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.2.cmml">O</mi><mi id="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.3.cmml">m</mi></msub><mo id="S3.E2.m1.4.4.4.2.1.1.1.1.1.4" stretchy="false" xref="S3.E2.m1.4.4.4.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.4.2.1.1.2" xref="S3.E2.m1.4.4.4.2.1.1.2.cmml">/</mo><mi id="S3.E2.m1.4.4.4.2.1.1.3" xref="S3.E2.m1.4.4.4.2.1.1.3.cmml">Ï„</mi></mrow><mo id="S3.E2.m1.4.4.4.2.1.3" stretchy="false" xref="S3.E2.m1.4.4.4.2.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.5.cmml" xref="S3.E2.m1.4.5"><eq id="S3.E2.m1.4.5.1.cmml" xref="S3.E2.m1.4.5.1"></eq><apply id="S3.E2.m1.4.5.2.cmml" xref="S3.E2.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.5.2.1.cmml" xref="S3.E2.m1.4.5.2">subscript</csymbol><apply id="S3.E2.m1.4.5.2.2.cmml" xref="S3.E2.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.5.2.2.1.cmml" xref="S3.E2.m1.4.5.2">superscript</csymbol><ci id="S3.E2.m1.4.5.2.2.2.cmml" xref="S3.E2.m1.4.5.2.2.2">ğ‘</ci><apply id="S3.E2.m1.4.5.2.2.3.cmml" xref="S3.E2.m1.4.5.2.2.3"><times id="S3.E2.m1.4.5.2.2.3.1.cmml" xref="S3.E2.m1.4.5.2.2.3.1"></times><ci id="S3.E2.m1.4.5.2.2.3.2.cmml" xref="S3.E2.m1.4.5.2.2.3.2">ğ‘¡</ci><cn id="S3.E2.m1.4.5.2.2.3.3.cmml" type="integer" xref="S3.E2.m1.4.5.2.2.3.3">2</cn><ci id="S3.E2.m1.4.5.2.2.3.4.cmml" xref="S3.E2.m1.4.5.2.2.3.4">ğ‘œ</ci></apply></apply><ci id="S3.E2.m1.4.5.2.3.cmml" xref="S3.E2.m1.4.5.2.3">ğ‘š</ci></apply><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><divide id="S3.E2.m1.4.4.5.cmml" xref="S3.E2.m1.4.4"></divide><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><times id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"></times><ci id="S3.E2.m1.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.4">ğ‘’</ci><ci id="S3.E2.m1.2.2.2.5.cmml" xref="S3.E2.m1.2.2.2.5">ğ‘¥</ci><ci id="S3.E2.m1.2.2.2.6.cmml" xref="S3.E2.m1.2.2.2.6">ğ‘</ci><apply id="S3.E2.m1.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1"><divide id="S3.E2.m1.2.2.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.2"></divide><apply id="S3.E2.m1.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1"><times id="S3.E2.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.2"></times><ci id="S3.E2.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3">ğ‘ </ci><interval closure="open" id="S3.E2.m1.2.2.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1.1"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ğ‘‡</ci><apply id="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.2">ğ‘‚</ci><ci id="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1.1.1.3">ğ‘š</ci></apply></interval></apply><ci id="S3.E2.m1.2.2.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.3">ğœ</ci></apply></apply><apply id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"><times id="S3.E2.m1.4.4.4.3.cmml" xref="S3.E2.m1.4.4.4.3"></times><apply id="S3.E2.m1.4.4.4.4.cmml" xref="S3.E2.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.4.1.cmml" xref="S3.E2.m1.4.4.4.4">superscript</csymbol><apply id="S3.E2.m1.4.4.4.4.2.cmml" xref="S3.E2.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.4.2.1.cmml" xref="S3.E2.m1.4.4.4.4">subscript</csymbol><ci id="S3.E2.m1.4.4.4.4.2.2.cmml" xref="S3.E2.m1.4.4.4.4.2.2">Î£</ci><ci id="S3.E2.m1.4.4.4.4.2.3.cmml" xref="S3.E2.m1.4.4.4.4.2.3">ğ‘š</ci></apply><ci id="S3.E2.m1.4.4.4.4.3.cmml" xref="S3.E2.m1.4.4.4.4.3">ğ‘€</ci></apply><ci id="S3.E2.m1.4.4.4.5.cmml" xref="S3.E2.m1.4.4.4.5">ğ‘’</ci><ci id="S3.E2.m1.4.4.4.6.cmml" xref="S3.E2.m1.4.4.4.6">ğ‘¥</ci><ci id="S3.E2.m1.4.4.4.7.cmml" xref="S3.E2.m1.4.4.4.7">ğ‘</ci><apply id="S3.E2.m1.4.4.4.2.1.1.cmml" xref="S3.E2.m1.4.4.4.2.1"><divide id="S3.E2.m1.4.4.4.2.1.1.2.cmml" xref="S3.E2.m1.4.4.4.2.1.1.2"></divide><apply id="S3.E2.m1.4.4.4.2.1.1.1.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1"><times id="S3.E2.m1.4.4.4.2.1.1.1.2.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1.2"></times><ci id="S3.E2.m1.4.4.4.2.1.1.1.3.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1.3">ğ‘ </ci><interval closure="open" id="S3.E2.m1.4.4.4.2.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1.1.1"><ci id="S3.E2.m1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.3.1">ğ‘‡</ci><apply id="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.2">ğ‘‚</ci><ci id="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1.1.1.1.3">ğ‘š</ci></apply></interval></apply><ci id="S3.E2.m1.4.4.4.2.1.1.3.cmml" xref="S3.E2.m1.4.4.4.2.1.1.3">ğœ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">p^{t2o}_{m}=\frac{exp(s(T,O_{m})/\tau)}{\Sigma_{m}^{M}exp(s(T,O_{m})/\tau)}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_p start_POSTSUPERSCRIPT italic_t 2 italic_o end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = divide start_ARG italic_e italic_x italic_p ( italic_s ( italic_T , italic_O start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) / italic_Ï„ ) end_ARG start_ARG roman_Î£ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_e italic_x italic_p ( italic_s ( italic_T , italic_O start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) / italic_Ï„ ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.5">Here, <math alttext="s(T,O)" class="ltx_Math" display="inline" id="S3.p6.1.m1.2"><semantics id="S3.p6.1.m1.2a"><mrow id="S3.p6.1.m1.2.3" xref="S3.p6.1.m1.2.3.cmml"><mi id="S3.p6.1.m1.2.3.2" xref="S3.p6.1.m1.2.3.2.cmml">s</mi><mo id="S3.p6.1.m1.2.3.1" xref="S3.p6.1.m1.2.3.1.cmml">â¢</mo><mrow id="S3.p6.1.m1.2.3.3.2" xref="S3.p6.1.m1.2.3.3.1.cmml"><mo id="S3.p6.1.m1.2.3.3.2.1" stretchy="false" xref="S3.p6.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">T</mi><mo id="S3.p6.1.m1.2.3.3.2.2" xref="S3.p6.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.p6.1.m1.2.2" xref="S3.p6.1.m1.2.2.cmml">O</mi><mo id="S3.p6.1.m1.2.3.3.2.3" stretchy="false" xref="S3.p6.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.2b"><apply id="S3.p6.1.m1.2.3.cmml" xref="S3.p6.1.m1.2.3"><times id="S3.p6.1.m1.2.3.1.cmml" xref="S3.p6.1.m1.2.3.1"></times><ci id="S3.p6.1.m1.2.3.2.cmml" xref="S3.p6.1.m1.2.3.2">ğ‘ </ci><interval closure="open" id="S3.p6.1.m1.2.3.3.1.cmml" xref="S3.p6.1.m1.2.3.3.2"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">ğ‘‡</ci><ci id="S3.p6.1.m1.2.2.cmml" xref="S3.p6.1.m1.2.2">ğ‘‚</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.2c">s(T,O)</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.2d">italic_s ( italic_T , italic_O )</annotation></semantics></math> is a similarity function between the original, untranslated data and the translated examples in a minibatch. We compute <math alttext="s(T,O)" class="ltx_Math" display="inline" id="S3.p6.2.m2.2"><semantics id="S3.p6.2.m2.2a"><mrow id="S3.p6.2.m2.2.3" xref="S3.p6.2.m2.2.3.cmml"><mi id="S3.p6.2.m2.2.3.2" xref="S3.p6.2.m2.2.3.2.cmml">s</mi><mo id="S3.p6.2.m2.2.3.1" xref="S3.p6.2.m2.2.3.1.cmml">â¢</mo><mrow id="S3.p6.2.m2.2.3.3.2" xref="S3.p6.2.m2.2.3.3.1.cmml"><mo id="S3.p6.2.m2.2.3.3.2.1" stretchy="false" xref="S3.p6.2.m2.2.3.3.1.cmml">(</mo><mi id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml">T</mi><mo id="S3.p6.2.m2.2.3.3.2.2" xref="S3.p6.2.m2.2.3.3.1.cmml">,</mo><mi id="S3.p6.2.m2.2.2" xref="S3.p6.2.m2.2.2.cmml">O</mi><mo id="S3.p6.2.m2.2.3.3.2.3" stretchy="false" xref="S3.p6.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.2b"><apply id="S3.p6.2.m2.2.3.cmml" xref="S3.p6.2.m2.2.3"><times id="S3.p6.2.m2.2.3.1.cmml" xref="S3.p6.2.m2.2.3.1"></times><ci id="S3.p6.2.m2.2.3.2.cmml" xref="S3.p6.2.m2.2.3.2">ğ‘ </ci><interval closure="open" id="S3.p6.2.m2.2.3.3.1.cmml" xref="S3.p6.2.m2.2.3.3.2"><ci id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1">ğ‘‡</ci><ci id="S3.p6.2.m2.2.2.cmml" xref="S3.p6.2.m2.2.2">ğ‘‚</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.2c">s(T,O)</annotation><annotation encoding="application/x-llamapun" id="S3.p6.2.m2.2d">italic_s ( italic_T , italic_O )</annotation></semantics></math> by first extracting and normalizing the embedding for the initial <span class="ltx_text ltx_font_smallcaps" id="S3.p6.5.1">[cls]</span> token after the final attention head of the encoder stack in <span class="ltx_text ltx_font_smallcaps" id="S3.p6.5.2">m-BERT</span>, computing a pairwise dot product for all possible pairs of original and translated data and dividing by <math alttext="\tau" class="ltx_Math" display="inline" id="S3.p6.3.m3.1"><semantics id="S3.p6.3.m3.1a"><mi id="S3.p6.3.m3.1.1" xref="S3.p6.3.m3.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.1b"><ci id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.p6.3.m3.1d">italic_Ï„</annotation></semantics></math>, which is a learnable parameter. We then apply the softmax function as a way to represent the likelihood of each original/translated match. Ideally, each correct original/translated pair will have the most similar embeddings, resulting in a value close to 1 after softmax. As a final step, we compute the cross-entropy between the result of the previous step and a target vector which encodes the correct original/translated pairs, weighting this by a hyperparameter, <math alttext="\alpha_{otc}" class="ltx_Math" display="inline" id="S3.p6.4.m4.1"><semantics id="S3.p6.4.m4.1a"><msub id="S3.p6.4.m4.1.1" xref="S3.p6.4.m4.1.1.cmml"><mi id="S3.p6.4.m4.1.1.2" xref="S3.p6.4.m4.1.1.2.cmml">Î±</mi><mrow id="S3.p6.4.m4.1.1.3" xref="S3.p6.4.m4.1.1.3.cmml"><mi id="S3.p6.4.m4.1.1.3.2" xref="S3.p6.4.m4.1.1.3.2.cmml">o</mi><mo id="S3.p6.4.m4.1.1.3.1" xref="S3.p6.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.p6.4.m4.1.1.3.3" xref="S3.p6.4.m4.1.1.3.3.cmml">t</mi><mo id="S3.p6.4.m4.1.1.3.1a" xref="S3.p6.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.p6.4.m4.1.1.3.4" xref="S3.p6.4.m4.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p6.4.m4.1b"><apply id="S3.p6.4.m4.1.1.cmml" xref="S3.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p6.4.m4.1.1.1.cmml" xref="S3.p6.4.m4.1.1">subscript</csymbol><ci id="S3.p6.4.m4.1.1.2.cmml" xref="S3.p6.4.m4.1.1.2">ğ›¼</ci><apply id="S3.p6.4.m4.1.1.3.cmml" xref="S3.p6.4.m4.1.1.3"><times id="S3.p6.4.m4.1.1.3.1.cmml" xref="S3.p6.4.m4.1.1.3.1"></times><ci id="S3.p6.4.m4.1.1.3.2.cmml" xref="S3.p6.4.m4.1.1.3.2">ğ‘œ</ci><ci id="S3.p6.4.m4.1.1.3.3.cmml" xref="S3.p6.4.m4.1.1.3.3">ğ‘¡</ci><ci id="S3.p6.4.m4.1.1.3.4.cmml" xref="S3.p6.4.m4.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.4.m4.1c">\alpha_{otc}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.4.m4.1d">italic_Î± start_POSTSUBSCRIPT italic_o italic_t italic_c end_POSTSUBSCRIPT</annotation></semantics></math>. Following <span class="ltx_text ltx_font_smallcaps" id="S3.p6.5.3">BLIP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx15" title="">Li et al., 2022</a>]</cite>, we set <math alttext="\alpha_{otc}=.4" class="ltx_Math" display="inline" id="S3.p6.5.m5.1"><semantics id="S3.p6.5.m5.1a"><mrow id="S3.p6.5.m5.1.1" xref="S3.p6.5.m5.1.1.cmml"><msub id="S3.p6.5.m5.1.1.2" xref="S3.p6.5.m5.1.1.2.cmml"><mi id="S3.p6.5.m5.1.1.2.2" xref="S3.p6.5.m5.1.1.2.2.cmml">Î±</mi><mrow id="S3.p6.5.m5.1.1.2.3" xref="S3.p6.5.m5.1.1.2.3.cmml"><mi id="S3.p6.5.m5.1.1.2.3.2" xref="S3.p6.5.m5.1.1.2.3.2.cmml">o</mi><mo id="S3.p6.5.m5.1.1.2.3.1" xref="S3.p6.5.m5.1.1.2.3.1.cmml">â¢</mo><mi id="S3.p6.5.m5.1.1.2.3.3" xref="S3.p6.5.m5.1.1.2.3.3.cmml">t</mi><mo id="S3.p6.5.m5.1.1.2.3.1a" xref="S3.p6.5.m5.1.1.2.3.1.cmml">â¢</mo><mi id="S3.p6.5.m5.1.1.2.3.4" xref="S3.p6.5.m5.1.1.2.3.4.cmml">c</mi></mrow></msub><mo id="S3.p6.5.m5.1.1.1" xref="S3.p6.5.m5.1.1.1.cmml">=</mo><mn id="S3.p6.5.m5.1.1.3" xref="S3.p6.5.m5.1.1.3.cmml">.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.5.m5.1b"><apply id="S3.p6.5.m5.1.1.cmml" xref="S3.p6.5.m5.1.1"><eq id="S3.p6.5.m5.1.1.1.cmml" xref="S3.p6.5.m5.1.1.1"></eq><apply id="S3.p6.5.m5.1.1.2.cmml" xref="S3.p6.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.p6.5.m5.1.1.2.1.cmml" xref="S3.p6.5.m5.1.1.2">subscript</csymbol><ci id="S3.p6.5.m5.1.1.2.2.cmml" xref="S3.p6.5.m5.1.1.2.2">ğ›¼</ci><apply id="S3.p6.5.m5.1.1.2.3.cmml" xref="S3.p6.5.m5.1.1.2.3"><times id="S3.p6.5.m5.1.1.2.3.1.cmml" xref="S3.p6.5.m5.1.1.2.3.1"></times><ci id="S3.p6.5.m5.1.1.2.3.2.cmml" xref="S3.p6.5.m5.1.1.2.3.2">ğ‘œ</ci><ci id="S3.p6.5.m5.1.1.2.3.3.cmml" xref="S3.p6.5.m5.1.1.2.3.3">ğ‘¡</ci><ci id="S3.p6.5.m5.1.1.2.3.4.cmml" xref="S3.p6.5.m5.1.1.2.3.4">ğ‘</ci></apply></apply><cn id="S3.p6.5.m5.1.1.3.cmml" type="float" xref="S3.p6.5.m5.1.1.3">.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.5.m5.1c">\alpha_{otc}=.4</annotation><annotation encoding="application/x-llamapun" id="S3.p6.5.m5.1d">italic_Î± start_POSTSUBSCRIPT italic_o italic_t italic_c end_POSTSUBSCRIPT = .4</annotation></semantics></math> for all runs.</p>
</div>
<div class="ltx_para" id="S3.p7">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\ell_{otc}=\alpha_{otc}*\frac{1}{2}\mathbb{E}_{(O,T)}[H(\textbf{y}^{o2t}(O),%
\textbf{p}^{o2t}(O)+\\
H(\textbf{y}^{t2o}(T),\textbf{p}^{t2o}(T)]" class="ltx_math_unparsed" display="block" id="S3.E3.m1.40"><semantics id="S3.E3.m1.40a"><mtable displaystyle="true" id="S3.E3.m1.40.40.2" rowspacing="0pt"><mtr id="S3.E3.m1.40.40.2a"><mtd class="ltx_align_left" columnalign="left" id="S3.E3.m1.40.40.2b"><mrow id="S3.E3.m1.24.24.24.24.24"><msub id="S3.E3.m1.24.24.24.24.24.25"><mi id="S3.E3.m1.1.1.1.1.1.1" mathvariant="normal">â„“</mi><mrow id="S3.E3.m1.2.2.2.2.2.2.1"><mi id="S3.E3.m1.2.2.2.2.2.2.1.2">o</mi><mo id="S3.E3.m1.2.2.2.2.2.2.1.1">â¢</mo><mi id="S3.E3.m1.2.2.2.2.2.2.1.3">t</mi><mo id="S3.E3.m1.2.2.2.2.2.2.1.1a">â¢</mo><mi id="S3.E3.m1.2.2.2.2.2.2.1.4">c</mi></mrow></msub><mo id="S3.E3.m1.3.3.3.3.3.3">=</mo><msub id="S3.E3.m1.24.24.24.24.24.26"><mi id="S3.E3.m1.4.4.4.4.4.4">Î±</mi><mrow id="S3.E3.m1.5.5.5.5.5.5.1"><mi id="S3.E3.m1.5.5.5.5.5.5.1.2">o</mi><mo id="S3.E3.m1.5.5.5.5.5.5.1.1">â¢</mo><mi id="S3.E3.m1.5.5.5.5.5.5.1.3">t</mi><mo id="S3.E3.m1.5.5.5.5.5.5.1.1a">â¢</mo><mi id="S3.E3.m1.5.5.5.5.5.5.1.4">c</mi></mrow></msub><mo id="S3.E3.m1.6.6.6.6.6.6" lspace="0.222em" rspace="0.222em">âˆ—</mo><mfrac id="S3.E3.m1.7.7.7.7.7.7"><mn id="S3.E3.m1.7.7.7.7.7.7.2">1</mn><mn id="S3.E3.m1.7.7.7.7.7.7.3">2</mn></mfrac><msub id="S3.E3.m1.24.24.24.24.24.27"><mi id="S3.E3.m1.8.8.8.8.8.8">ğ”¼</mi><mrow id="S3.E3.m1.9.9.9.9.9.9.1.4"><mo id="S3.E3.m1.9.9.9.9.9.9.1.4.1" stretchy="false">(</mo><mi id="S3.E3.m1.9.9.9.9.9.9.1.1">O</mi><mo id="S3.E3.m1.9.9.9.9.9.9.1.4.2">,</mo><mi id="S3.E3.m1.9.9.9.9.9.9.1.2">T</mi><mo id="S3.E3.m1.9.9.9.9.9.9.1.4.3" stretchy="false">)</mo></mrow></msub><mrow id="S3.E3.m1.24.24.24.24.24.28"><mo id="S3.E3.m1.10.10.10.10.10.10" stretchy="false">[</mo><mi id="S3.E3.m1.11.11.11.11.11.11">H</mi><mrow id="S3.E3.m1.24.24.24.24.24.28.1"><mo id="S3.E3.m1.12.12.12.12.12.12" stretchy="false">(</mo><msup id="S3.E3.m1.24.24.24.24.24.28.1.1"><mtext class="ltx_mathvariant_bold" id="S3.E3.m1.13.13.13.13.13.13">y</mtext><mrow id="S3.E3.m1.14.14.14.14.14.14.1"><mi id="S3.E3.m1.14.14.14.14.14.14.1.2">o</mi><mo id="S3.E3.m1.14.14.14.14.14.14.1.1">â¢</mo><mn id="S3.E3.m1.14.14.14.14.14.14.1.3">2</mn><mo id="S3.E3.m1.14.14.14.14.14.14.1.1a">â¢</mo><mi id="S3.E3.m1.14.14.14.14.14.14.1.4">t</mi></mrow></msup><mrow id="S3.E3.m1.24.24.24.24.24.28.1.2"><mo id="S3.E3.m1.15.15.15.15.15.15" stretchy="false">(</mo><mi id="S3.E3.m1.16.16.16.16.16.16">O</mi><mo id="S3.E3.m1.17.17.17.17.17.17" stretchy="false">)</mo></mrow><mo id="S3.E3.m1.18.18.18.18.18.18">,</mo><msup id="S3.E3.m1.24.24.24.24.24.28.1.3"><mtext class="ltx_mathvariant_bold" id="S3.E3.m1.19.19.19.19.19.19">p</mtext><mrow id="S3.E3.m1.20.20.20.20.20.20.1"><mi id="S3.E3.m1.20.20.20.20.20.20.1.2">o</mi><mo id="S3.E3.m1.20.20.20.20.20.20.1.1">â¢</mo><mn id="S3.E3.m1.20.20.20.20.20.20.1.3">2</mn><mo id="S3.E3.m1.20.20.20.20.20.20.1.1a">â¢</mo><mi id="S3.E3.m1.20.20.20.20.20.20.1.4">t</mi></mrow></msup><mrow id="S3.E3.m1.24.24.24.24.24.28.1.4"><mo id="S3.E3.m1.21.21.21.21.21.21" stretchy="false">(</mo><mi id="S3.E3.m1.22.22.22.22.22.22">O</mi><mo id="S3.E3.m1.23.23.23.23.23.23" stretchy="false">)</mo></mrow><mo id="S3.E3.m1.24.24.24.24.24.24">+</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E3.m1.40.40.2c"><mtd class="ltx_align_right" columnalign="right" id="S3.E3.m1.40.40.2d"><mrow id="S3.E3.m1.40.40.2.40.16.16"><mi id="S3.E3.m1.25.25.25.1.1.1">H</mi><mo id="S3.E3.m1.40.40.2.40.16.16.17">â¢</mo><mrow id="S3.E3.m1.40.40.2.40.16.16.16.2"><mo id="S3.E3.m1.26.26.26.2.2.2" stretchy="false">(</mo><mrow id="S3.E3.m1.39.39.1.39.15.15.15.1.1"><msup id="S3.E3.m1.39.39.1.39.15.15.15.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.E3.m1.27.27.27.3.3.3">y</mtext><mrow id="S3.E3.m1.28.28.28.4.4.4.1"><mi id="S3.E3.m1.28.28.28.4.4.4.1.2">t</mi><mo id="S3.E3.m1.28.28.28.4.4.4.1.1">â¢</mo><mn id="S3.E3.m1.28.28.28.4.4.4.1.3">2</mn><mo id="S3.E3.m1.28.28.28.4.4.4.1.1a">â¢</mo><mi id="S3.E3.m1.28.28.28.4.4.4.1.4">o</mi></mrow></msup><mo id="S3.E3.m1.39.39.1.39.15.15.15.1.1.1">â¢</mo><mrow id="S3.E3.m1.39.39.1.39.15.15.15.1.1.3"><mo id="S3.E3.m1.29.29.29.5.5.5" stretchy="false">(</mo><mi id="S3.E3.m1.30.30.30.6.6.6">T</mi><mo id="S3.E3.m1.31.31.31.7.7.7" stretchy="false">)</mo></mrow></mrow><mo id="S3.E3.m1.32.32.32.8.8.8">,</mo><mrow id="S3.E3.m1.40.40.2.40.16.16.16.2.2"><msup id="S3.E3.m1.40.40.2.40.16.16.16.2.2.2"><mtext class="ltx_mathvariant_bold" id="S3.E3.m1.33.33.33.9.9.9">p</mtext><mrow id="S3.E3.m1.34.34.34.10.10.10.1"><mi id="S3.E3.m1.34.34.34.10.10.10.1.2">t</mi><mo id="S3.E3.m1.34.34.34.10.10.10.1.1">â¢</mo><mn id="S3.E3.m1.34.34.34.10.10.10.1.3">2</mn><mo id="S3.E3.m1.34.34.34.10.10.10.1.1a">â¢</mo><mi id="S3.E3.m1.34.34.34.10.10.10.1.4">o</mi></mrow></msup><mo id="S3.E3.m1.40.40.2.40.16.16.16.2.2.1">â¢</mo><mrow id="S3.E3.m1.40.40.2.40.16.16.16.2.2.3"><mo id="S3.E3.m1.35.35.35.11.11.11" stretchy="false">(</mo><mi id="S3.E3.m1.36.36.36.12.12.12">T</mi><mo id="S3.E3.m1.37.37.37.13.13.13" stretchy="false">)</mo></mrow></mrow><mo id="S3.E3.m1.38.38.38.14.14.14" stretchy="false">]</mo></mrow></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" id="S3.E3.m1.40b">\ell_{otc}=\alpha_{otc}*\frac{1}{2}\mathbb{E}_{(O,T)}[H(\textbf{y}^{o2t}(O),%
\textbf{p}^{o2t}(O)+\\
H(\textbf{y}^{t2o}(T),\textbf{p}^{t2o}(T)]</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.40c">start_ROW start_CELL roman_â„“ start_POSTSUBSCRIPT italic_o italic_t italic_c end_POSTSUBSCRIPT = italic_Î± start_POSTSUBSCRIPT italic_o italic_t italic_c end_POSTSUBSCRIPT âˆ— divide start_ARG 1 end_ARG start_ARG 2 end_ARG blackboard_E start_POSTSUBSCRIPT ( italic_O , italic_T ) end_POSTSUBSCRIPT [ italic_H ( y start_POSTSUPERSCRIPT italic_o 2 italic_t end_POSTSUPERSCRIPT ( italic_O ) , p start_POSTSUPERSCRIPT italic_o 2 italic_t end_POSTSUPERSCRIPT ( italic_O ) + end_CELL end_ROW start_ROW start_CELL italic_H ( y start_POSTSUPERSCRIPT italic_t 2 italic_o end_POSTSUPERSCRIPT ( italic_T ) , p start_POSTSUPERSCRIPT italic_t 2 italic_o end_POSTSUPERSCRIPT ( italic_T ) ] end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">For these experiments, we use a multilingual dataset of Amazon product reviews across 6 languages: English, Spanish, French, German, Chinese and Japanese <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx12" title="">Keung et al., 2020</a>]</cite>. This dataset is comprised of over 1 million total examples, split into a train and test partition. The reviews are equally distributed across the six languages, as well as the total stars given to the reviewed product (1-5) for both the train and test partition, i.e., each number of stars comprises 20% of the examples for that language. This dataset is particularly useful due to its size, number of available languages and presence of an established training and test data split.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We began by translating each review from the training partition of the original dataset into each of the other respective languages and assigned the same star value to the review (see example <a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S4.F1" title="Figure 1 â€£ 4.1 Data â€£ 4 Experiments â€£ Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_tag">1</span></a>), i.e., if a review was originally in English and had star star, when translating it into French it would also be labeled with one star. We did this translation once before carrying out the rest of the experiment to ensure each classifier would be trained on the same set of translations. To translate, we used a single multilingual translation model, M2M100 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx8" title="">Fan et al., 2020</a>]</cite>. We chose to use a single multilingual translation model in order to mitigate any potential differences from translation quality coming from different machine translation architectures.</p>
</div>
<figure class="ltx_figure" id="S4.F1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.F1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.F1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.F1.1.1.1.1">id</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.F1.1.1.1.2">translated</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.F1.1.1.1.3">language</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S4.F1.1.1.1.4">text</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.F1.1.1.1.5">stars</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.F1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.F1.1.2.1.1">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.F1.1.2.1.2">0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.F1.1.2.1.3">en</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.F1.1.2.1.4">My daughter really likes the backpack and â€¦</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.F1.1.2.1.5">5</td>
</tr>
<tr class="ltx_tr" id="S4.F1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.3.2.1">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.3.2.2">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.3.2.3">es</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.F1.1.3.2.4">Mi hija realmente le gusta el bolsillo y â€¦</td>
<td class="ltx_td ltx_align_center" id="S4.F1.1.3.2.5">5</td>
</tr>
<tr class="ltx_tr" id="S4.F1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.4.3.1">â€¦</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.4.3.2">â€¦</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.4.3.3">â€¦</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.F1.1.4.3.4">â€¦</td>
<td class="ltx_td ltx_align_center" id="S4.F1.1.4.3.5">â€¦</td>
</tr>
<tr class="ltx_tr" id="S4.F1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.5.4.1">2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.5.4.2">0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.5.4.3">en</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.F1.1.5.4.4">This product is BS, I washed my face with hot water â€¦</td>
<td class="ltx_td ltx_align_center" id="S4.F1.1.5.4.5">1</td>
</tr>
<tr class="ltx_tr" id="S4.F1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.6.5.1">2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.6.5.2">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.6.5.3">fr</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.F1.1.6.5.4">Ce produit est BS, je me suis lavÃ© le visage Ã  lâ€™eau chaude â€¦</td>
<td class="ltx_td ltx_align_center" id="S4.F1.1.6.5.5">1</td>
</tr>
<tr class="ltx_tr" id="S4.F1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.7.6.1">â€¦</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.7.6.2">â€¦</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F1.1.7.6.3">â€¦</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.F1.1.7.6.4">â€¦</td>
<td class="ltx_td ltx_align_center" id="S4.F1.1.7.6.5">â€¦</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example original and translated data. Each unique review (id) in the original dataset was translated to the other languages and assigned the same star value. Texts truncated here for formatting.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiment design</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To investigate any potential improvement in classifier accuracy with the use OTC loss, we fine-tuned pretrained transformer models on datasets that included original, untranslated data for a single language<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We restricted the experimental conditions to only including a single languageâ€™s original data, rather than use the full set of <math alttext="6!=720" class="ltx_Math" display="inline" id="footnote1.m1.1"><semantics id="footnote1.m1.1b"><mrow id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml"><mrow id="footnote1.m1.1.1.2" xref="footnote1.m1.1.1.2.cmml"><mn id="footnote1.m1.1.1.2.2" xref="footnote1.m1.1.1.2.2.cmml">6</mn><mo id="footnote1.m1.1.1.2.1" xref="footnote1.m1.1.1.2.1.cmml">!</mo></mrow><mo id="footnote1.m1.1.1.1" xref="footnote1.m1.1.1.1.cmml">=</mo><mn id="footnote1.m1.1.1.3" xref="footnote1.m1.1.1.3.cmml">720</mn></mrow><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><apply id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1"><eq id="footnote1.m1.1.1.1.cmml" xref="footnote1.m1.1.1.1"></eq><apply id="footnote1.m1.1.1.2.cmml" xref="footnote1.m1.1.1.2"><factorial id="footnote1.m1.1.1.2.1.cmml" xref="footnote1.m1.1.1.2.1"></factorial><cn id="footnote1.m1.1.1.2.2.cmml" type="integer" xref="footnote1.m1.1.1.2.2">6</cn></apply><cn id="footnote1.m1.1.1.3.cmml" type="integer" xref="footnote1.m1.1.1.3">720</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">6!=720</annotation><annotation encoding="application/x-llamapun" id="footnote1.m1.1e">6 ! = 720</annotation></semantics></math> possible permutations of language combinations for the sake of efficiency and resources.</span></span></span> and only translated data for all others in the six language set. As an example, in one training run, the model would be tuned on the original English training data and only translated data for all other languages, which were translated from the set of the original English data. We did this for all six languages in the original set to ensure any results were not restricted to one language in the dataset. Though the exact training examples varied for each model, we tested each on the original testing split of the dataset, which was solely comprised of original data, i.e., non-translated, for the six languages.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">In each case, we tuned a multilingual <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p2.1.1">DistilBERT</span> model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx20" title="">Sanh et al., 2019</a>]</cite>, a distilled version of the original multilingual <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p2.1.2">m-BERT</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx7" title="">Devlin et al., 2018</a>]</cite>, to predict the number of stars on a review as a categorical classification problem, using categorical cross-entropy loss and varying between using OTC loss as an additional loss parameter between runs. We chose to use a distilled variant of <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p2.1.3">BERT</span> due to the distilled variants increased speed of training, while still maintaining 97% of overall language understanding of the original.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Because of the mechanics of OTC loss, each translated datum must have an original match in the minibatch and each original must have at least one translated variant. As such, we constructed minibatches during training such that half the samples were always original, untranslated data and the other half were a randomly selected translated example for each original datum. For each original example, we randomly selected a translated example from the other languages, meaning that the model saw an equal number of original and translated examples during tuning overall, though it saw far fewer individual examples of each translated language, i.e., roughly <math alttext="\frac{1}{5}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mfrac id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">1</mn><mn id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">5</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><divide id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"></divide><cn id="S4.SS2.p3.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.p3.1.m1.1.1.2">1</cn><cn id="S4.SS2.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\frac{1}{5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">divide start_ARG 1 end_ARG start_ARG 5 end_ARG</annotation></semantics></math>. For simplicity, we restricted our tests to a 1:1 original:translated ratio and we used the same batch sampling method for runs without OTC loss, to make results more easily comparable.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">For each tuning run, we used a batch size of 32 (16 original and 16 translated examples per batch)<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For baseline conditions where there was no translated data, mini-batching happened as normal with 32 examples original, untranslated data per batch.</span></span></span> and used the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx16" title="">Loshchilov and Hutter, 2017</a>]</cite> optimizer with a linear warm-up of 500 updates with a learning rate of 2e-5. All training was done on <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p4.1.1">g5.2xlarge</span> AWS instances which contain NVIDIA A10G GPUs. We tuned 3 separate tuning runs for each set of hyperparameters and report their mean values in the next section.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In these experiments, we asked two simple questions: 1) how feasible is it to tune a multilingual transformer model on translated data and 2) does the inclusion of OTC loss improve model performance for languages where only translated training data was used.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">In answer to the first, for each of the six languages in the original dataset, models fine-tuned with translated data showed higher F1-micro scores<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>F1-micro is an example-weighted version of the F1-score, which is the harmonic mean or precision and recall. For more details on F1-score, see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx11" title="">Jurafsky and Martin, 2008</a>]</cite>.</span></span></span> on the held-out test set, compared to models trained with only original data for a single language (see Table <a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S5.T1" title="Table 1 â€£ 5 Results â€£ Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_tag">1</span></a>). As was expected from Pires et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx19" title="">Pires et al., 2019</a>]</cite>, even if a model was never exposed to data for a language, original or translated, the final model did have F1-micro greater than chance for that language (which would be 20% for a balanced, 5-label problem), indicating there was interlingual knowledge transfer happening within the model during training. Moreover, it appears that there was more transfer between related, similar languages, compared to more dissimilar languages; models trained with data for a European language showed higher performance on other European languages, compared to Japanese or Chinese. Nevertheless, for all languages, the use of translated data did show a noticeable improvement (.02-.11), though for each language, models trained with only translated data did underperform models trained with the full set of origina, untranslated training examples for that language (.07-.12).</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">That said, it is clear that the use of translated training data does improve model performance, even if the trained model only sees translated examples for that language. It should also be noted that due to the batching and sampling strategy used here, models trained with translated data saw far fewer examples of each language where they only saw translated data. That is, because each original review was paired with a single translated example out of five possible translated, these models were exposed to roughly one fifth of the data for translated languages and still saw a sizable boost in performance.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T1.1.1.1.1">Language</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3" id="S5.T1.1.1.1.2">F1-micro</th>
</tr>
<tr class="ltx_tr" id="S5.T1.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.2.2.2">No data</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.2.2.3">Translated</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.2.2.4">Original</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.1.3.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T1.1.3.1.1.1">en</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.1.3.1.2">0.407</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.1.3.1.3">0.481</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.3.1.4">0.554</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.4.2.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T1.1.4.2.1.1">fr</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.4.2.2">0.379</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.4.2.3">0.468</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.4">0.544</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.5.3.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T1.1.5.3.1.1">de</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.5.3.2">0.359</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.5.3.3">0.465</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.3.4">0.581</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.6.4.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T1.1.6.4.1.1">es</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.6.4.2">0.376</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.6.4.3">0.474</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.6.4.4">0.55</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.7.5.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T1.1.7.5.1.1">ja</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.7.5.2">0.307</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.7.5.3">0.396</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.7.5.4">0.543</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.8.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T1.1.8.6.1.1">zh</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.8.6.2">0.352</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.8.6.3">0.372</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.8.6.4">0.458</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>F1-micro for models trained with no samples for the specified language (No data), with only translated samples (Translated) and with the original training data for that language (Original). All languages saw a sizeable boost to performance over their respective baselines when using translated data (.02-.11) but all languages did perform markedly better when given actual data for each language.</figcaption>
</figure>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Moving on to the effect of OTC loss, Table <a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S5.T3" title="Table 3 â€£ 5 Results â€£ Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_tag">3</span></a> shows the mean F1-micro per language in the testing set, for models fine-tuned using original data for the specified language and translations for all other languages. For all languages, models trained using OTC loss saw an improvement over models trained without for all languages except Chinese, which showed a mixed set of negligible differences or lowered performance. However, these values include runs where the specific language was included as original, untranslated data. When averaging across all runs where a language in the testing set was only represented by translated data, OTC loss shows an improvement over models trained without it for all languages. Table <a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#S5.T2" title="Table 2 â€£ 5 Results â€£ Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_tag">2</span></a> shows the mean F1-micro for all models trained where the specified language was not the original language.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T2.1.1.1.1">Language</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2" id="S5.T2.1.1.1.2">F1-micro</th>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.1.2.2.2">No OTC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.2.2.3">OTC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.1.3.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.3.1.1.1">en</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.1.3.1.2">0.479</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.3.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.3.1.3.1">0.483</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.1.4.2.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.4.2.1.1">fr</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.4.2.2">0.464</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.2.3.1">0.472</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.1.5.3.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.5.3.1.1">de</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.5.3.2">0.463</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.3.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.3.3.1">0.467</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.1.6.4.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.6.4.1.1">es</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.6.4.2">0.472</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.4.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.6.4.3.1">0.476</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.1.7.5.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.7.5.1.1">ja</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.7.5.2">0.393</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.7.5.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.7.5.3.1">0.399</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.1.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.8.6.1.1">zh</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.8.6.2">0.368</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.8.6.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.8.6.3.1">0.376</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison on final performance per language for models that only included translated examples for the specified language. Though the gain was less than .1, each language consistently performed better when trained with OTC loss.</figcaption>
</figure>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">To ensure that the results here were in fact statistically significant, we fit a linear mixed-effect model to predict final model F1-micro for a language, given the hyperparameters of a particular tuning run. Mixed-effect models are able to accurately evaluate the contribution of different fixed-effect independent variables, e.g., whether OTC was used when training a particular model, on dependent variables, e.g., the final accuracy of the trained model, all the while being robust to expected random variance between trials, e.g., because of random initialization and batching, some deep learning models score higher than others with identical hyperparameters (see Baayen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx2" title="">Baayen et al., 2008</a>]</cite>, Jaeger <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx10" title="">Jaeger, 2008</a>]</cite> for more).</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">This statistical model was fit to predict per-language test f1-micro, given a random effect of each model run and three fixed effects: i) the tested language, ii) the identity of the single original language and iii) whether OTC loss was added. OTC was found to have a significant, positive effect (<span class="ltx_text ltx_font_smallcaps" id="S5.p6.1.1">coef</span>=0.036, <span class="ltx_text ltx_font_smallcaps" id="S5.p6.1.2">Std.Error</span>=0.017, for all model details see <a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#A0.F2" title="Figure 2 â€£ Using Machine Translation to Augment Multilingual Classification"><span class="ltx_text ltx_ref_tag">2</span></a>), indicating that even after taking into consideration differences between languages and random variance for each multilingual model, the inclusion of OTC loss did yield an improved final model F1-micro.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.1.1">Orig. Training Language</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.1.2">OTC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.1.3"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.1.1.3.1">en</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.1.4"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.1.1.4.1">fr</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.1.5"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.1.1.5.1">de</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.1.6"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.1.1.6.1">es</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.1.7"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.1.1.7.1">ja</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.1.1.1.8"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.1.1.8.1">zh</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.2.1.1.1">en</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.2">No OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.3">0.548</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.4">0.488</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.5">0.493</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.6">0.489</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.7">0.425</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.2.1.8">0.423</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<td class="ltx_td ltx_border_r" id="S5.T3.1.3.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.3.2.2">OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.3.2.3.1">0.553</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.3.2.4.1">0.507</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.3.2.5.1">0.522</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.3.2.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.3.2.6.1">0.512</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.3.2.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.3.2.7.1">0.434</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.8">0.422</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.4.3.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.4.3.1.1">fr</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.4.3.2">No OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.4.3.3">0.504</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.4.3.4">0.539</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.4.3.5">0.504</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.4.3.6">0.493</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.4.3.7">0.424</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.4.3.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.3.8.1">0.426</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.4">
<td class="ltx_td ltx_border_r" id="S5.T3.1.5.4.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.5.4.2">OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.4.3.1">0.512</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.5.4.4">0.539</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.5.4.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.4.5.1">0.517</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.5.4.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.4.6.1">0.511</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.5.4.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.4.7.1">0.428</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.4.8">0.412</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.6.5.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.6.5.1.1">de</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.6.5.2">No OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.6.5.3">0.514</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.6.5.4">0.495</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.6.5.5">0.577</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.6.5.6">0.495</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.6.5.7">0.436</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.6.5.8">0.427</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.6">
<td class="ltx_td ltx_border_r" id="S5.T3.1.7.6.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.7.6.2">OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.3.1">0.524</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.4.1">0.506</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.7.6.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.5.1">0.581</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.7.6.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.6.1">0.506</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.7.6.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.7.1">0.449</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.7.6.8">0.425</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.8.7.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.8.7.1.1">es</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.8.7.2">No OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.8.7.3">0.506</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.8.7.4">0.497</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.8.7.5">0.500</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.8.7.6">0.544</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.8.7.7">0.433</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.8.7.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.8.7.8.1">0.419</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.9.8">
<td class="ltx_td ltx_border_r" id="S5.T3.1.9.8.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.9.8.2">OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.9.8.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.9.8.3.1">0.523</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.9.8.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.9.8.4.1">0.510</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.9.8.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.9.8.5.1">0.518</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.9.8.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.9.8.6.1">0.548</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.9.8.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.9.8.7.1">0.441</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.9.8.8">0.413</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.10.9.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.10.9.1.1">ja</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.10.9.2">No OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.10.9.3">0.470</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.10.9.4">0.460</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.10.9.5">0.477</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.10.9.6">0.468</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.10.9.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.9.7.1">0.526</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.10.9.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.9.8.1">0.436</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.11.10">
<td class="ltx_td ltx_border_r" id="S5.T3.1.11.10.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.11.10.2">OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.11.10.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.11.10.3.1">0.493</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.11.10.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.11.10.4.1">0.474</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.11.10.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.11.10.5.1">0.499</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.11.10.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.11.10.6.1">0.487</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.11.10.7">0.522</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.11.10.8">0.424</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.12.11.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.12.11.1.1">zh</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.12.11.2">No OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.12.11.3">0.486</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.12.11.4">0.439</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.12.11.5">0.441</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.12.11.6">0.444</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.12.11.7">0.398</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.12.11.8">0.482</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.13.12">
<td class="ltx_td ltx_border_r" id="S5.T3.1.13.12.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.13.12.2">OTC</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.13.12.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.13.12.3.1">0.488</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.13.12.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.13.12.4.1">0.467</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.13.12.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.13.12.5.1">0.473</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.13.12.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.13.12.6.1">0.472</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.13.12.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.13.12.7.1">0.421</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.13.12.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.13.12.8.1">0.503</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>F1-micro results on untranslated test data. Each row shows the per-language performance for models trained with original data for the specified language and translated data for all other languages, using OTC loss and without. Each cell shows the mean of 3 runs per condition. Bolded values show a difference of .03 or greater.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion and future directions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We investigated the feasibility of using translated text to fine-tune a multilingual transformer model, as well as any potential gains by utilizing a novel application of deep learning technique to improve performance. We found that models trained using only translated data for a language do show a noticeable improvement over baselines, though as expected, there was still a performance drop from using original, untranslated data for that language. We also found that slight further gains can be achieved by the use of OTC loss, suggesting that training the model in such a way where it is sensitive to potential data issues improves its ability to generalize.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Granted, this is a very open problem and results of using translated data to tune a multilingual classifier will vary highly depending on the quality of MT model used, architecture of the classifier being tuned and the type of classification being modeled. Nevertheless, the results here are exciting for multiple reasons. Firstly, as suggested by previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.05478v1#bib.bibx21" title="">Shalunts et al., 2016</a>, as an example]</cite>, MT is useful tool for language-specific dataset creation when creating a dataset for that language directly may prove difficult. In this case, we showed that <span class="ltx_text ltx_font_smallcaps" id="S6.p2.1.1">m-BERT</span> models tuned on translated examples showed large gains over simple multilingual transfer during training. This is particularly interesting given that for each translated language, the model was only given a fraction of samples compared to the original language due to the 1:1 ratio of original and translated data. A future direction for this work may be to adjust this ratio or the number of languages in the dataset to investigate how this affects model training. Secondly, the use of OTC loss was shown to lead to a small, but robust boost to performance. This suggests that methods of mitigating the natural effects of translation have a potential to bridge the gap, so to speak, between models trained on translated data and on datasets in the target language directly. Particularly relevant, Chinese, which is linguistically dissimilar from the majority of languages in the set used here, showed a mixed ability to benefit from training with other languages, but a clearer improvement using OTC loss. This may suggest that OTC loss is able to mitigate structural differences between languages and a future direction for this may be to explore exactly how OTC loss affects individual examples and how other noise-reduction techniques may lead to further gains in model performance.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Putting this together, this is an indication that MT-augmented datasets stand as a good first step for developing multilingual classification models. Given that MT can quickly and efficiently expand an annotated dataset from one language into another and that translated dataset is of sufficient quality to improve over basic interlingual transfer, this technique has great potential to expanding classification tasks to new languages quickly. In addition, OTC loss may be able to slightly but significantly increase the quality of these models with no additional data. All in all, we are confident that the use of MT augmentation is an exciting and interesting topic for future exploration.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This work was carried out as a part of the R&amp;D for GumGumâ€™s Verity product. Special thanks go to the members of the AI team (names, names, names) for their suggestions and audience during brainstorming, development and analysis of this project.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[Araujo et al., 2016] </span>
<span class="ltx_bibblock">
Araujo, Matheus, Julio Reis, Adriano Pereira, and Fabricio Benevenuto.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">An evaluation of machine translation for multilingual sentence-level sentiment analysis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx1.1.1">Proceedings of the 31st annual ACM symposium on applied computing</span>, pages 1140â€“1145.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[Baayen et al., 2008] </span>
<span class="ltx_bibblock">
Baayen, RÂ Harald, DouglasÂ J Davidson, and DouglasÂ M Bates.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock">Mixed-effects modeling with crossed random effects for subjects and items.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx2.1.1">Journal of memory and language</span>, 59(4):390â€“412.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[Barriere and Balahur, 2020] </span>
<span class="ltx_bibblock">
Barriere, Valentin and Alexandra Balahur.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Improving sentiment analysis over non-english tweets using multilingual transformers and automatic translation for data-augmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx3.1.1">arXiv preprint arXiv:2010.03486</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[Brown et al., 2020] </span>
<span class="ltx_bibblock">
Brown, TomÂ B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, DanielÂ M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx4.1.1">CoRR</span>, abs/2005.14165.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[Chung et al., 2022] </span>
<span class="ltx_bibblock">
Chung, HyungÂ Won, LeÂ Hou, Shayne Longpre, Barret Zoph, YiÂ Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, etÂ al.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx5.1.1">arXiv preprint arXiv:2210.11416</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[Conneau et al., 2019] </span>
<span class="ltx_bibblock">
Conneau, Alexis, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx6.1.1">CoRR</span>, abs/1911.02116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[Devlin et al., 2018] </span>
<span class="ltx_bibblock">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">BERT: pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx7.1.1">CoRR</span>, abs/1810.04805.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[Fan et al., 2020] </span>
<span class="ltx_bibblock">
Fan, Angela, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Beyond english-centric multilingual machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx8.1.1">CoRR</span>, abs/2010.11125.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[Ghafoor et al., 2021] </span>
<span class="ltx_bibblock">
Ghafoor, Abdul, AliÂ Shariq Imran, SherÂ Muhammad Daudpota, Zenun Kastrati, Rakhi Batra, MudasirÂ Ahmad Wani, etÂ al.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">The impact of translating resource-rich datasets to low-resource languages through multi-lingual text processing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx9.1.1">IEEE Access</span>, 9:124478â€“124490.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[Jaeger, 2008] </span>
<span class="ltx_bibblock">
Jaeger, TÂ Florian.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock">Categorical data analysis: Away from anovas (transformation or not) and towards logit mixed models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx10.1.1">Journal of memory and language</span>, 59(4):434â€“446.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[Jurafsky and Martin, 2008] </span>
<span class="ltx_bibblock">
Jurafsky, Daniel and JamesÂ H Martin.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock">Speech and language processing: An introduction to speech recognition, computational linguistics and natural language processing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx11.1.1">Upper Saddle River, NJ: Prentice Hall</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[Keung et al., 2020] </span>
<span class="ltx_bibblock">
Keung, Phillip, Yichao Lu, GyÃ¶rgy Szarvas, and NoahÂ A. Smith.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">The multilingual amazon reviews corpus.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx12.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[KocoÅ„ et al., 2023] </span>
<span class="ltx_bibblock">
KocoÅ„, Jan, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika SzydÅ‚o, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, etÂ al.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Chatgpt: Jack of all trades, master of none.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx13.1.1">Information Fusion</span>, page 101861.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[Li et al., 2021] </span>
<span class="ltx_bibblock">
Li, Junnan, RamprasaathÂ R. Selvaraju, AkhileshÂ Deepak Gotmare, ShafiqÂ R. Joty, Caiming Xiong, and Steven C.Â H. Hoi.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Align before fuse: Vision and language representation learning with momentum distillation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx14.1.1">CoRR</span>, abs/2107.07651.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[Li et al., 2022] </span>
<span class="ltx_bibblock">
Li, Junnan, Dongxu Li, Caiming Xiong, and Steven Hoi.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[Loshchilov and Hutter, 2017] </span>
<span class="ltx_bibblock">
Loshchilov, Ilya and Frank Hutter.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Fixing weight decay regularization in adam.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx16.1.1">CoRR</span>, abs/1711.05101.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[Ouyang et al., 2022] </span>
<span class="ltx_bibblock">
Ouyang, Long, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, etÂ al.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx17.1.1">Advances in Neural Information Processing Systems</span>, 35:27730â€“27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[Pan et al., 2011] </span>
<span class="ltx_bibblock">
Pan, Junfeng, Gui-Rong Xue, Yong Yu, and Yang Wang.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock">Cross-lingual sentiment classification via bi-view non-negative matrix tri-factorization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx18.1.1">Advances in Knowledge Discovery and Data Mining: 15th Pacific-Asia Conference, PAKDD 2011, Shenzhen, China, May 24-27, 2011, Proceedings, Part I 15</span>, pages 289â€“300. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[Pires et al., 2019] </span>
<span class="ltx_bibblock">
Pires, Telmo, Eva Schlinger, and Dan Garrette.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">How multilingual is multilingual bert?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx19.1.1">CoRR</span>, abs/1906.01502.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[Sanh et al., 2019] </span>
<span class="ltx_bibblock">
Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx20.1.1">ArXiv</span>, abs/1910.01108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[Shalunts et al., 2016] </span>
<span class="ltx_bibblock">
Shalunts, Gayane, Gerhard Backfried, and Nicolas Commeignes.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">The impact of machine translation on sentiment analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx21.1.1">Data Analytics</span>, 63:51â€“56.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[Stahlberg, 2020] </span>
<span class="ltx_bibblock">
Stahlberg, Felix.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Neural machine translation: A review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx22.1.1">Journal of Artificial Intelligence Research</span>, 69:343â€“418.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[Team et al., 2022] </span>
<span class="ltx_bibblock">
Team, NLLB, MartaÂ R. Costa-jussÃ , James Cross, Onur Ã‡elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, AlÂ Youngblood, Bapi Akula, Loic Barrault, GabrielÂ Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, KaushikÂ Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, NecipÂ Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco GuzmÃ¡n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[Tiedemann and Thottingal, 2020] </span>
<span class="ltx_bibblock">
Tiedemann, JÃ¶rg and Santhosh Thottingal.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">OPUS-MT â€“ building open translation services for the world.

</span>
<span class="ltx_bibblock">In Martins, AndrÃ©, Helena Moniz, Sara Fumega, Bruno Martins, Fernando Batista, Luisa Coheur, Carla Parra, Isabel Trancoso, Marco Turchi, Arianna Bisazza, Joss Moorkens, Ana Guerberof, Mary Nurminen, Lena Marg, and MikelÂ L. Forcada, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx24.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</span>, pages 479â€“480, Lisboa, Portugal, November. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[Vaswani et al., 2017] </span>
<span class="ltx_bibblock">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx25.1.1">Advances in neural information processing systems</span>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[Wei and Pal, 2010] </span>
<span class="ltx_bibblock">
Wei, Bin and Christopher Pal.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock">Cross lingual adaptation: an experiment on sentiment classifications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx26.1.1">Proceedings of the ACL 2010 conference short papers</span>, pages 258â€“262.

</span>
</li>
</ul>
</section>
<figure class="ltx_figure" id="A0.F2"><pre class="ltx_verbatim ltx_font_typewriter" id="A0.F2.1">
              Mixed Linear Model Regression Results
==========================================================
Model:              MixedLM  Dependent Variable:  test_acc
No. Observations:   108      Method:              REML
No. Groups:         18       Scale:               0.0038
Min. group size:    6        Log-Likelihood:      111.7634
Max. group size:    6        Converged:           Yes
Mean group size:    6.0
----------------------------------------------------------
                       Coef.  Std.Err.   z    P&gt;|z| [0.025 0.975]
----------------------------------------------------------
Intercept              0.465    0.024 19.128 0.000  0.418  0.513
otc[T.True]            0.036    0.017  2.105 0.035  0.002  0.069
original_lang[T.en]   -0.020    0.028 -0.714 0.475 -0.074  0.035
original_lang[T.es]   -0.012    0.028 -0.432 0.666 -0.066  0.042
original_lang[T.fr]   -0.015    0.028 -0.555 0.579 -0.070  0.039
original_lang[T.ja]   -0.030    0.028 -1.093 0.274 -0.085  0.024
original_lang[T.zh]   -0.050    0.028 -1.801 0.072 -0.104  0.004
test_lang[T.en]        0.016    0.020  0.762 0.446 -0.025  0.056
test_lang[T.es]        0.003    0.020  0.130 0.897 -0.037  0.043
test_lang[T.fr]       -0.000    0.020 -0.005 0.996 -0.040  0.040
test_lang[T.ja]       -0.061    0.020 -2.972 0.003 -0.101 -0.021
test_lang[T.zh]       -0.068    0.020 -3.336 0.001 -0.108 -0.028
Group Var              0.001    0.009
==========================================================
</pre>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Full model details for MLE model trained to predict F1-micro per laguage. <span class="ltx_text ltx_font_smallcaps" id="A0.F2.3.1">otc</span> has a positive contribution to an increase F1-micro score, even when controlling for variance between languages and model runs.</figcaption>
</figure>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue May 14 18:06:38 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
