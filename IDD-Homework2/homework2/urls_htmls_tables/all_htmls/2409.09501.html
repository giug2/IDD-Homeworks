<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Synthetic4Health: Generating Annotated Synthetic Clinical Letters</title>
<!--Generated on Sat Sep 14 18:09:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.09501v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S1" title="In Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2" title="In Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and Literature Review</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS1" title="In 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Development of Language Models (LMs)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS1.SSS1" title="In 2.1 Development of Language Models (LMs) ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Rule-Based Approach</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS1.SSS2" title="In 2.1 Development of Language Models (LMs) ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Supervised Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS1.SSS3" title="In 2.1 Development of Language Models (LMs) ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Unsupervised Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS2" title="In 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Language Models Applications in Clinical Domain</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS2.SSS1" title="In 2.2 Language Models Applications in Clinical Domain ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Named Entity Recognition (NER)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS2.SSS2" title="In 2.2 Language Models Applications in Clinical Domain ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>De-Identification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS2.SSS3" title="In 2.2 Language Models Applications in Clinical Domain ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Natural Language Generation (NLG)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3" title="In 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Generative Language Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3.SSS1" title="In 2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Transformer and Attention Mechanism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3.SSS2" title="In 2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Encoder-Only Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3.SSS3" title="In 2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.3 </span>Decoder-Only Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3.SSS4" title="In 2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.4 </span>Encoder-Decoder Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3.SSS5" title="In 2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.5 </span>Comparison and Limitations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS4" title="In 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Related Works on Clinical Text Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS4.SSS1" title="In 2.4 Related Works on Clinical Text Generation ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span>LT3: Label to Text Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS4.SSS2" title="In 2.4 Related Works on Clinical Text Generation ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span>Seq2Seq Generation for Medical Dataset Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS4.SSS3" title="In 2.4 Related Works on Clinical Text Generation ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.3 </span>Discharge Summary Generation Using Clinical Guidelines and Human Evaluation Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS4.SSS4" title="In 2.4 Related Works on Clinical Text Generation ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.4 </span>Comparison of Masked and Causal Language Modelling for Text Generation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3" title="In Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodologies and Experimental Design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS1" title="In 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Set</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS2" title="In 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Software and Environment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS3" title="In 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Pre-Processing</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS3.SSS1" title="In 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Merging Dataset and Annotated Entity Recognition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS3.SSS2" title="In 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Splitting Letters into Variable-Length Chunks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS3.SSS3" title="In 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Word Tokenisation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS3.SSS4" title="In 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>Feature Extraction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS4" title="In 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Clinical Letters Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS4.SSS1" title="In 3.4 Clinical Letters Generation ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Encoder-Only Models with Random Masking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS4.SSS2" title="In 3.4 Clinical Letters Generation ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Encoder-Decoder Models with Random Masking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS4.SSS3" title="In 3.4 Clinical Letters Generation ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Different Masking Strategies with Bio_ClinicalBERT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS4.SSS4" title="In 3.4 Clinical Letters Generation ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.4 </span>Determining Variable-Length-Chunk Size with Bio_ClinicalBERT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS5" title="In 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Evaluation Methods</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS5.SSS1" title="In 3.5 Evaluation Methods ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span>Quantitative Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS5.SSS2" title="In 3.5 Evaluation Methods ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.2 </span>Qualitative Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS5.SSS3" title="In 3.5 Evaluation Methods ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.3 </span>Downstream NER Task</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS6" title="In 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Post-Processing</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS6.SSS1" title="In 3.6 Post-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.1 </span>Filling in the blanks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS6.SSS2" title="In 3.6 Post-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.2 </span>Spelling Correction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS7" title="In 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Summary</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4" title="In Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS1" title="In 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Random Masking: Qualitative Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS1.SSS1" title="In 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Encoder-Only Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS1.SSS2" title="In 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Decoder-Only GPT-4o</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS1.SSS3" title="In 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Encoder-Decoder Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS2" title="In 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Random Masking: Quantitative Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS2.SSS1" title="In 4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Sentence-Level Quantitative Results: Encoder-Only Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS2.SSS2" title="In 4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Sentence-Level Quantitative Results: Encoder-Decoder Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS2.SSS3" title="In 4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Quantitative Results on the Full Dataset: Encoder-Only Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS3" title="In 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Variable-Length Chunk Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS4" title="In 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Other Masking Strategies Using Bio_ClinicalBERT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS4.SSS1" title="In 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Masking Only Nouns</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS4.SSS2" title="In 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Masking Only Verbs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS4.SSS3" title="In 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span>Masking Only Stopwords</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS4.SSS4" title="In 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.4 </span>Comparison of Identical Actual Masking Ratios</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS4.SSS5" title="In 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.5 </span>Hybrid Masking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS4.SSS6" title="In 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.6 </span>Comparison with and without (w/o) Entity Preservation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS5" title="In 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Downstream NER Task</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS6" title="In 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Post-Process Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS6.SSS1" title="In 4.6 Post-Process Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.1 </span>Filling in the Blanks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS6.SSS2" title="In 4.6 Post-Process Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.2 </span>Spelling Correction</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S5" title="In Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions and Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S5.SS1" title="In 5 Conclusions and Future Work ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Key Findings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S5.SS2" title="In 5 Conclusions and Future Work ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S5.SS3" title="In 5 Conclusions and Future Work ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Future Work</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps" id="id3.id1">Synthetic4Health</span>: Generating Annotated Synthetic Clinical Letters</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Libo Ren, Samuel Belkadi, Lifeng Han<sup class="ltx_sup" id="id4.2.id1"><span class="ltx_text ltx_font_italic" id="id4.2.id1.1">∗</span></sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id5.3.id2">Warren Del-Pinto</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id6.2.id1">Goran Nenadic
<br class="ltx_break"/></span>The University of Manchester, UK 
<br class="ltx_break"/><sup class="ltx_sup" id="id7.3.id2"><span class="ltx_text ltx_font_italic" id="id7.3.id2.1">∗</span></sup> <span class="ltx_text ltx_font_italic" id="id8.4.id3">corresponding author</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.5.id4">lifeng.han, warren.del-pinto, g.nenadic@manchester.ac.uk</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id10.6.id5">renlibo994, belkadisamuel@gmail.com
</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">Since clinical letters contain sensitive information, clinical-related datasets can not be widely applied in model training, medical research, and teaching. This work aims to generate reliable, various, and de-identified synthetic clinical letters. To achieve this goal, we explored different pre-trained language models (PLMs) for masking and generating text.
After that, we worked on Bio_ClinicalBERT, a high-performing model, and experimented with different masking strategies. Both qualitative and quantitative methods were used for evaluation. Additionally, a downstream task, Named Entity Recognition (NER), was also implemented to assess the usability of these synthetic letters.</p>
<p class="ltx_p" id="id12.id2">The results indicate that 1) encoder-only models outperform encoder-decoder models. 2) Among encoder-only models, those trained on general corpora perform comparably to those trained on clinical data when clinical information is preserved. 3) Additionally, preserving clinical entities and document structure better aligns with our objectives than simply fine-tuning the model. 4) Furthermore, different masking strategies can impact the quality of synthetic clinical letters. Masking stopwords has a positive impact, while masking nouns or verbs has a negative effect. 5) For evaluation, BERTScore should be the primary quantitative evaluation metric, with other metrics serving as supplementary references. 6) Contextual information does not significantly impact the models’ understanding, so the synthetic clinical letters have the potential to replace the original ones in downstream tasks.</p>
<p class="ltx_p" id="id13.id3">Unlike previous research, which focuses more on restoring the original letters by training language models, this project provides a basic framework for generating diverse, de-identified clinical letters. It offers a direction for utilising the model to process real-world clinical letters, thereby helping to expand datasets in the clinical domain. Our codes and trained models are available at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/HECTA-UoM/Synthetic4Health</span></p>
<p class="ltx_p" id="id14.id4"><span class="ltx_text ltx_font_bold" id="id14.id4.1">Keywords:</span> Pre-trained Language Models (PLMs); Encoder-Only Models; Encoder-Decoder Models; Masking and Generating; Named Entity Recognition (NER)</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the development of medical information systems, electronic clinical letters are increasingly used in communication between healthcare departments. These clinical letters typically contain detailed information about patients’ visits, including their symptoms, medical history, medications, etc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx72" title="">Rayner et al., 2020</a>]</cite>. They also often include sensitive information, such as patients’ names, phone numbers, and addresses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx80" title="">Tarur and Prasanna, 2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx83" title="">Tucker et al., 2016</a>]</cite>. As a result, these letters are difficult to share and nearly impossible to use widely in clinical education and research.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In 2018, 325 severe breaches of protected health information were reported by CynergisTek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx2" title="">Abouelmehdi et al., 2018</a>]</cite>. Among these, nearly 3,620,000 patients’ records were at risk <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx2" title="">Abouelmehdi et al., 2018</a>]</cite>. This is just the data from one year—similar privacy breaches are unfortunately common. The most severe hacking incident affected up to 16,612,985 patients <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx2" title="">Abouelmehdi et al., 2018</a>]</cite>. Therefore, generating synthetic letters and applying de-identification techniques seem to be indispensable.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Additionally, due to privacy concerns and access controls, insufficient data is the major challenge of clinical education, medical research, and system development <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx76" title="">Spasic and Nenadic, 2020</a>]</cite>. Some shared datasets offer de-identified annotated data. The MIMIC series is a typical example. These datasets are accessible through PhysioNet. MIMIC-IV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx43" title="">Johnson et al., 2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx44" title="">Johnson et al., 2024a</a>]</cite>, the latest version, contains data from 364,627 patients’ clinical information collected from 2008 to 2019 at a medical centre in Boston. It records details about hospitalisations, demographics, and transfers. Numerous research are based on this shared dataset. Another public dataset series in the clinical domain is i2b2/n2c2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx81" title="">the President and of Harvard College, 2023</a>]</cite>, They are accessible through the DBMI Data Portal. This series includes unstructured clinical notes such as process notes, radiology reports, and discharge summaries, and is published for clinical informatics sharing and NLP tasks challenges.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">However, these sharing datasets are often limited to specific regions and institutions, making them not comprehensive. Consequently, models and medical research outcomes derived from these datasets cannot be widely applied <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx38" title="">Humbert-Droz et al., 2022</a>]</cite>. Therefore, to address the lack of clinical datasets and reduce the workload for clinicians, it is essential to explore available technologies that can automatically generate de-identified clinical letters.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Existing systems generate clinical letters primarily by integrating structured data, while there are not many studies on how to use Natural Language Generation (<span class="ltx_text ltx_font_bold" id="S1.p5.1.1">NLG</span>) models for this task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx39" title="">HÜSKE-KRAUS, 2003</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx6" title="">Amin-Nejad et al., 2020a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx79" title="">Tang et al., 2023</a>]</cite>. NLG attempts to combine clinical knowledge with general linguistic expressions and aims to generate clinical letters that are both readable and medically sound. However, NLG technology is not yet mature enough for widespread use in healthcare systems. Additionally, it faces numerous challenges, including medical accuracy, format normalisation, and de-identification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx39" title="">HÜSKE-KRAUS, 2003</a>]</cite>. Therefore, this investigation focuses on how NLG technology can be used to generate reliable and anonymous clinical letters, which can benefit medical research, clinical education, and clinical decision-making.
The main aim of our work is to <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">generate de-identified clinical letters</span> that can <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">preserve clinical information</span> while <span class="ltx_text ltx_font_italic" id="S1.p5.1.4">differing from the original</span> letters. A brief example of our objective is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">1</span></a>.
Based on this objective, different generation models will be explored as a preliminary attempt.
Then we select the best models and try various techniques to improve the quality of the synthetic letters.
The synthetic letters are evaluated not only with quantitative and qualitative methods, but also in downstream tasks, i.e., Named Entity Recognition (NER). We hope this work will contribute to addressing the challenge of insufficient data in the clinical domain.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="S1.F1.g1" src="extracted/5855287/figures/exampleCh1.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An Example of the Objective: sentence/segment-level generations</figcaption>
</figure>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, this work is centered on the Research Question (RQ): ”How can we generate reliable and diverse clinical letters without including sensitive information?” Specifically, it will answer the following related sub-questions (RQs):</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">How do different models perform in masking and generating clinical letters?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">How should the text be segmented in clinical letter generation?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">How do different masking strategies affect the quality of synthetic letters?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">How can we evaluate the quality of synthetic letters?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">To answer these questions, we explored various LLMs for masking and generating clinical letters, ultimately focusing on one that performed well. The overall <span class="ltx_text ltx_font_bold" id="S1.p7.1.1">highlights</span> of this work are summarised as follows:</p>
</div>
<div class="ltx_para" id="S1.p8">
<ol class="ltx_enumerate" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">Mask and Generate clinical letters using different LLMs at the sentence level.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">Explore methods to improve synthetic clinical letters’ readability and clinical soundness.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">Initially evaluate synthetic letters using both qualitative and quantitative methods.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I2.i4.p1">
<p class="ltx_p" id="S1.I2.i4.p1.1">Apply synthetic letters in downstream tasks.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S1.I2.i5.p1">
<p class="ltx_p" id="S1.I2.i5.p1.1">Explore post-processing methods to further enhance the quality of de-identified letters.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Literature Review</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We first introduce general language models, followed by their applications, especially within the clinical domain. We then present the generative language models based on the Transformer architecture. These models serve as the technical foundation for most modern text generation tasks. Afterward, We review related works, discussing their relevance and how they are connected to ours.
Finally, all quantitative evaluation metrics used in this project are introduced.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Development of Language Models (LMs) </h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The development of Language Models can be divided into three stages: Rule-Based Approach, Supervised Modelling, and Unsupervised Modelling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx14" title="">Boonstra et al., 2024</a>]</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Rule-Based Approach</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">The Rule-Based Approach was first used in 1950s, which marks the beginning of NLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx66" title="">Nadkarni et al., 2011</a>]</cite>. This approach always uses a set of predefined rules, which were written and maintained manually by specialists <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx84" title="">van der Lee et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx74" title="">Satapathy et al., 2017</a>]</cite>. Although it can generate standardised text without being fed with extensive input data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx84" title="">van der Lee et al., 2018</a>]</cite>, there are still numerous limitations. Initially, manually crafted rules are often ambiguous, and the dependencies between different rules increase maintenance costs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx66" title="">Nadkarni et al., 2011</a>]</cite>. Secondly, these stylised models cannot perform well in understanding realistic oral English and ungrammatical text such as clinical discharge records, although these texts are still readable for humans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx66" title="">Nadkarni et al., 2011</a>]</cite>. Thirdly, they are not objective enough, as they are affected by the editors of the rule library. Additionally, they are not flexible enough to deal with special cases. Therefore, the Rule-Based Method is only suitable for analysing and generating highly standardised texts like prescriptions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx84" title="">van der Lee et al., 2018</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Supervised Language Models</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">To address the limitations of the Rule-Based Approach, Supervised Learning has been applied to NLP. The invention of Statistical Machine Translation (SMT) in 1990 marked the rise of supervised NLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx14" title="">Boonstra et al., 2024</a>]</cite>. It learns the correspondence rules between different languages by analysing the input of bilingual texts (parallel corpus) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx16" title="">Brown et al., 1990</a>]</cite>. Supervised NLP models are trained on annotated labels to learn rules automatically. The learned rules will be used in word prediction or text classification.
Hidden Markov Model (HMM) and Conditional Random Field (CRF) are two typical applications of this stage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx75" title="">Sharma et al., 2023</a>]</cite>. Both of them worked by tagging features of the input texts. HMM generates data by statistically analysing word frequencies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx27" title="">Eddy, 1996</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx59" title="">Masuko et al., 1998</a>]</cite>. CRF, however, searches globally and calculates joint probabilities to get an optimal solution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx78" title="">Sutton et al., 2012</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx18" title="">Bundschus et al., 2008</a>]</cite>. Long Short-Term Memory (LSTM) is another typical example of supervised language modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx36" title="">Hochreiter and Schmidhuber, 1997</a>]</cite>.
In the task of text generation, the input should be a set of labelled data or word vector sequences. By minimising the loss between the predicted word vector and the actual word vector, LSTM can capture the dependencies between words in long texts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx73" title="">Santhanam, 2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx33" title="">Graves and Graves, 2012</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1">Although Supervised Language Models performs better than the Rule-Based Approach, domain experts still need to annotate the training dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx14" title="">Boonstra et al., 2024</a>]</cite>. In addition, data in some domains are difficult to collect due to privacy issues (such as <span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p2.1.1">medical</span> and <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p2.1.2">legal</span> domains). This became an ongoing challenge in applying the supervised language models to specific tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Unsupervised Language Models</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">To address the high cost and difficulty of obtaining labelled data, unsupervised Neural Networks are applied to the language modelling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx11" title="">Bengio et al., 2000</a>]</cite>. The popularity of corpora such as Wikipedia and social media provides enough data for unsupervised models’ training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx14" title="">Boonstra et al., 2024</a>]</cite>. Word embedding is a significant technique in this stage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx45" title="">Johnson et al., 2024b</a>]</cite>. <span class="ltx_text ltx_font_bold" id="S2.SS1.SSS3.p1.1.1">Word2Vec</span> represents words using vectors with hundreds of dimensions. The context can be captured by training word vectors in the sliding window. By adjusting the hyperparameters to maximise the conditional probability of the target word, it can learn semantic information accurately <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx64" title="">Mikolov et al., 2013a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx65" title="">Mikolov et al., 2013b</a>]</cite> (e.g. ‘Beijing’-‘China’+‘America’ =<math alttext="&gt;" class="ltx_Math" display="inline" id="S2.SS1.SSS3.p1.1.m1.1"><semantics id="S2.SS1.SSS3.p1.1.m1.1a"><mo id="S2.SS1.SSS3.p1.1.m1.1.1" xref="S2.SS1.SSS3.p1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.1.m1.1b"><gt id="S2.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p1.1.m1.1d">&gt;</annotation></semantics></math> ‘Washington’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx58" title="">Ma and Zhang, 2015</a>]</cite>). After training, each word usually has a fixed word vector regardless of the context in which it appears (known as Static Word Embedding) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx73" title="">Santhanam, 2020</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p2">
<p class="ltx_p" id="S2.SS1.SSS3.p2.1">Unlike word2Vec, <span class="ltx_text ltx_font_bold" id="S2.SS1.SSS3.p2.1.1">BERT</span> and <span class="ltx_text ltx_font_bold" id="S2.SS1.SSS3.p2.1.2">GPT</span> use contextual word embedding. Their word vectors reflect the semantic information and are affected by the context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx20" title="">Camacho-Collados and Pilehvar, 2018</a>]</cite>. BERT focuses on contextual understanding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx91" title="">Zhang et al., 2020</a>]</cite> (e.g. The bank is full of lush willows, where ‘bank’ means the riverside, not the financial institution), while GPTs focus on text generation in a specific context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx53" title="">Liu et al., 2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx52" title="">Li et al., 2024</a>]</cite> (e.g. Prompt: “Do you know Big Ben?” Answer: “Yes, I know Big Ben. It is the nickname for the Great Bell of the Clock located in London.”) Although unsupervised language models have been able to train and understand text proficiently, they still face challenges in practical applications, such as difficulty handling ambiguity and high computing resource consumption. Therefore, language modelling still has a long way to go.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Language Models Applications in Clinical Domain</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Based on the modelling methods mentioned above, a variety of language models have been invented. They play an important role in scientific research and daily life, especially in the field of <span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">healthcare</span>. In this section, I will discuss the <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">clinical language model</span> applications in detail from two aspects: named entity recognition (<span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.3">NER</span>) and natural language generation (<span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.4">NLG</span>).</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Named Entity Recognition (NER)</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">NER was originally designed for text analysis and recognition of named entities, such as dates, organisations, and proper nouns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx34" title="">Grishman and Sundheim, 1996</a>]</cite>. In the clinical domain, NER is used to identify <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p1.1.1">clinical events</span> (e.g. symptoms, drugs, treatment plans, etc.) from unstructured documents with their <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p1.1.2">qualifiers</span> (e.g. chronic, acute, mild), classify them, and extract the relationship <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx15" title="">Bose et al., 2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx48" title="">Kundeti et al., 2016</a>]</cite>. Initially, NER relied on rule-based and machine-learning methods that required extensive manual feature engineering. In 2011, <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p1.1.3">?</span>) used word embeddings and neural networks in NER. Since then, research in NER has shifted to automatic feature extraction.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p2.1.1">SpaCy</span> <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://spacy.io/</span></span></span></span> is an open-source NLP library for tasks like POS tagging and text classification. Additionally, it offers a range of pre-trained NER models. <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p2.1.2">ScispaCy</span> <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://allenai.github.io/scispacy/</span></span></span></span>, a fine-tuned extension of spaCy on medical science datasets, can recognise entities such as “DISEASE”, “CHEMICAL”, and “CELL”, which are essential for medical research.
Although NER is useful in rapidly extracting clinical terms, there are still a lot of challenges, such as <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p2.1.3">non-standardisation</span> (extensive use of abbreviated words in clinical texts), <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p2.1.4">misspellings</span> (due to manual input by medical staff), and <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p2.1.5">ambiguity</span> (often influenced by context, e.g.whether ’back’ refers to an adverb or an anatomical entity) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx15" title="">Bose et al., 2021</a>]</cite>. Existing research mitigates these problems using <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p2.1.6">entity linking</span> (mapping extracted clinical entities to medical repositories such as UMLS and SNOMED). More deep-learning models and text analysis tools are being developed to solve these issues.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>De-Identification</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">The unprocessed clinical text has a risk of personal information leakage. Additionally, manual de-identification is not only prone to errors but also costly. Therefore, research on de-identification is indispensable for the secondary use of clinical data. Typically, de-identification is based on NER models to identify <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.p1.1.1">Protected Health Information (PHI)</span>. Then PHI will be processed by different strategies (such as synonym replacement, removal, or masking) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx13" title="">Berg et al., 2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx12" title="">Berg et al., 2020</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">Similar to NER, early de-identification relied heavily on rule-based systems, machine learning, or hybrid models. Physionet DeID, the VHA Best-of-Breed (BoB), and MITRE’s MIST are three typical examples <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx62" title="">Meystre, 2015</a>]</cite>. However, these algorithms require extensive handcrafted feature engineering. With the development of unsupervised learning, recurrent neural networks (RNNs) and Transformers are widely used in de-identification tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx25" title="">Dernoncourt et al., 2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx47" title="">Kovačević et al., 2024</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p3">
<p class="ltx_p" id="S2.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.p3.1.1">Philter</span>, a Protected Health Information filter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx67" title="">Norgeot et al., 2020</a>]</cite>, is a pioneering system that combines rule-based approaches with state-of-the-art NLP models to identify and remove PHI. Although Philter outperforms many existing tools like Physionet and Scrubber, particularly in recall and F2 score, it still requires large amounts of annotated data for training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx67" title="">Norgeot et al., 2020</a>]</cite>. Additionally, research has shown that while the impact of de-identification on downstream tasks is minimal, it cannot be completely ignored <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx61" title="">Meystre et al., 2014</a>]</cite>. Therefore, performing de-identification without mistakenly removing semantic information is still a challenge in this field.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Natural Language Generation (NLG)</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Both label-to-text and text-to-text generation are components of NLG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx30" title="">Gatt and Krahmer, 2018</a>]</cite>. NLG consists of six primary sub-tasks, covering most of the NLG process. NLG architectures can generally be divided into three categories <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx30" title="">Gatt and Krahmer, 2018</a>]</cite>:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Modular Architectures: </span>This architecture consists of three modules: the Text Planner (responsible for determining the content for generation), the Sentence Planner (which aggregates the synthetic text), and the Realiser (which generates grammatically correct sentences). These modules are closely related to the six sub-tasks, and each module operates independently.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Planning Perspectives: </span>This architecture considers NLG as a planning problem. It generates tokens dynamically based on the objectives, with potential dependencies between different steps.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Integrated or Global Approaches: </span>This is the dominant architecture for NLG, relying on statistical learning and deep learning. Common generative models, such as Transformers and conditional language models, are included in this architecture.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<p class="ltx_p" id="S2.SS2.SSS3.p2.1">In the field of healthcare, NLG applications include document generation and question-answering. Document generation involves discharge letters, diagnostic reports for patients, decision-making suggestions for experts, and personalised patient profiles for administrators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx21" title="">Cawsey et al., 1997</a>]</cite>. Some systems have already been implemented in practice. For instance, PIGLIT generates explanations of clinical terminology for diabetes patients <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx35" title="">Hirst et al., 1997</a>]</cite>, and MAGIC can generate reports for Intensive Care Unit (ICU) patients <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx60" title="">McKeown et al., 1997</a>]</cite>. Question answering is another application of NLG. Tools like chatbots can provide patients with answers to basic healthcare questions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx54" title="">Locke et al., 2021</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p3">
<p class="ltx_p" id="S2.SS2.SSS3.p3.1">Nowadays, NLG in the clinical field focuses on the development and training of transformer-based large language models (LLMs); examples of this work can be seen in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx6" title="">Amin-Nejad et al., 2020a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx57" title="">Luo et al., 2022</a>]</cite>. These models perform well in specific domains such as semantic query <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx46" title="">Kong et al., 2022</a>]</cite> and electronic health records (<span class="ltx_text ltx_font_bold" id="S2.SS2.SSS3.p3.1.1">EHRs</span>) generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx50" title="">Lee, 2018</a>]</cite>. However, very few systems can reliably produce concise, readable, and clinically sound reports across multiple sub-domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx21" title="">Cawsey et al., 1997</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Generative Language Models</h3>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Transformer and Attention Mechanism</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">Although Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) are effective at semantic understanding, their recursive structure not only prevents parallel computation, but also makes them prone to gradient vanishing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx31" title="">Gillioz et al., 2020</a>]</cite>. The introduction of the Transformer in 2017 addressed this issue by replacing the recurrent structure with a multi-head attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx85" title="">Vaswani et al., 2017</a>]</cite>. Since then, most deep learning models have been based on the Transformer. Transformer architecture is based on an encoder-decoder model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx85" title="">Vaswani et al., 2017</a>]</cite>. To understand this, we first need to overview Auto-Regressive Models and the Multi-Head Attention Mechanism.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.2"><span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p2.2.1">Auto-Regressive Models</span>
Predictions for each Auto-Regressive Model token depend on the previous output. Therefore, it can only access the preceding tokens and operate iteratively. When the input sequence is <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p2.2.2">X</span>, the Auto-Regressive Model aims to train parameters <math alttext="\mathit{\theta}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p2.1.m1.1"><semantics id="S2.SS3.SSS1.p2.1.m1.1a"><mi id="S2.SS3.SSS1.p2.1.m1.1.1" xref="S2.SS3.SSS1.p2.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p2.1.m1.1b"><ci id="S2.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p2.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p2.1.m1.1c">\mathit{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p2.1.m1.1d">italic_θ</annotation></semantics></math> to maximise the log-likelihood of the conditional probability <math alttext="\mathit{P}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p2.2.m2.1"><semantics id="S2.SS3.SSS1.p2.2.m2.1a"><mi id="S2.SS3.SSS1.p2.2.m2.1.1" xref="S2.SS3.SSS1.p2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p2.2.m2.1b"><ci id="S2.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS3.SSS1.p2.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p2.2.m2.1c">\mathit{P}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p2.2.m2.1d">italic_P</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx85" title="">Vaswani et al., 2017</a>]</cite>.</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L(X)=\sum_{i}\log P(x_{i}\mid x_{i-k},\ldots,x_{i-1};\Theta)" class="ltx_Math" display="block" id="S2.E1.m1.4"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml"><mrow id="S2.E1.m1.4.4.3" xref="S2.E1.m1.4.4.3.cmml"><mi id="S2.E1.m1.4.4.3.2" xref="S2.E1.m1.4.4.3.2.cmml">L</mi><mo id="S2.E1.m1.4.4.3.1" xref="S2.E1.m1.4.4.3.1.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.3.3.2" xref="S2.E1.m1.4.4.3.cmml"><mo id="S2.E1.m1.4.4.3.3.2.1" stretchy="false" xref="S2.E1.m1.4.4.3.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">X</mi><mo id="S2.E1.m1.4.4.3.3.2.2" stretchy="false" xref="S2.E1.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.4.2" rspace="0.111em" xref="S2.E1.m1.4.4.2.cmml">=</mo><mrow id="S2.E1.m1.4.4.1" xref="S2.E1.m1.4.4.1.cmml"><munder id="S2.E1.m1.4.4.1.2" xref="S2.E1.m1.4.4.1.2.cmml"><mo id="S2.E1.m1.4.4.1.2.2" movablelimits="false" xref="S2.E1.m1.4.4.1.2.2.cmml">∑</mo><mi id="S2.E1.m1.4.4.1.2.3" xref="S2.E1.m1.4.4.1.2.3.cmml">i</mi></munder><mrow id="S2.E1.m1.4.4.1.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1.3" xref="S2.E1.m1.4.4.1.1.3.cmml"><mi id="S2.E1.m1.4.4.1.1.3.1" xref="S2.E1.m1.4.4.1.1.3.1.cmml">log</mi><mo id="S2.E1.m1.4.4.1.1.3a" lspace="0.167em" xref="S2.E1.m1.4.4.1.1.3.cmml">⁡</mo><mi id="S2.E1.m1.4.4.1.1.3.2" xref="S2.E1.m1.4.4.1.1.3.2.cmml">P</mi></mrow><mo id="S2.E1.m1.4.4.1.1.2" xref="S2.E1.m1.4.4.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.cmml"><mo id="S2.E1.m1.4.4.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.cmml"><msub id="S2.E1.m1.4.4.1.1.1.1.1.4" xref="S2.E1.m1.4.4.1.1.1.1.1.4.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.4.2" xref="S2.E1.m1.4.4.1.1.1.1.1.4.2.cmml">x</mi><mi id="S2.E1.m1.4.4.1.1.1.1.1.4.3" xref="S2.E1.m1.4.4.1.1.1.1.1.4.3.cmml">i</mi></msub><mo id="S2.E1.m1.4.4.1.1.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.1.1.3.cmml">∣</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.2.2" xref="S2.E1.m1.4.4.1.1.1.1.1.2.3.cmml"><msub id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></mrow></msub><mo id="S2.E1.m1.4.4.1.1.1.1.1.2.2.3" xref="S2.E1.m1.4.4.1.1.1.1.1.2.3.cmml">,</mo><mi id="S2.E1.m1.2.2" mathvariant="normal" xref="S2.E1.m1.2.2.cmml">…</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.2.2.4" xref="S2.E1.m1.4.4.1.1.1.1.1.2.3.cmml">,</mo><msub id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.2.cmml">x</mi><mrow id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.2.cmml">i</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.1" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.1.cmml">−</mo><mn id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.3" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.E1.m1.4.4.1.1.1.1.1.2.2.5" xref="S2.E1.m1.4.4.1.1.1.1.1.2.3.cmml">;</mo><mi id="S2.E1.m1.3.3" mathvariant="normal" xref="S2.E1.m1.3.3.cmml">Θ</mi></mrow></mrow><mo id="S2.E1.m1.4.4.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4"><eq id="S2.E1.m1.4.4.2.cmml" xref="S2.E1.m1.4.4.2"></eq><apply id="S2.E1.m1.4.4.3.cmml" xref="S2.E1.m1.4.4.3"><times id="S2.E1.m1.4.4.3.1.cmml" xref="S2.E1.m1.4.4.3.1"></times><ci id="S2.E1.m1.4.4.3.2.cmml" xref="S2.E1.m1.4.4.3.2">𝐿</ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑋</ci></apply><apply id="S2.E1.m1.4.4.1.cmml" xref="S2.E1.m1.4.4.1"><apply id="S2.E1.m1.4.4.1.2.cmml" xref="S2.E1.m1.4.4.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.2.1.cmml" xref="S2.E1.m1.4.4.1.2">subscript</csymbol><sum id="S2.E1.m1.4.4.1.2.2.cmml" xref="S2.E1.m1.4.4.1.2.2"></sum><ci id="S2.E1.m1.4.4.1.2.3.cmml" xref="S2.E1.m1.4.4.1.2.3">𝑖</ci></apply><apply id="S2.E1.m1.4.4.1.1.cmml" xref="S2.E1.m1.4.4.1.1"><times id="S2.E1.m1.4.4.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2"></times><apply id="S2.E1.m1.4.4.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.3"><log id="S2.E1.m1.4.4.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.3.1"></log><ci id="S2.E1.m1.4.4.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2">𝑃</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.4.4.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.3">conditional</csymbol><apply id="S2.E1.m1.4.4.1.1.1.1.1.4.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.1.4.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.4">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.1.4.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.4.2">𝑥</ci><ci id="S2.E1.m1.4.4.1.1.1.1.1.4.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.4.3">𝑖</ci></apply><list id="S2.E1.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2"><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3"><minus id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3">𝑘</ci></apply></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">…</ci><apply id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.2">𝑥</ci><apply id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3"><minus id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.1"></minus><ci id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.2">𝑖</ci><cn id="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="S2.E1.m1.4.4.1.1.1.1.1.2.2.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">Θ</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">L(X)=\sum_{i}\log P(x_{i}\mid x_{i-k},\ldots,x_{i-1};\Theta)</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.4d">italic_L ( italic_X ) = ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∣ italic_x start_POSTSUBSCRIPT italic_i - italic_k end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; roman_Θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p3">
<p class="ltx_p" id="S2.SS3.SSS1.p3.7"><span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p3.7.1">Multi-Head Attention Mechanism</span>
Attention mechanism was initially proposed by <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p3.7.2">?</span>). It can not only focus on the element being processed, but also capture the context dependence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx85" title="">Vaswani et al., 2017</a>]</cite>. Multi-head attention consists of several single-head attention (Scaled Dot-Product Attention) layers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx85" title="">Vaswani et al., 2017</a>]</cite>.
Each word in the input sequence is converted into a high-dimensional vector representing semantic information by word embedding. These vectors pass linear transformation layers and get vectors for queries (<math alttext="\mathit{Q}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p3.1.m1.1"><semantics id="S2.SS3.SSS1.p3.1.m1.1a"><mi id="S2.SS3.SSS1.p3.1.m1.1.1" xref="S2.SS3.SSS1.p3.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p3.1.m1.1b"><ci id="S2.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p3.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p3.1.m1.1c">\mathit{Q}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p3.1.m1.1d">italic_Q</annotation></semantics></math>), keys (<math alttext="\mathit{K}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p3.2.m2.1"><semantics id="S2.SS3.SSS1.p3.2.m2.1a"><mi id="S2.SS3.SSS1.p3.2.m2.1.1" xref="S2.SS3.SSS1.p3.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p3.2.m2.1b"><ci id="S2.SS3.SSS1.p3.2.m2.1.1.cmml" xref="S2.SS3.SSS1.p3.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p3.2.m2.1c">\mathit{K}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p3.2.m2.1d">italic_K</annotation></semantics></math>), and values (<math alttext="\mathit{V}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p3.3.m3.1"><semantics id="S2.SS3.SSS1.p3.3.m3.1a"><mi id="S2.SS3.SSS1.p3.3.m3.1.1" xref="S2.SS3.SSS1.p3.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p3.3.m3.1b"><ci id="S2.SS3.SSS1.p3.3.m3.1.1.cmml" xref="S2.SS3.SSS1.p3.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p3.3.m3.1c">\mathit{V}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p3.3.m3.1d">italic_V</annotation></semantics></math>). For each word, <math alttext="\mathit{Q}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p3.4.m4.1"><semantics id="S2.SS3.SSS1.p3.4.m4.1a"><mi id="S2.SS3.SSS1.p3.4.m4.1.1" xref="S2.SS3.SSS1.p3.4.m4.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p3.4.m4.1b"><ci id="S2.SS3.SSS1.p3.4.m4.1.1.cmml" xref="S2.SS3.SSS1.p3.4.m4.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p3.4.m4.1c">\mathit{Q}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p3.4.m4.1d">italic_Q</annotation></semantics></math>, <math alttext="\mathit{K}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p3.5.m5.1"><semantics id="S2.SS3.SSS1.p3.5.m5.1a"><mi id="S2.SS3.SSS1.p3.5.m5.1.1" xref="S2.SS3.SSS1.p3.5.m5.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p3.5.m5.1b"><ci id="S2.SS3.SSS1.p3.5.m5.1.1.cmml" xref="S2.SS3.SSS1.p3.5.m5.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p3.5.m5.1c">\mathit{K}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p3.5.m5.1d">italic_K</annotation></semantics></math>, and <math alttext="\mathit{V}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p3.6.m6.1"><semantics id="S2.SS3.SSS1.p3.6.m6.1a"><mi id="S2.SS3.SSS1.p3.6.m6.1.1" xref="S2.SS3.SSS1.p3.6.m6.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p3.6.m6.1b"><ci id="S2.SS3.SSS1.p3.6.m6.1.1.cmml" xref="S2.SS3.SSS1.p3.6.m6.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p3.6.m6.1c">\mathit{V}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p3.6.m6.1d">italic_V</annotation></semantics></math> are inputs to this single-head attention layer.
The importance score of this word is calculated, and <math alttext="\mathit{V}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p3.7.m7.1"><semantics id="S2.SS3.SSS1.p3.7.m7.1a"><mi id="S2.SS3.SSS1.p3.7.m7.1.1" xref="S2.SS3.SSS1.p3.7.m7.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p3.7.m7.1b"><ci id="S2.SS3.SSS1.p3.7.m7.1.1.cmml" xref="S2.SS3.SSS1.p3.7.m7.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p3.7.m7.1c">\mathit{V}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p3.7.m7.1d">italic_V</annotation></semantics></math> corresponding to this word should be multiplied to get the output of this head (called Attention). Finally, outputs from all layers are concatenated to form a larger vector, which is the input to a feed-forward neural network (also the output of the multi-head attention layer) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx85" title="">Vaswani et al., 2017</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p4">
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V" class="ltx_Math" display="block" id="S2.E2.m1.4"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.5" xref="S2.E2.m1.4.5.cmml"><mrow id="S2.E2.m1.4.5.2" xref="S2.E2.m1.4.5.2.cmml"><mtext id="S2.E2.m1.4.5.2.2" xref="S2.E2.m1.4.5.2.2a.cmml">Attention</mtext><mo id="S2.E2.m1.4.5.2.1" xref="S2.E2.m1.4.5.2.1.cmml">⁢</mo><mrow id="S2.E2.m1.4.5.2.3.2" xref="S2.E2.m1.4.5.2.3.1.cmml"><mo id="S2.E2.m1.4.5.2.3.2.1" stretchy="false" xref="S2.E2.m1.4.5.2.3.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">Q</mi><mo id="S2.E2.m1.4.5.2.3.2.2" xref="S2.E2.m1.4.5.2.3.1.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">K</mi><mo id="S2.E2.m1.4.5.2.3.2.3" xref="S2.E2.m1.4.5.2.3.1.cmml">,</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">V</mi><mo id="S2.E2.m1.4.5.2.3.2.4" stretchy="false" xref="S2.E2.m1.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.5.1" xref="S2.E2.m1.4.5.1.cmml">=</mo><mrow id="S2.E2.m1.4.5.3" xref="S2.E2.m1.4.5.3.cmml"><mtext id="S2.E2.m1.4.5.3.2" xref="S2.E2.m1.4.5.3.2a.cmml">softmax</mtext><mo id="S2.E2.m1.4.5.3.1" xref="S2.E2.m1.4.5.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.4.5.3.3.2" xref="S2.E2.m1.4.4.cmml"><mo id="S2.E2.m1.4.5.3.3.2.1" xref="S2.E2.m1.4.4.cmml">(</mo><mfrac id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><mrow id="S2.E2.m1.4.4.2" xref="S2.E2.m1.4.4.2.cmml"><mi id="S2.E2.m1.4.4.2.2" xref="S2.E2.m1.4.4.2.2.cmml">Q</mi><mo id="S2.E2.m1.4.4.2.1" xref="S2.E2.m1.4.4.2.1.cmml">⁢</mo><msup id="S2.E2.m1.4.4.2.3" xref="S2.E2.m1.4.4.2.3.cmml"><mi id="S2.E2.m1.4.4.2.3.2" xref="S2.E2.m1.4.4.2.3.2.cmml">K</mi><mi id="S2.E2.m1.4.4.2.3.3" xref="S2.E2.m1.4.4.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S2.E2.m1.4.4.3" xref="S2.E2.m1.4.4.3.cmml"><msub id="S2.E2.m1.4.4.3.2" xref="S2.E2.m1.4.4.3.2.cmml"><mi id="S2.E2.m1.4.4.3.2.2" xref="S2.E2.m1.4.4.3.2.2.cmml">d</mi><mi id="S2.E2.m1.4.4.3.2.3" xref="S2.E2.m1.4.4.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo id="S2.E2.m1.4.5.3.3.2.2" xref="S2.E2.m1.4.4.cmml">)</mo></mrow><mo id="S2.E2.m1.4.5.3.1a" xref="S2.E2.m1.4.5.3.1.cmml">⁢</mo><mi id="S2.E2.m1.4.5.3.4" xref="S2.E2.m1.4.5.3.4.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.5.cmml" xref="S2.E2.m1.4.5"><eq id="S2.E2.m1.4.5.1.cmml" xref="S2.E2.m1.4.5.1"></eq><apply id="S2.E2.m1.4.5.2.cmml" xref="S2.E2.m1.4.5.2"><times id="S2.E2.m1.4.5.2.1.cmml" xref="S2.E2.m1.4.5.2.1"></times><ci id="S2.E2.m1.4.5.2.2a.cmml" xref="S2.E2.m1.4.5.2.2"><mtext id="S2.E2.m1.4.5.2.2.cmml" xref="S2.E2.m1.4.5.2.2">Attention</mtext></ci><vector id="S2.E2.m1.4.5.2.3.1.cmml" xref="S2.E2.m1.4.5.2.3.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑄</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝐾</ci><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">𝑉</ci></vector></apply><apply id="S2.E2.m1.4.5.3.cmml" xref="S2.E2.m1.4.5.3"><times id="S2.E2.m1.4.5.3.1.cmml" xref="S2.E2.m1.4.5.3.1"></times><ci id="S2.E2.m1.4.5.3.2a.cmml" xref="S2.E2.m1.4.5.3.2"><mtext id="S2.E2.m1.4.5.3.2.cmml" xref="S2.E2.m1.4.5.3.2">softmax</mtext></ci><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.5.3.3.2"><divide id="S2.E2.m1.4.4.1.cmml" xref="S2.E2.m1.4.5.3.3.2"></divide><apply id="S2.E2.m1.4.4.2.cmml" xref="S2.E2.m1.4.4.2"><times id="S2.E2.m1.4.4.2.1.cmml" xref="S2.E2.m1.4.4.2.1"></times><ci id="S2.E2.m1.4.4.2.2.cmml" xref="S2.E2.m1.4.4.2.2">𝑄</ci><apply id="S2.E2.m1.4.4.2.3.cmml" xref="S2.E2.m1.4.4.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.2.3.1.cmml" xref="S2.E2.m1.4.4.2.3">superscript</csymbol><ci id="S2.E2.m1.4.4.2.3.2.cmml" xref="S2.E2.m1.4.4.2.3.2">𝐾</ci><ci id="S2.E2.m1.4.4.2.3.3.cmml" xref="S2.E2.m1.4.4.2.3.3">𝑇</ci></apply></apply><apply id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.3"><root id="S2.E2.m1.4.4.3a.cmml" xref="S2.E2.m1.4.4.3"></root><apply id="S2.E2.m1.4.4.3.2.cmml" xref="S2.E2.m1.4.4.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.3.2.1.cmml" xref="S2.E2.m1.4.4.3.2">subscript</csymbol><ci id="S2.E2.m1.4.4.3.2.2.cmml" xref="S2.E2.m1.4.4.3.2.2">𝑑</ci><ci id="S2.E2.m1.4.4.3.2.3.cmml" xref="S2.E2.m1.4.4.3.2.3">𝑘</ci></apply></apply></apply><ci id="S2.E2.m1.4.5.3.4.cmml" xref="S2.E2.m1.4.5.3.4">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.4d">Attention ( italic_Q , italic_K , italic_V ) = softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p5">
<p class="ltx_p" id="S2.SS3.SSS1.p5.8"><span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p5.8.1">Transformer and Pre-training Language Models (PLMs)</span>
<span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p5.8.2">Transformer</span> consists of an encoder and a decoder. The Auto-Regressive Model is the basis of the decoder. When the input sequence is <math alttext="X=(x_{1},\ldots,x_{N})" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p5.1.m1.3"><semantics id="S2.SS3.SSS1.p5.1.m1.3a"><mrow id="S2.SS3.SSS1.p5.1.m1.3.3" xref="S2.SS3.SSS1.p5.1.m1.3.3.cmml"><mi id="S2.SS3.SSS1.p5.1.m1.3.3.4" xref="S2.SS3.SSS1.p5.1.m1.3.3.4.cmml">X</mi><mo id="S2.SS3.SSS1.p5.1.m1.3.3.3" xref="S2.SS3.SSS1.p5.1.m1.3.3.3.cmml">=</mo><mrow id="S2.SS3.SSS1.p5.1.m1.3.3.2.2" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.3.cmml"><mo id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.3" stretchy="false" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.3.cmml">(</mo><msub id="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1" xref="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.2" xref="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.3" xref="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.4" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.3.cmml">,</mo><mi id="S2.SS3.SSS1.p5.1.m1.1.1" mathvariant="normal" xref="S2.SS3.SSS1.p5.1.m1.1.1.cmml">…</mi><mo id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.5" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.3.cmml">,</mo><msub id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.2" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.2.cmml">x</mi><mi id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.3" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.3.cmml">N</mi></msub><mo id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.6" stretchy="false" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p5.1.m1.3b"><apply id="S2.SS3.SSS1.p5.1.m1.3.3.cmml" xref="S2.SS3.SSS1.p5.1.m1.3.3"><eq id="S2.SS3.SSS1.p5.1.m1.3.3.3.cmml" xref="S2.SS3.SSS1.p5.1.m1.3.3.3"></eq><ci id="S2.SS3.SSS1.p5.1.m1.3.3.4.cmml" xref="S2.SS3.SSS1.p5.1.m1.3.3.4">𝑋</ci><vector id="S2.SS3.SSS1.p5.1.m1.3.3.2.3.cmml" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.2"><apply id="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.cmml" xref="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.2">𝑥</ci><cn id="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS3.SSS1.p5.1.m1.2.2.1.1.1.3">1</cn></apply><ci id="S2.SS3.SSS1.p5.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p5.1.m1.1.1">…</ci><apply id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.cmml" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.2">𝑥</ci><ci id="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.3.cmml" xref="S2.SS3.SSS1.p5.1.m1.3.3.2.2.2.3">𝑁</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p5.1.m1.3c">X=(x_{1},\ldots,x_{N})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p5.1.m1.3d">italic_X = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )</annotation></semantics></math>, output sequence is <math alttext="Y_{M}=(y_{1},\ldots,y_{M})" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p5.2.m2.3"><semantics id="S2.SS3.SSS1.p5.2.m2.3a"><mrow id="S2.SS3.SSS1.p5.2.m2.3.3" xref="S2.SS3.SSS1.p5.2.m2.3.3.cmml"><msub id="S2.SS3.SSS1.p5.2.m2.3.3.4" xref="S2.SS3.SSS1.p5.2.m2.3.3.4.cmml"><mi id="S2.SS3.SSS1.p5.2.m2.3.3.4.2" xref="S2.SS3.SSS1.p5.2.m2.3.3.4.2.cmml">Y</mi><mi id="S2.SS3.SSS1.p5.2.m2.3.3.4.3" xref="S2.SS3.SSS1.p5.2.m2.3.3.4.3.cmml">M</mi></msub><mo id="S2.SS3.SSS1.p5.2.m2.3.3.3" xref="S2.SS3.SSS1.p5.2.m2.3.3.3.cmml">=</mo><mrow id="S2.SS3.SSS1.p5.2.m2.3.3.2.2" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.3.cmml"><mo id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.3" stretchy="false" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.3.cmml">(</mo><msub id="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1" xref="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.2" xref="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.2.cmml">y</mi><mn id="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.3" xref="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.4" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.3.cmml">,</mo><mi id="S2.SS3.SSS1.p5.2.m2.1.1" mathvariant="normal" xref="S2.SS3.SSS1.p5.2.m2.1.1.cmml">…</mi><mo id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.5" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.3.cmml">,</mo><msub id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.cmml"><mi id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.2" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.2.cmml">y</mi><mi id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.3" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.3.cmml">M</mi></msub><mo id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.6" stretchy="false" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p5.2.m2.3b"><apply id="S2.SS3.SSS1.p5.2.m2.3.3.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3"><eq id="S2.SS3.SSS1.p5.2.m2.3.3.3.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3.3"></eq><apply id="S2.SS3.SSS1.p5.2.m2.3.3.4.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3.4"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.2.m2.3.3.4.1.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3.4">subscript</csymbol><ci id="S2.SS3.SSS1.p5.2.m2.3.3.4.2.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3.4.2">𝑌</ci><ci id="S2.SS3.SSS1.p5.2.m2.3.3.4.3.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3.4.3">𝑀</ci></apply><vector id="S2.SS3.SSS1.p5.2.m2.3.3.2.3.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.2"><apply id="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.cmml" xref="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.2">𝑦</ci><cn id="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS3.SSS1.p5.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="S2.SS3.SSS1.p5.2.m2.1.1.cmml" xref="S2.SS3.SSS1.p5.2.m2.1.1">…</ci><apply id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.1.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.2">𝑦</ci><ci id="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.3.cmml" xref="S2.SS3.SSS1.p5.2.m2.3.3.2.2.2.3">𝑀</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p5.2.m2.3c">Y_{M}=(y_{1},\ldots,y_{M})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p5.2.m2.3d">italic_Y start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT = ( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT )</annotation></semantics></math>, the model can learn a latent feature representation <math alttext="Z=(z_{1},\ldots,z_{N})" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p5.3.m3.3"><semantics id="S2.SS3.SSS1.p5.3.m3.3a"><mrow id="S2.SS3.SSS1.p5.3.m3.3.3" xref="S2.SS3.SSS1.p5.3.m3.3.3.cmml"><mi id="S2.SS3.SSS1.p5.3.m3.3.3.4" xref="S2.SS3.SSS1.p5.3.m3.3.3.4.cmml">Z</mi><mo id="S2.SS3.SSS1.p5.3.m3.3.3.3" xref="S2.SS3.SSS1.p5.3.m3.3.3.3.cmml">=</mo><mrow id="S2.SS3.SSS1.p5.3.m3.3.3.2.2" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.3.cmml"><mo id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.3" stretchy="false" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.3.cmml">(</mo><msub id="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1" xref="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.cmml"><mi id="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.2" xref="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.2.cmml">z</mi><mn id="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.3" xref="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.4" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.3.cmml">,</mo><mi id="S2.SS3.SSS1.p5.3.m3.1.1" mathvariant="normal" xref="S2.SS3.SSS1.p5.3.m3.1.1.cmml">…</mi><mo id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.5" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.3.cmml">,</mo><msub id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.cmml"><mi id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.2" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.2.cmml">z</mi><mi id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.3" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.3.cmml">N</mi></msub><mo id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.6" stretchy="false" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p5.3.m3.3b"><apply id="S2.SS3.SSS1.p5.3.m3.3.3.cmml" xref="S2.SS3.SSS1.p5.3.m3.3.3"><eq id="S2.SS3.SSS1.p5.3.m3.3.3.3.cmml" xref="S2.SS3.SSS1.p5.3.m3.3.3.3"></eq><ci id="S2.SS3.SSS1.p5.3.m3.3.3.4.cmml" xref="S2.SS3.SSS1.p5.3.m3.3.3.4">𝑍</ci><vector id="S2.SS3.SSS1.p5.3.m3.3.3.2.3.cmml" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.2"><apply id="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.cmml" xref="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.1.cmml" xref="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.2.cmml" xref="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.2">𝑧</ci><cn id="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS3.SSS1.p5.3.m3.2.2.1.1.1.3">1</cn></apply><ci id="S2.SS3.SSS1.p5.3.m3.1.1.cmml" xref="S2.SS3.SSS1.p5.3.m3.1.1">…</ci><apply id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.cmml" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.1.cmml" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.2.cmml" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.2">𝑧</ci><ci id="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.3.cmml" xref="S2.SS3.SSS1.p5.3.m3.3.3.2.2.2.3">𝑁</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p5.3.m3.3c">Z=(z_{1},\ldots,z_{N})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p5.3.m3.3d">italic_Z = ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )</annotation></semantics></math> from <math alttext="X" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p5.4.m4.1"><semantics id="S2.SS3.SSS1.p5.4.m4.1a"><mi id="S2.SS3.SSS1.p5.4.m4.1.1" xref="S2.SS3.SSS1.p5.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p5.4.m4.1b"><ci id="S2.SS3.SSS1.p5.4.m4.1.1.cmml" xref="S2.SS3.SSS1.p5.4.m4.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p5.4.m4.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p5.4.m4.1d">italic_X</annotation></semantics></math> to <math alttext="Y" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p5.5.m5.1"><semantics id="S2.SS3.SSS1.p5.5.m5.1a"><mi id="S2.SS3.SSS1.p5.5.m5.1.1" xref="S2.SS3.SSS1.p5.5.m5.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p5.5.m5.1b"><ci id="S2.SS3.SSS1.p5.5.m5.1.1.cmml" xref="S2.SS3.SSS1.p5.5.m5.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p5.5.m5.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p5.5.m5.1d">italic_Y</annotation></semantics></math>. The generation of each new element <math alttext="Y_{M}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p5.6.m6.1"><semantics id="S2.SS3.SSS1.p5.6.m6.1a"><msub id="S2.SS3.SSS1.p5.6.m6.1.1" xref="S2.SS3.SSS1.p5.6.m6.1.1.cmml"><mi id="S2.SS3.SSS1.p5.6.m6.1.1.2" xref="S2.SS3.SSS1.p5.6.m6.1.1.2.cmml">Y</mi><mi id="S2.SS3.SSS1.p5.6.m6.1.1.3" xref="S2.SS3.SSS1.p5.6.m6.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p5.6.m6.1b"><apply id="S2.SS3.SSS1.p5.6.m6.1.1.cmml" xref="S2.SS3.SSS1.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.6.m6.1.1.1.cmml" xref="S2.SS3.SSS1.p5.6.m6.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.p5.6.m6.1.1.2.cmml" xref="S2.SS3.SSS1.p5.6.m6.1.1.2">𝑌</ci><ci id="S2.SS3.SSS1.p5.6.m6.1.1.3.cmml" xref="S2.SS3.SSS1.p5.6.m6.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p5.6.m6.1c">Y_{M}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p5.6.m6.1d">italic_Y start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> relies on the generated sequence <math alttext="Y_{M-1}=(y_{1},\ldots,y_{M-1})" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p5.7.m7.3"><semantics id="S2.SS3.SSS1.p5.7.m7.3a"><mrow id="S2.SS3.SSS1.p5.7.m7.3.3" xref="S2.SS3.SSS1.p5.7.m7.3.3.cmml"><msub id="S2.SS3.SSS1.p5.7.m7.3.3.4" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.cmml"><mi id="S2.SS3.SSS1.p5.7.m7.3.3.4.2" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.2.cmml">Y</mi><mrow id="S2.SS3.SSS1.p5.7.m7.3.3.4.3" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.3.cmml"><mi id="S2.SS3.SSS1.p5.7.m7.3.3.4.3.2" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.3.2.cmml">M</mi><mo id="S2.SS3.SSS1.p5.7.m7.3.3.4.3.1" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.3.1.cmml">−</mo><mn id="S2.SS3.SSS1.p5.7.m7.3.3.4.3.3" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS3.SSS1.p5.7.m7.3.3.3" xref="S2.SS3.SSS1.p5.7.m7.3.3.3.cmml">=</mo><mrow id="S2.SS3.SSS1.p5.7.m7.3.3.2.2" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.3.cmml"><mo id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.3" stretchy="false" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.3.cmml">(</mo><msub id="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1" xref="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.cmml"><mi id="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.2" xref="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.2.cmml">y</mi><mn id="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.3" xref="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.4" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.3.cmml">,</mo><mi id="S2.SS3.SSS1.p5.7.m7.1.1" mathvariant="normal" xref="S2.SS3.SSS1.p5.7.m7.1.1.cmml">…</mi><mo id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.5" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.3.cmml">,</mo><msub id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.cmml"><mi id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.2" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.2.cmml">y</mi><mrow id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.cmml"><mi id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.2" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.2.cmml">M</mi><mo id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.1" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.1.cmml">−</mo><mn id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.3" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.6" stretchy="false" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p5.7.m7.3b"><apply id="S2.SS3.SSS1.p5.7.m7.3.3.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3"><eq id="S2.SS3.SSS1.p5.7.m7.3.3.3.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.3"></eq><apply id="S2.SS3.SSS1.p5.7.m7.3.3.4.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.4"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.7.m7.3.3.4.1.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.4">subscript</csymbol><ci id="S2.SS3.SSS1.p5.7.m7.3.3.4.2.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.2">𝑌</ci><apply id="S2.SS3.SSS1.p5.7.m7.3.3.4.3.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.3"><minus id="S2.SS3.SSS1.p5.7.m7.3.3.4.3.1.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.3.1"></minus><ci id="S2.SS3.SSS1.p5.7.m7.3.3.4.3.2.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.3.2">𝑀</ci><cn id="S2.SS3.SSS1.p5.7.m7.3.3.4.3.3.cmml" type="integer" xref="S2.SS3.SSS1.p5.7.m7.3.3.4.3.3">1</cn></apply></apply><vector id="S2.SS3.SSS1.p5.7.m7.3.3.2.3.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2"><apply id="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.cmml" xref="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.1.cmml" xref="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.2.cmml" xref="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.2">𝑦</ci><cn id="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS3.SSS1.p5.7.m7.2.2.1.1.1.3">1</cn></apply><ci id="S2.SS3.SSS1.p5.7.m7.1.1.cmml" xref="S2.SS3.SSS1.p5.7.m7.1.1">…</ci><apply id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.1.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2">subscript</csymbol><ci id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.2.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.2">𝑦</ci><apply id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3"><minus id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.1.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.1"></minus><ci id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.2.cmml" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.2">𝑀</ci><cn id="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.3.cmml" type="integer" xref="S2.SS3.SSS1.p5.7.m7.3.3.2.2.2.3.3">1</cn></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p5.7.m7.3c">Y_{M-1}=(y_{1},\ldots,y_{M-1})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p5.7.m7.3d">italic_Y start_POSTSUBSCRIPT italic_M - 1 end_POSTSUBSCRIPT = ( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_M - 1 end_POSTSUBSCRIPT )</annotation></semantics></math> and feature representation <math alttext="Z" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p5.8.m8.1"><semantics id="S2.SS3.SSS1.p5.8.m8.1a"><mi id="S2.SS3.SSS1.p5.8.m8.1.1" xref="S2.SS3.SSS1.p5.8.m8.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p5.8.m8.1b"><ci id="S2.SS3.SSS1.p5.8.m8.1.1.cmml" xref="S2.SS3.SSS1.p5.8.m8.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p5.8.m8.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p5.8.m8.1d">italic_Z</annotation></semantics></math>. Both the encoder and the decoder use the multi-head attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx85" title="">Vaswani et al., 2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx31" title="">Gillioz et al., 2020</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p6">
<p class="ltx_p" id="S2.SS3.SSS1.p6.1">Many modern models are based entirely or partially on the Transformer. They compute general feature representations for the training set by unsupervised learning. This is the concept of Pre-training Language Models (PLMs). They can be fine-tuned to adapt to the specific tasks on particular datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx31" title="">Gillioz et al., 2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx52" title="">Li et al., 2024</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Encoder-Only Models</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">Since the Transformer’s encoder architecture can effectively capture the semantic features, some models only use this part for training. They are applied in text understanding tasks, such as text classification and NER. Bidirectional Encoder Representations from Transformers (BERT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx26" title="">Devlin et al., 2019</a>]</cite> is a representative model among them.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p2">
<p class="ltx_p" id="S2.SS3.SSS2.p2.1">Unlike the Transformer decoder, which uses an Auto-Regressive model, BERT is trained based on the Masked Language Model (MLM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx52" title="">Li et al., 2024</a>]</cite>. It masks the word in the input sequence, and uses the bidirectional encoder to understand the context semantically, which will be used in predicting the masked word <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx26" title="">Devlin et al., 2019</a>]</cite>.
It has already been pre-trained on a 16GB corpus.
To deploy it, we only need to replace the original fully connected layer with a new output layer, and then fine-tune the parameters on the dataset for specific tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx26" title="">Devlin et al., 2019</a>]</cite>. This approach consumes fewer computing resources and less time than training a model from scratch. In the clinical domain, Bio_ClinicalBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx5" title="">Alsentzer et al., 2019</a>]</cite> and medicalai/ClinicalBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx86" title="">Wang et al., 2023</a>]</cite> are fine-tuned in the clinical dataset based on BERT architecture. Initially, due to the BERT’s focus on semantic understanding, it was rarely used for text generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx89" title="">Yang et al., 2019</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p3">
<p class="ltx_p" id="S2.SS3.SSS2.p3.1">Robustly Optimized BERT Pretraining Approach (RoBERTa) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx93" title="">Zhuang et al., 2021</a>]</cite> improved some key hyperparameters based on BERT. Instead of BERT’s static mask, it uses a dynamic mask strategy, which helps it better adapt to multitasking. Additionally, it gained a stronger semantic understanding after training on five English datasets of 160GB. Unfortunately, compared to BERT, it requires significantly more computational resources and time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx3" title="">Acheampong et al., 2021</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p4">
<p class="ltx_p" id="S2.SS3.SSS2.p4.1">To better handle long sequences, Longformer introduces a sparse attention mechanism to reduce computation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx10" title="">Beltagy et al., 2020</a>]</cite>. This allows each token to focus only on nearby tokens rather than the entire sequence. Unlike traditional models like BERT and RoBERTa, which can only process no more than 512 tokens, Longformer can handle up to 4096 tokens. It consistently achieves better performance than RoBERTa in downstream tasks involving long documents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx10" title="">Beltagy et al., 2020</a>]</cite>. The <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS2.p4.1.1">Clinical-Longformer</span> model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx51" title="">Li et al., 2023</a>]</cite> was fine-tuned for the clinical domain.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p5">
<p class="ltx_p" id="S2.SS3.SSS2.p5.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.T1" title="Table 1 ‣ 2.3.2 Encoder-Only Models ‣ 2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">1</span></a> summarises the <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS2.p5.1.1">encoder-only models</span> used in our work and their corresponding fine-tuning datasets.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.1.2.1.1" style="width:256.1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1.1.1">Fine-tuned Dataset</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1">Bio_Clinical BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx5" title="">Alsentzer et al., 2019</a>]</cite>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.2.1">
<span class="ltx_p" id="S2.T1.1.2.1.2.1.1" style="width:256.1pt;">MIMIC-III</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.3.2.1">medicalai/ClinicalBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx86" title="">Wang et al., 2023</a>]</cite>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.2.1">
<span class="ltx_p" id="S2.T1.1.3.2.2.1.1" style="width:256.1pt;">A large corpus of 1.2B words of diverse diseases</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.4.3.1">RoBERTa-base <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx93" title="">Zhuang et al., 2021</a>]</cite>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.2.1">
<span class="ltx_p" id="S2.T1.1.4.3.2.1.1" style="width:256.1pt;">General Dataset (including BookCorpus, English Wikipedia, CC-News, OpenWebText, and Stories)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.5.4.1">Clinical-Longformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx51" title="">Li et al., 2023</a>]</cite>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.2.1">
<span class="ltx_p" id="S2.T1.1.5.4.2.1.1" style="width:256.1pt;">MIMIC-III</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Encoder-Only Models and Their Fine-tuned Datasets</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>Decoder-Only Models</h4>
<div class="ltx_para" id="S2.SS3.SSS3.p1">
<p class="ltx_p" id="S2.SS3.SSS3.p1.1">In 2020, the performance of ChatGPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx17" title="">Brown et al., 2020</a>]</cite> in question answering task caught researchers’ attention to decoder-only architectures.
As mentioned earlier,
the Transformer decoder is an auto-regressive model. It can only refer to the synthesised words on the left side to generate the new word, without considering the context (which is called masked self-attention). This method made it more flexible in generating coherent text. Compared with BERT, the GPT series performed well in zero-sample and small-sample learning tasks by enlarging the size of the model. Even without fine-tuning, a simple prompt can help GPT generate a reasonable answer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx87" title="">Wu, 2024</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS3.p2">
<p class="ltx_p" id="S2.SS3.SSS3.p2.1">Unlike GPT, which improves models’ performance by increasing dataset size and the number of parameters without limitations, Meta AI published a series of <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS3.p2.1.1">Llama</span> Models. These models aim to maximise the use of limited resources - in other words, by extending training, they reduce the overall demand on computing resources. The latest Llama3 model requires only 8B to 70B parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx4" title="">AI, 2023</a>]</cite>, significantly fewer than GPT-3’s 175 billion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx87" title="">Wu, 2024</a>]</cite>. Additionally, it outperforms GPT-3.5 Turbo in 5-shot learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx24" title="">Context.ai, 2024</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.4 </span>Encoder-Decoder Models</h4>
<div class="ltx_para" id="S2.SS3.SSS4.p1">
<p class="ltx_p" id="S2.SS3.SSS4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.SSS4.p1.1.1">T5</span> Family <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx70" title="">Raffel et al., 2020a</a>]</cite> is a classic example of the Encoder-Decoder Model. This architecture is particularly suitable for text generation tasks that require deep semantic understanding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx19" title="">Cai et al., 2022</a>]</cite>. T5 transforms all kinds of NLP tasks into a text-to-text format <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx82" title="">Tsirmpas et al., 2024</a>]</cite>. Unlike BERT, which uses word-based masking and prediction, T5 processes text at the <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS4.p1.1.2">fragment</span> level using ”span corruption” to understand semantics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx82" title="">Tsirmpas et al., 2024</a>]</cite>.
For the fill-in-the-blank task, instead of replacing the specific words with <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS4.p1.1.3">&lt;mask&gt;</span> like BERT, T5 replaces the text fragments with an ordered set of <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS4.p1.1.4">&lt;extra_id_n&gt;</span> to reassemble the long sequence text.
T5 needs to pre-process the input text according to the tasks’ requirements. A directive prefix should be added as a prompt.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS4.p2">
<p class="ltx_p" id="S2.SS3.SSS4.p2.1">Some language models fine-tuned with T5 on specific datasets, such as SciFive (fine-tuned in some science literature) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx68" title="">Phan et al., 2021</a>]</cite> and ClinicalT5 (fine-tuned in clinical dataset MIMIC-III notes) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx56" title="">Lu et al., 2022</a>]</cite>, have shown excellent performance in their respective fields. The <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS4.p2.1.1">T5 family models</span> used in this project and their corresponding fine-tuned datasets are summarised in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.T2" title="Table 2 ‣ 2.3.4 Encoder-Decoder Models ‣ 2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.1.1">
<span class="ltx_p" id="S2.T2.1.1.1.1.1.1" style="width:142.3pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.1.1.1">Model</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.2.1">
<span class="ltx_p" id="S2.T2.1.1.1.2.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.2.1.1.1">Pre-trained Dataset</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.3.1">
<span class="ltx_p" id="S2.T2.1.1.1.3.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.3.1.1.1">Weight Initialisation Method</span></span>
</span>
</th>
</tr>
<tr class="ltx_tr" id="S2.T2.1.2.2">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.2.1.1">
<span class="ltx_p" id="S2.T2.1.2.2.1.1.1" style="width:142.3pt;">T5-base <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx71" title="">Raffel et al., 2020b</a>]</cite></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.2.2.1">
<span class="ltx_p" id="S2.T2.1.2.2.2.1.1" style="width:113.8pt;">Colossal Clean Crawled Corpus (C4)</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.2.3.1">
<span class="ltx_p" id="S2.T2.1.2.2.3.1.1" style="width:113.8pt;">Randomly Initialised</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.1.3.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.3.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.1.1.1">
<span class="ltx_p" id="S2.T2.1.3.1.1.1.1" style="width:142.3pt;">Clinical-T5-Base</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.3.1.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.3.1.3"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.4.2">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T2.1.4.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.2.1.1">
<span class="ltx_p" id="S2.T2.1.4.2.1.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx29" title="">Eric and Johnson, 2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx32" title="">Goldberger et al., 2000</a>]</cite></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.4.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.2.2.1">
<span class="ltx_p" id="S2.T2.1.4.2.2.1.1" style="width:113.8pt;">MIMIC-<span class="ltx_text ltx_font_smallcaps" id="S2.T2.1.4.2.2.1.1.1">iii</span>, MIMIC-<span class="ltx_text ltx_font_smallcaps" id="S2.T2.1.4.2.2.1.1.2">iv</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.4.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.2.3.1">
<span class="ltx_p" id="S2.T2.1.4.2.3.1.1" style="width:113.8pt;">Initialised from T5-Base</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.5.3">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.5.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.3.1.1">
<span class="ltx_p" id="S2.T2.1.5.3.1.1.1" style="width:142.3pt;">Clinical-T5-Sci</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.5.3.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.5.3.3"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.6.4">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T2.1.6.4.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.4.1.1">
<span class="ltx_p" id="S2.T2.1.6.4.1.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx29" title="">Eric and Johnson, 2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx32" title="">Goldberger et al., 2000</a>]</cite></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.6.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.4.2.1">
<span class="ltx_p" id="S2.T2.1.6.4.2.1.1" style="width:113.8pt;">PubMed Abstracts, PubMed Central</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.6.4.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.4.3.1">
<span class="ltx_p" id="S2.T2.1.6.4.3.1.1" style="width:113.8pt;">Initialised from SciFive</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.7.5">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.7.5.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.5.1.1">
<span class="ltx_p" id="S2.T2.1.7.5.1.1.1" style="width:142.3pt;">Clinical-T5-Scratch</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.7.5.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T2.1.7.5.3"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.8.6">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S2.T2.1.8.6.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.6.1.1">
<span class="ltx_p" id="S2.T2.1.8.6.1.1.1" style="width:142.3pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx29" title="">Eric and Johnson, 2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx32" title="">Goldberger et al., 2000</a>]</cite></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S2.T2.1.8.6.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.6.2.1">
<span class="ltx_p" id="S2.T2.1.8.6.2.1.1" style="width:113.8pt;">MIMIC-<span class="ltx_text ltx_font_smallcaps" id="S2.T2.1.8.6.2.1.1.1">iii</span>, MIMIC-<span class="ltx_text ltx_font_smallcaps" id="S2.T2.1.8.6.2.1.1.2">iv</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S2.T2.1.8.6.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.8.6.3.1">
<span class="ltx_p" id="S2.T2.1.8.6.3.1.1" style="width:113.8pt;">Randomly Initialised</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The T5 Family Models used in our work</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.5 </span>Comparison and Limitations</h4>
<div class="ltx_para" id="S2.SS3.SSS5.p1">
<p class="ltx_p" id="S2.SS3.SSS5.p1.1">According to <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS5.p1.1.1">?</span>), the encoder-decoder architecture performs best with sufficient training data. However, challenges in data collection can negatively affect its performance. Despite these challenges, different architectures are well-suited to different tasks. For example, for tasks requiring semantic understanding, such as text summarisation, the encoder-decoder architecture is the most effective. In contrast, for tasks that involve minor word modifications, the encoder-only structure works better. However, the decoder-only structure is not suitable for tasks with insufficient training data and long text processing, but performs well in few-shot question-answering tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx7" title="">Amin-Nejad et al., 2020b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx19" title="">Cai et al., 2022</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS5.p2">
<p class="ltx_p" id="S2.SS3.SSS5.p2.1">Following these discussions, Transformer-based Pre-trained Language Models (PLMs) have demonstrated strong performance in NLP tasks, but many challenges still remain.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Related Works on Clinical Text Generation</h3>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>LT3: Label to Text Generation</h4>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<p class="ltx_p" id="S2.SS4.SSS1.p1.1">LT3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx9" title="">Belkadi et al., 2023</a>]</cite> uses an encoder-decoder architecture to generate synthetic text from labels. As shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.F2" title="Figure 2 ‣ 2.4.1 LT3: Label to Text Generation ‣ 2.4 Related Works on Clinical Text Generation ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">2</span></a>, labels such as medications are the input of the encoder, which can generate corresponding feature representations. The decoder generates prescription sequences based on these features. The pre-trained BERT tokenizer is used to split the input sequence into sub-words. LT3 is trained from scratch. Instead of using traditional greedy decoding, which may miss the global optimum, the authors proposed Beam Search Decoding with Backtracking (<span class="ltx_text ltx_font_bold" id="S2.SS4.SSS1.p1.1.1">B2SD</span>). This approach broadens the search range through a backtracking mechanism, preserving possible candidates for the optimal solution. To reduce time complexity, they used a probability difference function to avoid searching for low-probability words. Additionally, the algorithm penalises repeated sub-sequences and employs a logarithmic heuristic to guide the exploration of generation paths. The authors test LT3 on the 2018-n2c2 dataset, and evaluate the results using both quantitative metrics and downstream tasks. It was demonstrated that this model outperforms T5 in label-to-text generation. <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>LT3 has shown significant improvements over existing label-to-text generation models. Unfortunately, when we tried applying B2SD to generate clinical letters, the results were somehow disappointing. This may be due to the length of clinical letters, B2SD consumes a lot of time on long text generation. Despite this, it still shows great potential in generating clinical data.
</span></span></span></p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="109" id="S2.F2.g1" src="extracted/5855287/figures/ExamplesOfLT3.drawio.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An Example of LT3</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>Seq2Seq Generation for Medical Dataset Augmentation</h4>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.SSS2.p1.1.1">?</span>) compare the performance of the Vanilla Transformer and GPT-2 using the MIMIC-III dataset in seq2seq tasks. Specifically, they input a series of structured patient information as conditions, as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.F3" title="Figure 3 ‣ 2.4.2 Seq2Seq Generation for Medical Dataset Augmentation ‣ 2.4 Related Works on Clinical Text Generation ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">3</span></a>, to generate discharge summaries. They demonstrate that the augmented data outperforms the original data in downstream tasks (e.g. readmission prediction). Furthermore, they prove that Vanilla Transformer performs better with large samples, while GPT-2 excels in few-shot scenarios. However, GPT-2 is not suitable for augmenting long texts. Additionally, they used Bio_ClinicalBERT for the downstream tasks, and discovered that Bio_ClinicalBERT significantly outperformed the baseline model (BERT) in almost all experiments. It suggests that Bio_ClinicalBERT can potentially replace BERT in the biomedical field. Interestingly, although the synthetic data have a low score on internal metrics (such as ROUGE and BLEU), the performance on downstream tasks is notably enhanced. This may be because augmenting text can effectively introduce noise into the original text, improving the model’s generalisation to unseen data.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="231" id="S2.F3.g1" src="extracted/5855287/figures/ConditionalTextGenerationExample.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An Input Example of Conditional Text Generation</figcaption>
</figure>
<div class="ltx_para" id="S2.SS4.SSS2.p2">
<p class="ltx_p" id="S2.SS4.SSS2.p2.1">According to their findings, decoder-only models like GPT-2 are not suitable for processing long text. Bio_ClinicalBERT is particularly effective for tasks in the clinical area, and Clinical Transformer is promising in augmenting medical data. This provides more possibilities for my task of generating synthetic clinical letters.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.3 </span>Discharge Summary Generation Using Clinical Guidelines and Human Evaluation Framework</h4>
<div class="ltx_para" id="S2.SS4.SSS3.p1">
<p class="ltx_p" id="S2.SS4.SSS3.p1.1">Unlike the traditional supervised learning of fine-tuning language models (which requires a large amount of annotated data), <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS3.p1.1.1">?</span>) generated 53 discharge summaries using only a one-shot example and a clinical guideline. Their research consists of two aspects: generating discharge summaries and a manual evaluation framework.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS3.p2">
<p class="ltx_p" id="S2.SS4.SSS3.p2.1">As shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.F4" title="Figure 4 ‣ 2.4.3 Discharge Summary Generation Using Clinical Guidelines and Human Evaluation Framework ‣ 2.4 Related Works on Clinical Text Generation ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">4</span></a>, the authors used clinical notes from MIMIC-III as input, and incorporated a one-shot summary along with clinical guidance as prompts to generate discharge summaries by GPT-4-turbo. Initially, five sample synthetic summaries were evaluated by a clinician. Based on the feedback, the clinical guidance was revised to adapt to the generation task. Through iterative optimisation, the revised guidance, combined with the original one-shot sample, became the new prompt. Then the authors generated 53 discharge summaries using this method and invited 11 clinicians to do a final manual quantitative evaluation.
Clinicians were invited to evaluate the error rate at the section level (e.g., Diagnoses, Social Context, etc). It includes four dimensions:</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS3.p3">
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1">Minor omissions;</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1">Severe omissions;</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1">Unnecessary text;</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i4.p1">
<p class="ltx_p" id="S2.I2.i4.p1.1">Incorrect additional text.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="S2.F4.g1" src="extracted/5855287/figures/WorkflowofDischargeSummaryGenerationBasedonGuidelines.drawio.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Workflow of Discharge Summary Generation Using Clinical Guidelines</figcaption>
</figure>
<div class="ltx_para" id="S2.SS4.SSS3.p4">
<p class="ltx_p" id="S2.SS4.SSS3.p4.1">Each discharge summary was evaluated by at least two clinicians, and the authors calculated agreement scores to evaluate the subjectivity during the human evaluation stage. Unfortunately, the inter-rater agreement was only 59.72%, raising concerns that the revised prompts based on such feedback might result in subjective synthetic summaries.
Although this study partially addresses the issue of insufficient training data, and provides reliable human quantitative evaluation methods, it is still not well-suited for our investigation. Specifically,
It consumes significant time and manpower.
Therefore, there is still a long way to go before this technique can be used for large-scale text generation tasks.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.4 </span>Comparison of Masked and Causal Language Modelling for Text Generation</h4>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="S2.F5.g1" src="extracted/5855287/figures/NicoloWorkflow.drawio.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Workflow of MLM and CLM Comparison in Text Generation</figcaption>
</figure>
<div class="ltx_para" id="S2.SS4.SSS4.p1">
<p class="ltx_p" id="S2.SS4.SSS4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.SSS4.p1.1.1">?</span>) compared masked language modelling (MLM, including BERT, RoBERTa, BiomedNLP-PubMedBERT) and causal language modelling (CLM, including T5, BART, SciFive-large-Pubmed_PMC) across various datasets in masking and text generation tasks. They used qualitative and quantitative evaluations, as well as downstream tasks, to assess the quality of the synthetic texts. Their workflow is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.F5" title="Figure 5 ‣ 2.4.4 Comparison of Masked and Causal Language Modelling for Text Generation ‣ 2.4 Related Works on Clinical Text Generation ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">5</span></a>. Based on these evaluations, the study yielded the following results:</p>
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1">MLM models are better suited for text masking and generation tasks compared to CLM.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1">Introducing domain-specific knowledge does not significantly improve the model’s performance.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i3.p1">
<p class="ltx_p" id="S2.I3.i3.p1.1">Downstream tasks can adapt to the introduced noise. Although some synthetic texts might not achieve high quantitative evaluation scores, they can still perform well in downstream tasks. This matches the findings from Amin-Nejad <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx7" title="">Amin-Nejad et al., 2020b</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i4.p1">
<p class="ltx_p" id="S2.I3.i4.p1.1">A lower random masking ratio can generate higher-quality synthetic texts.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S2.SS4.SSS4.p1.2">These very recent findings provide insightful inspiration to our investigation. Our work will build on their research, expanding on masking strategies and focusing on the clinical domain.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodologies and Experimental Design</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Due to the sensitivity of clinical information, many clinical datasets are not accessible. As mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2" title="2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">2</span></a>, numerous studies use NLG techniques to generate clinical letters, and evaluate the feasibility of replacing the original raw clinical letters with synthetic letters. Most existing research involves fine-tuning PLMs or training Transformer-based models from scratch on their datasets through supervising learning.
These studies explore different ways to learn the mapping from original raw text to synthetic text and work on generating synthetic data that are similar (or even identical) to the original ones. Our work, however, aims to find a method that can generate clinical letters that can <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">keep the original clinical story, while not exactly being the same as the original letters</span>. To achieve this objective, we employed various models and masking strategies to generate clinical letters. The experiment will follow these steps:</p>
</div>
<div class="ltx_para" id="S3.p2">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Data Collecting and Pre-processing</span>: Access clinical letter examples <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx44" title="">Johnson et al., 2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx43" title="">Johnson et al., 2023b</a>]</cite>. Segment the text at sentence level. Extract entities and the letters’ templates to represent the clinical story and maintain clinical soundness.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Randomly mask</span> the context. Generate clinical letters by predicting masked tokens using different LLMs.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Evaluate</span> synthetic letters generated by different language models. Select one well-performed model - Bio_ClinicalBERT, and work on it.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Explore different <span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">masking strategies</span> to retain clinical stories and diversity while removing private information. After generating clinical letters using these strategies, evaluate their quality.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">Explore <span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">post-processing</span> methods, in order to further enhance the readability of synthetic letters.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1">Compare the performance of synthetic and original letters in a <span class="ltx_text ltx_font_bold" id="S3.I1.i6.p1.1.1">downstream NER</span> task, to evaluate the usability of these synthetic letters.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.p2.1">An overall investigation workflow is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F6" title="Figure 6 ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="316" id="S3.F6.g1" src="extracted/5855287/figures/overallWorkflow.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Overall Investigation Workflow for <span class="ltx_text ltx_font_smallcaps" id="S3.F6.2.1">Synthetic4Health</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Set</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Based on the objective of this project, we need a dataset that includes both clinical <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">notes</span> and some clinical <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.2">entities</span>. The dataset we used is from the SNOMED CT Entity Linking Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx44" title="">Johnson et al., 2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx43" title="">Johnson et al., 2023b</a>]</cite>. It includes 204 clinical letters and 51,574 manually annotated clinical entities.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Clinical Letters</span>
The clinical letters are from a subset of discharge summaries in MIMIC-IV-Note <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx42" title="">Johnson et al., 2023a</a>]</cite>. It uses clinical notes obtained from a healthcare system in the United States. These notes were de-identified by a hybrid method of the Rule-based Approach and Neural Networks. To avoid releasing sensitive data, the organisation also did a manual review of protected health information (PHI). In these letters, all PHI was replaced with three underscores ‘___’. The letters record the patient’s hospitalisation information (including the reason for visiting, consultation process, allergy history, discharge instructions, etc.). They are saved in a comma-separated value (CSV) format file ‘mimic-iv_notes_training_set.csv’. Each row of data represents an individual clinical letter. It consists of two columns, where the ”note_id” column is a unique identifier for each patient’s clinical letter, and the ‘text’ column contains the contents of the clinical letter. Since most language models have a limitation on the number of tokens to process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx77" title="">Sun and Iyyer, 2021</a>]</cite>, we tokenized the clinical letters into words using the ‘NLTK’ library and found that all clinical letters contained thousands of tokens. Therefore, it is necessary to split each clinical letter into multiple chunks to process them. These separated chunks should be merged in the end to generate the whole letter.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Annotated Clinical Entities</span>
The entities are manually annotated based on SNOMED CT. A total of 51,574 annotations cover 5,336 clinical concepts. They are saved in another CSV document which includes four columns: ‘note_id’, ‘start’, ‘end’, and ‘concept_id’. The ‘note_id’ column corresponds to the ‘note_id’ in ‘mimic-iv_notes_training_set.csv’ file. The ‘start’ and ‘end’ columns indicate the position of annotated entities. The ‘concept_id’ can be used for entity linking with SNOMED CT. For example, for the ‘note_id’‘10807423-DS-19’, , the annotated entity ‘No Known Allergies’ has a corresponding ‘concept_id’: ‘609328004’. This can be linked to SNOMED CT under the concept of ‘Allergic disposition’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx40" title="">International, 2024</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">An example of text excerpted from the original letter is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F7" title="Figure 7 ‣ 3.1 Data Set ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">7</span></a>. It contains the document structure and some free text. According to the dataset, document structure often corresponds to capital letters and colons ‘:’. Our primary goal is to mask the context that is neither part of the document structure nor annotated entities, and then generate a new letter, as both <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">structure</span> and clinical <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.2">entities</span> are essential for understanding clinical information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx61" title="">Meystre et al., 2014</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="257" id="S3.F7.g1" src="extracted/5855287/figures/TextExcerptFromOriginalLetter.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Text Excerpt from the Original Letter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx44" title="">Johnson et al., 2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx43" title="">Johnson et al., 2023b</a>]</cite> (‘note_id’: ’17656866-DS-6’)</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Software and Environment</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">All codes and experiments in this project are run in the integrated development environment (IDE) ‘Google Colab’. The built-in T4 GPU is used to accelerate the inference process. The primary tools used in the project include:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">Programming Language and Environment: </span>Python 3.8 serves as the main programming language.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">Deep Learning Framework: </span>PyTorch is the core framework used for loading and applying pre-trained language models (PLMs).</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">Natural Language Processing Libraries: </span>This includes Hugging Face’s Transformers, NLTK, BERTScore, etc. They are popular tools for text processing and evaluation in the NLP domain.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i4.p1.1.1">Auxiliary Tools: </span>Libraries such as Pandas and Math can support data management, mathematical operations, and other routine tasks.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Pre-Processing</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The collected dataset involves different files and is entirely raw data. It is necessary to pre-process them before using them in generation tasks. The pre-processing of this system contains five steps: ‘Merge dataset based on ‘note_id”, ‘Annotated Entity Recognition’, ‘Split Letters in Chunks’, ‘Word Tokenization’ and ‘Feature Extraction’. The pre-processing pipeline is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F8" title="Figure 8 ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="S3.F8.g1" src="extracted/5855287/figures/preprocessingPipeline.png" width="416"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Pre-Processing Pipeline</figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Merging Dataset and Annotated Entity Recognition</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Initially, we merged the clinical letters file and annotations file into a new DataFrame.
The method is detailed in Appendix <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:Appendix:_Method_for_Dataset_Merging_in_Preprocessing</span>.
After this, we extracted manually annotated entities based on their index. An excerpt from an original letter is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F9" title="Figure 9 ‣ 3.3.1 Merging Dataset and Annotated Entity Recognition ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">9</span></a>, and the manually annotated entities are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.T3" title="Table 3 ‣ 3.3.1 Merging Dataset and Annotated Entity Recognition ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="S3.F9.g1" src="extracted/5855287/figures/ExampleOfText.drawio.png" width="525"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Sample Text from Original Letters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx44" title="">Johnson et al., 2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx43" title="">Johnson et al., 2023b</a>]</cite> (‘note_id’: ’10807423-DS-19’)</figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1">Entity</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1">Start</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.3.1">End</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.4.1">Concept ID</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.2.1.1">fall</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.2.1.2">571</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.2.1.3">575</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.2.1.4">161898004</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.3.2.1">LLE</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.3.2.2">621</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.3.2.3">624</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.3.2.4">32153003</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.4.3.1">eversion</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.4.3.2">636</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.4.3.3">644</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.4.3.4">4196002</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.5.4.1">open fracture</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.5.4.2">660</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.5.4.3">673</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.5.4.4">397181002</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.6.5.1">dislocation</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.6.5.2">674</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.6.5.3">685</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.6.5.4">87642003</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.7.6.1">head strike</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.7.6.2">694</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.7.6.3">706</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.7.6.4">82271004</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.8.7.1">LOC</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.8.7.2">710</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.8.7.3">713</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.8.7.4">419045004</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.9.8.1">neck pain</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.9.8.2">722</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.9.8.3">731</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.9.8.4">81680005</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.10.9.1">back pain</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.10.9.2">733</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.10.9.3">742</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.10.9.4">161891005</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.11.10.1">chest pain</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.11.10.2">744</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.11.10.3">754</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.11.10.4">29857009</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.12.11.1">abd pain</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.12.11.2">756</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.12.11.3">765</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.12.11.4">21522001</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.13.12.1">pelvic</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.13.12.2">774</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.13.12.3">780</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.13.12.4">30473006</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.14.13.1">thigh pain</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.14.13.2">784</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.14.13.3">794</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.14.13.4">78514002</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.15.14.1">conscious sedation</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.15.14.2">833</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.15.14.3">851</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.15.14.4">314271007</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.16.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.16.15.1">vitals</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.16.15.2">874</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.16.15.3">880</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.16.15.4">118227000</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.17.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.17.16.1">neurovascular symptoms</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.17.16.2">963</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.17.16.3">986</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.17.16.4">308921004</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Extracted Entities and Their Details</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Splitting Letters into Variable-Length Chunks</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Typically, PLMs such as BERT, RoBERTa, and T5 have a limit on the number of input tokens, usually capped at 512 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx90" title="">Zeng et al., 2022</a>]</cite>. When dealing with text that exceeds this limit, common approaches include discarding the excess tokens or splitting the text into fixed-length chunks of 512 tokens. In addition, some studies evaluate the tokens’ importance to decide which parts should be discarded <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx37" title="">Hou et al., 2022</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">In this work, each clinical letter (‘note_id’) contains thousands of tokens, as mentioned in Section <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:subsec:_Dataset</span>, to preserve as much critical clinical information as possible, we avoided simply discarding tokens.
Instead, we adopted a splitting strategy based on <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p2.1.1">semantics</span>. Each block is not a fixed length. Rather, they are complete <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p2.1.2">paragraphs</span> that are as close as possible to the token limit. This approach aims to help the model better capture the meaning and structure of clinical letters, thereby improving its ability to retain essential clinical information while efficiently processing the text. In fact, we initially generated letters at the sentence level. However, it was found that processing at the sentence level is not only time-consuming, but also fails to provide the model with enough information for inference and prediction. This is why the letters are processed in chunks rather than in sentences.</p>
</div>
<figure class="ltx_figure" id="S3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="219" id="S3.F10.g1" src="extracted/5855287/figures/chunkSegment.png" width="628"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Text Chunking Workflow</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.1">As shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F10" title="Figure 10 ‣ 3.3.2 Splitting Letters into Variable-Length Chunks ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">10</span></a>, each raw letter is split into sentences first. We used the pre-trained models provided by the ‘NLTK’ library, which combines statistical and machine-learning approaches to identify sentence boundaries. Each clinical letter is treated as a separate processing unit, with the first sentence automatically assigned to the first text block (chunk). To control the length of each chunk, we set a <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p3.1.1">maximum line count</span> parameter (max_lines). If the first sentence already meets the value of ‘max_lines’, the chunk will only contain this one sentence. Otherwise, subsequent sentences will be added to the chunk until the line count up to the max_lines.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p4">
<p class="ltx_p" id="S3.SS3.SSS2.p4.1">Extra care is needed when handling text with specific formats, such as medication dosage descriptions, as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F11" title="Figure 11 ‣ 3.3.2 Splitting Letters into Variable-Length Chunks ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">11</span></a>. Because there is no clear sentence boundary, these sentences may exceed the tokens limitation. To address this, we first check whether the sentence being processed exceeds the token limit (max_tokens). If it does not, the sentence will be added to the current chunk. Otherwise, the sentence should be split into smaller chunks, each no longer than ‘max_tokens’. This operation helps <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p4.1.1">balance processing efficiency</span> while maintaining semantic integrity. In the example shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F11" title="Figure 11 ‣ 3.3.2 Splitting Letters into Variable-Length Chunks ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">11</span></a>, although using line breaks to split the text seems to be more flexible, considering time complexity and the requirement to index the annotated entities, this method was not chosen.</p>
</div>
<figure class="ltx_figure" id="S3.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="S3.F11.g1" src="extracted/5855287/figures/PartialViewofaLongSentenceExample.png" width="440"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Sentence Fragment Exceeding Token Limit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx44" title="">Johnson et al., 2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx43" title="">Johnson et al., 2023b</a>]</cite> (‘note_id’: ’10807423-DS-19’)</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Word Tokenisation</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">To prepare the text for model processing, we split each chunk of text into smaller units: tokens. The tokenisation methods can be categorised into two types: one for feature extraction and the others for masking and generation.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS3.p2">
<p class="ltx_p" id="S3.SS3.SSS3.p2.1">For the tokenization aimed at feature extraction, we used the ‘word_tokenize’ method from the ‘NLTK’ library. It is helpful to preserve the original features of the words, which is especially important for retaining clinical entities. For instance, in the sentence “Patient is a ___ yo male previously healthy presenting w/ fall from 6 feet, from ladder.” Word boundaries such as spaces can be automatically detected for tokenization. The results of different tokenization methods are shown in the Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.T4" title="Table 4 ‣ 3.3.3 Word Tokenisation ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS3.p3">
<p class="ltx_p" id="S3.SS3.SSS3.p3.1">As for the tokenization used for masking and generating, we retained the original models’ tokenization methods. The specific tokenization approach varies by model, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.T4" title="Table 4 ‣ 3.3.3 Word Tokenisation ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">4</span></a>. For example, BERT family models use Word-Piece tokenization, which initially splits text by spaces and then further divides the words into sub-words <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx93" title="">Zhuang et al., 2021</a>]</cite>. This approach is particularly effective for handling words that are not in the pre-training vocabulary and is especially useful for predicting masked words. For complex clinical terms, however, these models rely heavily on a predefined dictionary, which can result in unsatisfactory tokenization and hinder the model’s understanding. For instance, the word ‘COVID-19’ is tokenized by BERT into [‘co’, ‘##vid’, ‘-’, ‘19’]. In contrast, the T5 family models use Sentence-Piece tokenization. It does not rely on space to split the text. Instead, this method tokenizes directly from the raw text, making it better suited for handling abbreviations and non-standard characters (e.g. ‘COVID-19’), which are common in clinical letters.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS3.p4">
<p class="ltx_p" id="S3.SS3.SSS3.p4.1">It is important to note that although all BERT family models use Word-Piece tokenization, the results can still differ. This is because different models use different vocabularies during pre-training, leading to variations in tokenization granularity. The tokenization methods for each model are detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.T4" title="Table 4 ‣ 3.3.3 Word Tokenisation ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">4</span></a>. Each tokenization approach has its own advantages and disadvantages for processing clinical letters. Therefore, exploring how these models impact the clinical letter generation is also a requirement of my project.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.1.1.1.1">
<span class="ltx_p" id="S3.T4.1.1.1.1.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.1.1.1.1">Operation / Model</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T4.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.1.1.2.1">
<span class="ltx_p" id="S3.T4.1.1.1.2.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.2.1.1.1">Tokenization Method</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T4.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.1.1.3.1">
<span class="ltx_p" id="S3.T4.1.1.1.3.1.1" style="width:227.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.3.1.1.1">Tokenized Output</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.2.1.1.1">
<span class="ltx_p" id="S3.T4.1.2.1.1.1.1" style="width:113.8pt;">Feature Extraction</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.2.1.2.1">
<span class="ltx_p" id="S3.T4.1.2.1.2.1.1" style="width:85.4pt;">Word Tokenization</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.2.1.3.1">
<span class="ltx_p" id="S3.T4.1.2.1.3.1.1" style="width:227.6pt;">[‘Patient’, ‘is’, ‘a’, ‘___’, ‘yo’, ‘male’, ‘previously’, ‘healthy’, ‘presenting’, ‘w’, ‘fall’, ‘from’, ‘6’, ‘feet’, ‘,’, ‘from’, ‘ladder’, ‘.]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.3.2.1.1">
<span class="ltx_p" id="S3.T4.1.3.2.1.1.1" style="width:113.8pt;">medicalai / ClinicalBERT</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.3.2.2.1">
<span class="ltx_p" id="S3.T4.1.3.2.2.1.1" style="width:85.4pt;">Subword-Enhanced Word-Piece</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.3.2.3.1">
<span class="ltx_p" id="S3.T4.1.3.2.3.1.1" style="width:227.6pt;">[‘patient’, ‘is’, ‘a’, ‘_’, ‘_’, ‘_’, ‘yo’, ‘male’, ‘previously’, ‘healthy’, ‘presenting’, ‘w’, ‘/’, ‘fall’, ‘from’, ‘6’, ‘feet’, ‘,’, ‘from’, ‘la’, ‘##dder’, ‘.]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.4.3.1.1">
<span class="ltx_p" id="S3.T4.1.4.3.1.1.1" style="width:113.8pt;">BERT-base, Bio_ClinicalBERT</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.4.3.2.1">
<span class="ltx_p" id="S3.T4.1.4.3.2.1.1" style="width:85.4pt;">Standard Word-Piece</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.4.3.3.1">
<span class="ltx_p" id="S3.T4.1.4.3.3.1.1" style="width:227.6pt;">['patient', 'is', 'a', '_', '_', '_', 'yo', 'male', 'previously', 'healthy', 'presenting', 'w', '/', 'fall', 'from', '6', 'feet', ',', 'from', 'ladder', '.']</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.5.4.1.1">
<span class="ltx_p" id="S3.T4.1.5.4.1.1.1" style="width:113.8pt;">Clinical-Longformer, RoBERTa</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.5.4.2.1">
<span class="ltx_p" id="S3.T4.1.5.4.2.1.1" style="width:85.4pt;">Detailed WordPiece</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.5.4.3.1">
<span class="ltx_p" id="S3.T4.1.5.4.3.1.1" style="width:227.6pt;">['Pat', 'ient', 'Ġis', 'Ġa', 'Ġ___', 'Ġyo', 'Ġmale', 'Ġpreviously', 'Ġhealthy', 'Ġpresenting', 'Ġw', '/', 'Ġfall', 'Ġfrom', 'Ġ6', 'Ġfeet', ',', 'Ġfrom', 'Ġladder', '.']</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.6.5.1.1">
<span class="ltx_p" id="S3.T4.1.6.5.1.1.1" style="width:113.8pt;">T5 Family (T5 base, SCI T5, Clinical T5)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.6.5.2.1">
<span class="ltx_p" id="S3.T4.1.6.5.2.1.1" style="width:85.4pt;">Sentence-Piece</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.6.5.3.1">
<span class="ltx_p" id="S3.T4.1.6.5.3.1.1" style="width:227.6pt;">[‘__Patient’, ‘__is’, ‘__’, ‘a’, ‘__’, ‘_’, ‘_’, ‘_’, ‘__’, ‘y’, ‘o’, ‘__male’, ‘__previously’, ‘__healthy’, ‘__’, ‘presenting’, ‘__’, ‘w’, ‘/’, ‘__fall’, ‘__from’, ‘__6’, ‘__feet’, ‘,’, ‘__from’, ‘__ladder’, ‘.’]</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of Tokenization Methods for Different LMs on Sentence 
<br class="ltx_break"/>”Patient is a ___ yo male previously healthy presenting w/ fall from 6 feet, from ladder.”</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Feature Extraction</h4>
<div class="ltx_para" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">Since we aim to generate de-identified clinical letters that can preserve clinical narratives during masking and generation, it is necessary to extract certain features beforehand.
We extracted the following features, with an example provided in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F12" title="Figure 12 ‣ 3.3.4 Feature Extraction ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">12</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.T5" title="Table 5 ‣ 3.3.4 Feature Extraction ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS4.p2">
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i1.p1.1.1">Document Structure: </span>This feature is identified by a rule-based approach. As mentioned in Subsection <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:subsec:_Dataset</span>, structural elements (or templates) often correspond to the use of colons ‘:’. They should not be masked to preserve the clinical context.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i2.p1.1.1">Privacy Information Identification: </span>In this part, I used a hybrid approach. To identify sensitive information such as ‘Name’, ‘Date’, and ‘Location (LOC)’, I employed a NER toolkit from Stanza <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx69" title="">Qi et al., 2020</a>]</cite>. To handle privacy information like phone numbers, postal codes, and e-mail addresses, I implemented a rule-based approach. Specifically, I devised several regular expressions to match the common formats of these data types. These identified privacy information should be masked.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i3.p1.1.1">Medical Terminology Recognition: </span>A NER toolkit pre-trained on the dataset i2b2 is used here <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx92" title="">Zhang et al., 2021</a>]</cite>. It can identify terms like ‘Test’, ‘Treatment’, and ‘Problem’ in free text. Although our dataset has already been manually annotated, these identified terms can serve as a supplement to the pre-annotated terms.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i4.p1">
<p class="ltx_p" id="S3.I3.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i4.p1.1.1">Special Patterns Observed in Sample Text: </span>Some specific patterns, like medication dosages (e.g. enoxaparin 40 mg/0.4 mL) or special notations (e.g. ‘b.i.d.’), may carry significant meaning. I retained these terms unless they were identified as private information to preserve the clinical background of the raw letters.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i5.p1">
<p class="ltx_p" id="S3.I3.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i5.p1.1.1">Part of Speech (POS) Tagging: </span>Different parts of speech (POS) play distinct roles in interpreting clinical texts. I aim to explore how these POS influence the model’s understanding of clinical text. To achieve this, I used a toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx92" title="">Zhang et al., 2021</a>]</cite> trained on the MIMIC-III <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx41" title="">Johnson et al., 2016</a>]</cite> dataset for POS tagging.
It performs better than SpaCy <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://spacy.io/</span></span></span></span> and NLTK in handling clinical letters.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="S3.F12.g1" src="extracted/5855287/figures/FeatureExtractionExample.drawio.png" width="440"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Example Sentence for Feature Extraction (See Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.T5" title="Table 5 ‣ 3.3.4 Feature Extraction ‣ 3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">5</span></a> for Extracted Features)</figcaption>
</figure>
<figure class="ltx_table" id="S3.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T5.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.1.1.1.1">
<span class="ltx_p" id="S3.T5.1.1.1.1.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.1.1.1.1">Feature Extraction Operation</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T5.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.1.1.2.1">
<span class="ltx_p" id="S3.T5.1.1.1.2.1.1" style="width:227.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.2.1.1.1">Extracted Features</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T5.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.2.1.1.1">
<span class="ltx_p" id="S3.T5.1.2.1.1.1.1" style="width:170.7pt;">Structure Extraction</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T5.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.2.1.2.1">
<span class="ltx_p" id="S3.T5.1.2.1.2.1.1" style="width:227.6pt;">Discharge Medication:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.3.2.1.1">
<span class="ltx_p" id="S3.T5.1.3.2.1.1.1" style="width:170.7pt;">Privacy Information Identification</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T5.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.3.2.2.1">
<span class="ltx_p" id="S3.T5.1.3.2.2.1.1" style="width:227.6pt;">Jone (PERSON) 
<br class="ltx_break"/>06/03/2010 (DATE) 
<br class="ltx_break"/>Postal Code: M16 3JE</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.4.3.1.1">
<span class="ltx_p" id="S3.T5.1.4.3.1.1.1" style="width:170.7pt;">Medical Terminology Recognition</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T5.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.4.3.2.1">
<span class="ltx_p" id="S3.T5.1.4.3.2.1.1" style="width:227.6pt;">Deep Vein Thrombosis (PROBLEM) 
<br class="ltx_break"/>DVT (PROBLEM) 
<br class="ltx_break"/>enoxaparin (TREATMENT) 
<br class="ltx_break"/>the syringe (TREATMENT) 
<br class="ltx_break"/>RX ⁢enoxaparin (TREATMENT)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.5.4.1.1">
<span class="ltx_p" id="S3.T5.1.5.4.1.1.1" style="width:170.7pt;">Special Patterns Observed in Sample Text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T5.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.5.4.2.1">
<span class="ltx_p" id="S3.T5.1.5.4.2.1.1" style="width:227.6pt;">‘40 mg/0.4 mL’ (122, 134) 
<br class="ltx_break"/>‘40 mg/0.4 mL ’ (284, 297) 
<br class="ltx_break"/>‘#⁢14’ (323, 327) 
<br class="ltx_break"/>‘⁢0’ (344, 346)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.6.5.1.1">
<span class="ltx_p" id="S3.T5.1.6.5.1.1.1" style="width:170.7pt;">POS Tagging</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.6.5.2.1">
<span class="ltx_p" id="S3.T5.1.6.5.2.1.1" style="width:227.6pt;">‘Jone’, ‘PROPN’ 
<br class="ltx_break"/>‘is’, ‘AUX’ 
<br class="ltx_break"/>‘living’, ‘VERB’ 
<br class="ltx_break"/>‘in’, ‘ADP’ 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S3.T5.1.6.5.2.1.1.1">(Partial List)</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Example: Summary of Feature Extraction Operations and Extracted Features</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Clinical Letters Generation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We discuss the models and masking strategies that are used in generating synthetic clinical letters. It is important to clarify that our key objective is to generate letters that differ from the original ones, rather than being exact copies, as the same statement may indirectly reveal the patients’ privacy. Although fine-tuning the model can always improve precision and enhance the model’s semantic comprehension ability, it tends to produce letters that are too closely aligned with the originals. This also causes the fine-tuned model to rely too heavily on the original dataset, compromising its ability to generalise. Therefore, simply fine-tuning the model is not ideal if the PLMs can already generate the readable text. Instead, we should concentrate on how to <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">protect clinical terms and patient narratives as well as avoid privacy breaches</span>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3" title="2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">2.3</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS4" title="2.4 Related Works on Clinical Text Generation ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">2.4</span></a>, decoder-only models struggle with processing long texts that require contextual understanding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx7" title="">Amin-Nejad et al., 2020b</a>]</cite>. Additionally, deploying them requires substantial computing resources and time. Therefore, we explored various pre-trained language models (PLMs), including both encoder-only and encoder-decoder models in this project. After evaluating their ability to generate synthetic letters from our dataset, we focused on <span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">Bio_ClinicalBERT</span> - a well-performed model in our task, to experiment with different masking strategies. Additionally, from the discussion in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS3" title="3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we need to split the text into various-length-chunks. So the appropriate <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.2">length of these chunks</span> is also experimented with Bio_ClinicalBERT.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Encoder-Only Models with Random Masking</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">As mentioned earlier, the primary method for this project involves masking and generation. We focused extensively on encoder-only models because of their advantage in bi-directional semantic comprehension. These encoder-only models, including BERT, RoBERTa, and Longformer—detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3" title="2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">2.3</span></a>—were compared for their performance. Given the clinical focus of this task, we particularly explored model variants that were fine-tuned on clinical or biological datasets. However, as no clinically fine-tuned RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx93" title="">Zhuang et al., 2021</a>]</cite> variant was available, the <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p1.1.1">RoBERTa-base</span> was used for comparisons. Specifically, the encoder-only models we explored include Bio_ClinicalBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx5" title="">Alsentzer et al., 2019</a>]</cite>, medicalai/ClinicalBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx86" title="">Wang et al., 2023</a>]</cite>, RoBERTa-base <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx93" title="">Zhuang et al., 2021</a>]</cite>, and Clinical-Longformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx51" title="">Li et al., 2023</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1">We used the standard procedure for Masked Language Modelling (<span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.1">MLM</span>). First, the tokens that need to be masked are selected. They are then corrupted, resulting in masked text - that includes both masked and unmasked tokens. Next, the model predicts the masked tokens and replaces them with the ones that have the highest probabilities.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Encoder-Decoder Models with Random Masking</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Although encoder-decoder models are not typically used for masked language modelling, they are well-suited for text generation. The architecture of T5, in particular, is designed to maintain the coherence of the text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx70" title="">Raffel et al., 2020a</a>]</cite>. Therefore, we included the T5 family models in comparisons.</p>
</div>
<figure class="ltx_figure" id="S3.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="417" id="S3.F13.g1" src="extracted/5855287/figures/ComparisionArchitecture.drawio.png" width="440"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Comparison of Encoder-Only and Encoder-Decoder Model Architectures</figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1">The process of generating synthetic letters with encoder-decoder models is very similar to that with encoder-only models. The difference is that, unlike the BERT family, which automatically masks tokens and replaces them with ’<span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS2.p2.1.1">&lt;mask&gt;</span>‘, the T5 family models <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS2.p2.1.2">do not have any built-in masking function</span>. As a result, we identified the words that needed to be masked by <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p2.1.3">index</span> and removed them, which are represented as ‘extra_id_x’ in the T5 family models. The text, with these words removed, was then used for the generation, which we refer to as ‘<span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p2.1.4">text with blanks</span>’. To maintain consistency in the format, we later replaced ‘extra_id_x’ with ‘<span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS2.p2.1.5">&lt;mask&gt;</span>’ when displaying the masked text. Additionally, the T5 family models <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p2.1.6">require a prompt</span> as part of the input. For this task, the complete input was structured as ‘Fill in the blanks in the following sentence in the clinical background’ + ‘text with blanks’. In this project, we used T5-base <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx71" title="">Raffel et al., 2020b</a>]</cite>, Clinical-T5-Base <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx29" title="">Eric and Johnson, 2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx32" title="">Goldberger et al., 2000</a>]</cite>, Clinical-T5-Sci <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx29" title="">Eric and Johnson, 2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx32" title="">Goldberger et al., 2000</a>]</cite>, and Clinical-T5-Scratch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx29" title="">Eric and Johnson, 2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx32" title="">Goldberger et al., 2000</a>]</cite> for comparison. The comparison of encoder-only and encoder-decoder model architectures is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F13" title="Figure 13 ‣ 3.4.2 Encoder-Decoder Models with Random Masking ‣ 3.4 Clinical Letters Generation ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Different Masking Strategies with Bio_ClinicalBERT</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">To make the synthetic letters more readable, clinically sound, and privacy-protective, different masking strategies are tested based on the following principles.</p>
<ol class="ltx_enumerate" id="S3.I4">
<li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I4.i1.p1">
<p class="ltx_p" id="S3.I4.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I4.i1.p1.1.1">Preserve Annotated Entities: </span>The manually annotated entities should not be masked to retain the clinical knowledge and context.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I4.i2.p1">
<p class="ltx_p" id="S3.I4.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I4.i2.p1.1.1">Preserve Extracted Structures: </span>Tokens that are part of the document structure should be preserved as templates for clinical letters.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I4.i3.p1">
<p class="ltx_p" id="S3.I4.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I4.i3.p1.1.1">Mask Detected Private Information: </span>This is helpful in de-identification. Although the dataset we use is de-identified, this approach may be useful when this system is deployed with real-world data.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I4.i4.p1">
<p class="ltx_p" id="S3.I4.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I4.i4.p1.1.1">Preserve Medical Terminology: </span>It still aims to retain clinical knowledge, as some diseases and treatments were not manually annotated.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S3.I4.i5.p1">
<p class="ltx_p" id="S3.I4.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I4.i5.p1.1.1">Preserve Non-Private Numbers: </span>Certain numbers, such as drug dosage or heart rates, are indispensable for clinical diagnosis and treatment. However, only non-private numbers should be retained, while private information (such as phone numbers, ages, postal codes, dates, and email addresses) should be masked.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S3.I4.i6.p1">
<p class="ltx_p" id="S3.I4.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I4.i6.p1.1.1">Preserve Punctuation: </span> Punctuation marks such as periods (‘.’) and underscores (‘___’) should not be masked, as they clarify the sentence boundaries and make the synthetic letters more coherent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx49" title="">Lamprou et al., 2022</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S3.I4.i7.p1">
<p class="ltx_p" id="S3.I4.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I4.i7.p1.1.1">Retain Special Patterns in Samples: </span>Tokens that match specific patterns (e.g. ‘Vitamin C ^1000 mg’, ‘Ibuprofen &gt; 200 mg’, etc) should be retained, as they may contain important clinical details. These patterns are summarised by analysing raw sample letters.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p2">
<p class="ltx_p" id="S3.SS4.SSS3.p2.1">Based on the principles above, different masking strategies were experimented with:</p>
<ol class="ltx_enumerate" id="S3.I5">
<li class="ltx_item" id="S3.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I5.i1.p1">
<p class="ltx_p" id="S3.I5.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I5.i1.p1.1.1">Mask Randomly: </span>Tokens that can be masked are selected randomly from the text. We experimented with <span class="ltx_text ltx_font_italic" id="S3.I5.i1.p1.1.2">masking ratios</span> ranging from 0% to 100% in 10% increments. This approach helps to understand how the number of masked tokens influences the quality of synthetic letters, and provides a baseline for other masking strategies.</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I5.i2.p1">
<p class="ltx_p" id="S3.I5.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I5.i2.p1.1.1">Mask Based on POS Tagging: </span> We experimented with different configurations in this section, such as masking only <span class="ltx_text ltx_font_bold" id="S3.I5.i2.p1.1.2">nouns</span>, only <span class="ltx_text ltx_font_bold" id="S3.I5.i2.p1.1.3">verbs</span>, etc. It is helpful to understand how POS influences the models’ context understanding. Similar to the random masking approach, We selected the tokens based on their POS configuration and masked them in 10% increments from 0% to 100%.</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I5.i3.p1">
<p class="ltx_p" id="S3.I5.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I5.i3.p1.1.1">Mask Stopwords: </span>Stopwords generally contribute little to the text’s main idea. Masking stopwords serves two purposes: reducing the <span class="ltx_text ltx_font_italic" id="S3.I5.i3.p1.1.2">noise</span> for model understanding and increasing the <span class="ltx_text ltx_font_italic" id="S3.I5.i3.p1.1.3">variety</span> of synthetic text by predicting these words. Moreover, they do not influence crucial clinical information. This approach is highly similar to the one used in ‘Mask Based on POS Tagging’. The only difference is the criteria for selecting tokens. Specifically, tokens are selected based on whether they are stopwords rather than on their POS. ‘<span class="ltx_text ltx_font_bold" id="S3.I5.i3.p1.1.4">NLTK</span>’ library is used for detecting stopwords in the text.</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I5.i4.p1">
<p class="ltx_p" id="S3.I5.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I5.i4.p1.1.1">Hybrid Masking Using Different Ratio Settings: </span>After employing the aforementioned masking strategies, we observed the influence of these elements. Additionally, we experimented with their <span class="ltx_text ltx_font_italic" id="S3.I5.i4.p1.1.2">combinations</span> at different masking ratios based on the outcomes, such as masking 50% nouns and 50% stopwords simultaneously.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.4 </span>Determining Variable-Length-Chunk Size with Bio_ClinicalBERT</h4>
<div class="ltx_para" id="S3.SS4.SSS4.p1">
<p class="ltx_p" id="S3.SS4.SSS4.p1.1">As mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS3" title="3.3 Pre-Processing ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we utilise two parameters in our chunk segment procedure: ‘max_lines’ and ‘max_tokens’. ‘max_lines’ represents the desired length of each chunk, while ‘max_tokens’ is related to the computing resources and model limitations. These two parameters determine the final length of each chunk together. Although most models we used have a limit of 512 tokens (except for the Longformer, which can process up to 4096 tokens), we set 256 as the value for <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS4.p1.1.1">‘max_tokens’</span> due to the computing resources constraints.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS4.p2">
<p class="ltx_p" id="S3.SS4.SSS4.p2.1">As for ‘max_lines’, we experimented with values starting from 10 lines, increasing by 10 lines each time, and calculated the average tokens for each chunk. Once the token growth began to slow, we refined the search by using <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS4.p2.1.1">finer increments</span>. Finally, we selected the number of lines at which the average tokens per chunk stopped growing. This is because more lines in each chunk provide more information for the model to predict masked tokens. However, if the chunk length reaches a critical threshold, it indicates that the primary limitation is ‘max_tokens’, not ‘max_lines’. Continuing to increase ‘max_lines’ would lead to additional computational overhead, as the system would have to repeatedly check whether adding the next sentence meets the required line count.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Evaluation Methods</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Both quantitative and qualitative methods will be used to evaluate the performance. Additionally, a downstream task (NER) is employed to assess whether the synthetic clinical letters can replace the original raw data. The evaluation methods pipeline is illustrated in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F14" title="Figure 14 ‣ 3.5 Evaluation Methods ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="426" id="S3.F14.g1" src="extracted/5855287/figures/evaluationPipeline.png" width="497"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Evaluation Pipeline</figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1 </span>Quantitative Evaluation</h4>
<div class="ltx_para" id="S3.SS5.SSS1.p1">
<p class="ltx_p" id="S3.SS5.SSS1.p1.1">To comprehensively evaluate the quality of the synthetic letters, we used quantitative evaluation from multiple dimensions, including the model’s <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p1.1.1">inference</span> performance, the <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p1.1.2">readability</span> of the synthetic letters, and their <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p1.1.3">similarity</span> to the raw data. The specific metrics are listed below.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p2">
<p class="ltx_p" id="S3.SS5.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p2.1.1">Standard NLG Metrics</span>
It covers standard NLG evaluation methods such as ROUGE, BERTScore, and METEOR. ROUGE measures literal similarity, BERTScore evaluates semantic similarity, and METEOR builds on ROUGE by taking synonyms and word order into account. It provides a more comprehensive evaluation of synthetic text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx8" title="">Banerjee and Lavie, 2005</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p3">
<p class="ltx_p" id="S3.SS5.SSS1.p3.1">These evaluations will be performed by comparing synthetic text with the original text. Moreover, a baseline is calculated by comparing masked text to the original text. The evaluation score should exceed the baseline but remain below ‘1’, ensuring it does not exactly replicate the original text.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p4">
<p class="ltx_p" id="S3.SS5.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p4.1.1">Readability Metrics</span>
To evaluate the readability, we calculated <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p4.1.2">SMOG</span>, <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p4.1.3">Flesch Reading Ease</span>, and <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p4.1.4">Flesch-Kincaid Grade Level</span>. Given our clinical focus, we prioritise SMOG as the primary readability metric, with Flesch Reading Ease and Flesch-Kincaid Grade Level as reference standards. In this analysis, we will compare the readability metrics of the synthetic text with those of the original and masked texts. The evaluation results should closely approximate the original text’s metrics. Significant differences may suggest that the model can not preserve semantic coherence and readability adequately.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p5">
<p class="ltx_p" id="S3.SS5.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p5.1.1">Advanced Text Quality Metrics</span>
In this part, we calculated the <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p5.1.2">perplexity</span>, <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p5.1.3">subjectivity</span>, and <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p5.1.4">information entropy</span>. We want the synthetic letters to be useful in training clinical models. Therefore, perplexity should not be far away from the value of the original letters. As for subjectivity and information entropy, we expect the synthetic letters to be both subjective and informative.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p6">
<p class="ltx_p" id="S3.SS5.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p6.1.1">Invalid Prediction Rate</span>
We calculated the invalid prediction rate for each generation configuration. This ratio is determined by dividing the number of invalid predictions (such as punctuation marks or subwords) by the total number of masked words that need to be predicted. We expect the model to generate more meaningful words. Since punctuation marks are not masked, the model should avoid generating too many non-words. This metric can provide insights into the model’s inference capability.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p7">
<p class="ltx_p" id="S3.SS5.SSS1.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p7.1.1">Inference Time</span>
The inference time for each generation configuration across the whole dataset (204 clinical letters) was recorded. Shorter inference times indicate lower computational resource consumption. When this system is deployed on large datasets, it is expected to save both time and computing resources.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2 </span>Qualitative Evaluation</h4>
<div class="ltx_para" id="S3.SS5.SSS2.p1">
<p class="ltx_p" id="S3.SS5.SSS2.p1.1">In the quantitative evaluation, we not only calculated the evaluation metrics for the entire dataset, but also recorded the results for each individual synthetic clinical letter. Interestingly, while some synthetic texts exhibited strong performance according to most metrics, they did not always appear satisfactory upon “visual” inspection. Conversely, some synthetic letters with average metrics may appear more visually appealing.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS2.p2">
<p class="ltx_p" id="S3.SS5.SSS2.p2.1">Although human evaluation is the most reliable approach for evaluating clinical letters, it is limited by availability and cost. Therefore, combining qualitative and quantitative evaluations helps in identifying suitable quantitative metrics for assessing our model’s performance. Once identified, one of these metrics can be used as the <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS2.p2.1.1">primary standard</span>, while the others serve as supporting indicators. As a workaround, we selected a small sample of representative clinical letters based on the evaluation results. Subsequently, we reviewed the outcomes to better understand how different generation methods impacted these results, while also evaluating their correspondence with the quantitative metrics.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.3 </span>Downstream NER Task</h4>
<div class="ltx_para" id="S3.SS5.SSS3.p1">
<p class="ltx_p" id="S3.SS5.SSS3.p1.1">Beyond qualitative and quantitative evaluation, we can also apply synthetic clinical letters in a downstream NER task. This is helpful to further evaluate their quality and their potential to replace original ones in clinical research and model training.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS3.p2">
<p class="ltx_p" id="S3.SS5.SSS3.p2.1">ScispaCy <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://allenai.github.io/scispacy/</span></span></span></span> and spaCy <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://spacy.io/</span></span></span></span> are used in this part. As shown in Fig <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:_Training_Process_for_spaCy_NER_Model</span>, they extract features from the text and learn the weights of each feature through neural networks. These weights are updated by comparing the loss between the predicted probabilities and actual labels. If a word does not belong to any label, it is classified as ‘O’ (outside any entity). SpaCy initialises these weights randomly. However, the version of ScispaCy we use, (‘en_ner_bc5cdr_md’), is specifically fine-tuned on the <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p2.1.1">BC5CDR</span> corpus. It focuses more on the entities ‘chemical’ and ‘disease’ while retaining the original general features.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS3.p3">
<p class="ltx_p" id="S3.SS5.SSS3.p3.1">In this downstream NER task, as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F15" title="Figure 15 ‣ 3.5.3 Downstream NER Task ‣ 3.5 Evaluation Methods ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">15</span></a>, we initially extracted entities from letters using ScispaCy. Subsequently, these entities were used to train a base spaCy model. The trained model was then employed to extract entities from the testing set. Finally, we can compare these newly extracted entities with those originally extracted by ScispaCy, and the evaluation scores can be calculated. These steps were performed on both original clinical letters and synthetic letters, to assess whether the synthetic letters can potentially replace the original ones.</p>
</div>
<figure class="ltx_figure" id="S3.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="335" id="S3.F15.g1" src="extracted/5855287/figures/downstreamPipeline.drawio.png" width="628"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Workflow of Downstream NER Task</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Post-Processing</h3>
<section class="ltx_subsubsection" id="S3.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.1 </span>Filling in the blanks</h4>
<div class="ltx_para" id="S3.SS6.SSS1.p1">
<p class="ltx_p" id="S3.SS6.SSS1.p1.1">As described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS1" title="3.1 Data Set ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">3.1</span></a>, the dataset we used has been de-identified. All private information is replaced by three underscores ‘___’. We hope that the synthetic clinical letters can maintain a certain degree of clinical integrity without revealing any private patient information. To address this, a post-processing step was added to the synthetic results. This process involves masking the three underscores (‘___’) detected and using PLMs to predict the masked part again. For example, if the original text is ‘___ caught a cold’. The post-processing result should ideally be ‘John caught a cold’ or ‘Patient caught a cold’. Such synthetic clinical letters can better support clinical model training and teaching.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS1.p2">
<p class="ltx_p" id="S3.SS6.SSS1.p2.1">In this part, we used Bio_ClinicalBERT and BERT-base models. Although Bio_ClinicalBERT is better at clinical information understanding, this issue is not directly related to clinical practice, so we used BERT-base for comparison.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.2 </span>Spelling Correction</h4>
<div class="ltx_para" id="S3.SS6.SSS2.p1">
<p class="ltx_p" id="S3.SS6.SSS2.p1.1">Since our data comes from real-world sources, it is inevitable that some words may be misspelled by doctors. These spelling errors can negatively impact the model’s training process or hinder clinical practitioners’ understanding of the synthetic clinical letters. Although some errors are masked and re-generated, our masking ratio is not always 100%, so some incorrect words may still exist. A toolkit ‘TextBlob’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx55" title="">Loria, 2024</a>]</cite> is added to correct these errors. Specifically, it uses a rule-based approach that relies on a <span class="ltx_text ltx_font_bold" id="S3.SS6.SSS2.p1.1.1">built-in vocabulary library</span> to detect and correct misspellings.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Summary</h3>
<div class="ltx_para" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.1">In this section, we introduced the experimental design and subsequent implementation steps: from project requirement, data collection to environmental setup, pre-processing, masking and generating, post-processing, downstream NER task, and both qualitative and quantitative evaluation. An example of the entire process flow is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.F16" title="Figure 16 ‣ 3.7 Summary ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">16</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="655" id="S3.F16.g1" src="extracted/5855287/figures/1.ObjectiveExample.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>An Example of Masking and Generating</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results and Analysis</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Random Masking: Qualitative Results</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We employed both the encoder-only and encoder-decoder models to mask and generate the data, yielding
numerous interesting results for human evaluation. Given space constraints, only a simple example is provided here. Following the masking principles in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS4" title="3.4 Clinical Letters Generation ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">3.4</span></a>, the eligible tokens were randomly selected for masking. Although the initial intention was to mask 50% of tokens, the actual masking ratio was lower due to the requirement to preserve certain entities and structures.</p>
</div>
<figure class="ltx_figure" id="S4.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="127" id="S4.F17.g1" src="extracted/5855287/figures/OriginalSentence.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Original Unprocessed Example Sentence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx44" title="">Johnson et al., 2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx43" title="">Johnson et al., 2023b</a>]</cite> (‘note_id’: ’10807423-DS-19’) (The circled tokens will be masked)</figcaption>
</figure>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.1.1">Entity</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.2.1">Start</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.3.1">End</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T6.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.4.1">Concept ID</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.1.2.1.1">ankle pain</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.1.2.1.2">411</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.1.2.1.3">421</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.1.2.1.4">247373008</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.1.3.2.1">open fracture</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.1.3.2.2">468</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.1.3.2.3">481</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.1.3.2.4">397181002</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T6.1.4.3.1">fall</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.1.4.3.2">571</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.1.4.3.3">575</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T6.1.4.3.4">161898004</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Annotated Entities Extracted from the Example Sentence (They should be preserved from masking)</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Encoder-Only Models</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">The original sentence is displayed in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F17" title="Figure 17 ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">17</span></a>. After the feature extraction, the resulting structure is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F19" title="Figure 19 ‣ 4.1.1 Encoder-Only Models ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">19</span></a>.
As detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T6" title="Table 6 ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">6</span></a>, certain manually annotated entities are excluded from masking.
The output of this masking process can be seen in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F18" title="Figure 18 ‣ 4.1.1 Encoder-Only Models ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">18</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="135" id="S4.F18.g1" src="extracted/5855287/figures/MaskExample.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>An Example of the Masked Sentence</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F19"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="106" id="S4.F19.g1" src="extracted/5855287/figures/StructureForSampleSentence.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Extracted Structure from the Example Sentence</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F20"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="130" id="S4.F20.g1" src="extracted/5855287/figures/BioBERTSample.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Example Sentence Generated by Bio_ClinicalBERT</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">The generated text using Bio_ClinicalBERT is displayed in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F20" title="Figure 20 ‣ 4.1.1 Encoder-Only Models ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">20</span></a>. For
‘management of open fracture,’ the model produced ‘r’, which is commonly used to denote ‘right’ in clinical contexts, showing a relevant and logical prediction. Furthermore, the model’s input ‘R ankle’, despite not being in the figure due to space constraints, provided context for predicting ‘r’ instead of ‘left.’ Interestingly, the term ‘admitted’ was generated even though it was not in the input, indicating the model’s understanding of clinical context. Although the phrase ‘from 6 stairs, from home’ differs significantly from the original one, it remains contextually appropriate.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">Overall, Bio_ClinicalBERT produced a clinically sound sentence, even though no tokens matched the original. In other examples, the predicted words may partially overlap with the original text. Nonetheless, this model effectively retains clinical information and introduces diversity without altering the text’s meaning.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1">The results from medicalai/ClinicalBERT and Clinical-Longformer are shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F21" title="Figure 21 ‣ 4.1.1 Encoder-Only Models ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">21</span></a> and Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F22" title="Figure 22 ‣ 4.1.1 Encoder-Only Models ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">22</span></a>. All three clinical-related models correctly predicted ‘r’ from the input context. Medicalai/ClinicalBERT performs <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.1">comparably</span> to Bio_ClinicalBERT, despite adding an extra comma, which did not affect the text’s clarity.
However, Clinical-Longformer’s predictions, while understandable, were <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.2">repetitive</span> and less satisfactory. Importantly, none of these three models altered the original meaning.</p>
</div>
<figure class="ltx_figure" id="S4.F21"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="142" id="S4.F21.g1" src="extracted/5855287/figures/MedicalBERTSample.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>Example Sentence Generated by medicalai / ClinicalBERT</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F22"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="S4.F22.g1" src="extracted/5855287/figures/ClinicalLongformerSample.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>Example Sentence Generated by Clinical-Longformer</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F23"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="140" id="S4.F23.g1" src="extracted/5855287/figures/RoBERTaSample.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>Example Sentence Generated by RoBERTa-base</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p5">
<p class="ltx_p" id="S4.SS1.SSS1.p5.1">The result generated by RoBERTa-base is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F23" title="Figure 23 ‣ 4.1.1 Encoder-Only Models ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">23</span></a>. While the generated text initially seems reasonable, the predicted word ‘years’ shifts the focus to a temporal context, which was not intended.
This is likely because RoBERTa is pre-trained on a general corpus and lacks sufficient clinical knowledge for accurate text generation, or it could simply be a coincidence based on this specific sentence, where RoBERTa-base inferred ‘years’ from its training data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Decoder-Only GPT-4o</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Additionally, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.1">GPT-4o</span> was used for comparison, with the prompt “Replace ‘<span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS2.p1.1.2">&lt;mask&gt;</span>’ with words in the following sentence: ”. The results, shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F24" title="Figure 24 ‣ 4.1.2 Decoder-Only GPT-4o ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">24</span></a>, are satisfactory. As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3" title="2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">2.3</span></a>, decoder-only models excel in few-shot learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx87" title="">Wu, 2024</a>]</cite>, which is confirmed by this experiment. However, its performance may decline with long clinical letters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx7" title="">Amin-Nejad et al., 2020b</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F24"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="184" id="S4.F24.g1" src="extracted/5855287/figures/GPTSample.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 24: </span>Example Sentence Generated by GPT-4o</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Encoder-Decoder Models</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">To further evaluate different PLMs in generating synthetic letters, we tested the T5 Family models. The generated results for the same sentence are shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F25" title="Figure 25 ‣ 4.1.3 Encoder-Decoder Models ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">25</span></a>, Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F26" title="Figure 26 ‣ 4.1.3 Encoder-Decoder Models ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">26</span></a>, Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F27" title="Figure 27 ‣ 4.1.3 Encoder-Decoder Models ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">27</span></a>, and Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F28" title="Figure 28 ‣ 4.1.3 Encoder-Decoder Models ‣ 4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">28</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1">T5-base performs the best among these tested models. However, the results are still <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p2.1.1">not fully rational</span>, as it generated ‘open is a ___ yo male’.
The other three models tend to use de-identification (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p2.1.2">DEID</span>) tags to replace the masked words, as these tags are part of their corpora. Furthermore, the T5 family models may predict multiple words for each token, aligning with findings in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3" title="2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">2.3</span></a></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p3">
<p class="ltx_p" id="S4.SS1.SSS3.p3.1">All these four T5 family models perform <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p3.1.1">worse than the encoder-only</span> models. This is consistent with the findings from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx63" title="">Micheletti et al., 2024</a>]</cite> that Masked Language Modelling (MLM) models significantly outperform Causal Language Modeling (CLM) models in medical datasets.</p>
</div>
<figure class="ltx_figure" id="S4.F25"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="125" id="S4.F25.g1" src="extracted/5855287/figures/T5.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 25: </span>Example Sentence Generated by T5-base</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F26"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="144" id="S4.F26.g1" src="extracted/5855287/figures/ClinicalT5.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 26: </span>Example Sentence Generated by Clinical-T5-Base</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F27"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="158" id="S4.F27.g1" src="extracted/5855287/figures/T5Scratch.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 27: </span>Example Sentence Generated by Clinical-T5-Scratch</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F28"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="109" id="S4.F28.g1" src="extracted/5855287/figures/T5Sci.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 28: </span>Example Sentence Generated by Clinical-T5-Sci</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Random Masking: Quantitative Results</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Sentence-Level Quantitative Results: Encoder-Only Models</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">We first calculated representative quantitative metrics at the sentence level, matching the sample sentence used in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS1" title="4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">4.1</span></a>. This approach allows for a better integration of quantitative and qualitative evaluations.
Although SMOG is typically suited for medical datasets, it is less appropriate for sentence-level analysis, so the Flesch Reading Ease was used here. The results are shown in table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T7" title="Table 7 ‣ 4.2.1 Sentence-Level Quantitative Results: Encoder-Only Models ‣ 4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S4.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.2.1">Model Evaluation</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T7.1.2.2.1"></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.2.2.2.1">
<span class="ltx_p" id="S4.T7.1.2.2.2.1.1" style="width:68.3pt;">RoBERTa-base</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.2.2.3.1">
<span class="ltx_p" id="S4.T7.1.2.2.3.1.1" style="width:68.3pt;">medicalai / ClinicalBERT</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.2.2.4.1">
<span class="ltx_p" id="S4.T7.1.2.2.4.1.1" style="width:68.3pt;">Clinical-Longformer</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.2.2.5.1">
<span class="ltx_p" id="S4.T7.1.2.2.5.1.1" style="width:68.3pt;">Bio _ ClinicalBERT</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T7.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T7.1.3.3.1.1">ROUGE-1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T7.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.1.4.4.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.4.4.2.1">
<span class="ltx_p" id="S4.T7.1.4.4.2.1.1" style="width:68.3pt;">86.54</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.4.4.3.1">
<span class="ltx_p" id="S4.T7.1.4.4.3.1.1" style="width:68.3pt;">88.46</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.4.4.4.1">
<span class="ltx_p" id="S4.T7.1.4.4.4.1.1" style="width:68.3pt;">89.52</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.4.4.5.1">
<span class="ltx_p" id="S4.T7.1.4.4.5.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T7.1.5.5.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.5.5.2.1">
<span class="ltx_p" id="S4.T7.1.5.5.2.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.5.5.3.1">
<span class="ltx_p" id="S4.T7.1.5.5.3.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.5.5.4.1">
<span class="ltx_p" id="S4.T7.1.5.5.4.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.5.5.5.1">
<span class="ltx_p" id="S4.T7.1.5.5.5.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T7.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T7.1.6.6.1.1">ROUGE-2</span></th>
</tr>
<tr class="ltx_tr" id="S4.T7.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.1.7.7.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.7.7.2.1">
<span class="ltx_p" id="S4.T7.1.7.7.2.1.1" style="width:68.3pt;">74.51</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.7.7.3.1">
<span class="ltx_p" id="S4.T7.1.7.7.3.1.1" style="width:68.3pt;">78.43</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.7.7.4.1">
<span class="ltx_p" id="S4.T7.1.7.7.4.1.1" style="width:68.3pt;">79.61</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.7.7.5.1">
<span class="ltx_p" id="S4.T7.1.7.7.5.1.1" style="width:68.3pt;">73.08</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T7.1.8.8.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.8.8.2.1">
<span class="ltx_p" id="S4.T7.1.8.8.2.1.1" style="width:68.3pt;">73.08</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.8.8.3.1">
<span class="ltx_p" id="S4.T7.1.8.8.3.1.1" style="width:68.3pt;">73.08</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.8.8.4.1">
<span class="ltx_p" id="S4.T7.1.8.8.4.1.1" style="width:68.3pt;">73.08</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.8.8.5.1">
<span class="ltx_p" id="S4.T7.1.8.8.5.1.1" style="width:68.3pt;">73.08</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T7.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S4.T7.1.9.9.1.1">ROUGE-L</span></th>
</tr>
<tr class="ltx_tr" id="S4.T7.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.1.10.10.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.10.10.2.1">
<span class="ltx_p" id="S4.T7.1.10.10.2.1.1" style="width:68.3pt;">86.54</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.10.10.3.1">
<span class="ltx_p" id="S4.T7.1.10.10.3.1.1" style="width:68.3pt;">88.46</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.10.10.4.1">
<span class="ltx_p" id="S4.T7.1.10.10.4.1.1" style="width:68.3pt;">89.52</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.10.10.5.1">
<span class="ltx_p" id="S4.T7.1.10.10.5.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T7.1.11.11.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.11.11.2.1">
<span class="ltx_p" id="S4.T7.1.11.11.2.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.11.11.3.1">
<span class="ltx_p" id="S4.T7.1.11.11.3.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.11.11.4.1">
<span class="ltx_p" id="S4.T7.1.11.11.4.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.11.11.5.1">
<span class="ltx_p" id="S4.T7.1.11.11.5.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T7.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T7.1.12.12.1.1">BERTScore F1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T7.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.1.13.13.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.13.13.2.1">
<span class="ltx_p" id="S4.T7.1.13.13.2.1.1" style="width:68.3pt;">0.81</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.13.13.3.1">
<span class="ltx_p" id="S4.T7.1.13.13.3.1.1" style="width:68.3pt;">0.83</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.13.13.4.1">
<span class="ltx_p" id="S4.T7.1.13.13.4.1.1" style="width:68.3pt;">0.84</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.13.13.5.1">
<span class="ltx_p" id="S4.T7.1.13.13.5.1.1" style="width:68.3pt;">0.85</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T7.1.14.14.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.14.14.2.1">
<span class="ltx_p" id="S4.T7.1.14.14.2.1.1" style="width:68.3pt;">0.79</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.14.14.3.1">
<span class="ltx_p" id="S4.T7.1.14.14.3.1.1" style="width:68.3pt;">0.65</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.14.14.4.1">
<span class="ltx_p" id="S4.T7.1.14.14.4.1.1" style="width:68.3pt;">0.79</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.14.14.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.14.14.5.1">
<span class="ltx_p" id="S4.T7.1.14.14.5.1.1" style="width:68.3pt;">0.65</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T7.1.15.15.1"><span class="ltx_text ltx_font_bold" id="S4.T7.1.15.15.1.1">METEOR</span></th>
</tr>
<tr class="ltx_tr" id="S4.T7.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.1.16.16.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.16.16.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.16.16.2.1">
<span class="ltx_p" id="S4.T7.1.16.16.2.1.1" style="width:68.3pt;">0.87</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.16.16.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.16.16.3.1">
<span class="ltx_p" id="S4.T7.1.16.16.3.1.1" style="width:68.3pt;">0.88</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.16.16.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.16.16.4.1">
<span class="ltx_p" id="S4.T7.1.16.16.4.1.1" style="width:68.3pt;">0.90</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.16.16.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.16.16.5.1">
<span class="ltx_p" id="S4.T7.1.16.16.5.1.1" style="width:68.3pt;">0.86</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.17.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T7.1.17.17.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.17.17.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.17.17.2.1">
<span class="ltx_p" id="S4.T7.1.17.17.2.1.1" style="width:68.3pt;">0.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.17.17.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.17.17.3.1">
<span class="ltx_p" id="S4.T7.1.17.17.3.1.1" style="width:68.3pt;">0.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.17.17.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.17.17.4.1">
<span class="ltx_p" id="S4.T7.1.17.17.4.1.1" style="width:68.3pt;">0.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.17.17.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.17.17.5.1">
<span class="ltx_p" id="S4.T7.1.17.17.5.1.1" style="width:68.3pt;">0.85</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.18.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T7.1.18.18.1"><span class="ltx_text ltx_font_bold" id="S4.T7.1.18.18.1.1">Flesch Reading Ease</span></th>
</tr>
<tr class="ltx_tr" id="S4.T7.1.19.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T7.1.19.19.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.19.19.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.19.19.2.1">
<span class="ltx_p" id="S4.T7.1.19.19.2.1.1" style="width:68.3pt;">10.24</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.19.19.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.19.19.3.1">
<span class="ltx_p" id="S4.T7.1.19.19.3.1.1" style="width:68.3pt;">18.70</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.19.19.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.19.19.4.1">
<span class="ltx_p" id="S4.T7.1.19.19.4.1.1" style="width:68.3pt;">9.22</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T7.1.19.19.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.19.19.5.1">
<span class="ltx_p" id="S4.T7.1.19.19.5.1.1" style="width:68.3pt;">16.67</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.20.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T7.1.20.20.1">Baseline (Original)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.20.20.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.20.20.2.1">
<span class="ltx_p" id="S4.T7.1.20.20.2.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.20.20.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.20.20.3.1">
<span class="ltx_p" id="S4.T7.1.20.20.3.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.20.20.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.20.20.4.1">
<span class="ltx_p" id="S4.T7.1.20.20.4.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T7.1.20.20.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.20.20.5.1">
<span class="ltx_p" id="S4.T7.1.20.20.5.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.21.21">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S4.T7.1.21.21.1">Baseline (Mask)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T7.1.21.21.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.21.21.2.1">
<span class="ltx_p" id="S4.T7.1.21.21.2.1.1" style="width:68.3pt;">16.67</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T7.1.21.21.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.21.21.3.1">
<span class="ltx_p" id="S4.T7.1.21.21.3.1.1" style="width:68.3pt;">16.67</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T7.1.21.21.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.21.21.4.1">
<span class="ltx_p" id="S4.T7.1.21.21.4.1.1" style="width:68.3pt;">16.67</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T7.1.21.21.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T7.1.21.21.5.1">
<span class="ltx_p" id="S4.T7.1.21.21.5.1.1" style="width:68.3pt;">16.67</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Encoder-Only Models Comparison at the Sentence Level (The ‘Baseline’ without annotations was calculated by comparing masked text to the original text)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">Our objective is to generate letters that differ from the original while maintaining clinical semantics and structure. Thus, high ROUGE scores are not desired, as they indicate significant <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.1">word/string</span> overlap. BERTScore is particularly useful for assessing <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.2">semantic</span> similarity, while METEOR offers a comprehensive evaluation considering word forms and synonyms theoretically. Flesch Reading Ease, on the other hand, provides a direct measure of textual <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.3">readability</span>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">We observed that clinical-related encoder-only models generally outperform RoBERTa-base in qualitative evaluation (see Subsection <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS1" title="4.1 Random Masking: Qualitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">4.1</span></a>).
However, from the quantitative perspective, RoBERTa-base shows mediocre performance across most metrics except for BERTScore. In contrast, Bio_ClinicalBERT, despite no word overlap in this sample sentence, achieves a reasonable clinical context and the highest BERTScore among the models. Both Medicalai/Clinical BERT and Bio_ClinicalBERT excel in Flesch Reading Ease, likely because they tend to predict tokens with fewer syllables words that preserve the original meaning.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">Surprisingly, while METEOR is designed to closely reflect human evaluation, BERTScore appears to be more consistent with our evaluation criteria. This trend was observed in other sample texts as well. <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p4.1.1">Synthetic texts with higher BERTScore and lower ROUGE scores are more aligned with our objectives</span>. It is likely because BERTScore is calculated using word embeddings, which can capture deep semantic similarity more effectively. All evaluation results <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p4.1.2">meet or exceed the baseline, affirming the effectiveness of these four encoder-only models</span> in generating clinical letters.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Sentence-Level Quantitative Results: Encoder-Decoder Models</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">The evaluations for the encoder-decoder models, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T8" title="Table 8 ‣ 4.2.2 Sentence-Level Quantitative Results: Encoder-Decoder Models ‣ 4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">8</span></a>, generally underperform on most
metrics compared to encoder-only models, except for METEOR. Interestingly, while the Flesch Reading Ease scores suggest a minimal impact on readability, the BERTScores are significantly lower than the baseline, indicating major deviations from the original meaning. This is consistent with our qualitative observations that the outputs from encoder-decoder models are largely unintelligible.</p>
</div>
<figure class="ltx_table" id="S4.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T8.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T8.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S4.T8.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.1.2.1">Model Evaluation</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T8.1.2.2.1"></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.2.2.2.1">
<span class="ltx_p" id="S4.T8.1.2.2.2.1.1" style="width:68.3pt;">T5-base</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.2.2.3.1">
<span class="ltx_p" id="S4.T8.1.2.2.3.1.1" style="width:68.3pt;">Clinical-T5-base</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.2.2.4.1">
<span class="ltx_p" id="S4.T8.1.2.2.4.1.1" style="width:68.3pt;">Clinical-T5-Scratch</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.2.2.5.1">
<span class="ltx_p" id="S4.T8.1.2.2.5.1.1" style="width:68.3pt;">Clinical-T5-Sci</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T8.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.3.3.1.1">ROUGE-1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T8.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.1.4.4.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.4.4.2.1">
<span class="ltx_p" id="S4.T8.1.4.4.2.1.1" style="width:68.3pt;">86.79</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.4.4.3.1">
<span class="ltx_p" id="S4.T8.1.4.4.3.1.1" style="width:68.3pt;">85.19</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.4.4.4.1">
<span class="ltx_p" id="S4.T8.1.4.4.4.1.1" style="width:68.3pt;">87.38</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.4.4.5.1">
<span class="ltx_p" id="S4.T8.1.4.4.5.1.1" style="width:68.3pt;">80.36</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T8.1.5.5.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.5.5.2.1">
<span class="ltx_p" id="S4.T8.1.5.5.2.1.1" style="width:68.3pt;">73.77</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.5.5.3.1">
<span class="ltx_p" id="S4.T8.1.5.5.3.1.1" style="width:68.3pt;">73.77</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.5.5.4.1">
<span class="ltx_p" id="S4.T8.1.5.5.4.1.1" style="width:68.3pt;">73.77</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.5.5.5.1">
<span class="ltx_p" id="S4.T8.1.5.5.5.1.1" style="width:68.3pt;">73.77</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T8.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.6.6.1.1">ROUGE-2</span></th>
</tr>
<tr class="ltx_tr" id="S4.T8.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.1.7.7.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.7.7.2.1">
<span class="ltx_p" id="S4.T8.1.7.7.2.1.1" style="width:68.3pt;">75.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.7.7.3.1">
<span class="ltx_p" id="S4.T8.1.7.7.3.1.1" style="width:68.3pt;">71.70</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.7.7.4.1">
<span class="ltx_p" id="S4.T8.1.7.7.4.1.1" style="width:68.3pt;">75.25</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.7.7.5.1">
<span class="ltx_p" id="S4.T8.1.7.7.5.1.1" style="width:68.3pt;">69.09</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T8.1.8.8.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.8.8.2.1">
<span class="ltx_p" id="S4.T8.1.8.8.2.1.1" style="width:68.3pt;">63.33</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.8.8.3.1">
<span class="ltx_p" id="S4.T8.1.8.8.3.1.1" style="width:68.3pt;">63.33</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.8.8.4.1">
<span class="ltx_p" id="S4.T8.1.8.8.4.1.1" style="width:68.3pt;">63.33</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.8.8.5.1">
<span class="ltx_p" id="S4.T8.1.8.8.5.1.1" style="width:68.3pt;">63.33</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T8.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.9.9.1.1">ROUGE-L</span></th>
</tr>
<tr class="ltx_tr" id="S4.T8.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.1.10.10.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.10.10.2.1">
<span class="ltx_p" id="S4.T8.1.10.10.2.1.1" style="width:68.3pt;">84.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.10.10.3.1">
<span class="ltx_p" id="S4.T8.1.10.10.3.1.1" style="width:68.3pt;">83.33</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.10.10.4.1">
<span class="ltx_p" id="S4.T8.1.10.10.4.1.1" style="width:68.3pt;">87.38</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.10.10.5.1">
<span class="ltx_p" id="S4.T8.1.10.10.5.1.1" style="width:68.3pt;">80.36</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T8.1.11.11.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.11.11.2.1">
<span class="ltx_p" id="S4.T8.1.11.11.2.1.1" style="width:68.3pt;">73.77</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.11.11.3.1">
<span class="ltx_p" id="S4.T8.1.11.11.3.1.1" style="width:68.3pt;">73.77</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.11.11.4.1">
<span class="ltx_p" id="S4.T8.1.11.11.4.1.1" style="width:68.3pt;">73.77</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.11.11.5.1">
<span class="ltx_p" id="S4.T8.1.11.11.5.1.1" style="width:68.3pt;">73.77</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T8.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.12.12.1.1">BERTScore F1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T8.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.1.13.13.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.13.13.2.1">
<span class="ltx_p" id="S4.T8.1.13.13.2.1.1" style="width:68.3pt;">0.44</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.13.13.3.1">
<span class="ltx_p" id="S4.T8.1.13.13.3.1.1" style="width:68.3pt;">0.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.13.13.4.1">
<span class="ltx_p" id="S4.T8.1.13.13.4.1.1" style="width:68.3pt;">0.45</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.13.13.5.1">
<span class="ltx_p" id="S4.T8.1.13.13.5.1.1" style="width:68.3pt;">0.40</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T8.1.14.14.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.14.14.2.1">
<span class="ltx_p" id="S4.T8.1.14.14.2.1.1" style="width:68.3pt;">0.50</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.14.14.3.1">
<span class="ltx_p" id="S4.T8.1.14.14.3.1.1" style="width:68.3pt;">0.50</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.14.14.4.1">
<span class="ltx_p" id="S4.T8.1.14.14.4.1.1" style="width:68.3pt;">0.50</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.14.14.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.14.14.5.1">
<span class="ltx_p" id="S4.T8.1.14.14.5.1.1" style="width:68.3pt;">0.50</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T8.1.15.15.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.15.15.1.1">METEOR</span></th>
</tr>
<tr class="ltx_tr" id="S4.T8.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.1.16.16.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.16.16.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.16.16.2.1">
<span class="ltx_p" id="S4.T8.1.16.16.2.1.1" style="width:68.3pt;">0.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.16.16.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.16.16.3.1">
<span class="ltx_p" id="S4.T8.1.16.16.3.1.1" style="width:68.3pt;">0.83</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.16.16.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.16.16.4.1">
<span class="ltx_p" id="S4.T8.1.16.16.4.1.1" style="width:68.3pt;">0.83</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.16.16.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.16.16.5.1">
<span class="ltx_p" id="S4.T8.1.16.16.5.1.1" style="width:68.3pt;">0.82</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.17.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T8.1.17.17.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.17.17.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.17.17.2.1">
<span class="ltx_p" id="S4.T8.1.17.17.2.1.1" style="width:68.3pt;">0.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.17.17.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.17.17.3.1">
<span class="ltx_p" id="S4.T8.1.17.17.3.1.1" style="width:68.3pt;">0.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.17.17.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.17.17.4.1">
<span class="ltx_p" id="S4.T8.1.17.17.4.1.1" style="width:68.3pt;">0.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.17.17.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.17.17.5.1">
<span class="ltx_p" id="S4.T8.1.17.17.5.1.1" style="width:68.3pt;">0.85</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.18.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T8.1.18.18.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.18.18.1.1">Flesch Reading Ease</span></th>
</tr>
<tr class="ltx_tr" id="S4.T8.1.19.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T8.1.19.19.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.19.19.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.19.19.2.1">
<span class="ltx_p" id="S4.T8.1.19.19.2.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.19.19.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.19.19.3.1">
<span class="ltx_p" id="S4.T8.1.19.19.3.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.19.19.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.19.19.4.1">
<span class="ltx_p" id="S4.T8.1.19.19.4.1.1" style="width:68.3pt;">19.71</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T8.1.19.19.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.19.19.5.1">
<span class="ltx_p" id="S4.T8.1.19.19.5.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.20.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T8.1.20.20.1">Baseline (Original)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.20.20.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.20.20.2.1">
<span class="ltx_p" id="S4.T8.1.20.20.2.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.20.20.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.20.20.3.1">
<span class="ltx_p" id="S4.T8.1.20.20.3.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.20.20.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.20.20.4.1">
<span class="ltx_p" id="S4.T8.1.20.20.4.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T8.1.20.20.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.20.20.5.1">
<span class="ltx_p" id="S4.T8.1.20.20.5.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.21.21">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S4.T8.1.21.21.1">Baseline (Mask)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T8.1.21.21.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.21.21.2.1">
<span class="ltx_p" id="S4.T8.1.21.21.2.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T8.1.21.21.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.21.21.3.1">
<span class="ltx_p" id="S4.T8.1.21.21.3.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T8.1.21.21.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.21.21.4.1">
<span class="ltx_p" id="S4.T8.1.21.21.4.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T8.1.21.21.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.1.21.21.5.1">
<span class="ltx_p" id="S4.T8.1.21.21.5.1.1" style="width:68.3pt;">8.21</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Encoder-Decoder Models Comparison at the Sentence Level (The Baseline without annotations was calculated by comparing masked text to the original text)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">Collectively, the quantitative and qualitative results demonstrate that <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS2.SSS2.p2.1.1">encoder-decoder models are not well-suited for generating clinical letters</span>, as they fail to preserve the original narratives. These results also support the validity of using BERTScore as the primary evaluation metric, with other metrics serving as supplementary references. We also tested this on the <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p2.1.2">entire dataset</span>, which produced <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p2.1.3">consistent</span> results.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Quantitative Results on the Full Dataset: Encoder-Only Models</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Based on the findings above, we expect a higher BERTScore and a lower ROUGE Score. We used the 0.4 masking ratio to illustrate the model comparison on the full dataset in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T9" title="Table 9 ‣ 4.2.3 Quantitative Results on the Full Dataset: Encoder-Only Models ‣ 4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">9</span></a>. The other masking ratios show similar trends. Surprisingly, all encoder-only models this time showed comparable results, which contradicts our hypothesis that ‘Clinical-related’ models would outperform base models. This suggests that <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p1.1.1">training on the clinical dataset does not significantly impact the quality of synthetic letters</span>. This may be because most clinical-related tokens are preserved, with only the remaining tokens being eligible for masking. Consequently, the normal encoder-only models can effectively understand the context and predict appropriate words while preserving clinical information. This differs slightly from the sentence-level comparisons, likely because the evaluation of a single sentence cannot fully represent the overall results. Despite this, BERTScore as a primary evaluation metric remains useful, as the correspondence between qualitative and quantitative evaluation is consistent, whether at the sentence or dataset level.</p>
</div>
<figure class="ltx_table" id="S4.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T9.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T9.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T9.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S4.T9.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.1.2.1">Model Evaluation</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T9.1.2.2.1"></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.2.2.2.1">
<span class="ltx_p" id="S4.T9.1.2.2.2.1.1" style="width:68.3pt;">RoBERTa-base</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.2.2.3.1">
<span class="ltx_p" id="S4.T9.1.2.2.3.1.1" style="width:68.3pt;">medicalai / ClinicalBERT</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.2.2.4.1">
<span class="ltx_p" id="S4.T9.1.2.2.4.1.1" style="width:68.3pt;">Clinical-Longformer</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.2.2.5.1">
<span class="ltx_p" id="S4.T9.1.2.2.5.1.1" style="width:68.3pt;">Bio_ ClinicalBERT</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T9.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T9.1.3.3.1.1">ROUGE-1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T9.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T9.1.4.4.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.4.4.2.1">
<span class="ltx_p" id="S4.T9.1.4.4.2.1.1" style="width:68.3pt;">92.98</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.4.4.3.1">
<span class="ltx_p" id="S4.T9.1.4.4.3.1.1" style="width:68.3pt;">93.63</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.4.4.4.1">
<span class="ltx_p" id="S4.T9.1.4.4.4.1.1" style="width:68.3pt;">94.66</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.4.4.5.1">
<span class="ltx_p" id="S4.T9.1.4.4.5.1.1" style="width:68.3pt;">93.18</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T9.1.5.5.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.5.5.2.1">
<span class="ltx_p" id="S4.T9.1.5.5.2.1.1" style="width:68.3pt;">85.64</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.5.5.3.1">
<span class="ltx_p" id="S4.T9.1.5.5.3.1.1" style="width:68.3pt;">85.44</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.5.5.4.1">
<span class="ltx_p" id="S4.T9.1.5.5.4.1.1" style="width:68.3pt;">85.64</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.5.5.5.1">
<span class="ltx_p" id="S4.T9.1.5.5.5.1.1" style="width:68.3pt;">85.61</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T9.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T9.1.6.6.1.1">ROUGE-2</span></th>
</tr>
<tr class="ltx_tr" id="S4.T9.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T9.1.7.7.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.7.7.2.1">
<span class="ltx_p" id="S4.T9.1.7.7.2.1.1" style="width:68.3pt;">86.10</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.7.7.3.1">
<span class="ltx_p" id="S4.T9.1.7.7.3.1.1" style="width:68.3pt;">87.42</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.7.7.4.1">
<span class="ltx_p" id="S4.T9.1.7.7.4.1.1" style="width:68.3pt;">89.50</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.7.7.5.1">
<span class="ltx_p" id="S4.T9.1.7.7.5.1.1" style="width:68.3pt;">86.50</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T9.1.8.8.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.8.8.2.1">
<span class="ltx_p" id="S4.T9.1.8.8.2.1.1" style="width:68.3pt;">74.96</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.8.8.3.1">
<span class="ltx_p" id="S4.T9.1.8.8.3.1.1" style="width:68.3pt;">74.64</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.8.8.4.1">
<span class="ltx_p" id="S4.T9.1.8.8.4.1.1" style="width:68.3pt;">74.96</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.8.8.5.1">
<span class="ltx_p" id="S4.T9.1.8.8.5.1.1" style="width:68.3pt;">74.92</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T9.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S4.T9.1.9.9.1.1">ROUGE-L</span></th>
</tr>
<tr class="ltx_tr" id="S4.T9.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T9.1.10.10.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.10.10.2.1">
<span class="ltx_p" id="S4.T9.1.10.10.2.1.1" style="width:68.3pt;">92.54</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.10.10.3.1">
<span class="ltx_p" id="S4.T9.1.10.10.3.1.1" style="width:68.3pt;">93.22</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.10.10.4.1">
<span class="ltx_p" id="S4.T9.1.10.10.4.1.1" style="width:68.3pt;">94.38</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.10.10.5.1">
<span class="ltx_p" id="S4.T9.1.10.10.5.1.1" style="width:68.3pt;">92.71</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T9.1.11.11.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.11.11.2.1">
<span class="ltx_p" id="S4.T9.1.11.11.2.1.1" style="width:68.3pt;">85.64</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.11.11.3.1">
<span class="ltx_p" id="S4.T9.1.11.11.3.1.1" style="width:68.3pt;">85.44</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.11.11.4.1">
<span class="ltx_p" id="S4.T9.1.11.11.4.1.1" style="width:68.3pt;">85.64</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T9.1.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.11.11.5.1">
<span class="ltx_p" id="S4.T9.1.11.11.5.1.1" style="width:68.3pt;">85.61</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T9.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T9.1.12.12.1.1">BERTScore F1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T9.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T9.1.13.13.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.13.13.2.1">
<span class="ltx_p" id="S4.T9.1.13.13.2.1.1" style="width:68.3pt;">0.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.13.13.3.1">
<span class="ltx_p" id="S4.T9.1.13.13.3.1.1" style="width:68.3pt;">0.90</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.13.13.4.1">
<span class="ltx_p" id="S4.T9.1.13.13.4.1.1" style="width:68.3pt;">0.92</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T9.1.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.13.13.5.1">
<span class="ltx_p" id="S4.T9.1.13.13.5.1.1" style="width:68.3pt;">0.90</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S4.T9.1.14.14.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T9.1.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.14.14.2.1">
<span class="ltx_p" id="S4.T9.1.14.14.2.1.1" style="width:68.3pt;">0.82</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T9.1.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.14.14.3.1">
<span class="ltx_p" id="S4.T9.1.14.14.3.1.1" style="width:68.3pt;">0.63</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T9.1.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.14.14.4.1">
<span class="ltx_p" id="S4.T9.1.14.14.4.1.1" style="width:68.3pt;">0.82</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T9.1.14.14.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.1.14.14.5.1">
<span class="ltx_p" id="S4.T9.1.14.14.5.1.1" style="width:68.3pt;">0.63</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Encoder-Only Models Comparison on the Full Dataset with Masking Ratio 0.4 (The Baseline was calculated by comparing masked text to the original text)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">We will now explore how different <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p2.1.1">masking ratios</span> affect the quality of synthetic clinical letters. For each model, we generated data with masking ratios from 0.0 to 1.0, in increments of 0.1 (the masking ratios here refer only to the eligible tokens, as described in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS4.SSS3" title="3.4.3 Different Masking Strategies with Bio_ClinicalBERT ‣ 3.4 Clinical Letters Generation ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">3.4.3</span></a>, and do not represent the actual overall masking ratio). Due to space limitations, we will present only the results for Bio_ClinicalBERT with a 0.2 increment here.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T10" title="Table 10 ‣ 4.2.3 Quantitative Results on the Full Dataset: Encoder-Only Models ‣ 4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">10</span></a> shows that the higher masking ratio, the lower the similarity (metrics’ scores) will be. As we expected, all evaluation values are higher than the baseline, but still below ‘1’. This means the model can understand the clinical context and generate understandable text. It is surprising that with the masking ratio of 1.0, BERTScore increased from the baseline (0.29) to 0.63. Although this score is not very high, it still reflects that Bio_ClinicalBERT can generate clinical text effectively.</p>
</div>
<figure class="ltx_table" id="S4.T10">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T10.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T10.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T10.1.1.1.1.1">Bio_ClinicalBERT</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="6" id="S4.T10.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T10.1.1.1.2.1">Masking Ratio</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.2.2">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T10.1.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T10.1.2.2.2.1">1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T10.1.2.2.3.1">0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T10.1.2.2.4.1">0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T10.1.2.2.5.1">0.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T10.1.2.2.6.1">0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T10.1.2.2.7.1">0.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T10.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T10.1.3.3.1.1">ROUGE-1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.4.4.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.4.4.2">76.28</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.4.4.3">83.75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.4.4.4">88.91</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.4.4.5">93.18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.4.4.6">96.76</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.4.4.7">99.51</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.5.5.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.5.5.2">64.05</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.5.5.3">71.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.5.5.4">78.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.5.5.5">85.61</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.5.5.6">92.63</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.5.5.7">99.22</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T10.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T10.1.6.6.1.1">ROUGE-2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.7.7.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.7.7.2">62.60</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.7.7.3">70.77</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.7.7.4">78.81</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.7.7.5">86.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.7.7.6">93.42</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.7.7.7">99.02</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.8.8.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.8.8.2">51.72</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.8.8.3">57.88</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.8.8.4">65.38</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.8.8.5">74.92</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.8.8.6">86.27</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.8.8.7">98.61</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T10.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S4.T10.1.9.9.1.1">ROUGE-L</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.10.10.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.10.10.2">74.33</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.10.10.3">81.69</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.10.10.4">87.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.10.10.5">92.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.10.10.6">96.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.10.10.7">99.50</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.11.11.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.11.11.2">64.05</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.11.11.3">71.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.11.11.4">78.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.11.11.5">85.61</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.11.11.6">92.63</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.11.11.7">99.22</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T10.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T10.1.12.12.1.1">BERTScore</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.13.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.13.13.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.13.13.2">0.63</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.13.13.3">0.75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.13.13.4">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.13.13.5">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.13.13.6">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.13.13.7">0.99</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.14.14.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.14.14.2">0.29</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.14.14.3">0.39</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.14.14.4">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.14.14.5">0.63</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.14.14.6">0.79</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.14.14.7">0.98</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.15.15">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T10.1.15.15.1"><span class="ltx_text ltx_font_bold" id="S4.T10.1.15.15.1.1">METEOR</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.16.16">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.16.16.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.16.16.2">0.70</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.16.16.3">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.16.16.4">0.87</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.16.16.5">0.93</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.16.16.6">0.97</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.1.16.16.7">1.00</td>
</tr>
<tr class="ltx_tr" id="S4.T10.1.17.17">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T10.1.17.17.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T10.1.17.17.2">0.66</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T10.1.17.17.3">0.72</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T10.1.17.17.4">0.78</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T10.1.17.17.5">0.85</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T10.1.17.17.6">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T10.1.17.17.7">0.99</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Standard NLG Metrics Across Different Masking Ratios Using Bio_ClinicalBERT (The Baseline was calculated by comparing masked text to the original text)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T11" title="Table 11 ‣ 4.2.3 Quantitative Results on the Full Dataset: Encoder-Only Models ‣ 4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">11</span></a>, we calculated three <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p4.1.1">readability</span> metrics, which were mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS5" title="3.5 Evaluation Methods ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">3.5</span></a>. All these metrics have not shown significant changes from the original ones. However, it is strange that the SMOG and Flesh-Kincaid Grade are not always between the original baseline and mask baseline. When the masking ratio is high, the evaluation values even fall below both the masking and the original baseline. This may be because a <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p4.1.2">higher masking ratio leads to a lower valid prediction rate</span>. If the predicted words include many spaces or punctuation marks, the readability will decrease obviously.</p>
</div>
<figure class="ltx_table" id="S4.T11">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T11.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T11.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T11.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T11.1.1.1.1.1">Bio_ClinicalBERT</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="6" id="S4.T11.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T11.1.1.1.2.1">Masking Ratio</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.2.2">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T11.1.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T11.1.2.2.2.1">1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T11.1.2.2.3.1">0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T11.1.2.2.4.1">0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T11.1.2.2.5.1">0.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T11.1.2.2.6.1">0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T11.1.2.2.7.1">0.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T11.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T11.1.3.3.1.1">SMOG</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T11.1.4.4.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.4.4.2">8.91</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.4.4.3">9.18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.4.4.4">9.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.4.4.5">9.79</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.4.4.6">10.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.4.4.7">10.13</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T11.1.5.5.1">Baseline (Original)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.5.5.2">10.16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.5.5.3">10.15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.5.5.4">10.15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.5.5.5">10.15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.5.5.6">10.15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.5.5.7">10.15</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T11.1.6.6.1">Baseline (Mask)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.6.6.2">9.04</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.6.6.3">9.29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.6.6.4">9.52</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.6.6.5">9.74</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.6.6.6">9.95</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.6.6.7">10.13</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T11.1.7.7.1"><span class="ltx_text ltx_font_bold" id="S4.T11.1.7.7.1.1">Flesch Reading Ease</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T11.1.8.8.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.8.8.2">63.77</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.8.8.3">63.44</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.8.8.4">61.41</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.8.8.5">59.54</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.8.8.6">58.06</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.8.8.7">57.02</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T11.1.9.9.1">Baseline (Original)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.9.9.2">56.85</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.9.9.3">56.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.9.9.4">56.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.9.9.5">56.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.9.9.6">56.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.9.9.7">56.87</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T11.1.10.10.1">Baseline (Mask)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.10.10.2">70.11</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.10.10.3">67.39</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.10.10.4">64.75</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.10.10.5">62.15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.10.10.6">59.62</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.10.10.7">57.13</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T11.1.11.11.1"><span class="ltx_text ltx_font_bold" id="S4.T11.1.11.11.1.1">Flesch-Kincaid Grade</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T11.1.12.12.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.12.12.2">7.32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.12.12.3">7.70</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.12.12.4">8.24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.12.12.5">8.66</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.12.12.6">9.01</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.1.12.12.7">9.22</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.13.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T11.1.13.13.1">Baseline (Original)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.13.13.2">9.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.13.13.3">9.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.13.13.4">9.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.13.13.5">9.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.13.13.6">9.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.1.13.13.7">9.26</td>
</tr>
<tr class="ltx_tr" id="S4.T11.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" id="S4.T11.1.14.14.1">Baseline (Mask)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T11.1.14.14.2">7.41</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T11.1.14.14.3">7.79</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T11.1.14.14.4">8.16</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T11.1.14.14.5">8.52</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T11.1.14.14.6">8.87</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T11.1.14.14.7">9.22</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Readability Metrics Across Different Masking Ratios Using Bio_ClinicalBERT (The Baseline without annotations was calculated by comparing masked text to the original text)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p5">
<p class="ltx_p" id="S4.SS2.SSS3.p5.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T12" title="Table 12 ‣ 4.2.3 Quantitative Results on the Full Dataset: Encoder-Only Models ‣ 4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">12</span></a>, considering the perplexity, the masking baseline is very high, while the values for synthetic letters are close to the original ones. This indicates that the synthetic letters are useful for training clinical models. For information entropy, regardless of the masking ratio, it can <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p5.1.1">effectively preserve the amount of information</span>. As for subjectivity, since all the values are close, we don’t need to worry that the synthetic letters will be biased.</p>
</div>
<figure class="ltx_table" id="S4.T12">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T12.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T12.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T12.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T12.1.1.1.1.1">Bio_ClinicalBERT</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="6" id="S4.T12.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T12.1.1.1.2.1">Masking Ratio</span></td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.2.2">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T12.1.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T12.1.2.2.2.1">1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T12.1.2.2.3.1">0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T12.1.2.2.4.1">0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T12.1.2.2.5.1">0.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T12.1.2.2.6.1">0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T12.1.2.2.7.1">0.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T12.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T12.1.3.3.1.1">Perplexity</span></td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T12.1.4.4.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.4.4.2">2.24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.4.4.3">2.32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.4.4.4">2.31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.4.4.5">2.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.4.4.6">2.29</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.4.4.7">2.29</td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T12.1.5.5.1">Baseline (Original)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.5.5.2">2.22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.5.5.3">2.28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.5.5.4">2.28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.5.5.5">2.28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.5.5.6">2.28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.5.5.7">2.28</td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T12.1.6.6.1">Baseline (Mask)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.6.6.2">250.37</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.6.6.3">65.42</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.6.6.4">24.29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.6.6.5">8.95</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.6.6.6">4.03</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.6.6.7">2.39</td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T12.1.7.7.1"><span class="ltx_text ltx_font_bold" id="S4.T12.1.7.7.1.1">Information Entropy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T12.1.8.8.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.8.8.2">5.46</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.8.8.3">5.80</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.8.8.4">5.92</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.8.8.5">5.96</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.8.8.6">5.98</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.8.8.7">5.98</td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T12.1.9.9.1">Baseline (Original)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.9.9.2">5.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.9.9.3">5.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.9.9.4">5.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.9.9.5">5.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.9.9.6">5.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.9.9.7">5.98</td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T12.1.10.10.1">Baseline (Mask)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.10.10.2">4.51</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.10.10.3">4.93</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.10.10.4">5.29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.10.10.5">5.60</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.10.10.6">5.85</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.10.10.7">5.97</td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T12.1.11.11.1"><span class="ltx_text ltx_font_bold" id="S4.T12.1.11.11.1.1">Subjectivity</span></td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T12.1.12.12.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.12.12.2">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.12.12.3">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.12.12.4">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.12.12.5">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.12.12.6">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T12.1.12.12.7">0.33</td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.13.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T12.1.13.13.1">Baseline (Original)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.13.13.2">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.13.13.3">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.13.13.4">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.13.13.5">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.13.13.6">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T12.1.13.13.7">0.33</td>
</tr>
<tr class="ltx_tr" id="S4.T12.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" id="S4.T12.1.14.14.1">Baseline (Mask)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T12.1.14.14.2">0.41</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T12.1.14.14.3">0.39</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T12.1.14.14.4">0.38</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T12.1.14.14.5">0.37</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T12.1.14.14.6">0.35</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T12.1.14.14.7">0.33</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Advanced Text Quality Metrics Across Different Masking Ratios Using Bio_ClinicalBERT (The Baseline without annotations was calculated by comparing masked text to the original text)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p6">
<p class="ltx_p" id="S4.SS2.SSS3.p6.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T13" title="Table 13 ‣ 4.2.3 Quantitative Results on the Full Dataset: Encoder-Only Models ‣ 4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">13</span></a>, <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p6.1.1">inference time</span> for the entire dataset consistently ranges between 3 to 4 hours. However, it decreases with either very high or very low masking ratios. A mid-range masking ratio of approximately 0.6 results in longer inference times, likely because lower ratios reduce the number of masked tokens to process, while higher ratios provide less context, reducing the computational load. This lack of effective context also increases the invalid prediction rate. Conversely, with a masking ratio of ‘0’, even a small number of prediction errors can significantly impact the overall accuracy due to the few masked tokens.</p>
</div>
<figure class="ltx_table" id="S4.T13">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T13.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T13.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T13.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T13.1.1.1.1.1">Masking Ratio</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T13.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T13.1.1.1.2.1">1.0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T13.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T13.1.1.1.3.1">0.8</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T13.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T13.1.1.1.4.1">0.6</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T13.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T13.1.1.1.5.1">0.4</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T13.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T13.1.1.1.6.1">0.2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T13.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T13.1.1.1.7.1">0.0</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T13.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T13.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T13.1.2.1.1.1">Inference Time</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T13.1.2.1.2">3:12:05</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T13.1.2.1.3">3:28:56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T13.1.2.1.4">3:33:26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T13.1.2.1.5">3:25:16</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T13.1.2.1.6">3:13:26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T13.1.2.1.7">3:01:11</td>
</tr>
<tr class="ltx_tr" id="S4.T13.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T13.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T13.1.3.2.1.1">Invalid Prediction Rate</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T13.1.3.2.2">0.72</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T13.1.3.2.3">0.47</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T13.1.3.2.4">0.34</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T13.1.3.2.5">0.28</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T13.1.3.2.6">0.25</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T13.1.3.2.7">0.37</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 13: </span>Inference Time and Invalid Prediction Rate Across Different Masking Ratios Using Bio_ClinicalBERT</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Variable-Length Chunk Segmentation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">As mentioned in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS4.SSS4" title="3.4.4 Determining Variable-Length-Chunk Size with Bio_ClinicalBERT ‣ 3.4 Clinical Letters Generation ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">3.4.4</span></a>, we set ‘max_lines’ as a variable and the ‘max_tokens’ equal to 256. A series of increasing ‘max_lines’ were tested until the average tokens per chunk reached a peak.
We initially did this on a small sample (7 letters). The results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T14" title="Table 14 ‣ 4.3 Variable-Length Chunk Segmentation ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">We can see that the average tokens per chunk reach a peak as the ‘max_lines’ parameter increases to 41. Similarly, inference time decreases as ’max_lines’ increases up to 41, but starts rising again once it exceeds this value. This experiment was also carried out on small samples of 10 and 30 letters. All of them showed the same trend. However, the inference time here may only reflect an overall trend, not exact results, as it is influenced by many factors, not only the chunk size but also the internet speed, etc.</p>
</div>
<figure class="ltx_table" id="S4.T14">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T14.1" style="width:455.2pt;height:104.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(22.9pt,-5.3pt) scale(1.11212399866955,1.11212399866955) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T14.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T14.1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T14.1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T14.1.1.1.1.1.1">
<span class="ltx_p" id="S4.T14.1.1.1.1.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.1.1.1.1.1.1">max_lines</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T14.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.1.1.2.1">10</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T14.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.1.1.3.1">20</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T14.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.1.1.4.1">30</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T14.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.1.1.5.1">35</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T14.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.1.1.6.1">40</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T14.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.1.1.7.1">41</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T14.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.1.1.8.1">42</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T14.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.1.1.9.1">45</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T14.1.1.1.1.10"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.1.1.10.1">50</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T14.1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T14.1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T14.1.1.2.1.1.1">
<span class="ltx_p" id="S4.T14.1.1.2.1.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.2.1.1.1.1.1">Inference Time (min)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T14.1.1.2.1.2">13:47</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T14.1.1.2.1.3">8:10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T14.1.1.2.1.4">6:44</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T14.1.1.2.1.5">5:24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T14.1.1.2.1.6">5:10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T14.1.1.2.1.7">5:01</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T14.1.1.2.1.8">5:12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T14.1.1.2.1.9">5:54</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T14.1.1.2.1.10">6:05</td>
</tr>
<tr class="ltx_tr" id="S4.T14.1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T14.1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T14.1.1.3.2.1.1">
<span class="ltx_p" id="S4.T14.1.1.3.2.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T14.1.1.3.2.1.1.1.1">Average Tokens Per Chunk</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T14.1.1.3.2.2">51.59</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T14.1.1.3.2.3">90.23</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T14.1.1.3.2.4">131.26</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T14.1.1.3.2.5">136.55</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T14.1.1.3.2.6">144.34</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T14.1.1.3.2.7">146.43</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T14.1.1.3.2.8">146.43</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T14.1.1.3.2.9">146.43</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T14.1.1.3.2.10">146.43</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 14: </span>Comparison for different Chunk Size</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Other Masking Strategies Using Bio_ClinicalBERT</h3>
<figure class="ltx_figure" id="S4.F29"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="253" id="S4.F29.g1" src="extracted/5855287/figures/Example1ByPositions.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 29: </span>Example Sentence 1 with Different Masked Tokens</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F30"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="S4.F30.g1" src="extracted/5855287/figures/Example2ByPositions.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 30: </span>Example Sentence 2 with Different Masked Tokens</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">There is a random selection when masking tokens at certain ratios. Masking different types of tokens will lead to different results, as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F29" title="Figure 29 ‣ 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">29</span></a> and Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F30" title="Figure 30 ‣ 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">30</span></a>. This variability is understandable since the encoder-only models use bidirectional attention, as mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S2.SS3" title="2.3 Generative Language Models ‣ 2 Background and Literature Review ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">2.3</span></a>. <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">These models need to predict the masked tokens based on the context</span>. Therefore, it is necessary to experiment with different masking strategies based on the types of tokens. We used <span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.2">POS</span> tagging and <span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.3">stopwords</span> to observe how these strategies influence the quality of synthetic letters.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">As discussed in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS2" title="4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">4.2</span></a>, BERTScore should be the primary evaluation metric for our objective. Additionally, the invalid prediction rate is useful for assessing the model’s ability to generate informative predictions, and ROUGE scores help evaluate literal diversity. Therefore, these quantitative metrics, calculated using different masking strategies, will be shown in this section. Similar to Subsection <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS2" title="4.2 Random Masking: Quantitative Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we experimented with different masking ratios calculated from the eligible tokens (masked tokens divided by eligible tokens). The ratios are increased in increments of 0.1, ranging from 0.0 to 1.0. Due to space constraints, only metrics with increments of 0.2 will be shown here. A comparison with the same actual masking ratio (masked tokens divided by total tokens in the text) will also be presented in this subsection.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Masking Only Nouns</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">Nouns often correspond to Personally Identifiable Information (<span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.p1.1.1">PII</span>), so masking nouns can serve as a verification step for de-identification. This experiment also helps identify which types of tokens for masking can introduce variations without significantly affecting the original clinical narratives.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T15" title="Table 15 ‣ 4.4.1 Masking Only Nouns ‣ 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">15</span></a>, <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.p2.1.1">the fewer nouns we mask, the better all these metrics perform</span>. This trend is consistent with random masking. When the noun masking ratio is 1.0, meaning all nouns are masked, BERTScore increases from a baseline of 0.70 to 0.89. This means the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS1.p2.1.2">model predicted meaningful nouns</span>. A similar trend is observed in the ROUGE scores. All evaluations are higher than the baseline but lower than ‘1’. However, ROUGE scores show a smaller improvement than BERTScore. This may be because the model generates synonyms or paraphrases that retain the original meaning. With the increase in nouns masking ratio, the BERTScore decreases significantly.</p>
</div>
<figure class="ltx_table" id="S4.T15">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T15.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T15.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T15.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T15.1.1.1.1.1">Bio_ClinicalBERT</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="6" id="S4.T15.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T15.1.1.1.2.1">Nouns Masking Ratio</span></td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.2.2">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T15.1.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T15.1.2.2.2.1">1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T15.1.2.2.3.1">0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T15.1.2.2.4.1">0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T15.1.2.2.5.1">0.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T15.1.2.2.6.1">0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T15.1.2.2.7.1">0.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T15.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T15.1.3.3.1.1">ROUGE-1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T15.1.4.4.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.4.4.2">93.29</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.4.4.3">95.16</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.4.4.4">96.48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.4.4.5">97.62</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.4.4.6">98.66</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.4.4.7">99.51</td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T15.1.5.5.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.5.5.2">88.13</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.5.5.3">90.79</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.5.5.4">92.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.5.5.5">95.19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.5.5.6">97.39</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.5.5.7">99.22</td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T15.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T15.1.6.6.1.1">ROUGE-2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T15.1.7.7.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.7.7.2">86.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.7.7.3">90.29</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.7.7.4">92.84</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.7.7.5">95.12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.7.7.6">97.28</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.7.7.7">99.02</td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T15.1.8.8.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.8.8.2">78.32</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.8.8.3">82.92</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.8.8.4">86.79</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.8.8.5">90.82</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.8.8.6">95.01</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.8.8.7">98.61</td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T15.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S4.T15.1.9.9.1.1">ROUGE-L</span></td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T15.1.10.10.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.10.10.2">93.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.10.10.3">94.96</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.10.10.4">96.35</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.10.10.5">97.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.10.10.6">98.64</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.10.10.7">99.50</td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T15.1.11.11.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.11.11.2">88.13</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.11.11.3">90.79</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.11.11.4">92.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.11.11.5">95.19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.11.11.6">97.39</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.11.11.7">99.22</td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T15.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T15.1.12.12.1.1">BERTScore</span></td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.13.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T15.1.13.13.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.13.13.2">0.89</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.13.13.3">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.13.13.4">0.94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.13.13.5">0.96</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.13.13.6">0.98</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T15.1.13.13.7">0.99</td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T15.1.14.14.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.14.14.2">0.70</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.14.14.3">0.76</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.14.14.4">0.81</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.14.14.5">0.86</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.14.14.6">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T15.1.14.14.7">0.98</td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.15.15">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T15.1.15.15.1"><span class="ltx_text ltx_font_bold" id="S4.T15.1.15.15.1.1">Invalid Prediction Rate</span></td>
</tr>
<tr class="ltx_tr" id="S4.T15.1.16.16">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T15.1.16.16.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T15.1.16.16.2">0.37</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T15.1.16.16.3">0.34</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T15.1.16.16.4">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T15.1.16.16.5">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T15.1.16.16.6">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T15.1.16.16.7">0.37</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 15: </span>Quantitative Comparisons of Nouns Masking Ratios (The ‘Baseline’ was calculated by comparing masked text to the original text)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.SSS1.p3">
<p class="ltx_p" id="S4.SS4.SSS1.p3.1">Therefore, to generate synthetic clinical letters that are distinguishable but still retain the original clinical information, we can only partially mask nouns (around a 0.8 masking ratio). It helps maintain balanced evaluation scores. If we mask all nouns, the quality of synthetic letters will decrease significantly.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Masking Only Verbs</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">Masking only verbs also helps identify which token types are appropriate for masking to achieve our objective. While verbs are essential to describing clinical events, some can still be inferred from context. Therefore, <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.1.1">masking verbs may have a slight effect on the quality of synthetic clinical letters, but it can also introduce some variation</span>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T16" title="Table 16 ‣ 4.4.2 Masking Only Verbs ‣ 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">16</span></a> shows a similar trend for masking verbs as observed with other masking strategies in standard NLG metrics. However, it is surprising that as the masking ratio increases, both the invalid prediction rate and NLG metrics decrease. This phenomenon can be attributed to two main reasons. <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS2.p2.1.1">First</span>, the model seems to prioritise predicting meaningful tokens (rather than punctuation, spaces, etc.) to generate coherent sentences. Contextual relevance is only considered after the sentence structure is complete. This may be due to the important role of verbs in sentences. <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS2.p2.1.2">Second</span>, the original raw data may contain fewer verbs than nouns. Therefore, the number of actual masking tokens changes slightly when verbs are masked, making the model less sensitive to them. This is also reflected in BERTScore. If all verbs are masked, the BERTScore remains high at 0.95, whereas if all nouns are masked, the BERTScore drops to 0.89.</p>
</div>
<figure class="ltx_table" id="S4.T16">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T16.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T16.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T16.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T16.1.1.1.1.1">Bio_ClinicalBERT</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="6" id="S4.T16.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T16.1.1.1.2.1">Verb Masking Ratio</span></td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.2.2">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T16.1.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T16.1.2.2.2.1">1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T16.1.2.2.3.1">0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T16.1.2.2.4.1">0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T16.1.2.2.5.1">0.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T16.1.2.2.6.1">0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T16.1.2.2.7.1">0.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T16.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T16.1.3.3.1.1">ROUGE-1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T16.1.4.4.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.4.4.2">96.48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.4.4.3">97.38</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.4.4.4">97.97</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.4.4.5">98.54</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.4.4.6">99.08</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.4.4.7">99.51</td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T16.1.5.5.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.5.5.2">94.11</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.5.5.3">95.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.5.5.4">96.48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.5.5.5">97.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.5.5.6">98.48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.5.5.7">99.22</td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T16.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T16.1.6.6.1.1">ROUGE-2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T16.1.7.7.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.7.7.2">92.79</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.7.7.3">94.63</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.7.7.4">95.84</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.7.7.5">97.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.7.7.6">98.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.7.7.7">99.02</td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T16.1.8.8.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.8.8.2">88.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.8.8.3">91.26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.8.8.4">93.18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.8.8.5">95.19</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.8.8.6">97.14</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.8.8.7">98.61</td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T16.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S4.T16.1.9.9.1.1">ROUGE-L</span></td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T16.1.10.10.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.10.10.2">96.37</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.10.10.3">97.31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.10.10.4">97.92</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.10.10.5">98.51</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.10.10.6">99.07</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.10.10.7">99.50</td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T16.1.11.11.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.11.11.2">94.11</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.11.11.3">96.48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.11.11.4">96.48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.11.11.5">97.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.11.11.6">98.48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.11.11.7">99.22</td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T16.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T16.1.12.12.1.1">BERTScore</span></td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.13.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T16.1.13.13.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.13.13.2">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.13.13.3">0.97</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.13.13.4">0.97</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.13.13.5">0.98</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.13.13.6">0.99</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.13.13.7">0.99</td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T16.1.14.14.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.14.14.2">0.82</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.14.14.3">0.86</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.14.14.4">0.89</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.14.14.5">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.14.14.6">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T16.1.14.14.7">0.98</td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.15.15">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T16.1.15.15.1"><span class="ltx_text ltx_font_bold" id="S4.T16.1.15.15.1.1">Invalid Prediction Rate</span></td>
</tr>
<tr class="ltx_tr" id="S4.T16.1.16.16">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T16.1.16.16.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T16.1.16.16.2">0.31</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T16.1.16.16.3">0.31</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T16.1.16.16.4">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T16.1.16.16.5">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T16.1.16.16.6">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T16.1.16.16.7">0.37</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 16: </span>Quantitative Comparisons of Verb Masking Ratios (The ‘Baseline’ was calculated by comparing masked text to the original text)</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Masking Only Stopwords</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.1">As mentioned in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S3.SS4.SSS3" title="3.4.3 Different Masking Strategies with Bio_ClinicalBERT ‣ 3.4 Clinical Letters Generation ‣ 3 Methodologies and Experimental Design ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">3.4.3</span></a>, masking stopwords aims to reduce noise for model understanding while introducing variation in synthetic clinical letters. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T17" title="Table 17 ‣ 4.4.3 Masking Only Stopwords ‣ 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">17</span></a> shows that <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS3.p1.1.1">masking only stopwords follows a similar trend to random masking</span>, where a higher masking ratio leads to lower ROUGE Score and BERTScore. Additionally, the Invalid Prediction Rate is at its lowest with a medium masking ratio. This is because higher masking ratios always result in more information loss. On the other hand, lower masking ratios lead to fewer tokens being masked, which makes small prediction errors more influential. The results show an overall low Invalid Prediction Rate and high BERTScore, indicating that <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS3.p1.1.2">stopwords have only a limited influence on the model’s understanding of context</span>. This is not because the original raw letters contain very few stopwords. In fact, there are even more stopwords than nouns and verbs, as seen in sample texts.</p>
</div>
<figure class="ltx_table" id="S4.T17">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T17.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T17.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T17.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T17.1.1.1.1.1">Bio_ClinicalBERT</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="6" id="S4.T17.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T17.1.1.1.2.1">Stopwords Masking Ratio</span></td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.2.2">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T17.1.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T17.1.2.2.2.1">1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T17.1.2.2.3.1">0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T17.1.2.2.4.1">0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T17.1.2.2.5.1">0.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T17.1.2.2.6.1">0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T17.1.2.2.7.1">0.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T17.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T17.1.3.3.1.1">ROUGE-1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T17.1.4.4.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.4.4.2">92.84</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.4.4.3">95.17</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.4.4.4">96.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.4.4.5">97.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.4.4.6">98.69</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.4.4.7">99.51</td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T17.1.5.5.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.5.5.2">81.52</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.5.5.3">85.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.5.5.4">89.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.5.5.5">92.54</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.5.5.6">96.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.5.5.7">99.22</td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T17.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T17.1.6.6.1.1">ROUGE-2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T17.1.7.7.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.7.7.2">84.64</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.7.7.3">89.41</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.7.7.4">92.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.7.7.5">95.05</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.7.7.6">97.24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.7.7.7">99.02</td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T17.1.8.8.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.8.8.2">68.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.8.8.3">74.35</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.8.8.4">79.99</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.8.8.5">86.02</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.8.8.6">92.44</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.8.8.7">98.61</td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T17.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S4.T17.1.9.9.1.1">ROUGE-L</span></td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T17.1.10.10.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.10.10.2">91.84</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.10.10.3">94.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.10.10.4">96.23</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.10.10.5">97.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.10.10.6">98.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.10.10.7">99.50</td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T17.1.11.11.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.11.11.2">81.52</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.11.11.3">85.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.11.11.4">89.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.11.11.5">92.54</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.11.11.6">96.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.11.11.7">99.22</td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T17.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T17.1.12.12.1.1">BERTScore</span></td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.13.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T17.1.13.13.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.13.13.2">0.89</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.13.13.3">0.93</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.13.13.4">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.13.13.5">0.97</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.13.13.6">0.98</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.13.13.7">0.99</td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T17.1.14.14.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.14.14.2">0.57</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.14.14.3">0.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.14.14.4">0.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.14.14.5">0.79</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.14.14.6">0.88</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T17.1.14.14.7">0.98</td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.15.15">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7" id="S4.T17.1.15.15.1"><span class="ltx_text ltx_font_bold" id="S4.T17.1.15.15.1.1">Invalid Prediction Rate</span></td>
</tr>
<tr class="ltx_tr" id="S4.T17.1.16.16">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T17.1.16.16.1">Generation Performance</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T17.1.16.16.2">0.29</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T17.1.16.16.3">0.22</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T17.1.16.16.4">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T17.1.16.16.5">0.18</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T17.1.16.16.6">0.19</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T17.1.16.16.7">0.37</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 17: </span>Quantitative Comparisons of Stopwords Masking Ratios (The ‘Baseline’ was calculated by comparing masked text to the original text)</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>Comparison of Identical Actual Masking Ratios</h4>
<div class="ltx_para" id="S4.SS4.SSS4.p1">
<p class="ltx_p" id="S4.SS4.SSS4.p1.1">To further observe how different masking strategies influence the generation of clinical letters, we compared the results using the same actual masking ratios but with different strategies. In other words, the number of masked tokens is fixed, so the only variable is <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p1.1.1">the type of tokens being masked</span>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T18" title="Table 18 ‣ 4.4.4 Comparison of Identical Actual Masking Ratios ‣ 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">18</span></a> shows the results with a 0.04 actual masking ratio, and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T19" title="Table 19 ‣ 4.4.4 Comparison of Identical Actual Masking Ratios ‣ 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">19</span></a> shows the results with a 0.1 actual masking ratio.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS4.p2">
<p class="ltx_p" id="S4.SS4.SSS4.p2.1">As we can see, masking only stopwords got the highest BERTScore and lowest invalid prediction rate. Therefore, stopwords have little influence on the overall meaning of the text, which is consistent with our earlier findings. Additionally, masking nouns and verbs perform worse than random masking. Therefore, if we want to preserve the original meaning, we cannot mask too many nouns and verbs.</p>
</div>
<figure class="ltx_table" id="S4.T18">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T18.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T18.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T18.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T18.1.1.1.1.1">Bio_ ClinicalBERT</span></th>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S4.T18.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T18.1.1.1.2.1">Masking Strategies</span></td>
</tr>
<tr class="ltx_tr" id="S4.T18.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T18.1.2.2.1"></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.2.2.2.1">
<span class="ltx_p" id="S4.T18.1.2.2.2.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T18.1.2.2.2.1.1.1">Noun Masking (0.4)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.2.2.3.1">
<span class="ltx_p" id="S4.T18.1.2.2.3.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T18.1.2.2.3.1.1.1">Stopwords Masking (0.2)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.2.2.4.1">
<span class="ltx_p" id="S4.T18.1.2.2.4.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T18.1.2.2.4.1.1.1">Verbs Masking (0.8)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.2.2.5.1">
<span class="ltx_p" id="S4.T18.1.2.2.5.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T18.1.2.2.5.1.1.1">Randomly Masking (0.1)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T18.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T18.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T18.1.3.3.1.1">ROUGE-1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T18.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T18.1.4.4.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.4.4.2.1">
<span class="ltx_p" id="S4.T18.1.4.4.2.1.1" style="width:56.9pt;">97.62</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.4.4.3.1">
<span class="ltx_p" id="S4.T18.1.4.4.3.1.1" style="width:56.9pt;">98.69</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.4.4.4.1">
<span class="ltx_p" id="S4.T18.1.4.4.4.1.1" style="width:56.9pt;">97.62</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.4.4.5.1">
<span class="ltx_p" id="S4.T18.1.4.4.5.1.1" style="width:56.9pt;">98.28</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T18.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T18.1.5.5.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.5.5.2.1">
<span class="ltx_p" id="S4.T18.1.5.5.2.1.1" style="width:56.9pt;">95.19</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.5.5.3.1">
<span class="ltx_p" id="S4.T18.1.5.5.3.1.1" style="width:56.9pt;">96.04</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.5.5.4.1">
<span class="ltx_p" id="S4.T18.1.5.5.4.1.1" style="width:56.9pt;">95.19</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.5.5.5.1">
<span class="ltx_p" id="S4.T18.1.5.5.5.1.1" style="width:56.9pt;">96.16</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T18.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T18.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T18.1.6.6.1.1">ROUGE-2</span></th>
</tr>
<tr class="ltx_tr" id="S4.T18.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T18.1.7.7.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.7.7.2.1">
<span class="ltx_p" id="S4.T18.1.7.7.2.1.1" style="width:56.9pt;">95.12</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.7.7.3.1">
<span class="ltx_p" id="S4.T18.1.7.7.3.1.1" style="width:56.9pt;">97.24</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.7.7.4.1">
<span class="ltx_p" id="S4.T18.1.7.7.4.1.1" style="width:56.9pt;">95.12</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.7.7.5.1">
<span class="ltx_p" id="S4.T18.1.7.7.5.1.1" style="width:56.9pt;">96.50</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T18.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T18.1.8.8.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.8.8.2.1">
<span class="ltx_p" id="S4.T18.1.8.8.2.1.1" style="width:56.9pt;">90.82</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.8.8.3.1">
<span class="ltx_p" id="S4.T18.1.8.8.3.1.1" style="width:56.9pt;">92.44</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.8.8.4.1">
<span class="ltx_p" id="S4.T18.1.8.8.4.1.1" style="width:56.9pt;">90.82</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.8.8.5.1">
<span class="ltx_p" id="S4.T18.1.8.8.5.1.1" style="width:56.9pt;">92.68</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T18.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T18.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S4.T18.1.9.9.1.1">ROUGE-L</span></th>
</tr>
<tr class="ltx_tr" id="S4.T18.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T18.1.10.10.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.10.10.2.1">
<span class="ltx_p" id="S4.T18.1.10.10.2.1.1" style="width:56.9pt;">97.56</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.10.10.3.1">
<span class="ltx_p" id="S4.T18.1.10.10.3.1.1" style="width:56.9pt;">98.65</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.10.10.4.1">
<span class="ltx_p" id="S4.T18.1.10.10.4.1.1" style="width:56.9pt;">97.56</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.10.10.5.1">
<span class="ltx_p" id="S4.T18.1.10.10.5.1.1" style="width:56.9pt;">98.25</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T18.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T18.1.11.11.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.11.11.2.1">
<span class="ltx_p" id="S4.T18.1.11.11.2.1.1" style="width:56.9pt;">95.19</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.11.11.3.1">
<span class="ltx_p" id="S4.T18.1.11.11.3.1.1" style="width:56.9pt;">96.04</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.11.11.4.1">
<span class="ltx_p" id="S4.T18.1.11.11.4.1.1" style="width:56.9pt;">95.19</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.11.11.5.1">
<span class="ltx_p" id="S4.T18.1.11.11.5.1.1" style="width:56.9pt;">96.16</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T18.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T18.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T18.1.12.12.1.1">BERTScore</span></th>
</tr>
<tr class="ltx_tr" id="S4.T18.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T18.1.13.13.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.13.13.2.1">
<span class="ltx_p" id="S4.T18.1.13.13.2.1.1" style="width:56.9pt;">0.96</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.13.13.3.1">
<span class="ltx_p" id="S4.T18.1.13.13.3.1.1" style="width:56.9pt;">0.98</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.13.13.4.1">
<span class="ltx_p" id="S4.T18.1.13.13.4.1.1" style="width:56.9pt;">0.96</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.13.13.5.1">
<span class="ltx_p" id="S4.T18.1.13.13.5.1.1" style="width:56.9pt;">0.97</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T18.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T18.1.14.14.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.14.14.2.1">
<span class="ltx_p" id="S4.T18.1.14.14.2.1.1" style="width:56.9pt;">0.86</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.14.14.3.1">
<span class="ltx_p" id="S4.T18.1.14.14.3.1.1" style="width:56.9pt;">0.88</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.14.14.4.1">
<span class="ltx_p" id="S4.T18.1.14.14.4.1.1" style="width:56.9pt;">0.86</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T18.1.14.14.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.14.14.5.1">
<span class="ltx_p" id="S4.T18.1.14.14.5.1.1" style="width:56.9pt;">0.88</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T18.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T18.1.15.15.1"><span class="ltx_text ltx_font_bold" id="S4.T18.1.15.15.1.1">Invalid Prediction Rate</span></th>
</tr>
<tr class="ltx_tr" id="S4.T18.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T18.1.16.16.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T18.1.16.16.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.16.16.2.1">
<span class="ltx_p" id="S4.T18.1.16.16.2.1.1" style="width:56.9pt;">0.32</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T18.1.16.16.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.16.16.3.1">
<span class="ltx_p" id="S4.T18.1.16.16.3.1.1" style="width:56.9pt;">0.19</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T18.1.16.16.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.16.16.4.1">
<span class="ltx_p" id="S4.T18.1.16.16.4.1.1" style="width:56.9pt;">0.32</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T18.1.16.16.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T18.1.16.16.5.1">
<span class="ltx_p" id="S4.T18.1.16.16.5.1.1" style="width:56.9pt;">0.25</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 18: </span>Quantitative Comparison of Different Masking Strategies at a 0.04 Actual Masking Ratio (The ‘Baseline’ was calculated by comparing masked text to the original text)</figcaption>
</figure>
<figure class="ltx_table" id="S4.T19">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T19.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T19.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T19.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T19.1.1.1.1.1">Bio_ClinicalBERT</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.1.1.2.1">
<span class="ltx_p" id="S4.T19.1.1.1.2.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T19.1.1.1.2.1.1.1">Nouns Masking (1.0)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.1.1.3.1">
<span class="ltx_p" id="S4.T19.1.1.1.3.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T19.1.1.1.3.1.1.1">Stopwords Masking (0.6)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.1.1.4.1">
<span class="ltx_p" id="S4.T19.1.1.1.4.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T19.1.1.1.4.1.1.1">Random Masking (0.3)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T19.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S4.T19.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T19.1.2.2.1.1">ROUGE-1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T19.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T19.1.3.3.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.3.3.2.1">
<span class="ltx_p" id="S4.T19.1.3.3.2.1.1" style="width:56.9pt;">93.29</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.3.3.3.1">
<span class="ltx_p" id="S4.T19.1.3.3.3.1.1" style="width:56.9pt;">96.56</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.3.3.4.1">
<span class="ltx_p" id="S4.T19.1.3.3.4.1.1" style="width:56.9pt;">95.10</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T19.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T19.1.4.4.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.4.4.2.1">
<span class="ltx_p" id="S4.T19.1.4.4.2.1.1" style="width:56.9pt;">88.13</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.4.4.3.1">
<span class="ltx_p" id="S4.T19.1.4.4.3.1.1" style="width:56.9pt;">89.04</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.4.4.4.1">
<span class="ltx_p" id="S4.T19.1.4.4.4.1.1" style="width:56.9pt;">89.16</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T19.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S4.T19.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S4.T19.1.5.5.1.1">ROUGE-2</span></th>
</tr>
<tr class="ltx_tr" id="S4.T19.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T19.1.6.6.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.6.6.2.1">
<span class="ltx_p" id="S4.T19.1.6.6.2.1.1" style="width:56.9pt;">86.71</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.6.6.3.1">
<span class="ltx_p" id="S4.T19.1.6.6.3.1.1" style="width:56.9pt;">92.53</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.6.6.4.1">
<span class="ltx_p" id="S4.T19.1.6.6.4.1.1" style="width:56.9pt;">90.17</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T19.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T19.1.7.7.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.7.7.2.1">
<span class="ltx_p" id="S4.T19.1.7.7.2.1.1" style="width:56.9pt;">78.32</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.7.7.3.1">
<span class="ltx_p" id="S4.T19.1.7.7.3.1.1" style="width:56.9pt;">79.99</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.7.7.4.1">
<span class="ltx_p" id="S4.T19.1.7.7.4.1.1" style="width:56.9pt;">80.44</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T19.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S4.T19.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S4.T19.1.8.8.1.1">ROUGE-L</span></th>
</tr>
<tr class="ltx_tr" id="S4.T19.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T19.1.9.9.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.9.9.2.1">
<span class="ltx_p" id="S4.T19.1.9.9.2.1.1" style="width:56.9pt;">93.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.9.9.3.1">
<span class="ltx_p" id="S4.T19.1.9.9.3.1.1" style="width:56.9pt;">96.23</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.9.9.4.1">
<span class="ltx_p" id="S4.T19.1.9.9.4.1.1" style="width:56.9pt;">94.86</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T19.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T19.1.10.10.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.10.10.2.1">
<span class="ltx_p" id="S4.T19.1.10.10.2.1.1" style="width:56.9pt;">88.13</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.10.10.3.1">
<span class="ltx_p" id="S4.T19.1.10.10.3.1.1" style="width:56.9pt;">89.04</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.10.10.4.1">
<span class="ltx_p" id="S4.T19.1.10.10.4.1.1" style="width:56.9pt;">89.16</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T19.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S4.T19.1.11.11.1"><span class="ltx_text ltx_font_bold" id="S4.T19.1.11.11.1.1">BERTScore</span></th>
</tr>
<tr class="ltx_tr" id="S4.T19.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T19.1.12.12.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.12.12.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.12.12.2.1">
<span class="ltx_p" id="S4.T19.1.12.12.2.1.1" style="width:56.9pt;">0.89</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.12.12.3.1">
<span class="ltx_p" id="S4.T19.1.12.12.3.1.1" style="width:56.9pt;">0.95</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.12.12.4.1">
<span class="ltx_p" id="S4.T19.1.12.12.4.1.1" style="width:56.9pt;">0.93</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T19.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T19.1.13.13.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.13.13.2.1">
<span class="ltx_p" id="S4.T19.1.13.13.2.1.1" style="width:56.9pt;">0.70</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.13.13.3.1">
<span class="ltx_p" id="S4.T19.1.13.13.3.1.1" style="width:56.9pt;">0.71</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T19.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.13.13.4.1">
<span class="ltx_p" id="S4.T19.1.13.13.4.1.1" style="width:56.9pt;">0.71</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T19.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S4.T19.1.14.14.1"><span class="ltx_text ltx_font_bold" id="S4.T19.1.14.14.1.1">Invalid Prediction Rate</span></th>
</tr>
<tr class="ltx_tr" id="S4.T19.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T19.1.15.15.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T19.1.15.15.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.15.15.2.1">
<span class="ltx_p" id="S4.T19.1.15.15.2.1.1" style="width:56.9pt;">0.37</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T19.1.15.15.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.15.15.3.1">
<span class="ltx_p" id="S4.T19.1.15.15.3.1.1" style="width:56.9pt;">0.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T19.1.15.15.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T19.1.15.15.4.1">
<span class="ltx_p" id="S4.T19.1.15.15.4.1.1" style="width:56.9pt;">0.26</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 19: </span>Quantitative Comparisons of 0.1 Actual Masking Ratio (The Baseline was calculated by comparing masked text to the original text)</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.5 </span>Hybrid Masking</h4>
<div class="ltx_para" id="S4.SS4.SSS5.p1">
<p class="ltx_p" id="S4.SS4.SSS5.p1.1">After comparing different strategies with the same actual masking ratio, we explored hybrid masking strategies and compared them with other strategies at the same actual ratio. The results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T20" title="Table 20 ‣ 4.4.5 Hybrid Masking ‣ 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">20</span></a>. The first three columns have the same actual masking ratio. Masking only stopwords achieved the strongest performance among these strategies. However, when nouns are also masked along with stopwords, performance decreases, as masking nouns negatively affects the results. Despite this, it still performs better than random masking, indicating that stopwords have a greater influence than nouns. Next, we compared the last two columns. If 0.5 of nouns and 0.5 of stopwords are masked, adding an additional 0.5 of masked verbs leads to worse performance, showing that verbs also negatively influence the model’s performance.</p>
</div>
<figure class="ltx_table" id="S4.T20">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T20.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T20.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T20.1.1.1.1.1">Bio_ClinicalBERT</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.1.1.2.1">
<span class="ltx_p" id="S4.T20.1.1.1.2.1.1" style="width:68.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T20.1.1.1.2.1.1.1">Stopwords Masking (0.8)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.1.1.3.1">
<span class="ltx_p" id="S4.T20.1.1.1.3.1.1" style="width:68.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T20.1.1.1.3.1.1.1">Random Masking (0.4)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.1.1.4.1">
<span class="ltx_p" id="S4.T20.1.1.1.4.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T20.1.1.1.4.1.1.1">Nouns (0.5) and Stopwords (0.5)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.1.1.5.1">
<span class="ltx_p" id="S4.T20.1.1.1.5.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T20.1.1.1.5.1.1.1">Nouns (0.5), Verbs (0.5), Stopwords (0.8)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T20.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T20.1.2.2.1.1">Actual Masking Ratio</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.2.2.2.1">
<span class="ltx_p" id="S4.T20.1.2.2.2.1.1" style="width:68.3pt;">0.13</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.2.2.3.1">
<span class="ltx_p" id="S4.T20.1.2.2.3.1.1" style="width:68.3pt;">0.13</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.2.2.4.1">
<span class="ltx_p" id="S4.T20.1.2.2.4.1.1" style="width:71.1pt;">0.13</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.2.2.5.1">
<span class="ltx_p" id="S4.T20.1.2.2.5.1.1" style="width:71.1pt;">0.16</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T20.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T20.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T20.1.3.3.1.1">ROUGE-1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T20.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.4.4.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.4.4.2.1">
<span class="ltx_p" id="S4.T20.1.4.4.2.1.1" style="width:68.3pt;">95.17</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.4.4.3.1">
<span class="ltx_p" id="S4.T20.1.4.4.3.1.1" style="width:68.3pt;">93.18</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.4.4.4.1">
<span class="ltx_p" id="S4.T20.1.4.4.4.1.1" style="width:71.1pt;">94.29</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.4.4.5.1">
<span class="ltx_p" id="S4.T20.1.4.4.5.1.1" style="width:71.1pt;">91.34</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T20.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.5.5.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.5.5.2.1">
<span class="ltx_p" id="S4.T20.1.5.5.2.1.1" style="width:68.3pt;">85.53</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.5.5.3.1">
<span class="ltx_p" id="S4.T20.1.5.5.3.1.1" style="width:68.3pt;">85.61</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.5.5.4.1">
<span class="ltx_p" id="S4.T20.1.5.5.4.1.1" style="width:71.1pt;">85.98</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.5.5.5.1">
<span class="ltx_p" id="S4.T20.1.5.5.5.1.1" style="width:71.1pt;">82.47</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T20.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T20.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T20.1.6.6.1.1">ROUGE-2</span></th>
</tr>
<tr class="ltx_tr" id="S4.T20.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.7.7.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.7.7.2.1">
<span class="ltx_p" id="S4.T20.1.7.7.2.1.1" style="width:68.3pt;">89.41</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.7.7.3.1">
<span class="ltx_p" id="S4.T20.1.7.7.3.1.1" style="width:68.3pt;">86.50</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.7.7.4.1">
<span class="ltx_p" id="S4.T20.1.7.7.4.1.1" style="width:71.1pt;">88.34</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.7.7.5.1">
<span class="ltx_p" id="S4.T20.1.7.7.5.1.1" style="width:71.1pt;">83.08</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T20.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.8.8.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.8.8.2.1">
<span class="ltx_p" id="S4.T20.1.8.8.2.1.1" style="width:68.3pt;">74.35</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.8.8.3.1">
<span class="ltx_p" id="S4.T20.1.8.8.3.1.1" style="width:68.3pt;">74.92</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.8.8.4.1">
<span class="ltx_p" id="S4.T20.1.8.8.4.1.1" style="width:71.1pt;">75.30</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.8.8.5.1">
<span class="ltx_p" id="S4.T20.1.8.8.5.1.1" style="width:71.1pt;">70.73</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T20.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T20.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S4.T20.1.9.9.1.1">ROUGE-L</span></th>
</tr>
<tr class="ltx_tr" id="S4.T20.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.10.10.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.10.10.2.1">
<span class="ltx_p" id="S4.T20.1.10.10.2.1.1" style="width:68.3pt;">94.53</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.10.10.3.1">
<span class="ltx_p" id="S4.T20.1.10.10.3.1.1" style="width:68.3pt;">92.71</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.10.10.4.1">
<span class="ltx_p" id="S4.T20.1.10.10.4.1.1" style="width:71.1pt;">93.80</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.10.10.5.1">
<span class="ltx_p" id="S4.T20.1.10.10.5.1.1" style="width:71.1pt;">90.50</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T20.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.11.11.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.11.11.2.1">
<span class="ltx_p" id="S4.T20.1.11.11.2.1.1" style="width:68.3pt;">85.53</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.11.11.3.1">
<span class="ltx_p" id="S4.T20.1.11.11.3.1.1" style="width:68.3pt;">85.61</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.11.11.4.1">
<span class="ltx_p" id="S4.T20.1.11.11.4.1.1" style="width:71.1pt;">85.98</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.11.11.5.1">
<span class="ltx_p" id="S4.T20.1.11.11.5.1.1" style="width:71.1pt;">82.47</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T20.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T20.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T20.1.12.12.1.1">BERTScore</span></th>
</tr>
<tr class="ltx_tr" id="S4.T20.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.13.13.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.13.13.2.1">
<span class="ltx_p" id="S4.T20.1.13.13.2.1.1" style="width:68.3pt;">0.93</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.13.13.3.1">
<span class="ltx_p" id="S4.T20.1.13.13.3.1.1" style="width:68.3pt;">0.90</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.13.13.4.1">
<span class="ltx_p" id="S4.T20.1.13.13.4.1.1" style="width:71.1pt;">0.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.13.13.5.1">
<span class="ltx_p" id="S4.T20.1.13.13.5.1.1" style="width:71.1pt;">0.87</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T20.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.14.14.1">Baseline</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.14.14.2.1">
<span class="ltx_p" id="S4.T20.1.14.14.2.1.1" style="width:68.3pt;">0.65</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.14.14.3.1">
<span class="ltx_p" id="S4.T20.1.14.14.3.1.1" style="width:68.3pt;">0.63</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.14.14.4.1">
<span class="ltx_p" id="S4.T20.1.14.14.4.1.1" style="width:71.1pt;">0.65</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T20.1.14.14.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.14.14.5.1">
<span class="ltx_p" id="S4.T20.1.14.14.5.1.1" style="width:71.1pt;">0.57</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T20.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T20.1.15.15.1"><span class="ltx_text ltx_font_bold" id="S4.T20.1.15.15.1.1">Invalid Prediction Rate</span></th>
</tr>
<tr class="ltx_tr" id="S4.T20.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T20.1.16.16.1">Generation Performance</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T20.1.16.16.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.16.16.2.1">
<span class="ltx_p" id="S4.T20.1.16.16.2.1.1" style="width:68.3pt;">0.22</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T20.1.16.16.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.16.16.3.1">
<span class="ltx_p" id="S4.T20.1.16.16.3.1.1" style="width:68.3pt;">0.28</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T20.1.16.16.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.16.16.4.1">
<span class="ltx_p" id="S4.T20.1.16.16.4.1.1" style="width:71.1pt;">0.28</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T20.1.16.16.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T20.1.16.16.5.1">
<span class="ltx_p" id="S4.T20.1.16.16.5.1.1" style="width:71.1pt;">0.31</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 20: </span>Quantitative Comparisons for Hybrid Masking (The Baseline was calculated by comparing masked text to the original text)</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.6 </span>Comparison with and without (w/o) Entity Preservation</h4>
<div class="ltx_para" id="S4.SS4.SSS6.p1">
<p class="ltx_p" id="S4.SS4.SSS6.p1.1">To further explore whether keeping entities is useful for our task, we compared our results with a baseline that does not retain any entities. The baseline was trained with four epochs of fine-tuning on our dataset. Specifically, 0.4 of nouns from all tokens were randomly masked during the baseline training. In contrast, in our experiments, only eligible tokens—excluding clinical information—were selected for masking. The comparisons are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T21" title="Table 21 ‣ 4.4.6 Comparison with and without (w/o) Entity Preservation ‣ 4.4 Other Masking Strategies Using Bio_ClinicalBERT ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">21</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T21">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T21.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T21.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T21.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T21.1.1.1.1.1">Bio_ClinicalBERT</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.1.1.2.1">
<span class="ltx_p" id="S4.T21.1.1.1.2.1.1" style="width:99.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T21.1.1.1.2.1.1.1">With Entity Preservation</span> (0.4 Nouns Masking)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.1.1.3.1">
<span class="ltx_p" id="S4.T21.1.1.1.3.1.1" style="width:99.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T21.1.1.1.3.1.1.1">With Entity Preservation</span> (0.3 Random Masking)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.1.1.4.1">
<span class="ltx_p" id="S4.T21.1.1.1.4.1.1" style="width:99.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T21.1.1.1.4.1.1.1">Without Entity Preservation</span> (0.4 Nouns Masking)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T21.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T21.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T21.1.2.2.1.1">ROUGE-1</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.2.2.2.1">
<span class="ltx_p" id="S4.T21.1.2.2.2.1.1" style="width:99.6pt;">97.62</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.2.2.3.1">
<span class="ltx_p" id="S4.T21.1.2.2.3.1.1" style="width:99.6pt;">95.10</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.2.2.4.1">
<span class="ltx_p" id="S4.T21.1.2.2.4.1.1" style="width:99.6pt;">97.31</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T21.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T21.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T21.1.3.3.1.1">ROUGE-2</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.3.3.2.1">
<span class="ltx_p" id="S4.T21.1.3.3.2.1.1" style="width:99.6pt;">95.12</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.3.3.3.1">
<span class="ltx_p" id="S4.T21.1.3.3.3.1.1" style="width:99.6pt;">90.17</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.3.3.4.1">
<span class="ltx_p" id="S4.T21.1.3.3.4.1.1" style="width:99.6pt;">94.46</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T21.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T21.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S4.T21.1.4.4.1.1">ROUGE-L</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.4.4.2.1">
<span class="ltx_p" id="S4.T21.1.4.4.2.1.1" style="width:99.6pt;">97.56</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.4.4.3.1">
<span class="ltx_p" id="S4.T21.1.4.4.3.1.1" style="width:99.6pt;">94.86</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T21.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.4.4.4.1">
<span class="ltx_p" id="S4.T21.1.4.4.4.1.1" style="width:99.6pt;">93.71</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T21.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T21.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S4.T21.1.5.5.1.1">BERTScore</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T21.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.5.5.2.1">
<span class="ltx_p" id="S4.T21.1.5.5.2.1.1" style="width:99.6pt;">0.96</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T21.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.5.5.3.1">
<span class="ltx_p" id="S4.T21.1.5.5.3.1.1" style="width:99.6pt;">0.93</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T21.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T21.1.5.5.4.1">
<span class="ltx_p" id="S4.T21.1.5.5.4.1.1" style="width:99.6pt;">0.91</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 21: </span>Comparison with and without Entity Preservation Using Bio_ClinicalBERT</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.SSS6.p2">
<p class="ltx_p" id="S4.SS4.SSS6.p2.1">As we can see, <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS6.p2.1.1">when 0.4 nouns are masked while preserving entities, the models perform much better</span> than those without any entity preservation. Interestingly, when we randomly mask 0.3 while preserving entities, the model achieves lower ROUGE-1 and ROUGE-2 scores but higher ROUGE-L and BERTScores compared to models without entity preservation. This trend is consistent across different settings. It suggests that models preserving entities have less overlap with the original text, while they can retain the original narrative better. Additionally, the higher ROUGE-L score suggests that the step of <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS6.p2.1.2">preserving document structure is indeed effective</span>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS6.p3">
<p class="ltx_p" id="S4.SS4.SSS6.p3.1">These results also confirm our initial hypothesis that, for our objective — generating clinical letters that can keep the original meaning while adding some variety — <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS6.p3.1.1">retaining entities</span> is much more effective than just fine-tuning the model. Moreover, this approach can effectively preserve useful information while avoiding overfitting.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Downstream NER Task</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">To further evaluate whether synthetic letters have the potential to replace the original raw letters, particularly in the domains of clinical research and model training, a downstream NER task was implemented. Two <span class="ltx_text ltx_font_bold" id="S4.SS5.p1.1.1">spaCy NER</span> models were trained separately on <span class="ltx_text ltx_font_bold" id="S4.SS5.p1.1.2">original raw letters and synthetic letters</span>. Specifically, the synthetic letters were generated with 0.3 random masking while preserving entities.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.T22" title="Table 22 ‣ 4.5 Downstream NER Task ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">22</span></a>, spaCy models trained on original and synthetic letters showed <span class="ltx_text ltx_font_bold" id="S4.SS5.p2.1.1">similar evaluation scores</span>. They even achieved F1 scores comparable to ScispaCy’s score of 0.843. Therefore, the unmasked context does not significantly influence model understanding. Consequently, <span class="ltx_text ltx_font_italic" id="S4.SS5.p2.1.2">our synthetic letters can be used in NER tasks to replace real-world clinical letters, thereby further protecting sensitive information</span>.</p>
</div>
<figure class="ltx_table" id="S4.T22">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T22.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T22.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T22.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.1.2.1">
<span class="ltx_p" id="S4.T22.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T22.1.1.2.1.1.1">Metric</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T22.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.1.3.1">
<span class="ltx_p" id="S4.T22.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T22.1.1.3.1.1.1">spaCy Trained on Original Letters</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T22.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.1.4.1">
<span class="ltx_p" id="S4.T22.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T22.1.1.4.1.1.1">spaCy Trained on Synthetic Letters</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T22.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.1.1.1">
<span class="ltx_p" id="S4.T22.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T22.1.1.1.1.1.1">Performance Delta (<math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T22.1.1.1.1.1.1.m1.1"><semantics id="S4.T22.1.1.1.1.1.1.m1.1a"><mi id="S4.T22.1.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T22.1.1.1.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T22.1.1.1.1.1.1.m1.1b"><ci id="S4.T22.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T22.1.1.1.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T22.1.1.1.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T22.1.1.1.1.1.1.m1.1d">roman_Δ</annotation></semantics></math>)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T22.1.2.1">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T22.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.2.1.1.1">
<span class="ltx_p" id="S4.T22.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T22.1.2.1.1.1.1.1">F1 Score</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T22.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.2.1.2.1">
<span class="ltx_p" id="S4.T22.1.2.1.2.1.1">0.855</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T22.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.2.1.3.1">
<span class="ltx_p" id="S4.T22.1.2.1.3.1.1">0.853</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T22.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.2.1.4.1">
<span class="ltx_p" id="S4.T22.1.2.1.4.1.1">-0.002</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T22.1.3.2">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S4.T22.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.3.2.1.1">
<span class="ltx_p" id="S4.T22.1.3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T22.1.3.2.1.1.1.1">Precision</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T22.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.3.2.2.1">
<span class="ltx_p" id="S4.T22.1.3.2.2.1.1">0.865</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T22.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.3.2.3.1">
<span class="ltx_p" id="S4.T22.1.3.2.3.1.1">0.863</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T22.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.3.2.4.1">
<span class="ltx_p" id="S4.T22.1.3.2.4.1.1">-0.002</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T22.1.4.3">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T22.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.4.3.1.1">
<span class="ltx_p" id="S4.T22.1.4.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T22.1.4.3.1.1.1.1">Recall</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S4.T22.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.4.3.2.1">
<span class="ltx_p" id="S4.T22.1.4.3.2.1.1">0.846</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S4.T22.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.4.3.3.1">
<span class="ltx_p" id="S4.T22.1.4.3.3.1.1">0.843</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S4.T22.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T22.1.4.3.4.1">
<span class="ltx_p" id="S4.T22.1.4.3.4.1.1">-0.003</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 22: </span>Comparisons on Downstream NER Task</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Post-Process Results</h3>
<section class="ltx_subsubsection" id="S4.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.6.1 </span>Filling in the Blanks</h4>
<div class="ltx_para" id="S4.SS6.SSS1.p1">
<p class="ltx_p" id="S4.SS6.SSS1.p1.1">One example text without post-processing is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F31" title="Figure 31 ‣ 4.6.1 Filling in the Blanks ‣ 4.6 Post-Process Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">31</span></a>. After filling in the blanks, the results with BERT-base and Bio_ClinicalBERT are shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F32" title="Figure 32 ‣ 4.6.1 Filling in the Blanks ‣ 4.6 Post-Process Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">32</span></a> and Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F33" title="Figure 33 ‣ 4.6.1 Filling in the Blanks ‣ 4.6 Post-Process Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">33</span></a> separately. We can see that both models can partially achieve the goal of making the text more complete. However, neither of them created a coherent story to fill in these blanks. They just used general terms like “hospital” and “clinic”. Perhaps other decoder-only models, more suitable for generating stories like GPT, could perform better and should be explored in the future.</p>
</div>
<figure class="ltx_figure" id="S4.F31"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="154" id="S4.F31.g1" src="extracted/5855287/figures/OriginalText.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 31: </span>Example Sentence Before Post-Processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx44" title="">Johnson et al., 2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx43" title="">Johnson et al., 2023b</a>]</cite> (‘note_id’: ‘16441224-DS-19’)</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F32"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="141" id="S4.F32.g1" src="extracted/5855287/figures/BERTBasePostprocess.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 32: </span>Post-Processing Results with BERT-Base</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F33"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="162" id="S4.F33.g1" src="extracted/5855287/figures/BioBERTpostprocess.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 33: </span>Post-Processing Results with Bio_ClinicalBERT</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.6.2 </span>Spelling Correction</h4>
<div class="ltx_para" id="S4.SS6.SSS2.p1">
<p class="ltx_p" id="S4.SS6.SSS2.p1.1">Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F34" title="Figure 34 ‣ 4.6.2 Spelling Correction ‣ 4.6 Post-Process Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">34</span></a> shows that if the incorrect words are masked, the models may be able to correct the misspelled tokens by predicting them. However, the masking process is random. Additionally, sometimes the predicted words will be incorrect because some models tokenize the sentence into word-pieces. Therefore, a post-processing step is necessary for correcting spelling.</p>
</div>
<figure class="ltx_figure" id="S4.F34"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S4.F34.g1" src="extracted/5855287/figures/ExampleOfWordCollectionByMasking.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 34: </span>Spelling Correction by masking and Generating <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx44" title="">Johnson et al., 2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx43" title="">Johnson et al., 2023b</a>]</cite> (‘note_id’: ‘10807423-DS-19’)</figcaption>
</figure>
<div class="ltx_para" id="S4.SS6.SSS2.p2">
<p class="ltx_p" id="S4.SS6.SSS2.p2.1">As shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.F35" title="Figure 35 ‣ 4.6.2 Spelling Correction ‣ 4.6 Post-Process Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">35</span></a>, Tooltik ‘TextBlob’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx55" title="">Loria, 2024</a>]</cite> can successfully correct misspelled words (‘healhty’) in our sample text. However, if clinical entities are not preserved during the pre-processing step, ‘TextBlob’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx55" title="">Loria, 2024</a>]</cite> may misidentify some clinical terms as spelling errors. It may be because ‘TextBlob’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx55" title="">Loria, 2024</a>]</cite> was developed on the general corpus, not a clinical one. Additionally, its corrections are limited to the word level and do not consider any context. Therefore, if words are misspelled deliberately, they could be processed incorrectly. Thus, <span class="ltx_text ltx_font_italic" id="S4.SS6.SSS2.p2.1.1">developing a clinical misspelling correction toolkit is a promising</span> research direction in the future.</p>
</div>
<figure class="ltx_figure" id="S4.F35"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S4.F35.g1" src="extracted/5855287/figures/SpellingCorrection.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 35: </span>Spelling Correction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx1" title="">A et al., 2000</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx44" title="">Johnson et al., 2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx43" title="">Johnson et al., 2023b</a>]</cite> (‘note_id’: ‘10807423-DS-19’)</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and Future Work</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Key Findings</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">These results provided some useful findings in generating clinical letters, including:</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Encoder-only models generally perform much better</span> in clinical-letter masking and generation tasks, which is consistent with a very recent study by <span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.2">?</span>). When clinical information is preserved, <span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.3">base encoder-only models perform comparably to clinical-related models</span>.
</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">To generate clinical letters that preserve clinical narrative while adding variety, <span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">BERTScore should be the primary evaluation metric</span>, with other metrics serving as supporting references. This is because BERTScore focuses more on semantic rather than literal similarity, and it is consistent with qualitative assessment results.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Different types of masked tokens influence</span> the quality of synthetic clinical letters. <span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.2">Stopwords</span> have a <span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.3">positive</span> impact, while <span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.4">nouns and verbs</span> have <span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.5">negative</span> impacts.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">For our objective, <span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">preserving useful tokens</span> is more effective than just fine-tuning the model without preserving any entities.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">The <span class="ltx_text ltx_font_bold" id="S5.I1.i5.p1.1.1">unmasked context</span> does not significantly influence the model’s understanding. As a result, the <span class="ltx_text ltx_font_bold" id="S5.I1.i5.p1.1.2">synthetic letters can be effectively used in downstream NER task</span> to replace original real-world letters.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Limitations</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Although the strategies mentioned above help generate diverse, de-identified synthetic clinical letters, there are still some limitations in applying this method generally.</p>
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i1.p1.1.1">Challenges in the Dataset: </span> Since these clinical letters are derived from the real world, some problems are inevitable. For example, there may be spelling errors in the dataset, such as in note_id “10807423-DS-19”, ‘healthy’ is misspelled as ‘healhty’. These errors may hurt the usability of the synthetic text. In addition, some polysemous words may also cause ambiguity due to context. For example, the word ‘back’ can refer to an anatomical entity (such as the back), and can also be used as an adverb.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i2.p1.1.1">Data Volume: </span>Due to the difficulty in collecting annotated data, only 204 clinical letters are used in our investigation. They may not be representative enough, which could limit the applicability of our findings to a broader scenario. Additionally, the data we used were already de-identified. Although we considered de-identification and took steps to mask all privacy information, the effectiveness of these approaches cannot be fully tested because we do not have access to sensitive datasets.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i3.p1.1.1">Evaluation Metrics: </span>In this project, we primarily use BERTScore for evaluation, while also incorporating other metrics such as ROUGE and readability metrics. Currently, there is <span class="ltx_text ltx_font_bold" id="S5.I2.i3.p1.1.2">no evaluation framework that can assess all aspects </span>simultaneously, including maintaining the original meaning, diversity, readability, and even clinical soundness.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i4.p1">
<p class="ltx_p" id="S5.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i4.p1.1.1">Clinical Knowledge Understanding: </span>While the model can often preserve clinical entities and generate contextually reasonable tokens, it sometimes encounters comprehension errors. For example, in a context where ‘LLE’ (‘left lower extremity’) is used, Bio_ClinicalBERT incorrectly predicted the nearby masked token as ‘R ankle’ (‘right ankle’). In this case, the model fails to capture the side knowledge accurately.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i5.p1">
<p class="ltx_p" id="S5.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i5.p1.1.1">Computing Resources: </span>Due to the limitations of computing resources, we explored a limited range of language generation models. There may be other models, such as improved decoder-only models, that could better adapt to our task.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Future Work</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Based on the limitations mentioned above, we outlined some potential directions to further explore:</p>
<ul class="ltx_itemize" id="S5.I3">
<li class="ltx_item" id="S5.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i1.p1">
<p class="ltx_p" id="S5.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i1.p1.1.1">Test on More Clinical Datasets: </span>To further evaluate the effectiveness of these masking strategies, more annotated clinical letters should be tested to assess system generalisation.</p>
</div>
</li>
<li class="ltx_item" id="S5.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i2.p1">
<p class="ltx_p" id="S5.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i2.p1.1.1">Assess de-identification Performance: </span> A quantitative metric for de-identification evaluation should be included in the future. Non-anonymous synthetic datasets can be used to evaluate the de-identification process, so that this system can be applied directly to real-world clinical letters in the future.</p>
</div>
</li>
<li class="ltx_item" id="S5.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i3.p1">
<p class="ltx_p" id="S5.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i3.p1.1.1">Evaluation Benchmark: </span>A new metric that is suitable for our task should be developed. Specifically, this metric should consider both similarity and diversity. Weighting parameters for each dimension could be useful and can be obtained through neural networks. For evaluating clinical soundness, it may be necessary to invite clinicians to assess the synthetic letters based on multiple dimensions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx28" title="">Ellershaw et al., 2024</a>]</cite>. Furthermore, the mapping from clinical letters to their quality scores can be learned using deep learning.</p>
</div>
</li>
<li class="ltx_item" id="S5.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i4.p1">
<p class="ltx_p" id="S5.I3.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i4.p1.1.1">Balancing Knowledge from Both Clinical and General Domains: </span>Although there are numerous clinical-related encoder-only models, only a few can effectively integrate clinical and general knowledge. <span class="ltx_text ltx_font_bold" id="S5.I3.i4.p1.1.2">?</span>) demonstrated that mixing the clinical dataset with the general dataset in a certain proportion can help the model better understand clinical knowledge. Therefore, a new BERT-based model should be trained from scratch using both clinical and general domain datasets.</p>
</div>
</li>
<li class="ltx_item" id="S5.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i5.p1">
<p class="ltx_p" id="S5.I3.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i5.p1.1.1">Synonymous Substitution: </span> We focused on exploring the range of eligible tokens for masking. Additionally, a masking strategy similar to BERT’s can be integrated with our results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#bib.bibx26" title="">Devlin et al., 2019</a>]</cite>. Specifically, we can select certain tokens to mask, some to retain, and replace others with synonyms. This approach can further enhance the variety of synthetic clinical letters. Moreover, the retained clinical entities can also be substituted using the entity linking to SNOMED CT.</p>
</div>
</li>
<li class="ltx_item" id="S5.I3.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i6.p1">
<p class="ltx_p" id="S5.I3.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i6.p1.1.1">Spelling Correction: </span>As mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09501v1#S4.SS6" title="4.6 Post-Process Results ‣ 4 Experimental Results and Analysis ‣ Synthetic4Health: Generating Annotated Synthetic Clinical Letters"><span class="ltx_text ltx_ref_tag">4.6</span></a>, there are very few toolkits available for spelling correction in the clinical domain. Standard spelling correction tools may misidentify clinical terms as misspelled words. Therefore, it is necessary to develop a specialised spell-checking tool adapted to the clinical domain.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Ethics Consideration</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">There are no ethical concerns in this project. Before accessing the dataset, we completed the necessary training (CITI Data or Specimens Only Research) and signed the Data Use Agreement (DUA). In addition, the results of this project will not be used for any commercial purpose.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">We thank the insightful feedback from Nicolo Micheletti which is very valuable to the start of this work.
LH, WDP, and GN are grateful for the support from the grant “Assembling the Data Jigsaw: Powering Robust Research on the
Causes, Determinants and Outcomes of MSK Disease.” The project has been funded by the Nuffield
Foundation, but the views expressed are those of the authors and not necessarily of the Foundation.
Visit www.nuffieldfoundation.org.
LH, WDP, and GN are also supported by the grant “Integrating hospital outpatient letters into the healthcare data space” (EP/V047949/1; funder: UKRI/EPSRC).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[A et al., 2000] </span>
<span class="ltx_bibblock">
Goldberger A, Amaral L, Glass L, Hausdorff J, Ivanov PC, Mark R, Mietus JE, Moody GB, Peng CK, and Stanley HE.

</span>
<span class="ltx_bibblock">2000.

</span>
<span class="ltx_bibblock">Physiobank, physiotoolkit, and physionet: Components of a new research resource for complex physiologic signals.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://physionet.org/content/</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[Abouelmehdi et al., 2018] </span>
<span class="ltx_bibblock">
Karim Abouelmehdi, Abderrahim Beni-Hessane, and Hayat Khaloufi.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Big healthcare data: preserving security and privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx2.1.1">Journal of big data</span>, 5(1):1–18.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[Acheampong et al., 2021] </span>
<span class="ltx_bibblock">
Francisca Adoma Acheampong, Henry Nunoo-Mensah, and Wenyu Chen.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Transformer models for text-based emotion detection: a review of bert-based approaches.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx3.1.1">Artificial Intelligence Review</span>, 54(8):5789–5829.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[AI, 2023] </span>
<span class="ltx_bibblock">
Meta AI.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Llama3 model card.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md</span>.

</span>
<span class="ltx_bibblock">Accessed on: 04/05/2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[Alsentzer et al., 2019] </span>
<span class="ltx_bibblock">
Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan Naumann, and Matthew McDermott.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Publicly available clinical bert embeddings.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx5.1.1">Proceedings of the 2nd Clinical Natural Language Processing Workshop</span>, pages 72–78.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[Amin-Nejad et al., 2020a] </span>
<span class="ltx_bibblock">
Ali Amin-Nejad, Julia Ive, and Sumithra Velupillai.

</span>
<span class="ltx_bibblock">2020a.

</span>
<span class="ltx_bibblock">Exploring transformer text generation for medical dataset augmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx6.1.1">Proceedings of the Twelfth Language Resources and Evaluation Conference</span>, pages 4699–4708.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[Amin-Nejad et al., 2020b] </span>
<span class="ltx_bibblock">
Ali Amin-Nejad, Julia Ive, and Sumithra Velupillai.

</span>
<span class="ltx_bibblock">2020b.

</span>
<span class="ltx_bibblock">Exploring transformer text generation for medical dataset augmentation.

</span>
<span class="ltx_bibblock">In Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx7.1.1">Proceedings of the Twelfth Language Resources and Evaluation Conference</span>, pages 4699–4708, Marseille, France, May. European Language Resources Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[Banerjee and Lavie, 2005] </span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.

</span>
<span class="ltx_bibblock">In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx8.1.1">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</span>, pages 65–72, Ann Arbor, Michigan, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[Belkadi et al., 2023] </span>
<span class="ltx_bibblock">
Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Warren Del-Pinto, and Goran Nenadic.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Generating medical instructions with conditional transformer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx9.1.1">NeurIPS 2023 Workshop on Synthetic Data Generation with Generative AI</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[Beltagy et al., 2020] </span>
<span class="ltx_bibblock">
Iz Beltagy, Matthew E Peters, and Arman Cohan.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx10.1.1">arXiv preprint arXiv:2004.05150</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[Bengio et al., 2000] </span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent.

</span>
<span class="ltx_bibblock">2000.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx11.1.1">Advances in neural information processing systems</span>, 13.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[Berg et al., 2020] </span>
<span class="ltx_bibblock">
Hanna Berg, Aron Henriksson, and Hercules Dalianis.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">The impact of de-identification on downstream named entity recognition in clinical text.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx12.1.1">Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis</span>, pages 1–11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[Berg et al., 2021] </span>
<span class="ltx_bibblock">
Hanna Berg, Aron Henriksson, Uno Fors, and Hercules Dalianis.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">De-identification of clinical text for secondary use: Research issues.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx13.1.1">14th International Joint Conference on Biomedical Engineering Systems and Technologies-BIOSTEC 2021, 11-13 Februar, 2021, Vienna, Austria</span>, pages 592–599. SciTePress.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[Boonstra et al., 2024] </span>
<span class="ltx_bibblock">
Machteld J Boonstra, Davy Weissenbacher, Jason H Moore, Graciela Gonzalez-Hernandez, and Folkert W Asselbergs.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Artificial intelligence: revolutionizing cardiology with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx14.1.1">European Heart Journal</span>, 45(5):332–345.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[Bose et al., 2021] </span>
<span class="ltx_bibblock">
Priyankar Bose, Sriram Srinivasan, William C Sleeman IV, Jatinder Palta, Rishabh Kapoor, and Preetam Ghosh.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">A survey on recent named entity recognition and relationship extraction techniques on clinical texts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx15.1.1">Applied Sciences</span>, 11(18):8319.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[Brown et al., 1990] </span>
<span class="ltx_bibblock">
Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Frederick Jelinek, John Lafferty, Robert L Mercer, and Paul S Roossin.

</span>
<span class="ltx_bibblock">1990.

</span>
<span class="ltx_bibblock">A statistical approach to machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx16.1.1">Computational linguistics</span>, 16(2):79–85.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[Brown et al., 2020] </span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx17.1.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</span>, NIPS ’20, Red Hook, NY, USA. Curran Associates Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[Bundschus et al., 2008] </span>
<span class="ltx_bibblock">
Markus Bundschus, Mathaeus Dejori, Martin Stetter, Volker Tresp, and Hans-Peter Kriegel.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock">Extraction of semantic biomedical relations from text using conditional random fields.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx18.1.1">BMC bioinformatics</span>, 9:1–14.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[Cai et al., 2022] </span>
<span class="ltx_bibblock">
Pei-Xuan Cai, Yao-Chung Fan, and Fang-Yie Leu.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Compare encoder-decoder, encoder-only, and decoder-only architectures for text generation on low-resource datasets.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx19.1.1">Advances on Broad-Band Wireless Computing, Communication and Applications: Proceedings of the 16th International Conference on Broad-Band Wireless Computing, Communication and Applications (BWCCA-2021)</span>, pages 216–225. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[Camacho-Collados and Pilehvar, 2018] </span>
<span class="ltx_bibblock">
Jose Camacho-Collados and Mohammad Taher Pilehvar.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">From word to sense embeddings: A survey on vector representations of meaning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx20.1.1">Journal of Artificial Intelligence Research</span>, 63:743–788.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[Cawsey et al., 1997] </span>
<span class="ltx_bibblock">
Alison J Cawsey, Bonnie L Webber, and Ray B Jones.

</span>
<span class="ltx_bibblock">1997.

</span>
<span class="ltx_bibblock">Natural language generation in health care.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx21.1.1">Journal of the American Medical Informatics Association</span>, 4(6):473–482.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[Cho et al., 2014] </span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">Learning phrase representations using rnn encoder–decoder for statistical machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx22.1.1">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, page 1724. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[Collobert et al., 2011] </span>
<span class="ltx_bibblock">
Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock">Natural language processing (almost) from scratch.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx23.1.1">Journal of machine learning research</span>, 12:2493–2537.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[Context.ai, 2024] </span>
<span class="ltx_bibblock">
Context.ai.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Compare llama 3 70b instruct to gpt-3.5 turbo.

</span>
<span class="ltx_bibblock">[Online; accessed 14-August-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[Dernoncourt et al., 2017] </span>
<span class="ltx_bibblock">
Franck Dernoncourt, Ji Young Lee, Ozlem Uzuner, and Peter Szolovits.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">De-identification of patient notes with recurrent neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx25.1.1">Journal of the American Medical Informatics Association</span>, 24(3):596–606.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[Devlin et al., 2019] </span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx26.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</span>, pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx27">
<span class="ltx_tag ltx_tag_bibitem">[Eddy, 1996] </span>
<span class="ltx_bibblock">
Sean R Eddy.

</span>
<span class="ltx_bibblock">1996.

</span>
<span class="ltx_bibblock">Hidden markov models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx27.1.1">Current opinion in structural biology</span>, 6(3):361–365.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx28">
<span class="ltx_tag ltx_tag_bibitem">[Ellershaw et al., 2024] </span>
<span class="ltx_bibblock">
Simon Ellershaw, Christopher Tomlinson, Oliver E Burton, Thomas Frost, John Gerrard Hanrahan, Danyal Zaman Khan, Hugo Layard Horsfall, Mollie Little, Evaleen Malgapo, Joachim Starup-Hansen, et al.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Automated generation of hospital discharge summaries using clinical guidelines and large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx28.1.1">AAAI 2024 Spring Symposium on Clinical Foundation Models</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx29">
<span class="ltx_tag ltx_tag_bibitem">[Eric and Johnson, 2023] </span>
<span class="ltx_bibblock">
Lehman Eric and Alistair Johnson.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Clinical-T5: Large Language Models Built Using MIMIC Clinical Text.

</span>
<span class="ltx_bibblock">PhysioNet.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx30">
<span class="ltx_tag ltx_tag_bibitem">[Gatt and Krahmer, 2018] </span>
<span class="ltx_bibblock">
Albert Gatt and Emiel Krahmer.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx30.1.1">Journal of Artificial Intelligence Research</span>, 61:65–170.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx31">
<span class="ltx_tag ltx_tag_bibitem">[Gillioz et al., 2020] </span>
<span class="ltx_bibblock">
Anthony Gillioz, Jacky Casas, Elena Mugellini, and Omar Abou Khaled.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Overview of the transformer-based models for nlp tasks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx31.1.1">2020 15th Conference on computer science and information systems (FedCSIS)</span>, pages 179–183. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx32">
<span class="ltx_tag ltx_tag_bibitem">[Goldberger et al., 2000] </span>
<span class="ltx_bibblock">
Ary L. Goldberger, Luis A. N. Amaral, Leon Glass, Jeffrey M. Hausdorff, Plamen Ch. Ivanov, Roger G. Mark, Joseph E. Mietus, George B. Moody, Chung-Kang Peng, and H. Eugene Stanley.

</span>
<span class="ltx_bibblock">2000.

</span>
<span class="ltx_bibblock">Physiobank, physiotoolkit, and physionet.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx32.1.1">Circulation</span>, 101(23):e215–e220.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx33">
<span class="ltx_tag ltx_tag_bibitem">[Graves and Graves, 2012] </span>
<span class="ltx_bibblock">
Alex Graves and Alex Graves.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx33.1.1">Supervised sequence labelling with recurrent neural networks</span>, pages 37–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx34">
<span class="ltx_tag ltx_tag_bibitem">[Grishman and Sundheim, 1996] </span>
<span class="ltx_bibblock">
Ralph Grishman and Beth M Sundheim.

</span>
<span class="ltx_bibblock">1996.

</span>
<span class="ltx_bibblock">Message understanding conference-6: A brief history.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx34.1.1">COLING 1996 volume 1: The 16th international conference on computational linguistics</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx35">
<span class="ltx_tag ltx_tag_bibitem">[Hirst et al., 1997] </span>
<span class="ltx_bibblock">
Graeme Hirst, Chrysanne DiMarco, Eduard Hovy, and Kimberley Parsons.

</span>
<span class="ltx_bibblock">1997.

</span>
<span class="ltx_bibblock">Authoring and generating health-education documents that are tailored to the needs of the individual patient.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx35.1.1">User Modeling: Proceedings of the Sixth International Conference UM97 Chia Laguna, Sardinia, Italy June 2–5 1997</span>, pages 107–118. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx36">
<span class="ltx_tag ltx_tag_bibitem">[Hochreiter and Schmidhuber, 1997] </span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx36.1.1">Neural computation</span>, 9(8):1735–1780.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx37">
<span class="ltx_tag ltx_tag_bibitem">[Hou et al., 2022] </span>
<span class="ltx_bibblock">
Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, and Denny Zhou.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Token dropping for efficient bert pretraining.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx37.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 3774–3784.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx38">
<span class="ltx_tag ltx_tag_bibitem">[Humbert-Droz et al., 2022] </span>
<span class="ltx_bibblock">
Marie Humbert-Droz, Pritam Mukherjee, and Olivier Gevaert.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Strategies to address the lack of labeled data for supervised machine learning training with electronic health records: Case study for the extraction of symptoms from clinical notes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx38.1.1">JMIR Medical Informatics</span>, 10(3):e32903.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx39">
<span class="ltx_tag ltx_tag_bibitem">[HÜSKE-KRAUS, 2003] </span>
<span class="ltx_bibblock">
D HÜSKE-KRAUS.

</span>
<span class="ltx_bibblock">2003.

</span>
<span class="ltx_bibblock">Text generation in clinical medicine: A review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx39.1.1">Methods of information in medicine</span>, 42(1):51–60.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx40">
<span class="ltx_tag ltx_tag_bibitem">[International, 2024] </span>
<span class="ltx_bibblock">
SNOMED International.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Snomed ct browser.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://browser.ihtsdotools.org/?</span>
</span>
<span class="ltx_bibblock">Accessed: 2024-09-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx41">
<span class="ltx_tag ltx_tag_bibitem">[Johnson et al., 2016] </span>
<span class="ltx_bibblock">
Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Mimic-iii, a freely accessible critical care database.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx41.1.1">Scientific data</span>, 3(1):1–9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx42">
<span class="ltx_tag ltx_tag_bibitem">[Johnson et al., 2023a] </span>
<span class="ltx_bibblock">
Alistair Johnson, Tom Pollard, Steven Horng, Leo Anthony Celi, and Roger Mark.

</span>
<span class="ltx_bibblock">2023a.

</span>
<span class="ltx_bibblock">MIMIC-IV-Note: Deidentified free-text clinical notes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx42.1.1">PhysioNet</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx43">
<span class="ltx_tag ltx_tag_bibitem">[Johnson et al., 2023b] </span>
<span class="ltx_bibblock">
Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al.

</span>
<span class="ltx_bibblock">2023b.

</span>
<span class="ltx_bibblock">Mimic-iv, a freely accessible electronic health record dataset.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx43.1.1">Scientific data</span>, 10(1):1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx44">
<span class="ltx_tag ltx_tag_bibitem">[Johnson et al., 2024a] </span>
<span class="ltx_bibblock">
Alistair Johnson, Lucas Bulgarelli, Tom Pollard, Brian Gow, Benjamin Moody, Steven Horng, Leo Anthony Celi, and Roger Mark.

</span>
<span class="ltx_bibblock">2024a.

</span>
<span class="ltx_bibblock">Mimic-iv.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx45">
<span class="ltx_tag ltx_tag_bibitem">[Johnson et al., 2024b] </span>
<span class="ltx_bibblock">
S Joshua Johnson, M Ramakrishna Murty, and I Navakanth.

</span>
<span class="ltx_bibblock">2024b.

</span>
<span class="ltx_bibblock">A detailed review on word embedding techniques with emphasis on word2vec.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx45.1.1">Multimedia Tools and Applications</span>, 83(13):37979–38007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx46">
<span class="ltx_tag ltx_tag_bibitem">[Kong et al., 2022] </span>
<span class="ltx_bibblock">
Ming Kong, Zhengxing Huang, Kun Kuang, Qiang Zhu, and Fei Wu.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Transq: Transformer-based semantic query for medical report generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx46.1.1">International Conference on Medical Image Computing and Computer-Assisted Intervention</span>, pages 610–620. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx47">
<span class="ltx_tag ltx_tag_bibitem">[Kovačević et al., 2024] </span>
<span class="ltx_bibblock">
Aleksandar Kovačević, Bojana Bašaragin, Nikola Milošević, and Goran Nenadić.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">De-identification of clinical free text using natural language processing: A systematic review of current approaches.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx47.1.1">Artificial Intelligence in Medicine</span>, page 102845.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx48">
<span class="ltx_tag ltx_tag_bibitem">[Kundeti et al., 2016] </span>
<span class="ltx_bibblock">
Srinivasa Rao Kundeti, J Vijayananda, Srikanth Mujjiga, and M Kalyan.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Clinical named entity recognition: Challenges and opportunities.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx48.1.1">2016 IEEE International Conference on Big Data (Big Data)</span>, pages 1937–1945. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx49">
<span class="ltx_tag ltx_tag_bibitem">[Lamprou et al., 2022] </span>
<span class="ltx_bibblock">
Zenon Lamprou, Frank Pollick, and Yashar Moshfeghi.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Role of punctuation in semantic mapping between brain and transformer models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx49.1.1">International Conference on Machine Learning, Optimization, and Data Science</span>, pages 458–472. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx50">
<span class="ltx_tag ltx_tag_bibitem">[Lee, 2018] </span>
<span class="ltx_bibblock">
Scott H Lee.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Natural language generation for electronic health records.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx50.1.1">NPJ digital medicine</span>, 1(1):63.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx51">
<span class="ltx_tag ltx_tag_bibitem">[Li et al., 2023] </span>
<span class="ltx_bibblock">
Yikuan Li, Ramsey M Wehbe, Faraz S Ahmad, Hanyin Wang, and Yuan Luo.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">A comparative study of pretrained language models for long clinical text.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx51.1.1">Journal of the American Medical Informatics Association</span>, 30(2):340–347.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx52">
<span class="ltx_tag ltx_tag_bibitem">[Li et al., 2024] </span>
<span class="ltx_bibblock">
Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Pre-trained language models for text generation: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx52.1.1">ACM Computing Surveys</span>, 56(9):1–39.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx53">
<span class="ltx_tag ltx_tag_bibitem">[Liu et al., 2022] </span>
<span class="ltx_bibblock">
Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">What makes good in-context examples for gpt-3?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx53.1.1">Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</span>, pages 100–114.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx54">
<span class="ltx_tag ltx_tag_bibitem">[Locke et al., 2021] </span>
<span class="ltx_bibblock">
Saskia Locke, Anthony Bashall, Sarah Al-Adely, John Moore, Anthony Wilson, and Gareth B Kitchen.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Natural language processing in medicine: a review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx54.1.1">Trends in Anaesthesia and Critical Care</span>, 38:4–9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx55">
<span class="ltx_tag ltx_tag_bibitem">[Loria, 2024] </span>
<span class="ltx_bibblock">
S. Loria.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Textblob: Simplified text processing.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://textblob.readthedocs.io/en/dev/</span>.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx56">
<span class="ltx_tag ltx_tag_bibitem">[Lu et al., 2022] </span>
<span class="ltx_bibblock">
Qiuhao Lu, Dejing Dou, and Thien Nguyen.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">ClinicalT5: A generative language model for clinical text.

</span>
<span class="ltx_bibblock">In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx56.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022</span>, pages 5436–5443, Abu Dhabi, United Arab Emirates, December. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx57">
<span class="ltx_tag ltx_tag_bibitem">[Luo et al., 2022] </span>
<span class="ltx_bibblock">
Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Biogpt: generative pre-trained transformer for biomedical text generation and mining.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx57.1.1">Briefings in bioinformatics</span>, 23(6):bbac409.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx58">
<span class="ltx_tag ltx_tag_bibitem">[Ma and Zhang, 2015] </span>
<span class="ltx_bibblock">
Long Ma and Yanqing Zhang.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Using word2vec to process big text data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx58.1.1">2015 IEEE International Conference on Big Data (Big Data)</span>, pages 2895–2897. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx59">
<span class="ltx_tag ltx_tag_bibitem">[Masuko et al., 1998] </span>
<span class="ltx_bibblock">
Takashi Masuko, Takao Kobayashi, Masatsune Tamura, Jun Masubuchi, and Keiichi Tokuda.

</span>
<span class="ltx_bibblock">1998.

</span>
<span class="ltx_bibblock">Text-to-visual speech synthesis based on parameter generation from hmm.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx59.1.1">Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP’98 (Cat. No. 98CH36181)</span>, volume 6, pages 3745–3748. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx60">
<span class="ltx_tag ltx_tag_bibitem">[McKeown et al., 1997] </span>
<span class="ltx_bibblock">
Kathleen McKeown, Desmond A Jordan, Shimei Pan, James Shaw, and Barry A Allen.

</span>
<span class="ltx_bibblock">1997.

</span>
<span class="ltx_bibblock">Language generation for multimedia healthcare briefings.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx60.1.1">Fifth Conference on Applied Natural Language Processing</span>, pages 277–282.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx61">
<span class="ltx_tag ltx_tag_bibitem">[Meystre et al., 2014] </span>
<span class="ltx_bibblock">
Stéphane M Meystre, Oscar Ferrández, F Jeffrey Friedlin, Brett R South, Shuying Shen, and Matthew H Samore.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">Text de-identification for privacy protection: a study of its impact on clinical text information content.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx61.1.1">Journal of biomedical informatics</span>, 50:142–150.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx62">
<span class="ltx_tag ltx_tag_bibitem">[Meystre, 2015] </span>
<span class="ltx_bibblock">
Stephane M Meystre.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">De-identification of unstructured clinical data for patient privacy protection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx62.1.1">Medical Data Privacy Handbook</span>, pages 697–716. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx63">
<span class="ltx_tag ltx_tag_bibitem">[Micheletti et al., 2024] </span>
<span class="ltx_bibblock">
Nicolo Micheletti, Samuel Belkadi, Lifeng Han, and Goran Nenadic.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Exploration of masked and causal language modelling for text generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx63.1.1">CoRR</span>, abs/2405.12630.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx64">
<span class="ltx_tag ltx_tag_bibitem">[Mikolov et al., 2013a] </span>
<span class="ltx_bibblock">
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.

</span>
<span class="ltx_bibblock">2013a.

</span>
<span class="ltx_bibblock">Efficient estimation of word representations in vector space.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx64.1.1">arXiv preprint arXiv:1301.3781</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx65">
<span class="ltx_tag ltx_tag_bibitem">[Mikolov et al., 2013b] </span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.

</span>
<span class="ltx_bibblock">2013b.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their compositionality.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx65.1.1">Advances in neural information processing systems</span>, 26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx66">
<span class="ltx_tag ltx_tag_bibitem">[Nadkarni et al., 2011] </span>
<span class="ltx_bibblock">
Prakash M Nadkarni, Lucila Ohno-Machado, and Wendy W Chapman.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock">Natural language processing: an introduction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx66.1.1">Journal of the American Medical Informatics Association</span>, 18(5):544–551.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx67">
<span class="ltx_tag ltx_tag_bibitem">[Norgeot et al., 2020] </span>
<span class="ltx_bibblock">
Beau Norgeot, Kathleen Muenzen, Thomas A Peterson, Xuancheng Fan, Benjamin S Glicksberg, Gundolf Schenk, Eugenia Rutenberg, Boris Oskotsky, Marina Sirota, Jinoos Yazdany, et al.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Protected health information filter (philter): accurately and securely de-identifying free-text clinical notes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx67.1.1">NPJ digital medicine</span>, 3(1):57.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx68">
<span class="ltx_tag ltx_tag_bibitem">[Phan et al., 2021] </span>
<span class="ltx_bibblock">
Long Phan, James T. Anibal, Hieu Trung Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, and Grégoire Altan-Bonnet.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Scifive: a text-to-text transformer model for biomedical literature.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx68.1.1">ArXiv</span>, abs/2106.03598.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx69">
<span class="ltx_tag ltx_tag_bibitem">[Qi et al., 2020] </span>
<span class="ltx_bibblock">
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D Manning.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Stanza: A python natural language processing toolkit for many human languages.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx69.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</span>, pages 101–108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx70">
<span class="ltx_tag ltx_tag_bibitem">[Raffel et al., 2020a] </span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.

</span>
<span class="ltx_bibblock">2020a.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx70.1.1">Journal of machine learning research</span>, 21(140):1–67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx71">
<span class="ltx_tag ltx_tag_bibitem">[Raffel et al., 2020b] </span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.

</span>
<span class="ltx_bibblock">2020b.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx71.1.1">Journal of Machine Learning Research</span>, 21(140):1–67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx72">
<span class="ltx_tag ltx_tag_bibitem">[Rayner et al., 2020] </span>
<span class="ltx_bibblock">
Hugh Rayner, Martha Hickey, Ian Logan, Nigel Mathers, Peter Rees, and Robina Shah.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Writing outpatient letters to patients.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx72.1.1">BMJ</span>, 368.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx73">
<span class="ltx_tag ltx_tag_bibitem">[Santhanam, 2020] </span>
<span class="ltx_bibblock">
Sivasurya Santhanam.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Context based text-generation using lstm networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx73.1.1">arXiv e-prints</span>, pages arXiv–2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx74">
<span class="ltx_tag ltx_tag_bibitem">[Satapathy et al., 2017] </span>
<span class="ltx_bibblock">
Ranjan Satapathy, Erik Cambria, and Amir Hussain.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx74.1.1">Sentiment analysis in the bio-medical domain</span>.

</span>
<span class="ltx_bibblock">Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx75">
<span class="ltx_tag ltx_tag_bibitem">[Sharma et al., 2023] </span>
<span class="ltx_bibblock">
Sonali Sharma, Manoj Diwakar, Prabhishek Singh, Vijendra Singh, Seifedine Kadry, and Jungeun Kim.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Machine translation systems based on classical-statistical-deep-learning approaches.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx75.1.1">Electronics</span>, 12(7):1716.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx76">
<span class="ltx_tag ltx_tag_bibitem">[Spasic and Nenadic, 2020] </span>
<span class="ltx_bibblock">
Irena Spasic and Goran Nenadic.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Clinical text data in machine learning: Systematic review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx76.1.1">JMIR medical informatics</span>, 8(3):e17984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx77">
<span class="ltx_tag ltx_tag_bibitem">[Sun and Iyyer, 2021] </span>
<span class="ltx_bibblock">
Simeng Sun and Mohit Iyyer.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Revisiting simple neural probabilistic language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx77.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</span>, pages 5181–5188.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx78">
<span class="ltx_tag ltx_tag_bibitem">[Sutton et al., 2012] </span>
<span class="ltx_bibblock">
Charles Sutton, Andrew McCallum, et al.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">An introduction to conditional random fields.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx78.1.1">Foundations and Trends® in Machine Learning</span>, 4(4):267–373.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx79">
<span class="ltx_tag ltx_tag_bibitem">[Tang et al., 2023] </span>
<span class="ltx_bibblock">
Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Does synthetic data generation of llms help clinical text mining?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx79.1.1">ArXiv</span>, abs/2303.04360.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx80">
<span class="ltx_tag ltx_tag_bibitem">[Tarur and Prasanna, 2021] </span>
<span class="ltx_bibblock">
Sumitha Udayashankar Tarur and Sudhakar Prasanna.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">clinical case letter.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx80.1.1">Indian Pediatr</span>, 58(188):189.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx81">
<span class="ltx_tag ltx_tag_bibitem">[the President and of Harvard College, 2023] </span>
<span class="ltx_bibblock">
the President and Fellows of Harvard College.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Dbmi data portal.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://portal.dbmi.hms.harvard.edu/</span>.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-04.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx82">
<span class="ltx_tag ltx_tag_bibitem">[Tsirmpas et al., 2024] </span>
<span class="ltx_bibblock">
Dimitrios Tsirmpas, Ioannis Gkionis, Georgios Th Papadopoulos, and Ioannis Mademlis.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Neural natural language processing for long texts: A survey on classification and summarization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx82.1.1">Engineering Applications of Artificial Intelligence</span>, 133:108231.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx83">
<span class="ltx_tag ltx_tag_bibitem">[Tucker et al., 2016] </span>
<span class="ltx_bibblock">
Katherine Tucker, Janice Branson, Maria Dilleen, Sally Hollis, Paul Loughlin, Mark J Nixon, and Zoë Williams.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Protecting patient privacy when sharing patient-level data from clinical trials.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx83.1.1">BMC medical research methodology</span>, 16:5–14.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx84">
<span class="ltx_tag ltx_tag_bibitem">[van der Lee et al., 2018] </span>
<span class="ltx_bibblock">
Chris van der Lee, Emiel Krahmer, and Sander Wubben.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Automated learning of templates for data-to-text generation: comparing rule-based, statistical and neural methods.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx84.1.1">Proceedings of the 11th International Conference on Natural Language Generation</span>, pages 35–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx85">
<span class="ltx_tag ltx_tag_bibitem">[Vaswani et al., 2017] </span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx85.1.1">Proceedings of the 31st International Conference on Neural Information Processing Systems</span>, NIPS’17, page 6000–6010, Red Hook, NY, USA. Curran Associates Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx86">
<span class="ltx_tag ltx_tag_bibitem">[Wang et al., 2023] </span>
<span class="ltx_bibblock">
Guangyu Wang, Xiaohong Liu, Zhen Ying, Guoxing Yang, Zhiwei Chen, Zhiwen Liu, Min Zhang, Hongmei Yan, Yuxing Lu, Yuanxu Gao, et al.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Optimized glycemic control of type 2 diabetes with reinforcement learning: a proof-of-concept trial.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx86.1.1">Nature Medicine</span>, 29(10):2633–2642.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx87">
<span class="ltx_tag ltx_tag_bibitem">[Wu, 2024] </span>
<span class="ltx_bibblock">
Yonghui Wu.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Large language model and text generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx87.1.1">Natural Language Processing in Biomedicine: A Practical Guide</span>, pages 265–297. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx88">
<span class="ltx_tag ltx_tag_bibitem">[Xie et al., 2024] </span>
<span class="ltx_bibblock">
Q Xie, Q Chen, A Chen, C Peng, Y Hu, F Lin, X Peng, J Huang, J Zhang, V Keloth, et al.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Me-llama: Foundation large language models for medical applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx88.1.1">Research Square</span>, pages rs–3.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx89">
<span class="ltx_tag ltx_tag_bibitem">[Yang et al., 2019] </span>
<span class="ltx_bibblock">
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Xlnet: Generalized autoregressive pretraining for language understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx89.1.1">Advances in neural information processing systems</span>, 32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx90">
<span class="ltx_tag ltx_tag_bibitem">[Zeng et al., 2022] </span>
<span class="ltx_bibblock">
Wang Zeng, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo, Wanli Ouyang, and Xiaogang Wang.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Not all tokens are equal: Human-centric visual analysis via token clustering transformer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx90.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 11101–11111.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx91">
<span class="ltx_tag ltx_tag_bibitem">[Zhang et al., 2020] </span>
<span class="ltx_bibblock">
Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, and Xiang Zhou.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Semantics-aware bert for language understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx91.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volume 34, pages 9628–9635.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx92">
<span class="ltx_tag ltx_tag_bibitem">[Zhang et al., 2021] </span>
<span class="ltx_bibblock">
Yuhao Zhang, Yuhui Zhang, Peng Qi, Christopher D Manning, and Curtis P Langlotz.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Biomedical and clinical English model packages for the Stanza Python NLP library.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx92.1.1">Journal of the American Medical Informatics Association</span>, 06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx93">
<span class="ltx_tag ltx_tag_bibitem">[Zhuang et al., 2021] </span>
<span class="ltx_bibblock">
Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">A robustly optimized BERT pre-training approach with post-training.

</span>
<span class="ltx_bibblock">In Sheng Li, Maosong Sun, Yang Liu, Hua Wu, Kang Liu, Wanxiang Che, Shizhu He, and Gaoqi Rao, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx93.1.1">Proceedings of the 20th Chinese National Conference on Computational Linguistics</span>, pages 1218–1227, Huhhot, China, August. Chinese Information Processing Society of China.

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Appendix</h2>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 14 18:09:18 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
