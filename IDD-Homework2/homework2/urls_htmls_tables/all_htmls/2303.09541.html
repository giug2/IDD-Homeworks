<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.09541] Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains</title><meta property="og:description" content="Recent text-to-image generative models have exhibited remarkable abilities in generating high-fidelity and photo-realistic images. However, despite the visually impressive results, these models often struggle to preser‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.09541">

<!--Generated on Thu Feb 29 19:57:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenzhen Weng, Laura Bravo-S√°nchez, Serena Yeung-Levy 
<br class="ltx_break">Stanford University 
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{zzweng, lmbravo, syyeung}@stanford.edu </span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Recent text-to-image generative models have exhibited remarkable abilities in generating high-fidelity and photo-realistic images. However, despite the visually impressive results, these models often struggle to preserve plausible human structure in the generations. Due to this reason, while generative models have shown promising results in aiding downstream image recognition tasks by generating large volumes of synthetic data,
they are not suitable for improving downstream human pose perception and understanding. In this work, we propose a Diffusion model with Human Pose Correction (Diffusion-HPC), a text-conditioned method that generates photo-realistic images with plausible posed humans by injecting prior knowledge about human body structure. Our generated images are accompanied by 3D meshes that serve as ground truths for improving Human Mesh Recovery tasks, where a shortage of 3D training data has long been an issue. Furthermore, we show that Diffusion-HPC effectively improves the realism of human generations under varying conditioning strategies.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code: <a target="_blank" href="https://github.com/ZZWENG/Diffusion_HPC" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ZZWENG/Diffusion_HPC</a></span></span></span></p>
</div>
<div id="id2" class="ltx_logical-block">
<div id="id2.p1" class="ltx_para">
<img src="/html/2303.09541/assets/x1.png" id="id1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="204" alt="[Uncaptioned image]">
</div>
<figure id="S0.F1" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.3.2" class="ltx_text" style="font-size:90%;">
We propose Diffusion model with Human Pose Correction (Diffusion-HPC), a synthetic image generation strategy with paired with ground-truth meshes to improve the performance of Human Mesh Recovery (HMR) models on domains with challenging poses and/or limited data. Diffusion-HPC is a text-conditioned method that addresses the implausibility of human generations from Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, a large pre-trained text-conditioned generative model, while preserving the inherent flexibility of such models.
</span></figcaption>
</figure>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, large-scale text-conditioned image generation models such as GLIDE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>, Imagen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> and Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> have impressed the research community with their exceptional generative and compositional capabilities, owing to their training on extremely large image-text datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> and use of advanced model architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>. Not only do these generative models drastically elevate the quality and efficiency of content creation, but they also exhibit promising potential for enhancing other visual tasks. As shown in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">He et¬†al.</span> [<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, large text-conditioned generative models such as GLIDE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> are able to generate high-quality images targeted for a specific label space (i.e. domain customization), thus making it an ideal choice for synthetic data generation to aid in downstream image recognition tasks such as single-view Human Mesh Recovery (HMR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> where securing annotations can be not only costly, but incompatible in the wild.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite the benefits of synthetic data for image recognition tasks, these text-conditioned generative models have thus far lacked the capability of advancing human pose understanding tasks.
This is because these models do not explicitly model the underlying structure of human bodies and thus frequently encounter difficulties in preserving realistic human anatomy in their generated outputs. As Figure <a href="#S0.F1" title="Figure 1 ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (a) shows, generating realistic human poses embedded in plausible scenes is a known
limitation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> of generative diffusion models such as Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we present Diffusion model with Human Pose Correction (Diffusion-HPC), a method that addresses the implausibility of human generations from large pre-trained text-conditioned generative models. Our intuition is that we can rectify the generated unrealistic humans (e.g. with additional limbs in non-anatomical locations) by integrating stronger human pose priors within the generation process.
Thereby, we extend the capability of pre-trained diffusion models, such as Stable Diffusion, to produce a large variety of synthetic scenes for a target domain with minimal user input. Further, unlike base diffusion models our approach produces pairs of images and ground truth meshes as a result of including body pose priors in the generation process.
These image-mesh pairs can then be employed to improve existing single-view Human Mesh Recovery methods on challenging data-scarce domains (See Figure <a href="#S0.F1" title="Figure 1 ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>b, c). In summary, we make the following contributions.
</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Motivated by the implausible humans produced by diffusion models, we propose a simple and effective method Diffusion-HPC to rectify the implausibility of human generations that often occur in Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> results. To the best of our knowledge, our work presents the first training-free method that addresses the challenges in generating realistic humans by injecting human body structure priors within the generation process.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We show that the synthetic images with corresponding 3D ground truth produced by our method are capable of adapting Human Mesh Recovery models to challenging domains (e.g. competitive sports) where supervision is limited and hard to obtain. Models finetuned with Diffusion-HPC‚Äôs synthetic data achieve 2.6% PCK and 4.6 PA-MPJPE improvement on SMART <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> and Ski-Pose 3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>, respectively.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We quantitatively validate the improved quality of our generated images over existing text-to-image as well as state-of-the-art pose-to-image generative models.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Using synthetic data to improve HMR</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> have recognized the capability of state-of-the-art text-conditioned generative models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> for generating training data for downstream image recognition tasks. However, the poor quality of the generated person images effectively precludes the extension of this capacity to tasks such as 3D human pose understanding (e.g. human mesh recovery).
Due to the challenge in collecting 3D ground truths for end-to-end training of human mesh recovery models, many previous works have considered leveraging synthetic data. Typically, these works create 2D renderings of 3D posed human models from graphics engines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, with <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Black et¬†al.</span> [<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> being the most comprehensive effort. However, this approach possesses multiple disadvantages. First,
the variety of the generated poses is limited by the pose data source. Second, a large and diverse training set is needed to cover all possible poses of interest, which makes storing and sharing such data costly and inefficient. To address these, recent work <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Weng et¬†al.</span> [<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> proposes a data-efficient way by rendering SMPL bodies with poses sampled from the estimated pose distributions from real data, but since the body textures are predicted and warped from real images, the renderings are not photo-realistic. Analogously, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Sengupta et¬†al.</span> [<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> generates synthetic data online to improve diverse body shape estimation.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In contrast, using conditional generative models <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> such as ours to synthesize data has a few advantages. First, large generative models can produce high-fidelity photo-realistic images closer to real data since they are trained on internet-scale real-world data (e.g. LAION-5B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>). Second, they allow easy control of the generation style via detailed prompting, and stochasticity in the generation process results in more diverse and potentially unlimited synthetic data. However, although there have been some attempts to explore the use of generative models for image classification and object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, their usage in human mesh recovery has not yet been investigated due to the poor quality of the generated person images. Diffusion-HPC is the first approach that uses conditional generative models to produce synthetic data that are useful for human mesh recovery, broadening the range of downstream utilities of SoTA generative models.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Conditional generation of posed humans</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Recently, there has been a growing focus on conditional generation of posed humans in the form of images/videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>, body models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> or NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>. For synthesizing posed humans, most works focus either on text-conditioned or pose-conditioned generation.
In terms of text-conditioned approaches state-of-the-art general image generation models such as Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> have shown impressive capability in producing high-resolution and realistic images. But as a known limitation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, they frequently struggle to preserve the correct anatomy of human bodies. An alternate line of research focuses directly on text-conditioned human pose or motion generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>. These generative models are trained on large 3D human motion database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> with paired textual descriptions. But since they output parameters of human body models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, they bypass the issue of preserving anatomical structure. However, since human motion databases do not come with paired RGB data, these works are unable to produce human textures and background.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">On the other hand, previous works have explored generating images of full-body humans conditioning on body pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">AlBahar et¬†al.</span> [<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>], <span class="ltx_text" style="font-size:90%;">Knoche et¬†al.</span> [<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>], <span class="ltx_text" style="font-size:90%;">Men et¬†al.</span> [<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> consider the task of ‚Äúreposing‚Äù, where the goal is to synthesize images of people in a novel pose, based on a reference image of that person and the new pose. More recently and relevant to our work, Brooks <em id="S2.SS2.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> proposed a pose-conditioned image synthesis model that dispenses with the reference images by generating reasonable backgrounds. In contrast to the above works, our proposed Diffusion-HPC is a person image synthesis method flexible enough to allow for both text and pose conditioned generation and does not require additional training or explicit pose annotations from a target domain to produce diverse humans and scenes. Another closely related work is ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>, where posed-conditioned images are obtained from generative models, yet unlike our method ControlNet requires finetuning on large amounts of real paired data (i.e. 2D keypoints, images and captions).</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Editing &amp; composing large pre-trained models</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Foundation models (e.g., Imagen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>) that are trained on large amounts of broad data have demonstrated impressive generative and few-shot learning capabilities across a wide spectrum of tasks. Additionally, the scale of information they have seen and learned, allows these models to be adapted to further downstream tasks. For these reasons, editing or composing large pre-trained models has been widely studied recently. Among these works the most closely related to our approach are the training-free methods that utilize pre-trained diffusion models to perform global or local image editing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> (e.g. inpainting, style transfer, etc.). They are ‚Äútraining-free‚Äù in the sense that editing is done by injecting knowledge into the denoising process during inference and therefore no additional model finetuning is needed. Analogously, our method Diffusion-HPC improves plausibility of human generations by injecting human body priors in the form of posed SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> body models.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2303.09541/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.21.9.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.16.8" class="ltx_text" style="font-size:90%;">Overview of Diffusion-HPC. The generation process can be broken down into 3 steps. <span id="S2.F2.16.8.1" class="ltx_text" style="color:#00FFFF;">Step 1</span>: Obtaining image latents <math id="S2.F2.9.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.F2.9.1.m1.1b"><mi id="S2.F2.9.1.m1.1.1" xref="S2.F2.9.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.F2.9.1.m1.1c"><ci id="S2.F2.9.1.m1.1.1.cmml" xref="S2.F2.9.1.m1.1.1">ùëß</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.9.1.m1.1d">z</annotation></semantics></math> from the initial generation <math id="S2.F2.10.2.m2.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S2.F2.10.2.m2.1b"><mi class="ltx_font_mathcaligraphic" id="S2.F2.10.2.m2.1.1" xref="S2.F2.10.2.m2.1.1.cmml">‚Ñê</mi><annotation-xml encoding="MathML-Content" id="S2.F2.10.2.m2.1c"><ci id="S2.F2.10.2.m2.1.1.cmml" xref="S2.F2.10.2.m2.1.1">‚Ñê</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.10.2.m2.1d">\mathcal{I}</annotation></semantics></math> of a pre-trained text-to-image model (i.e. Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>) and injecting noise. <span id="S2.F2.16.8.2" class="ltx_text" style="color:#FFBFBF;">Step 2</span>: Estimating human body mesh <math id="S2.F2.11.3.m3.2" class="ltx_Math" alttext="\mathcal{M}(\theta,\beta)" display="inline"><semantics id="S2.F2.11.3.m3.2b"><mrow id="S2.F2.11.3.m3.2.3" xref="S2.F2.11.3.m3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.F2.11.3.m3.2.3.2" xref="S2.F2.11.3.m3.2.3.2.cmml">‚Ñ≥</mi><mo lspace="0em" rspace="0em" id="S2.F2.11.3.m3.2.3.1" xref="S2.F2.11.3.m3.2.3.1.cmml">‚Äã</mo><mrow id="S2.F2.11.3.m3.2.3.3.2" xref="S2.F2.11.3.m3.2.3.3.1.cmml"><mo stretchy="false" id="S2.F2.11.3.m3.2.3.3.2.1" xref="S2.F2.11.3.m3.2.3.3.1.cmml">(</mo><mi id="S2.F2.11.3.m3.1.1" xref="S2.F2.11.3.m3.1.1.cmml">Œ∏</mi><mo id="S2.F2.11.3.m3.2.3.3.2.2" xref="S2.F2.11.3.m3.2.3.3.1.cmml">,</mo><mi id="S2.F2.11.3.m3.2.2" xref="S2.F2.11.3.m3.2.2.cmml">Œ≤</mi><mo stretchy="false" id="S2.F2.11.3.m3.2.3.3.2.3" xref="S2.F2.11.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.11.3.m3.2c"><apply id="S2.F2.11.3.m3.2.3.cmml" xref="S2.F2.11.3.m3.2.3"><times id="S2.F2.11.3.m3.2.3.1.cmml" xref="S2.F2.11.3.m3.2.3.1"></times><ci id="S2.F2.11.3.m3.2.3.2.cmml" xref="S2.F2.11.3.m3.2.3.2">‚Ñ≥</ci><interval closure="open" id="S2.F2.11.3.m3.2.3.3.1.cmml" xref="S2.F2.11.3.m3.2.3.3.2"><ci id="S2.F2.11.3.m3.1.1.cmml" xref="S2.F2.11.3.m3.1.1">ùúÉ</ci><ci id="S2.F2.11.3.m3.2.2.cmml" xref="S2.F2.11.3.m3.2.2">ùõΩ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.11.3.m3.2d">\mathcal{M}(\theta,\beta)</annotation></semantics></math> from <math id="S2.F2.12.4.m4.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S2.F2.12.4.m4.1b"><mi class="ltx_font_mathcaligraphic" id="S2.F2.12.4.m4.1.1" xref="S2.F2.12.4.m4.1.1.cmml">‚Ñê</mi><annotation-xml encoding="MathML-Content" id="S2.F2.12.4.m4.1c"><ci id="S2.F2.12.4.m4.1.1.cmml" xref="S2.F2.12.4.m4.1.1">‚Ñê</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.12.4.m4.1d">\mathcal{I}</annotation></semantics></math>. If the pose is challenging based on a pose prior (i.e. VPoser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>) then render the mesh‚Äôs depth map <math id="S2.F2.13.5.m5.1" class="ltx_Math" alttext="d_{fg}" display="inline"><semantics id="S2.F2.13.5.m5.1b"><msub id="S2.F2.13.5.m5.1.1" xref="S2.F2.13.5.m5.1.1.cmml"><mi id="S2.F2.13.5.m5.1.1.2" xref="S2.F2.13.5.m5.1.1.2.cmml">d</mi><mrow id="S2.F2.13.5.m5.1.1.3" xref="S2.F2.13.5.m5.1.1.3.cmml"><mi id="S2.F2.13.5.m5.1.1.3.2" xref="S2.F2.13.5.m5.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.F2.13.5.m5.1.1.3.1" xref="S2.F2.13.5.m5.1.1.3.1.cmml">‚Äã</mo><mi id="S2.F2.13.5.m5.1.1.3.3" xref="S2.F2.13.5.m5.1.1.3.3.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.F2.13.5.m5.1c"><apply id="S2.F2.13.5.m5.1.1.cmml" xref="S2.F2.13.5.m5.1.1"><csymbol cd="ambiguous" id="S2.F2.13.5.m5.1.1.1.cmml" xref="S2.F2.13.5.m5.1.1">subscript</csymbol><ci id="S2.F2.13.5.m5.1.1.2.cmml" xref="S2.F2.13.5.m5.1.1.2">ùëë</ci><apply id="S2.F2.13.5.m5.1.1.3.cmml" xref="S2.F2.13.5.m5.1.1.3"><times id="S2.F2.13.5.m5.1.1.3.1.cmml" xref="S2.F2.13.5.m5.1.1.3.1"></times><ci id="S2.F2.13.5.m5.1.1.3.2.cmml" xref="S2.F2.13.5.m5.1.1.3.2">ùëì</ci><ci id="S2.F2.13.5.m5.1.1.3.3.cmml" xref="S2.F2.13.5.m5.1.1.3.3">ùëî</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.13.5.m5.1d">d_{fg}</annotation></semantics></math> and introduce occlusions via object masks obtained from a segmentation model. <span id="S2.F2.16.8.3" class="ltx_text" style="color:#008080;">Step 3</span>: Using the latents <math id="S2.F2.14.6.m6.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.F2.14.6.m6.1b"><mi id="S2.F2.14.6.m6.1.1" xref="S2.F2.14.6.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.F2.14.6.m6.1c"><ci id="S2.F2.14.6.m6.1.1.cmml" xref="S2.F2.14.6.m6.1.1">ùëß</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.14.6.m6.1d">z</annotation></semantics></math>, foreground depths, and the text embeddings <math id="S2.F2.15.7.m7.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.F2.15.7.m7.1b"><mi id="S2.F2.15.7.m7.1.1" xref="S2.F2.15.7.m7.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.F2.15.7.m7.1c"><ci id="S2.F2.15.7.m7.1.1.cmml" xref="S2.F2.15.7.m7.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.15.7.m7.1d">t</annotation></semantics></math> as guide for the final generation <math id="S2.F2.16.8.m8.1" class="ltx_Math" alttext="\mathcal{I^{*}}" display="inline"><semantics id="S2.F2.16.8.m8.1b"><msup id="S2.F2.16.8.m8.1.1" xref="S2.F2.16.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.F2.16.8.m8.1.1.2" xref="S2.F2.16.8.m8.1.1.2.cmml">‚Ñê</mi><mo id="S2.F2.16.8.m8.1.1.3" xref="S2.F2.16.8.m8.1.1.3.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="S2.F2.16.8.m8.1c"><apply id="S2.F2.16.8.m8.1.1.cmml" xref="S2.F2.16.8.m8.1.1"><csymbol cd="ambiguous" id="S2.F2.16.8.m8.1.1.1.cmml" xref="S2.F2.16.8.m8.1.1">superscript</csymbol><ci id="S2.F2.16.8.m8.1.1.2.cmml" xref="S2.F2.16.8.m8.1.1.2">‚Ñê</ci><times id="S2.F2.16.8.m8.1.1.3.cmml" xref="S2.F2.16.8.m8.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.16.8.m8.1d">\mathcal{I^{*}}</annotation></semantics></math>.
</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Diffusion-HPC</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Background</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Latent diffusion models.</span> Diffusion models are deep generative models that generate samples from a desired distribution by learning to reverse a gradual noising process. The sampling process starts from noise sampled from a standard normal distribution, which are refined into a series of less-noisy latents that eventually lead to the desired generation. For more details, please refer to Dhariwal <em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.p1.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> and Ho <em id="S3.SS1.p1.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.p1.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. Latent Diffusion uses a perceptual compression model, a variational autoencoder (VAE) that projects the data distribution into a latent space, where the conditional diffusion process operates. Previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> have shown that editing in the latent space is faster than pixel space editing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite> and helps to avoid pixel-level artifacts. Our method uses two latent diffusion models under the hood, a text-to-image model where the denoising is conditioned on the text input, and a depth-to-image model where the depth map is used as additional conditioning.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.8" class="ltx_p"><span id="S3.SS1.p2.8.1" class="ltx_text ltx_font_bold">SMPL body model.</span> We use the Skinned Multi-Person Linear (SMPL) model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> to represent the 3D mesh of the human body. SMPL is a differentiable function <math id="S3.SS1.p2.1.m1.2" class="ltx_Math" alttext="\mathcal{M}(\theta,\beta)" display="inline"><semantics id="S3.SS1.p2.1.m1.2a"><mrow id="S3.SS1.p2.1.m1.2.3" xref="S3.SS1.p2.1.m1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.2.3.2" xref="S3.SS1.p2.1.m1.2.3.2.cmml">‚Ñ≥</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.1.m1.2.3.1" xref="S3.SS1.p2.1.m1.2.3.1.cmml">‚Äã</mo><mrow id="S3.SS1.p2.1.m1.2.3.3.2" xref="S3.SS1.p2.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.2.3.3.2.1" xref="S3.SS1.p2.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Œ∏</mi><mo id="S3.SS1.p2.1.m1.2.3.3.2.2" xref="S3.SS1.p2.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">Œ≤</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.2.3.3.2.3" xref="S3.SS1.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.2b"><apply id="S3.SS1.p2.1.m1.2.3.cmml" xref="S3.SS1.p2.1.m1.2.3"><times id="S3.SS1.p2.1.m1.2.3.1.cmml" xref="S3.SS1.p2.1.m1.2.3.1"></times><ci id="S3.SS1.p2.1.m1.2.3.2.cmml" xref="S3.SS1.p2.1.m1.2.3.2">‚Ñ≥</ci><interval closure="open" id="S3.SS1.p2.1.m1.2.3.3.1.cmml" xref="S3.SS1.p2.1.m1.2.3.3.2"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ùúÉ</ci><ci id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">ùõΩ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.2c">\mathcal{M}(\theta,\beta)</annotation></semantics></math> that takes a pose parameter <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\theta\in\mathbb{R}^{69}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">Œ∏</mi><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">‚àà</mo><msup id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.cmml">‚Ñù</mi><mn id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml">69</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><in id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></in><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">ùúÉ</ci><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">‚Ñù</ci><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3">69</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\theta\in\mathbb{R}^{69}</annotation></semantics></math> and shape parameter <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\beta\in\mathbb{R}^{10}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">Œ≤</mi><mo id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">‚àà</mo><msup id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.3.2.cmml">‚Ñù</mi><mn id="S3.SS1.p2.3.m3.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.cmml">10</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><in id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"></in><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">ùõΩ</ci><apply id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.2">‚Ñù</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\beta\in\mathbb{R}^{10}</annotation></semantics></math>, and returns the body mesh <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{M}\in\mathbb{R}^{6890\times 3}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">‚Ñ≥</mi><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">‚àà</mo><msup id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml"><mn id="S3.SS1.p2.4.m4.1.1.3.3.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.cmml">6890</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.3.1" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS1.p2.4.m4.1.1.3.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><in id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></in><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">‚Ñ≥</ci><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">‚Ñù</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3"><times id="S3.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.1"></times><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2">6890</cn><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\mathcal{M}\in\mathbb{R}^{6890\times 3}</annotation></semantics></math> with <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="6890" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mn id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">6890</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><cn type="integer" id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">6890</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">6890</annotation></semantics></math> vertices. The 3D joint locations <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="X\in\mathbb{R}^{k\times 3}=\mathcal{W}\mathcal{M}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">X</mi><mo id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">‚àà</mo><msup id="S3.SS1.p2.6.m6.1.1.4" xref="S3.SS1.p2.6.m6.1.1.4.cmml"><mi id="S3.SS1.p2.6.m6.1.1.4.2" xref="S3.SS1.p2.6.m6.1.1.4.2.cmml">‚Ñù</mi><mrow id="S3.SS1.p2.6.m6.1.1.4.3" xref="S3.SS1.p2.6.m6.1.1.4.3.cmml"><mi id="S3.SS1.p2.6.m6.1.1.4.3.2" xref="S3.SS1.p2.6.m6.1.1.4.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.6.m6.1.1.4.3.1" xref="S3.SS1.p2.6.m6.1.1.4.3.1.cmml">√ó</mo><mn id="S3.SS1.p2.6.m6.1.1.4.3.3" xref="S3.SS1.p2.6.m6.1.1.4.3.3.cmml">3</mn></mrow></msup><mo id="S3.SS1.p2.6.m6.1.1.5" xref="S3.SS1.p2.6.m6.1.1.5.cmml">=</mo><mrow id="S3.SS1.p2.6.m6.1.1.6" xref="S3.SS1.p2.6.m6.1.1.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m6.1.1.6.2" xref="S3.SS1.p2.6.m6.1.1.6.2.cmml">ùí≤</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.6.m6.1.1.6.1" xref="S3.SS1.p2.6.m6.1.1.6.1.cmml">‚Äã</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m6.1.1.6.3" xref="S3.SS1.p2.6.m6.1.1.6.3.cmml">‚Ñ≥</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><and id="S3.SS1.p2.6.m6.1.1a.cmml" xref="S3.SS1.p2.6.m6.1.1"></and><apply id="S3.SS1.p2.6.m6.1.1b.cmml" xref="S3.SS1.p2.6.m6.1.1"><in id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3"></in><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">ùëã</ci><apply id="S3.SS1.p2.6.m6.1.1.4.cmml" xref="S3.SS1.p2.6.m6.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.4.1.cmml" xref="S3.SS1.p2.6.m6.1.1.4">superscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.4.2.cmml" xref="S3.SS1.p2.6.m6.1.1.4.2">‚Ñù</ci><apply id="S3.SS1.p2.6.m6.1.1.4.3.cmml" xref="S3.SS1.p2.6.m6.1.1.4.3"><times id="S3.SS1.p2.6.m6.1.1.4.3.1.cmml" xref="S3.SS1.p2.6.m6.1.1.4.3.1"></times><ci id="S3.SS1.p2.6.m6.1.1.4.3.2.cmml" xref="S3.SS1.p2.6.m6.1.1.4.3.2">ùëò</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.4.3.3.cmml" xref="S3.SS1.p2.6.m6.1.1.4.3.3">3</cn></apply></apply></apply><apply id="S3.SS1.p2.6.m6.1.1c.cmml" xref="S3.SS1.p2.6.m6.1.1"><eq id="S3.SS1.p2.6.m6.1.1.5.cmml" xref="S3.SS1.p2.6.m6.1.1.5"></eq><share href="#S3.SS1.p2.6.m6.1.1.4.cmml" id="S3.SS1.p2.6.m6.1.1d.cmml" xref="S3.SS1.p2.6.m6.1.1"></share><apply id="S3.SS1.p2.6.m6.1.1.6.cmml" xref="S3.SS1.p2.6.m6.1.1.6"><times id="S3.SS1.p2.6.m6.1.1.6.1.cmml" xref="S3.SS1.p2.6.m6.1.1.6.1"></times><ci id="S3.SS1.p2.6.m6.1.1.6.2.cmml" xref="S3.SS1.p2.6.m6.1.1.6.2">ùí≤</ci><ci id="S3.SS1.p2.6.m6.1.1.6.3.cmml" xref="S3.SS1.p2.6.m6.1.1.6.3">‚Ñ≥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">X\in\mathbb{R}^{k\times 3}=\mathcal{W}\mathcal{M}</annotation></semantics></math> are regressed from the vertices, using a pre-trained linear regressor <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">ùí≤</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">ùí≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\mathcal{W}</annotation></semantics></math>, where <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mi id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><ci id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">k</annotation></semantics></math> is the number of joints.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data generation process</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Diffusion-HPC consists of three main steps. As a first step, we leverage a text-to-image model (i.e. Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>) to produce an initial generation of a posed person. Second, we predict the pose of the person and determine the difficulty level of the pose in the initial generation using a pose prior. We observe that Stable Diffusion tends to generate worse anatomy on more difficult poses. Thus, if the initial generation contains a hard pose, we render the depth map of the predicted body mesh taking into consideration the occlusion from other objects in the image. The depth map serves as the human structure prior. In the final step, we use the context information (in the form of image latents) from initial generation as a starting point, and leverage a depth-to-image model (i.e. a fine-tuned version of Stable Diffusion) to produce final generations by conditioning on the depth map from previous step.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.11" class="ltx_p">Figure <a href="#S2.F2" title="Figure 2 ‚Ä£ 2.3 Editing &amp; composing large pre-trained models ‚Ä£ 2 Related Work ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an overview of our method.
Concretely, given a text prompt <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">t</annotation></semantics></math> that describes the action of a person, we first encode it to text embeddings <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">z</mi><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ùëß</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">z_{t}</annotation></semantics></math>, and then
use a text-to-image Stable Diffusion model (<math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">ùí¢</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ùí¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\mathcal{G}</annotation></semantics></math>) to generate an image <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{I}=\mathcal{G}(z_{t})" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">‚Ñê</mi><mo id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">=</mo><mrow id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.4.m4.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.3.cmml">ùí¢</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.SS2.p2.4.m4.1.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.4.m4.1.1.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p2.4.m4.1.1.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.SS2.p2.4.m4.1.1.1.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.SS2.p2.4.m4.1.1.1.1.1.3" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><eq id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2"></eq><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">‚Ñê</ci><apply id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.2"></times><ci id="S3.SS2.p2.4.m4.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.3">ùí¢</ci><apply id="S3.SS2.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.2">ùëß</ci><ci id="S3.SS2.p2.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1.1.1.3">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\mathcal{I}=\mathcal{G}(z_{t})</annotation></semantics></math>. The image is passed to encoder <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{E}_{\mathcal{G}_{d}}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">‚Ñ∞</mi><msub id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">ùí¢</mi><mi id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml">d</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">‚Ñ∞</ci><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">ùí¢</ci><ci id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3">ùëë</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\mathcal{E}_{\mathcal{G}_{d}}</annotation></semantics></math> of the compression module of <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{G}_{d}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">ùí¢</mi><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">ùí¢</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\mathcal{G}_{d}</annotation></semantics></math>, a depth-to-image diffusion model, to get the compressed image latents <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="z\in\mathbb{R}^{4\times 64\times 64}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mrow id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">z</mi><mo id="S3.SS2.p2.7.m7.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.cmml">‚àà</mo><msup id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml"><mi id="S3.SS2.p2.7.m7.1.1.3.2" xref="S3.SS2.p2.7.m7.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS2.p2.7.m7.1.1.3.3" xref="S3.SS2.p2.7.m7.1.1.3.3.cmml"><mn id="S3.SS2.p2.7.m7.1.1.3.3.2" xref="S3.SS2.p2.7.m7.1.1.3.3.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.7.m7.1.1.3.3.1" xref="S3.SS2.p2.7.m7.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS2.p2.7.m7.1.1.3.3.3" xref="S3.SS2.p2.7.m7.1.1.3.3.3.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.7.m7.1.1.3.3.1a" xref="S3.SS2.p2.7.m7.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS2.p2.7.m7.1.1.3.3.4" xref="S3.SS2.p2.7.m7.1.1.3.3.4.cmml">64</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><in id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"></in><ci id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2">ùëß</ci><apply id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.3.2">‚Ñù</ci><apply id="S3.SS2.p2.7.m7.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3"><times id="S3.SS2.p2.7.m7.1.1.3.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3.1"></times><cn type="integer" id="S3.SS2.p2.7.m7.1.1.3.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3.2">4</cn><cn type="integer" id="S3.SS2.p2.7.m7.1.1.3.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3.3">64</cn><cn type="integer" id="S3.SS2.p2.7.m7.1.1.3.3.4.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3.4">64</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">z\in\mathbb{R}^{4\times 64\times 64}</annotation></semantics></math>, i.e. <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="z=\mathcal{E}_{\mathcal{G}_{d}}(\mathcal{I})" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mrow id="S3.SS2.p2.8.m8.1.2" xref="S3.SS2.p2.8.m8.1.2.cmml"><mi id="S3.SS2.p2.8.m8.1.2.2" xref="S3.SS2.p2.8.m8.1.2.2.cmml">z</mi><mo id="S3.SS2.p2.8.m8.1.2.1" xref="S3.SS2.p2.8.m8.1.2.1.cmml">=</mo><mrow id="S3.SS2.p2.8.m8.1.2.3" xref="S3.SS2.p2.8.m8.1.2.3.cmml"><msub id="S3.SS2.p2.8.m8.1.2.3.2" xref="S3.SS2.p2.8.m8.1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.8.m8.1.2.3.2.2" xref="S3.SS2.p2.8.m8.1.2.3.2.2.cmml">‚Ñ∞</mi><msub id="S3.SS2.p2.8.m8.1.2.3.2.3" xref="S3.SS2.p2.8.m8.1.2.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.8.m8.1.2.3.2.3.2" xref="S3.SS2.p2.8.m8.1.2.3.2.3.2.cmml">ùí¢</mi><mi id="S3.SS2.p2.8.m8.1.2.3.2.3.3" xref="S3.SS2.p2.8.m8.1.2.3.2.3.3.cmml">d</mi></msub></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m8.1.2.3.1" xref="S3.SS2.p2.8.m8.1.2.3.1.cmml">‚Äã</mo><mrow id="S3.SS2.p2.8.m8.1.2.3.3.2" xref="S3.SS2.p2.8.m8.1.2.3.cmml"><mo stretchy="false" id="S3.SS2.p2.8.m8.1.2.3.3.2.1" xref="S3.SS2.p2.8.m8.1.2.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">‚Ñê</mi><mo stretchy="false" id="S3.SS2.p2.8.m8.1.2.3.3.2.2" xref="S3.SS2.p2.8.m8.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.2.cmml" xref="S3.SS2.p2.8.m8.1.2"><eq id="S3.SS2.p2.8.m8.1.2.1.cmml" xref="S3.SS2.p2.8.m8.1.2.1"></eq><ci id="S3.SS2.p2.8.m8.1.2.2.cmml" xref="S3.SS2.p2.8.m8.1.2.2">ùëß</ci><apply id="S3.SS2.p2.8.m8.1.2.3.cmml" xref="S3.SS2.p2.8.m8.1.2.3"><times id="S3.SS2.p2.8.m8.1.2.3.1.cmml" xref="S3.SS2.p2.8.m8.1.2.3.1"></times><apply id="S3.SS2.p2.8.m8.1.2.3.2.cmml" xref="S3.SS2.p2.8.m8.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.2.3.2.1.cmml" xref="S3.SS2.p2.8.m8.1.2.3.2">subscript</csymbol><ci id="S3.SS2.p2.8.m8.1.2.3.2.2.cmml" xref="S3.SS2.p2.8.m8.1.2.3.2.2">‚Ñ∞</ci><apply id="S3.SS2.p2.8.m8.1.2.3.2.3.cmml" xref="S3.SS2.p2.8.m8.1.2.3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.2.3.2.3.1.cmml" xref="S3.SS2.p2.8.m8.1.2.3.2.3">subscript</csymbol><ci id="S3.SS2.p2.8.m8.1.2.3.2.3.2.cmml" xref="S3.SS2.p2.8.m8.1.2.3.2.3.2">ùí¢</ci><ci id="S3.SS2.p2.8.m8.1.2.3.2.3.3.cmml" xref="S3.SS2.p2.8.m8.1.2.3.2.3.3">ùëë</ci></apply></apply><ci id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">‚Ñê</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">z=\mathcal{E}_{\mathcal{G}_{d}}(\mathcal{I})</annotation></semantics></math>.
Image latents <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><mi id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><ci id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1">ùëß</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">z</annotation></semantics></math> contain context information about <math id="S3.SS2.p2.10.m10.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S3.SS2.p2.10.m10.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.10.m10.1.1" xref="S3.SS2.p2.10.m10.1.1.cmml">‚Ñê</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m10.1b"><ci id="S3.SS2.p2.10.m10.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1">‚Ñê</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m10.1c">\mathcal{I}</annotation></semantics></math> such as the texture of the image and background layout. We can use <math id="S3.SS2.p2.11.m11.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS2.p2.11.m11.1a"><mi id="S3.SS2.p2.11.m11.1.1" xref="S3.SS2.p2.11.m11.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m11.1b"><ci id="S3.SS2.p2.11.m11.1.1.cmml" xref="S3.SS2.p2.11.m11.1.1">ùëß</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m11.1c">z</annotation></semantics></math> as a starting point in the final generation process so the context from the inital generation is roughly preserved.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.13" class="ltx_p">Next, we use an off-the-shelf model to reconstruct the 3D mesh of the person in <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">‚Ñê</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">‚Ñê</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathcal{I}</annotation></semantics></math>. Specifically, we estimate the human pose <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\theta</annotation></semantics></math>, shape <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">Œ≤</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">ùõΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\beta</annotation></semantics></math>, and parameters of a weak perspective camera <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="\Pi" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mi mathvariant="normal" id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">Œ†</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">Œ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\Pi</annotation></semantics></math> from the image using an off-the-shelf HMR model <math id="S3.SS2.p3.5.m5.3" class="ltx_Math" alttext="f:\mathcal{I}\rightarrow(\theta,\beta,\Pi)" display="inline"><semantics id="S3.SS2.p3.5.m5.3a"><mrow id="S3.SS2.p3.5.m5.3.4" xref="S3.SS2.p3.5.m5.3.4.cmml"><mi id="S3.SS2.p3.5.m5.3.4.2" xref="S3.SS2.p3.5.m5.3.4.2.cmml">f</mi><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p3.5.m5.3.4.1" xref="S3.SS2.p3.5.m5.3.4.1.cmml">:</mo><mrow id="S3.SS2.p3.5.m5.3.4.3" xref="S3.SS2.p3.5.m5.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.5.m5.3.4.3.2" xref="S3.SS2.p3.5.m5.3.4.3.2.cmml">‚Ñê</mi><mo stretchy="false" id="S3.SS2.p3.5.m5.3.4.3.1" xref="S3.SS2.p3.5.m5.3.4.3.1.cmml">‚Üí</mo><mrow id="S3.SS2.p3.5.m5.3.4.3.3.2" xref="S3.SS2.p3.5.m5.3.4.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.5.m5.3.4.3.3.2.1" xref="S3.SS2.p3.5.m5.3.4.3.3.1.cmml">(</mo><mi id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">Œ∏</mi><mo id="S3.SS2.p3.5.m5.3.4.3.3.2.2" xref="S3.SS2.p3.5.m5.3.4.3.3.1.cmml">,</mo><mi id="S3.SS2.p3.5.m5.2.2" xref="S3.SS2.p3.5.m5.2.2.cmml">Œ≤</mi><mo id="S3.SS2.p3.5.m5.3.4.3.3.2.3" xref="S3.SS2.p3.5.m5.3.4.3.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.5.m5.3.3" xref="S3.SS2.p3.5.m5.3.3.cmml">Œ†</mi><mo stretchy="false" id="S3.SS2.p3.5.m5.3.4.3.3.2.4" xref="S3.SS2.p3.5.m5.3.4.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.3b"><apply id="S3.SS2.p3.5.m5.3.4.cmml" xref="S3.SS2.p3.5.m5.3.4"><ci id="S3.SS2.p3.5.m5.3.4.1.cmml" xref="S3.SS2.p3.5.m5.3.4.1">:</ci><ci id="S3.SS2.p3.5.m5.3.4.2.cmml" xref="S3.SS2.p3.5.m5.3.4.2">ùëì</ci><apply id="S3.SS2.p3.5.m5.3.4.3.cmml" xref="S3.SS2.p3.5.m5.3.4.3"><ci id="S3.SS2.p3.5.m5.3.4.3.1.cmml" xref="S3.SS2.p3.5.m5.3.4.3.1">‚Üí</ci><ci id="S3.SS2.p3.5.m5.3.4.3.2.cmml" xref="S3.SS2.p3.5.m5.3.4.3.2">‚Ñê</ci><vector id="S3.SS2.p3.5.m5.3.4.3.3.1.cmml" xref="S3.SS2.p3.5.m5.3.4.3.3.2"><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">ùúÉ</ci><ci id="S3.SS2.p3.5.m5.2.2.cmml" xref="S3.SS2.p3.5.m5.2.2">ùõΩ</ci><ci id="S3.SS2.p3.5.m5.3.3.cmml" xref="S3.SS2.p3.5.m5.3.3">Œ†</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.3c">f:\mathcal{I}\rightarrow(\theta,\beta,\Pi)</annotation></semantics></math>. Since our method focuses on rectifying implausible human generations, we determine whether the initial generation is likely to contain implausible humans and only apply our rectification process on images with hard poses. We observe that Stable Diffusion tend to generate worse anatomy on more difficult poses. Hence, we use a pre-trained human pose prior VPoser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> as a proxy for determining if the person in image <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">‚Ñê</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">‚Ñê</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">\mathcal{I}</annotation></semantics></math> has a challenging pose.
VPoser is a Variational Auto-Encoder (VAE) that is trained on a massive database of realistic human poses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>. By design, poses that are farther away from the canonical pose (i.e. challenging poses) have larger variance in the embedding space. Therefore, we identify a difficult pose <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><mi id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><ci id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">\theta</annotation></semantics></math> if its embedding <math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="e_{\theta}" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><msub id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">e</mi><mi id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml">Œ∏</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2">ùëí</ci><ci id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">e_{\theta}</annotation></semantics></math> have larger norm, i.e. <math id="S3.SS2.p3.9.m9.1" class="ltx_Math" alttext="||e_{\theta}||_{2}&gt;\tau" display="inline"><semantics id="S3.SS2.p3.9.m9.1a"><mrow id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml"><msub id="S3.SS2.p3.9.m9.1.1.1" xref="S3.SS2.p3.9.m9.1.1.1.cmml"><mrow id="S3.SS2.p3.9.m9.1.1.1.1.1" xref="S3.SS2.p3.9.m9.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.9.m9.1.1.1.1.1.2" xref="S3.SS2.p3.9.m9.1.1.1.1.2.1.cmml">‚Äñ</mo><msub id="S3.SS2.p3.9.m9.1.1.1.1.1.1" xref="S3.SS2.p3.9.m9.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.9.m9.1.1.1.1.1.1.2" xref="S3.SS2.p3.9.m9.1.1.1.1.1.1.2.cmml">e</mi><mi id="S3.SS2.p3.9.m9.1.1.1.1.1.1.3" xref="S3.SS2.p3.9.m9.1.1.1.1.1.1.3.cmml">Œ∏</mi></msub><mo stretchy="false" id="S3.SS2.p3.9.m9.1.1.1.1.1.3" xref="S3.SS2.p3.9.m9.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.SS2.p3.9.m9.1.1.1.3" xref="S3.SS2.p3.9.m9.1.1.1.3.cmml">2</mn></msub><mo id="S3.SS2.p3.9.m9.1.1.2" xref="S3.SS2.p3.9.m9.1.1.2.cmml">&gt;</mo><mi id="S3.SS2.p3.9.m9.1.1.3" xref="S3.SS2.p3.9.m9.1.1.3.cmml">œÑ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.1b"><apply id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1"><gt id="S3.SS2.p3.9.m9.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.2"></gt><apply id="S3.SS2.p3.9.m9.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.1">subscript</csymbol><apply id="S3.SS2.p3.9.m9.1.1.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p3.9.m9.1.1.1.1.2.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.2">norm</csymbol><apply id="S3.SS2.p3.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.9.m9.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.1.2">ùëí</ci><ci id="S3.SS2.p3.9.m9.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.1.3">ùúÉ</ci></apply></apply><cn type="integer" id="S3.SS2.p3.9.m9.1.1.1.3.cmml" xref="S3.SS2.p3.9.m9.1.1.1.3">2</cn></apply><ci id="S3.SS2.p3.9.m9.1.1.3.cmml" xref="S3.SS2.p3.9.m9.1.1.3">ùúè</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.1c">||e_{\theta}||_{2}&gt;\tau</annotation></semantics></math>, where <math id="S3.SS2.p3.10.m10.3" class="ltx_Math" alttext="\{\mu,\sigma\}=\mathcal{E}_{v}(\theta)" display="inline"><semantics id="S3.SS2.p3.10.m10.3a"><mrow id="S3.SS2.p3.10.m10.3.4" xref="S3.SS2.p3.10.m10.3.4.cmml"><mrow id="S3.SS2.p3.10.m10.3.4.2.2" xref="S3.SS2.p3.10.m10.3.4.2.1.cmml"><mo stretchy="false" id="S3.SS2.p3.10.m10.3.4.2.2.1" xref="S3.SS2.p3.10.m10.3.4.2.1.cmml">{</mo><mi id="S3.SS2.p3.10.m10.1.1" xref="S3.SS2.p3.10.m10.1.1.cmml">Œº</mi><mo id="S3.SS2.p3.10.m10.3.4.2.2.2" xref="S3.SS2.p3.10.m10.3.4.2.1.cmml">,</mo><mi id="S3.SS2.p3.10.m10.2.2" xref="S3.SS2.p3.10.m10.2.2.cmml">œÉ</mi><mo stretchy="false" id="S3.SS2.p3.10.m10.3.4.2.2.3" xref="S3.SS2.p3.10.m10.3.4.2.1.cmml">}</mo></mrow><mo id="S3.SS2.p3.10.m10.3.4.1" xref="S3.SS2.p3.10.m10.3.4.1.cmml">=</mo><mrow id="S3.SS2.p3.10.m10.3.4.3" xref="S3.SS2.p3.10.m10.3.4.3.cmml"><msub id="S3.SS2.p3.10.m10.3.4.3.2" xref="S3.SS2.p3.10.m10.3.4.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.10.m10.3.4.3.2.2" xref="S3.SS2.p3.10.m10.3.4.3.2.2.cmml">‚Ñ∞</mi><mi id="S3.SS2.p3.10.m10.3.4.3.2.3" xref="S3.SS2.p3.10.m10.3.4.3.2.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p3.10.m10.3.4.3.1" xref="S3.SS2.p3.10.m10.3.4.3.1.cmml">‚Äã</mo><mrow id="S3.SS2.p3.10.m10.3.4.3.3.2" xref="S3.SS2.p3.10.m10.3.4.3.cmml"><mo stretchy="false" id="S3.SS2.p3.10.m10.3.4.3.3.2.1" xref="S3.SS2.p3.10.m10.3.4.3.cmml">(</mo><mi id="S3.SS2.p3.10.m10.3.3" xref="S3.SS2.p3.10.m10.3.3.cmml">Œ∏</mi><mo stretchy="false" id="S3.SS2.p3.10.m10.3.4.3.3.2.2" xref="S3.SS2.p3.10.m10.3.4.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m10.3b"><apply id="S3.SS2.p3.10.m10.3.4.cmml" xref="S3.SS2.p3.10.m10.3.4"><eq id="S3.SS2.p3.10.m10.3.4.1.cmml" xref="S3.SS2.p3.10.m10.3.4.1"></eq><set id="S3.SS2.p3.10.m10.3.4.2.1.cmml" xref="S3.SS2.p3.10.m10.3.4.2.2"><ci id="S3.SS2.p3.10.m10.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1">ùúá</ci><ci id="S3.SS2.p3.10.m10.2.2.cmml" xref="S3.SS2.p3.10.m10.2.2">ùúé</ci></set><apply id="S3.SS2.p3.10.m10.3.4.3.cmml" xref="S3.SS2.p3.10.m10.3.4.3"><times id="S3.SS2.p3.10.m10.3.4.3.1.cmml" xref="S3.SS2.p3.10.m10.3.4.3.1"></times><apply id="S3.SS2.p3.10.m10.3.4.3.2.cmml" xref="S3.SS2.p3.10.m10.3.4.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.10.m10.3.4.3.2.1.cmml" xref="S3.SS2.p3.10.m10.3.4.3.2">subscript</csymbol><ci id="S3.SS2.p3.10.m10.3.4.3.2.2.cmml" xref="S3.SS2.p3.10.m10.3.4.3.2.2">‚Ñ∞</ci><ci id="S3.SS2.p3.10.m10.3.4.3.2.3.cmml" xref="S3.SS2.p3.10.m10.3.4.3.2.3">ùë£</ci></apply><ci id="S3.SS2.p3.10.m10.3.3.cmml" xref="S3.SS2.p3.10.m10.3.3">ùúÉ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.m10.3c">\{\mu,\sigma\}=\mathcal{E}_{v}(\theta)</annotation></semantics></math> and <math id="S3.SS2.p3.11.m11.2" class="ltx_Math" alttext="e_{\theta}\sim\mathcal{N}(\mu,\sigma)" display="inline"><semantics id="S3.SS2.p3.11.m11.2a"><mrow id="S3.SS2.p3.11.m11.2.3" xref="S3.SS2.p3.11.m11.2.3.cmml"><msub id="S3.SS2.p3.11.m11.2.3.2" xref="S3.SS2.p3.11.m11.2.3.2.cmml"><mi id="S3.SS2.p3.11.m11.2.3.2.2" xref="S3.SS2.p3.11.m11.2.3.2.2.cmml">e</mi><mi id="S3.SS2.p3.11.m11.2.3.2.3" xref="S3.SS2.p3.11.m11.2.3.2.3.cmml">Œ∏</mi></msub><mo id="S3.SS2.p3.11.m11.2.3.1" xref="S3.SS2.p3.11.m11.2.3.1.cmml">‚àº</mo><mrow id="S3.SS2.p3.11.m11.2.3.3" xref="S3.SS2.p3.11.m11.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.11.m11.2.3.3.2" xref="S3.SS2.p3.11.m11.2.3.3.2.cmml">ùí©</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.11.m11.2.3.3.1" xref="S3.SS2.p3.11.m11.2.3.3.1.cmml">‚Äã</mo><mrow id="S3.SS2.p3.11.m11.2.3.3.3.2" xref="S3.SS2.p3.11.m11.2.3.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.11.m11.2.3.3.3.2.1" xref="S3.SS2.p3.11.m11.2.3.3.3.1.cmml">(</mo><mi id="S3.SS2.p3.11.m11.1.1" xref="S3.SS2.p3.11.m11.1.1.cmml">Œº</mi><mo id="S3.SS2.p3.11.m11.2.3.3.3.2.2" xref="S3.SS2.p3.11.m11.2.3.3.3.1.cmml">,</mo><mi id="S3.SS2.p3.11.m11.2.2" xref="S3.SS2.p3.11.m11.2.2.cmml">œÉ</mi><mo stretchy="false" id="S3.SS2.p3.11.m11.2.3.3.3.2.3" xref="S3.SS2.p3.11.m11.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.11.m11.2b"><apply id="S3.SS2.p3.11.m11.2.3.cmml" xref="S3.SS2.p3.11.m11.2.3"><csymbol cd="latexml" id="S3.SS2.p3.11.m11.2.3.1.cmml" xref="S3.SS2.p3.11.m11.2.3.1">similar-to</csymbol><apply id="S3.SS2.p3.11.m11.2.3.2.cmml" xref="S3.SS2.p3.11.m11.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.11.m11.2.3.2.1.cmml" xref="S3.SS2.p3.11.m11.2.3.2">subscript</csymbol><ci id="S3.SS2.p3.11.m11.2.3.2.2.cmml" xref="S3.SS2.p3.11.m11.2.3.2.2">ùëí</ci><ci id="S3.SS2.p3.11.m11.2.3.2.3.cmml" xref="S3.SS2.p3.11.m11.2.3.2.3">ùúÉ</ci></apply><apply id="S3.SS2.p3.11.m11.2.3.3.cmml" xref="S3.SS2.p3.11.m11.2.3.3"><times id="S3.SS2.p3.11.m11.2.3.3.1.cmml" xref="S3.SS2.p3.11.m11.2.3.3.1"></times><ci id="S3.SS2.p3.11.m11.2.3.3.2.cmml" xref="S3.SS2.p3.11.m11.2.3.3.2">ùí©</ci><interval closure="open" id="S3.SS2.p3.11.m11.2.3.3.3.1.cmml" xref="S3.SS2.p3.11.m11.2.3.3.3.2"><ci id="S3.SS2.p3.11.m11.1.1.cmml" xref="S3.SS2.p3.11.m11.1.1">ùúá</ci><ci id="S3.SS2.p3.11.m11.2.2.cmml" xref="S3.SS2.p3.11.m11.2.2">ùúé</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.11.m11.2c">e_{\theta}\sim\mathcal{N}(\mu,\sigma)</annotation></semantics></math>.
<math id="S3.SS2.p3.12.m12.1" class="ltx_Math" alttext="\mathcal{E}_{v}" display="inline"><semantics id="S3.SS2.p3.12.m12.1a"><msub id="S3.SS2.p3.12.m12.1.1" xref="S3.SS2.p3.12.m12.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.12.m12.1.1.2" xref="S3.SS2.p3.12.m12.1.1.2.cmml">‚Ñ∞</mi><mi id="S3.SS2.p3.12.m12.1.1.3" xref="S3.SS2.p3.12.m12.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.12.m12.1b"><apply id="S3.SS2.p3.12.m12.1.1.cmml" xref="S3.SS2.p3.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.12.m12.1.1.1.cmml" xref="S3.SS2.p3.12.m12.1.1">subscript</csymbol><ci id="S3.SS2.p3.12.m12.1.1.2.cmml" xref="S3.SS2.p3.12.m12.1.1.2">‚Ñ∞</ci><ci id="S3.SS2.p3.12.m12.1.1.3.cmml" xref="S3.SS2.p3.12.m12.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.12.m12.1c">\mathcal{E}_{v}</annotation></semantics></math> is the encoder of VPoser. <math id="S3.SS2.p3.13.m13.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS2.p3.13.m13.1a"><mi id="S3.SS2.p3.13.m13.1.1" xref="S3.SS2.p3.13.m13.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.13.m13.1b"><ci id="S3.SS2.p3.13.m13.1.1.cmml" xref="S3.SS2.p3.13.m13.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.13.m13.1c">\tau</annotation></semantics></math> is determined empirically and set to 30.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.5" class="ltx_p">Now that we have the predicted human pose from <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">‚Ñê</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">‚Ñê</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\mathcal{I}</annotation></semantics></math>, we move on to the final step of our method where we inject pose information <math id="S3.SS2.p4.2.m2.2" class="ltx_Math" alttext="\mathcal{M}(\theta,\beta)" display="inline"><semantics id="S3.SS2.p4.2.m2.2a"><mrow id="S3.SS2.p4.2.m2.2.3" xref="S3.SS2.p4.2.m2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p4.2.m2.2.3.2" xref="S3.SS2.p4.2.m2.2.3.2.cmml">‚Ñ≥</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m2.2.3.1" xref="S3.SS2.p4.2.m2.2.3.1.cmml">‚Äã</mo><mrow id="S3.SS2.p4.2.m2.2.3.3.2" xref="S3.SS2.p4.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p4.2.m2.2.3.3.2.1" xref="S3.SS2.p4.2.m2.2.3.3.1.cmml">(</mo><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">Œ∏</mi><mo id="S3.SS2.p4.2.m2.2.3.3.2.2" xref="S3.SS2.p4.2.m2.2.3.3.1.cmml">,</mo><mi id="S3.SS2.p4.2.m2.2.2" xref="S3.SS2.p4.2.m2.2.2.cmml">Œ≤</mi><mo stretchy="false" id="S3.SS2.p4.2.m2.2.3.3.2.3" xref="S3.SS2.p4.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.2b"><apply id="S3.SS2.p4.2.m2.2.3.cmml" xref="S3.SS2.p4.2.m2.2.3"><times id="S3.SS2.p4.2.m2.2.3.1.cmml" xref="S3.SS2.p4.2.m2.2.3.1"></times><ci id="S3.SS2.p4.2.m2.2.3.2.cmml" xref="S3.SS2.p4.2.m2.2.3.2">‚Ñ≥</ci><interval closure="open" id="S3.SS2.p4.2.m2.2.3.3.1.cmml" xref="S3.SS2.p4.2.m2.2.3.3.2"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">ùúÉ</ci><ci id="S3.SS2.p4.2.m2.2.2.cmml" xref="S3.SS2.p4.2.m2.2.2">ùõΩ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.2c">\mathcal{M}(\theta,\beta)</annotation></semantics></math> into the generation process to produce a more plausible image of a person with the predicted pose <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\theta</annotation></semantics></math>. We achieve this by leveraging a depth-to-image version of Stable Diffusion, and using the depth values of the predicted human body as conditioning information in the generation process. Specifically, we render the 3D mesh to obtain the depth map <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="d_{fg}\in\mathbb{R}^{64\times 64}" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><mrow id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><msub id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2.2" xref="S3.SS2.p4.4.m4.1.1.2.2.cmml">d</mi><mrow id="S3.SS2.p4.4.m4.1.1.2.3" xref="S3.SS2.p4.4.m4.1.1.2.3.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2.3.2" xref="S3.SS2.p4.4.m4.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m4.1.1.2.3.1" xref="S3.SS2.p4.4.m4.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p4.4.m4.1.1.2.3.3" xref="S3.SS2.p4.4.m4.1.1.2.3.3.cmml">g</mi></mrow></msub><mo id="S3.SS2.p4.4.m4.1.1.1" xref="S3.SS2.p4.4.m4.1.1.1.cmml">‚àà</mo><msup id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml"><mi id="S3.SS2.p4.4.m4.1.1.3.2" xref="S3.SS2.p4.4.m4.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS2.p4.4.m4.1.1.3.3" xref="S3.SS2.p4.4.m4.1.1.3.3.cmml"><mn id="S3.SS2.p4.4.m4.1.1.3.3.2" xref="S3.SS2.p4.4.m4.1.1.3.3.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p4.4.m4.1.1.3.3.1" xref="S3.SS2.p4.4.m4.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS2.p4.4.m4.1.1.3.3.3" xref="S3.SS2.p4.4.m4.1.1.3.3.3.cmml">64</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><in id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1.1"></in><apply id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.1.1.2.1.cmml" xref="S3.SS2.p4.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.p4.4.m4.1.1.2.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2.2">ùëë</ci><apply id="S3.SS2.p4.4.m4.1.1.2.3.cmml" xref="S3.SS2.p4.4.m4.1.1.2.3"><times id="S3.SS2.p4.4.m4.1.1.2.3.1.cmml" xref="S3.SS2.p4.4.m4.1.1.2.3.1"></times><ci id="S3.SS2.p4.4.m4.1.1.2.3.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2.3.2">ùëì</ci><ci id="S3.SS2.p4.4.m4.1.1.2.3.3.cmml" xref="S3.SS2.p4.4.m4.1.1.2.3.3">ùëî</ci></apply></apply><apply id="S3.SS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.1.1.3.1.cmml" xref="S3.SS2.p4.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.p4.4.m4.1.1.3.2.cmml" xref="S3.SS2.p4.4.m4.1.1.3.2">‚Ñù</ci><apply id="S3.SS2.p4.4.m4.1.1.3.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3.3"><times id="S3.SS2.p4.4.m4.1.1.3.3.1.cmml" xref="S3.SS2.p4.4.m4.1.1.3.3.1"></times><cn type="integer" id="S3.SS2.p4.4.m4.1.1.3.3.2.cmml" xref="S3.SS2.p4.4.m4.1.1.3.3.2">64</cn><cn type="integer" id="S3.SS2.p4.4.m4.1.1.3.3.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3.3.3">64</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">d_{fg}\in\mathbb{R}^{64\times 64}</annotation></semantics></math>. Since there might be other objects in the image that occlude part of the person, we use Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> (pre-trained on COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>) to segment the non-human objects in the image and use the segmentation masks to mask out the occluded body part in the depth map <math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="d_{fg}" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><msub id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">d</mi><mrow id="S3.SS2.p4.5.m5.1.1.3" xref="S3.SS2.p4.5.m5.1.1.3.cmml"><mi id="S3.SS2.p4.5.m5.1.1.3.2" xref="S3.SS2.p4.5.m5.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.5.m5.1.1.3.1" xref="S3.SS2.p4.5.m5.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p4.5.m5.1.1.3.3" xref="S3.SS2.p4.5.m5.1.1.3.3.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">ùëë</ci><apply id="S3.SS2.p4.5.m5.1.1.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3"><times id="S3.SS2.p4.5.m5.1.1.3.1.cmml" xref="S3.SS2.p4.5.m5.1.1.3.1"></times><ci id="S3.SS2.p4.5.m5.1.1.3.2.cmml" xref="S3.SS2.p4.5.m5.1.1.3.2">ùëì</ci><ci id="S3.SS2.p4.5.m5.1.1.3.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3.3">ùëî</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">d_{fg}</annotation></semantics></math>. Formally,</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\displaystyle\{\theta,\beta,\Pi\}" display="inline"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.4.2" xref="S3.E1.m1.3.4.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.4.2.1" xref="S3.E1.m1.3.4.1.cmml">{</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">Œ∏</mi><mo id="S3.E1.m1.3.4.2.2" xref="S3.E1.m1.3.4.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">Œ≤</mi><mo id="S3.E1.m1.3.4.2.3" xref="S3.E1.m1.3.4.1.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">Œ†</mi><mo stretchy="false" id="S3.E1.m1.3.4.2.4" xref="S3.E1.m1.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><set id="S3.E1.m1.3.4.1.cmml" xref="S3.E1.m1.3.4.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ùúÉ</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ùõΩ</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">Œ†</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\displaystyle\{\theta,\beta,\Pi\}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.1" class="ltx_Math" alttext="\displaystyle=f(\mathcal{I})" display="inline"><semantics id="S3.E1.m2.1a"><mrow id="S3.E1.m2.1.2" xref="S3.E1.m2.1.2.cmml"><mi id="S3.E1.m2.1.2.2" xref="S3.E1.m2.1.2.2.cmml"></mi><mo id="S3.E1.m2.1.2.1" xref="S3.E1.m2.1.2.1.cmml">=</mo><mrow id="S3.E1.m2.1.2.3" xref="S3.E1.m2.1.2.3.cmml"><mi id="S3.E1.m2.1.2.3.2" xref="S3.E1.m2.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.1.2.3.1" xref="S3.E1.m2.1.2.3.1.cmml">‚Äã</mo><mrow id="S3.E1.m2.1.2.3.3.2" xref="S3.E1.m2.1.2.3.cmml"><mo stretchy="false" id="S3.E1.m2.1.2.3.3.2.1" xref="S3.E1.m2.1.2.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml">‚Ñê</mi><mo stretchy="false" id="S3.E1.m2.1.2.3.3.2.2" xref="S3.E1.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.1b"><apply id="S3.E1.m2.1.2.cmml" xref="S3.E1.m2.1.2"><eq id="S3.E1.m2.1.2.1.cmml" xref="S3.E1.m2.1.2.1"></eq><csymbol cd="latexml" id="S3.E1.m2.1.2.2.cmml" xref="S3.E1.m2.1.2.2">absent</csymbol><apply id="S3.E1.m2.1.2.3.cmml" xref="S3.E1.m2.1.2.3"><times id="S3.E1.m2.1.2.3.1.cmml" xref="S3.E1.m2.1.2.3.1"></times><ci id="S3.E1.m2.1.2.3.2.cmml" xref="S3.E1.m2.1.2.3.2">ùëì</ci><ci id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1">‚Ñê</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.1c">\displaystyle=f(\mathcal{I})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle d_{fg}" display="inline"><semantics id="S3.E2.m1.1a"><msub id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">d</mi><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">ùëë</ci><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><times id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">ùëì</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">ùëî</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle d_{fg}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.4" class="ltx_Math" alttext="\displaystyle=\mathcal{R}_{d}(\Pi,\mathcal{M}(\theta,\beta))" display="inline"><semantics id="S3.E2.m2.4a"><mrow id="S3.E2.m2.4.4" xref="S3.E2.m2.4.4.cmml"><mi id="S3.E2.m2.4.4.3" xref="S3.E2.m2.4.4.3.cmml"></mi><mo id="S3.E2.m2.4.4.2" xref="S3.E2.m2.4.4.2.cmml">=</mo><mrow id="S3.E2.m2.4.4.1" xref="S3.E2.m2.4.4.1.cmml"><msub id="S3.E2.m2.4.4.1.3" xref="S3.E2.m2.4.4.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m2.4.4.1.3.2" xref="S3.E2.m2.4.4.1.3.2.cmml">‚Ñõ</mi><mi id="S3.E2.m2.4.4.1.3.3" xref="S3.E2.m2.4.4.1.3.3.cmml">d</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m2.4.4.1.2" xref="S3.E2.m2.4.4.1.2.cmml">‚Äã</mo><mrow id="S3.E2.m2.4.4.1.1.1" xref="S3.E2.m2.4.4.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m2.4.4.1.1.1.2" xref="S3.E2.m2.4.4.1.1.2.cmml">(</mo><mi mathvariant="normal" id="S3.E2.m2.3.3" xref="S3.E2.m2.3.3.cmml">Œ†</mi><mo id="S3.E2.m2.4.4.1.1.1.3" xref="S3.E2.m2.4.4.1.1.2.cmml">,</mo><mrow id="S3.E2.m2.4.4.1.1.1.1" xref="S3.E2.m2.4.4.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m2.4.4.1.1.1.1.2" xref="S3.E2.m2.4.4.1.1.1.1.2.cmml">‚Ñ≥</mi><mo lspace="0em" rspace="0em" id="S3.E2.m2.4.4.1.1.1.1.1" xref="S3.E2.m2.4.4.1.1.1.1.1.cmml">‚Äã</mo><mrow id="S3.E2.m2.4.4.1.1.1.1.3.2" xref="S3.E2.m2.4.4.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.E2.m2.4.4.1.1.1.1.3.2.1" xref="S3.E2.m2.4.4.1.1.1.1.3.1.cmml">(</mo><mi id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml">Œ∏</mi><mo id="S3.E2.m2.4.4.1.1.1.1.3.2.2" xref="S3.E2.m2.4.4.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E2.m2.2.2" xref="S3.E2.m2.2.2.cmml">Œ≤</mi><mo stretchy="false" id="S3.E2.m2.4.4.1.1.1.1.3.2.3" xref="S3.E2.m2.4.4.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m2.4.4.1.1.1.4" xref="S3.E2.m2.4.4.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.4b"><apply id="S3.E2.m2.4.4.cmml" xref="S3.E2.m2.4.4"><eq id="S3.E2.m2.4.4.2.cmml" xref="S3.E2.m2.4.4.2"></eq><csymbol cd="latexml" id="S3.E2.m2.4.4.3.cmml" xref="S3.E2.m2.4.4.3">absent</csymbol><apply id="S3.E2.m2.4.4.1.cmml" xref="S3.E2.m2.4.4.1"><times id="S3.E2.m2.4.4.1.2.cmml" xref="S3.E2.m2.4.4.1.2"></times><apply id="S3.E2.m2.4.4.1.3.cmml" xref="S3.E2.m2.4.4.1.3"><csymbol cd="ambiguous" id="S3.E2.m2.4.4.1.3.1.cmml" xref="S3.E2.m2.4.4.1.3">subscript</csymbol><ci id="S3.E2.m2.4.4.1.3.2.cmml" xref="S3.E2.m2.4.4.1.3.2">‚Ñõ</ci><ci id="S3.E2.m2.4.4.1.3.3.cmml" xref="S3.E2.m2.4.4.1.3.3">ùëë</ci></apply><interval closure="open" id="S3.E2.m2.4.4.1.1.2.cmml" xref="S3.E2.m2.4.4.1.1.1"><ci id="S3.E2.m2.3.3.cmml" xref="S3.E2.m2.3.3">Œ†</ci><apply id="S3.E2.m2.4.4.1.1.1.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1"><times id="S3.E2.m2.4.4.1.1.1.1.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.1"></times><ci id="S3.E2.m2.4.4.1.1.1.1.2.cmml" xref="S3.E2.m2.4.4.1.1.1.1.2">‚Ñ≥</ci><interval closure="open" id="S3.E2.m2.4.4.1.1.1.1.3.1.cmml" xref="S3.E2.m2.4.4.1.1.1.1.3.2"><ci id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1">ùúÉ</ci><ci id="S3.E2.m2.2.2.cmml" xref="S3.E2.m2.2.2">ùõΩ</ci></interval></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.4c">\displaystyle=\mathcal{R}_{d}(\Pi,\mathcal{M}(\theta,\beta))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle d_{fg}^{*}" display="inline"><semantics id="S3.E3.m1.1a"><msubsup id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml">d</mi><mrow id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3.cmml"><mi id="S3.E3.m1.1.1.2.3.2" xref="S3.E3.m1.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.3.1" xref="S3.E3.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.2.3.3" xref="S3.E3.m1.1.1.2.3.3.cmml">g</mi></mrow><mo id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml">‚àó</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1">superscript</csymbol><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">ùëë</ci><apply id="S3.E3.m1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.2.3"><times id="S3.E3.m1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.2.3.1"></times><ci id="S3.E3.m1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.2.3.2">ùëì</ci><ci id="S3.E3.m1.1.1.2.3.3.cmml" xref="S3.E3.m1.1.1.2.3.3">ùëî</ci></apply></apply><times id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle d_{fg}^{*}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m2.1" class="ltx_Math" alttext="\displaystyle=d_{fg}\odot((1-m)\cap(d_{fg}&gt;0))" display="inline"><semantics id="S3.E3.m2.1a"><mrow id="S3.E3.m2.1.1" xref="S3.E3.m2.1.1.cmml"><mi id="S3.E3.m2.1.1.3" xref="S3.E3.m2.1.1.3.cmml"></mi><mo id="S3.E3.m2.1.1.2" xref="S3.E3.m2.1.1.2.cmml">=</mo><mrow id="S3.E3.m2.1.1.1" xref="S3.E3.m2.1.1.1.cmml"><msub id="S3.E3.m2.1.1.1.3" xref="S3.E3.m2.1.1.1.3.cmml"><mi id="S3.E3.m2.1.1.1.3.2" xref="S3.E3.m2.1.1.1.3.2.cmml">d</mi><mrow id="S3.E3.m2.1.1.1.3.3" xref="S3.E3.m2.1.1.1.3.3.cmml"><mi id="S3.E3.m2.1.1.1.3.3.2" xref="S3.E3.m2.1.1.1.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E3.m2.1.1.1.3.3.1" xref="S3.E3.m2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E3.m2.1.1.1.3.3.3" xref="S3.E3.m2.1.1.1.3.3.3.cmml">g</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m2.1.1.1.2" xref="S3.E3.m2.1.1.1.2.cmml">‚äô</mo><mrow id="S3.E3.m2.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m2.1.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m2.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m2.1.1.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m2.1.1.1.1.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m2.1.1.1.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E3.m2.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E3.m2.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><mi id="S3.E3.m2.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.3.cmml">m</mi></mrow><mo stretchy="false" id="S3.E3.m2.1.1.1.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E3.m2.1.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.3.cmml">‚à©</mo><mrow id="S3.E3.m2.1.1.1.1.1.1.2.1" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.cmml"><mo stretchy="false" id="S3.E3.m2.1.1.1.1.1.1.2.1.2" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.cmml">(</mo><mrow id="S3.E3.m2.1.1.1.1.1.1.2.1.1" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.cmml"><msub id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.cmml"><mi id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.2" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.2.cmml">d</mi><mrow id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.cmml"><mi id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.2" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.1" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.3" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.3.cmml">g</mi></mrow></msub><mo id="S3.E3.m2.1.1.1.1.1.1.2.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.1.cmml">&gt;</mo><mn id="S3.E3.m2.1.1.1.1.1.1.2.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.3.cmml">0</mn></mrow><mo stretchy="false" id="S3.E3.m2.1.1.1.1.1.1.2.1.3" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m2.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.1b"><apply id="S3.E3.m2.1.1.cmml" xref="S3.E3.m2.1.1"><eq id="S3.E3.m2.1.1.2.cmml" xref="S3.E3.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E3.m2.1.1.3.cmml" xref="S3.E3.m2.1.1.3">absent</csymbol><apply id="S3.E3.m2.1.1.1.cmml" xref="S3.E3.m2.1.1.1"><csymbol cd="latexml" id="S3.E3.m2.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.2">direct-product</csymbol><apply id="S3.E3.m2.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.3.1.cmml" xref="S3.E3.m2.1.1.1.3">subscript</csymbol><ci id="S3.E3.m2.1.1.1.3.2.cmml" xref="S3.E3.m2.1.1.1.3.2">ùëë</ci><apply id="S3.E3.m2.1.1.1.3.3.cmml" xref="S3.E3.m2.1.1.1.3.3"><times id="S3.E3.m2.1.1.1.3.3.1.cmml" xref="S3.E3.m2.1.1.1.3.3.1"></times><ci id="S3.E3.m2.1.1.1.3.3.2.cmml" xref="S3.E3.m2.1.1.1.3.3.2">ùëì</ci><ci id="S3.E3.m2.1.1.1.3.3.3.cmml" xref="S3.E3.m2.1.1.1.3.3.3">ùëî</ci></apply></apply><apply id="S3.E3.m2.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1"><intersect id="S3.E3.m2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.3"></intersect><apply id="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1"><minus id="S3.E3.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E3.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E3.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1.1.3">ùëö</ci></apply><apply id="S3.E3.m2.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2.1"><gt id="S3.E3.m2.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.1"></gt><apply id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2">subscript</csymbol><ci id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.2">ùëë</ci><apply id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3"><times id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.1"></times><ci id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.2">ùëì</ci><ci id="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.2.3.3">ùëî</ci></apply></apply><cn type="integer" id="S3.E3.m2.1.1.1.1.1.1.2.1.1.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2.1.1.3">0</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.1c">\displaystyle=d_{fg}\odot((1-m)\cap(d_{fg}&gt;0))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.8" class="ltx_p">where <math id="S3.SS2.p4.6.m1.1" class="ltx_Math" alttext="\mathcal{R}_{d}" display="inline"><semantics id="S3.SS2.p4.6.m1.1a"><msub id="S3.SS2.p4.6.m1.1.1" xref="S3.SS2.p4.6.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p4.6.m1.1.1.2" xref="S3.SS2.p4.6.m1.1.1.2.cmml">‚Ñõ</mi><mi id="S3.SS2.p4.6.m1.1.1.3" xref="S3.SS2.p4.6.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m1.1b"><apply id="S3.SS2.p4.6.m1.1.1.cmml" xref="S3.SS2.p4.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.6.m1.1.1.1.cmml" xref="S3.SS2.p4.6.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.6.m1.1.1.2.cmml" xref="S3.SS2.p4.6.m1.1.1.2">‚Ñõ</ci><ci id="S3.SS2.p4.6.m1.1.1.3.cmml" xref="S3.SS2.p4.6.m1.1.1.3">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m1.1c">\mathcal{R}_{d}</annotation></semantics></math> is a depth renderer that renders the depth map of a mesh, <math id="S3.SS2.p4.7.m2.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS2.p4.7.m2.1a"><mo id="S3.SS2.p4.7.m2.1.1" xref="S3.SS2.p4.7.m2.1.1.cmml">‚äô</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m2.1b"><csymbol cd="latexml" id="S3.SS2.p4.7.m2.1.1.cmml" xref="S3.SS2.p4.7.m2.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m2.1c">\odot</annotation></semantics></math> denotes the Hadamard product, and <math id="S3.SS2.p4.8.m3.1" class="ltx_Math" alttext="(d_{fg}&gt;0)" display="inline"><semantics id="S3.SS2.p4.8.m3.1a"><mrow id="S3.SS2.p4.8.m3.1.1.1" xref="S3.SS2.p4.8.m3.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p4.8.m3.1.1.1.2" xref="S3.SS2.p4.8.m3.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p4.8.m3.1.1.1.1" xref="S3.SS2.p4.8.m3.1.1.1.1.cmml"><msub id="S3.SS2.p4.8.m3.1.1.1.1.2" xref="S3.SS2.p4.8.m3.1.1.1.1.2.cmml"><mi id="S3.SS2.p4.8.m3.1.1.1.1.2.2" xref="S3.SS2.p4.8.m3.1.1.1.1.2.2.cmml">d</mi><mrow id="S3.SS2.p4.8.m3.1.1.1.1.2.3" xref="S3.SS2.p4.8.m3.1.1.1.1.2.3.cmml"><mi id="S3.SS2.p4.8.m3.1.1.1.1.2.3.2" xref="S3.SS2.p4.8.m3.1.1.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.8.m3.1.1.1.1.2.3.1" xref="S3.SS2.p4.8.m3.1.1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p4.8.m3.1.1.1.1.2.3.3" xref="S3.SS2.p4.8.m3.1.1.1.1.2.3.3.cmml">g</mi></mrow></msub><mo id="S3.SS2.p4.8.m3.1.1.1.1.1" xref="S3.SS2.p4.8.m3.1.1.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.p4.8.m3.1.1.1.1.3" xref="S3.SS2.p4.8.m3.1.1.1.1.3.cmml">0</mn></mrow><mo stretchy="false" id="S3.SS2.p4.8.m3.1.1.1.3" xref="S3.SS2.p4.8.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m3.1b"><apply id="S3.SS2.p4.8.m3.1.1.1.1.cmml" xref="S3.SS2.p4.8.m3.1.1.1"><gt id="S3.SS2.p4.8.m3.1.1.1.1.1.cmml" xref="S3.SS2.p4.8.m3.1.1.1.1.1"></gt><apply id="S3.SS2.p4.8.m3.1.1.1.1.2.cmml" xref="S3.SS2.p4.8.m3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p4.8.m3.1.1.1.1.2.1.cmml" xref="S3.SS2.p4.8.m3.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p4.8.m3.1.1.1.1.2.2.cmml" xref="S3.SS2.p4.8.m3.1.1.1.1.2.2">ùëë</ci><apply id="S3.SS2.p4.8.m3.1.1.1.1.2.3.cmml" xref="S3.SS2.p4.8.m3.1.1.1.1.2.3"><times id="S3.SS2.p4.8.m3.1.1.1.1.2.3.1.cmml" xref="S3.SS2.p4.8.m3.1.1.1.1.2.3.1"></times><ci id="S3.SS2.p4.8.m3.1.1.1.1.2.3.2.cmml" xref="S3.SS2.p4.8.m3.1.1.1.1.2.3.2">ùëì</ci><ci id="S3.SS2.p4.8.m3.1.1.1.1.2.3.3.cmml" xref="S3.SS2.p4.8.m3.1.1.1.1.2.3.3">ùëî</ci></apply></apply><cn type="integer" id="S3.SS2.p4.8.m3.1.1.1.1.3.cmml" xref="S3.SS2.p4.8.m3.1.1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m3.1c">(d_{fg}&gt;0)</annotation></semantics></math> is the silhouette of the rendered person.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.6" class="ltx_p">Finally, to preserve the context information (e.g. texture and background layout) of the initial generation, we use initial image latents as a starting point in the final generation process. We add noise to <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">ùëß</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">z</annotation></semantics></math>, and use a pre-trained denoising model (i.e. depth-to-image Stable Diffusion) to perform sequential denoising steps which produces the final image latents <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="z^{*}" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><msup id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">z</mi><mo id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">ùëß</ci><times id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">z^{*}</annotation></semantics></math>. The denoising process (achieved through a pretrained UNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>) is guided by both the depth map <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="d_{fg}^{*}" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><msubsup id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml"><mi id="S3.SS2.p5.3.m3.1.1.2.2" xref="S3.SS2.p5.3.m3.1.1.2.2.cmml">d</mi><mrow id="S3.SS2.p5.3.m3.1.1.2.3" xref="S3.SS2.p5.3.m3.1.1.2.3.cmml"><mi id="S3.SS2.p5.3.m3.1.1.2.3.2" xref="S3.SS2.p5.3.m3.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.3.m3.1.1.2.3.1" xref="S3.SS2.p5.3.m3.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS2.p5.3.m3.1.1.2.3.3" xref="S3.SS2.p5.3.m3.1.1.2.3.3.cmml">g</mi></mrow><mo id="S3.SS2.p5.3.m3.1.1.3" xref="S3.SS2.p5.3.m3.1.1.3.cmml">‚àó</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">superscript</csymbol><apply id="S3.SS2.p5.3.m3.1.1.2.cmml" xref="S3.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.1.1.2.1.cmml" xref="S3.SS2.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p5.3.m3.1.1.2.2.cmml" xref="S3.SS2.p5.3.m3.1.1.2.2">ùëë</ci><apply id="S3.SS2.p5.3.m3.1.1.2.3.cmml" xref="S3.SS2.p5.3.m3.1.1.2.3"><times id="S3.SS2.p5.3.m3.1.1.2.3.1.cmml" xref="S3.SS2.p5.3.m3.1.1.2.3.1"></times><ci id="S3.SS2.p5.3.m3.1.1.2.3.2.cmml" xref="S3.SS2.p5.3.m3.1.1.2.3.2">ùëì</ci><ci id="S3.SS2.p5.3.m3.1.1.2.3.3.cmml" xref="S3.SS2.p5.3.m3.1.1.2.3.3">ùëî</ci></apply></apply><times id="S3.SS2.p5.3.m3.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">d_{fg}^{*}</annotation></semantics></math> and text embeddings <math id="S3.SS2.p5.4.m4.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS2.p5.4.m4.1a"><msub id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml"><mi id="S3.SS2.p5.4.m4.1.1.2" xref="S3.SS2.p5.4.m4.1.1.2.cmml">z</mi><mi id="S3.SS2.p5.4.m4.1.1.3" xref="S3.SS2.p5.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p5.4.m4.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1.2">ùëß</ci><ci id="S3.SS2.p5.4.m4.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">z_{t}</annotation></semantics></math>. Final generation is obtained by decoding the <math id="S3.SS2.p5.5.m5.1" class="ltx_Math" alttext="z^{*}" display="inline"><semantics id="S3.SS2.p5.5.m5.1a"><msup id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml"><mi id="S3.SS2.p5.5.m5.1.1.2" xref="S3.SS2.p5.5.m5.1.1.2.cmml">z</mi><mo id="S3.SS2.p5.5.m5.1.1.3" xref="S3.SS2.p5.5.m5.1.1.3.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><apply id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1">superscript</csymbol><ci id="S3.SS2.p5.5.m5.1.1.2.cmml" xref="S3.SS2.p5.5.m5.1.1.2">ùëß</ci><times id="S3.SS2.p5.5.m5.1.1.3.cmml" xref="S3.SS2.p5.5.m5.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">z^{*}</annotation></semantics></math> with the compression module‚Äôs decoder <math id="S3.SS2.p5.6.m6.1" class="ltx_Math" alttext="\mathcal{D}_{\mathcal{G}_{d}}" display="inline"><semantics id="S3.SS2.p5.6.m6.1a"><msub id="S3.SS2.p5.6.m6.1.1" xref="S3.SS2.p5.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.6.m6.1.1.2" xref="S3.SS2.p5.6.m6.1.1.2.cmml">ùíü</mi><msub id="S3.SS2.p5.6.m6.1.1.3" xref="S3.SS2.p5.6.m6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.6.m6.1.1.3.2" xref="S3.SS2.p5.6.m6.1.1.3.2.cmml">ùí¢</mi><mi id="S3.SS2.p5.6.m6.1.1.3.3" xref="S3.SS2.p5.6.m6.1.1.3.3.cmml">d</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m6.1b"><apply id="S3.SS2.p5.6.m6.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.1.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p5.6.m6.1.1.2.cmml" xref="S3.SS2.p5.6.m6.1.1.2">ùíü</ci><apply id="S3.SS2.p5.6.m6.1.1.3.cmml" xref="S3.SS2.p5.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.1.1.3.1.cmml" xref="S3.SS2.p5.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.6.m6.1.1.3.2.cmml" xref="S3.SS2.p5.6.m6.1.1.3.2">ùí¢</ci><ci id="S3.SS2.p5.6.m6.1.1.3.3.cmml" xref="S3.SS2.p5.6.m6.1.1.3.3">ùëë</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m6.1c">\mathcal{D}_{\mathcal{G}_{d}}</annotation></semantics></math>.</p>
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle z^{noised}" display="inline"><semantics id="S3.E4.m1.1a"><msup id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">z</mi><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.1a" xref="S3.E4.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.4" xref="S3.E4.m1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.1b" xref="S3.E4.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.5" xref="S3.E4.m1.1.1.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.1c" xref="S3.E4.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.6" xref="S3.E4.m1.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.1d" xref="S3.E4.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.7" xref="S3.E4.m1.1.1.3.7.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1">superscript</csymbol><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">ùëß</ci><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><times id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></times><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">ùëõ</ci><ci id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3">ùëú</ci><ci id="S3.E4.m1.1.1.3.4.cmml" xref="S3.E4.m1.1.1.3.4">ùëñ</ci><ci id="S3.E4.m1.1.1.3.5.cmml" xref="S3.E4.m1.1.1.3.5">ùë†</ci><ci id="S3.E4.m1.1.1.3.6.cmml" xref="S3.E4.m1.1.1.3.6">ùëí</ci><ci id="S3.E4.m1.1.1.3.7.cmml" xref="S3.E4.m1.1.1.3.7">ùëë</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle z^{noised}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.1" class="ltx_Math" alttext="\displaystyle=noise(z)" display="inline"><semantics id="S3.E4.m2.1a"><mrow id="S3.E4.m2.1.2" xref="S3.E4.m2.1.2.cmml"><mi id="S3.E4.m2.1.2.2" xref="S3.E4.m2.1.2.2.cmml"></mi><mo id="S3.E4.m2.1.2.1" xref="S3.E4.m2.1.2.1.cmml">=</mo><mrow id="S3.E4.m2.1.2.3" xref="S3.E4.m2.1.2.3.cmml"><mi id="S3.E4.m2.1.2.3.2" xref="S3.E4.m2.1.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.2.3.1" xref="S3.E4.m2.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E4.m2.1.2.3.3" xref="S3.E4.m2.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.2.3.1a" xref="S3.E4.m2.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E4.m2.1.2.3.4" xref="S3.E4.m2.1.2.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.2.3.1b" xref="S3.E4.m2.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E4.m2.1.2.3.5" xref="S3.E4.m2.1.2.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.2.3.1c" xref="S3.E4.m2.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E4.m2.1.2.3.6" xref="S3.E4.m2.1.2.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.2.3.1d" xref="S3.E4.m2.1.2.3.1.cmml">‚Äã</mo><mrow id="S3.E4.m2.1.2.3.7.2" xref="S3.E4.m2.1.2.3.cmml"><mo stretchy="false" id="S3.E4.m2.1.2.3.7.2.1" xref="S3.E4.m2.1.2.3.cmml">(</mo><mi id="S3.E4.m2.1.1" xref="S3.E4.m2.1.1.cmml">z</mi><mo stretchy="false" id="S3.E4.m2.1.2.3.7.2.2" xref="S3.E4.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.1b"><apply id="S3.E4.m2.1.2.cmml" xref="S3.E4.m2.1.2"><eq id="S3.E4.m2.1.2.1.cmml" xref="S3.E4.m2.1.2.1"></eq><csymbol cd="latexml" id="S3.E4.m2.1.2.2.cmml" xref="S3.E4.m2.1.2.2">absent</csymbol><apply id="S3.E4.m2.1.2.3.cmml" xref="S3.E4.m2.1.2.3"><times id="S3.E4.m2.1.2.3.1.cmml" xref="S3.E4.m2.1.2.3.1"></times><ci id="S3.E4.m2.1.2.3.2.cmml" xref="S3.E4.m2.1.2.3.2">ùëõ</ci><ci id="S3.E4.m2.1.2.3.3.cmml" xref="S3.E4.m2.1.2.3.3">ùëú</ci><ci id="S3.E4.m2.1.2.3.4.cmml" xref="S3.E4.m2.1.2.3.4">ùëñ</ci><ci id="S3.E4.m2.1.2.3.5.cmml" xref="S3.E4.m2.1.2.3.5">ùë†</ci><ci id="S3.E4.m2.1.2.3.6.cmml" xref="S3.E4.m2.1.2.3.6">ùëí</ci><ci id="S3.E4.m2.1.1.cmml" xref="S3.E4.m2.1.1">ùëß</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.1c">\displaystyle=noise(z)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle z^{*}" display="inline"><semantics id="S3.E5.m1.1a"><msup id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mi id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">z</mi><mo id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1">superscript</csymbol><ci id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2">ùëß</ci><times id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle z^{*}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E5.m2.3" class="ltx_Math" alttext="\displaystyle=denoise(z^{noised};d_{fg}^{*},z_{t})" display="inline"><semantics id="S3.E5.m2.3a"><mrow id="S3.E5.m2.3.3" xref="S3.E5.m2.3.3.cmml"><mi id="S3.E5.m2.3.3.5" xref="S3.E5.m2.3.3.5.cmml"></mi><mo id="S3.E5.m2.3.3.4" xref="S3.E5.m2.3.3.4.cmml">=</mo><mrow id="S3.E5.m2.3.3.3" xref="S3.E5.m2.3.3.3.cmml"><mi id="S3.E5.m2.3.3.3.5" xref="S3.E5.m2.3.3.3.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.3.3.3.4" xref="S3.E5.m2.3.3.3.4.cmml">‚Äã</mo><mi id="S3.E5.m2.3.3.3.6" xref="S3.E5.m2.3.3.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.3.3.3.4a" xref="S3.E5.m2.3.3.3.4.cmml">‚Äã</mo><mi id="S3.E5.m2.3.3.3.7" xref="S3.E5.m2.3.3.3.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.3.3.3.4b" xref="S3.E5.m2.3.3.3.4.cmml">‚Äã</mo><mi id="S3.E5.m2.3.3.3.8" xref="S3.E5.m2.3.3.3.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.3.3.3.4c" xref="S3.E5.m2.3.3.3.4.cmml">‚Äã</mo><mi id="S3.E5.m2.3.3.3.9" xref="S3.E5.m2.3.3.3.9.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.3.3.3.4d" xref="S3.E5.m2.3.3.3.4.cmml">‚Äã</mo><mi id="S3.E5.m2.3.3.3.10" xref="S3.E5.m2.3.3.3.10.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.3.3.3.4e" xref="S3.E5.m2.3.3.3.4.cmml">‚Äã</mo><mi id="S3.E5.m2.3.3.3.11" xref="S3.E5.m2.3.3.3.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.3.3.3.4f" xref="S3.E5.m2.3.3.3.4.cmml">‚Äã</mo><mrow id="S3.E5.m2.3.3.3.3.3" xref="S3.E5.m2.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.E5.m2.3.3.3.3.3.4" xref="S3.E5.m2.3.3.3.3.4.cmml">(</mo><msup id="S3.E5.m2.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.2.cmml">z</mi><mrow id="S3.E5.m2.1.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.3.2" xref="S3.E5.m2.1.1.1.1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.1.1.1.1.1.3.1" xref="S3.E5.m2.1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E5.m2.1.1.1.1.1.1.3.3" xref="S3.E5.m2.1.1.1.1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.1.1.1.1.1.3.1a" xref="S3.E5.m2.1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E5.m2.1.1.1.1.1.1.3.4" xref="S3.E5.m2.1.1.1.1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.1.1.1.1.1.3.1b" xref="S3.E5.m2.1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E5.m2.1.1.1.1.1.1.3.5" xref="S3.E5.m2.1.1.1.1.1.1.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.1.1.1.1.1.3.1c" xref="S3.E5.m2.1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E5.m2.1.1.1.1.1.1.3.6" xref="S3.E5.m2.1.1.1.1.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.1.1.1.1.1.3.1d" xref="S3.E5.m2.1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E5.m2.1.1.1.1.1.1.3.7" xref="S3.E5.m2.1.1.1.1.1.1.3.7.cmml">d</mi></mrow></msup><mo id="S3.E5.m2.3.3.3.3.3.5" xref="S3.E5.m2.3.3.3.3.4.cmml">;</mo><msubsup id="S3.E5.m2.2.2.2.2.2.2" xref="S3.E5.m2.2.2.2.2.2.2.cmml"><mi id="S3.E5.m2.2.2.2.2.2.2.2.2" xref="S3.E5.m2.2.2.2.2.2.2.2.2.cmml">d</mi><mrow id="S3.E5.m2.2.2.2.2.2.2.2.3" xref="S3.E5.m2.2.2.2.2.2.2.2.3.cmml"><mi id="S3.E5.m2.2.2.2.2.2.2.2.3.2" xref="S3.E5.m2.2.2.2.2.2.2.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.2.2.2.2.2.2.2.3.1" xref="S3.E5.m2.2.2.2.2.2.2.2.3.1.cmml">‚Äã</mo><mi id="S3.E5.m2.2.2.2.2.2.2.2.3.3" xref="S3.E5.m2.2.2.2.2.2.2.2.3.3.cmml">g</mi></mrow><mo id="S3.E5.m2.2.2.2.2.2.2.3" xref="S3.E5.m2.2.2.2.2.2.2.3.cmml">‚àó</mo></msubsup><mo id="S3.E5.m2.3.3.3.3.3.6" xref="S3.E5.m2.3.3.3.3.4.cmml">,</mo><msub id="S3.E5.m2.3.3.3.3.3.3" xref="S3.E5.m2.3.3.3.3.3.3.cmml"><mi id="S3.E5.m2.3.3.3.3.3.3.2" xref="S3.E5.m2.3.3.3.3.3.3.2.cmml">z</mi><mi id="S3.E5.m2.3.3.3.3.3.3.3" xref="S3.E5.m2.3.3.3.3.3.3.3.cmml">t</mi></msub><mo stretchy="false" id="S3.E5.m2.3.3.3.3.3.7" xref="S3.E5.m2.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.3b"><apply id="S3.E5.m2.3.3.cmml" xref="S3.E5.m2.3.3"><eq id="S3.E5.m2.3.3.4.cmml" xref="S3.E5.m2.3.3.4"></eq><csymbol cd="latexml" id="S3.E5.m2.3.3.5.cmml" xref="S3.E5.m2.3.3.5">absent</csymbol><apply id="S3.E5.m2.3.3.3.cmml" xref="S3.E5.m2.3.3.3"><times id="S3.E5.m2.3.3.3.4.cmml" xref="S3.E5.m2.3.3.3.4"></times><ci id="S3.E5.m2.3.3.3.5.cmml" xref="S3.E5.m2.3.3.3.5">ùëë</ci><ci id="S3.E5.m2.3.3.3.6.cmml" xref="S3.E5.m2.3.3.3.6">ùëí</ci><ci id="S3.E5.m2.3.3.3.7.cmml" xref="S3.E5.m2.3.3.3.7">ùëõ</ci><ci id="S3.E5.m2.3.3.3.8.cmml" xref="S3.E5.m2.3.3.3.8">ùëú</ci><ci id="S3.E5.m2.3.3.3.9.cmml" xref="S3.E5.m2.3.3.3.9">ùëñ</ci><ci id="S3.E5.m2.3.3.3.10.cmml" xref="S3.E5.m2.3.3.3.10">ùë†</ci><ci id="S3.E5.m2.3.3.3.11.cmml" xref="S3.E5.m2.3.3.3.11">ùëí</ci><list id="S3.E5.m2.3.3.3.3.4.cmml" xref="S3.E5.m2.3.3.3.3.3"><apply id="S3.E5.m2.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E5.m2.1.1.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.2">ùëß</ci><apply id="S3.E5.m2.1.1.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.3"><times id="S3.E5.m2.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.3.1"></times><ci id="S3.E5.m2.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.3.2">ùëõ</ci><ci id="S3.E5.m2.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.3.3">ùëú</ci><ci id="S3.E5.m2.1.1.1.1.1.1.3.4.cmml" xref="S3.E5.m2.1.1.1.1.1.1.3.4">ùëñ</ci><ci id="S3.E5.m2.1.1.1.1.1.1.3.5.cmml" xref="S3.E5.m2.1.1.1.1.1.1.3.5">ùë†</ci><ci id="S3.E5.m2.1.1.1.1.1.1.3.6.cmml" xref="S3.E5.m2.1.1.1.1.1.1.3.6">ùëí</ci><ci id="S3.E5.m2.1.1.1.1.1.1.3.7.cmml" xref="S3.E5.m2.1.1.1.1.1.1.3.7">ùëë</ci></apply></apply><apply id="S3.E5.m2.2.2.2.2.2.2.cmml" xref="S3.E5.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.2.2.2.2.1.cmml" xref="S3.E5.m2.2.2.2.2.2.2">superscript</csymbol><apply id="S3.E5.m2.2.2.2.2.2.2.2.cmml" xref="S3.E5.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.2.2.2.2.2.1.cmml" xref="S3.E5.m2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E5.m2.2.2.2.2.2.2.2.2.cmml" xref="S3.E5.m2.2.2.2.2.2.2.2.2">ùëë</ci><apply id="S3.E5.m2.2.2.2.2.2.2.2.3.cmml" xref="S3.E5.m2.2.2.2.2.2.2.2.3"><times id="S3.E5.m2.2.2.2.2.2.2.2.3.1.cmml" xref="S3.E5.m2.2.2.2.2.2.2.2.3.1"></times><ci id="S3.E5.m2.2.2.2.2.2.2.2.3.2.cmml" xref="S3.E5.m2.2.2.2.2.2.2.2.3.2">ùëì</ci><ci id="S3.E5.m2.2.2.2.2.2.2.2.3.3.cmml" xref="S3.E5.m2.2.2.2.2.2.2.2.3.3">ùëî</ci></apply></apply><times id="S3.E5.m2.2.2.2.2.2.2.3.cmml" xref="S3.E5.m2.2.2.2.2.2.2.3"></times></apply><apply id="S3.E5.m2.3.3.3.3.3.3.cmml" xref="S3.E5.m2.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E5.m2.3.3.3.3.3.3.1.cmml" xref="S3.E5.m2.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E5.m2.3.3.3.3.3.3.2.cmml" xref="S3.E5.m2.3.3.3.3.3.3.2">ùëß</ci><ci id="S3.E5.m2.3.3.3.3.3.3.3.cmml" xref="S3.E5.m2.3.3.3.3.3.3.3">ùë°</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.3c">\displaystyle=denoise(z^{noised};d_{fg}^{*},z_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{I^{*}}" display="inline"><semantics id="S3.E6.m1.1a"><msup id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml">‚Ñê</mi><mo id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1">superscript</csymbol><ci id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2">‚Ñê</ci><times id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\displaystyle\mathcal{I^{*}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E6.m2.1" class="ltx_Math" alttext="\displaystyle=\mathcal{D}_{\mathcal{G}_{d}}(z^{*})" display="inline"><semantics id="S3.E6.m2.1a"><mrow id="S3.E6.m2.1.1" xref="S3.E6.m2.1.1.cmml"><mi id="S3.E6.m2.1.1.3" xref="S3.E6.m2.1.1.3.cmml"></mi><mo id="S3.E6.m2.1.1.2" xref="S3.E6.m2.1.1.2.cmml">=</mo><mrow id="S3.E6.m2.1.1.1" xref="S3.E6.m2.1.1.1.cmml"><msub id="S3.E6.m2.1.1.1.3" xref="S3.E6.m2.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m2.1.1.1.3.2" xref="S3.E6.m2.1.1.1.3.2.cmml">ùíü</mi><msub id="S3.E6.m2.1.1.1.3.3" xref="S3.E6.m2.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m2.1.1.1.3.3.2" xref="S3.E6.m2.1.1.1.3.3.2.cmml">ùí¢</mi><mi id="S3.E6.m2.1.1.1.3.3.3" xref="S3.E6.m2.1.1.1.3.3.3.cmml">d</mi></msub></msub><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.2" xref="S3.E6.m2.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.E6.m2.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m2.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E6.m2.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.1.2.cmml">z</mi><mo id="S3.E6.m2.1.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.1.3.cmml">‚àó</mo></msup><mo stretchy="false" id="S3.E6.m2.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m2.1b"><apply id="S3.E6.m2.1.1.cmml" xref="S3.E6.m2.1.1"><eq id="S3.E6.m2.1.1.2.cmml" xref="S3.E6.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E6.m2.1.1.3.cmml" xref="S3.E6.m2.1.1.3">absent</csymbol><apply id="S3.E6.m2.1.1.1.cmml" xref="S3.E6.m2.1.1.1"><times id="S3.E6.m2.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.2"></times><apply id="S3.E6.m2.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.3.1.cmml" xref="S3.E6.m2.1.1.1.3">subscript</csymbol><ci id="S3.E6.m2.1.1.1.3.2.cmml" xref="S3.E6.m2.1.1.1.3.2">ùíü</ci><apply id="S3.E6.m2.1.1.1.3.3.cmml" xref="S3.E6.m2.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.3.3.1.cmml" xref="S3.E6.m2.1.1.1.3.3">subscript</csymbol><ci id="S3.E6.m2.1.1.1.3.3.2.cmml" xref="S3.E6.m2.1.1.1.3.3.2">ùí¢</ci><ci id="S3.E6.m2.1.1.1.3.3.3.cmml" xref="S3.E6.m2.1.1.1.3.3.3">ùëë</ci></apply></apply><apply id="S3.E6.m2.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1">superscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.2">ùëß</ci><times id="S3.E6.m2.1.1.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m2.1c">\displaystyle=\mathcal{D}_{\mathcal{G}_{d}}(z^{*})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p5.7" class="ltx_p">As shown in Figure <a href="#S2.F2" title="Figure 2 ‚Ä£ 2.3 Editing &amp; composing large pre-trained models ‚Ä£ 2 Related Work ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, final generation <math id="S3.SS2.p5.7.m1.1" class="ltx_Math" alttext="\mathcal{I}^{*}" display="inline"><semantics id="S3.SS2.p5.7.m1.1a"><msup id="S3.SS2.p5.7.m1.1.1" xref="S3.SS2.p5.7.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.7.m1.1.1.2" xref="S3.SS2.p5.7.m1.1.1.2.cmml">‚Ñê</mi><mo id="S3.SS2.p5.7.m1.1.1.3" xref="S3.SS2.p5.7.m1.1.1.3.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m1.1b"><apply id="S3.SS2.p5.7.m1.1.1.cmml" xref="S3.SS2.p5.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.7.m1.1.1.1.cmml" xref="S3.SS2.p5.7.m1.1.1">superscript</csymbol><ci id="S3.SS2.p5.7.m1.1.1.2.cmml" xref="S3.SS2.p5.7.m1.1.1.2">‚Ñê</ci><times id="S3.SS2.p5.7.m1.1.1.3.cmml" xref="S3.SS2.p5.7.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m1.1c">\mathcal{I}^{*}</annotation></semantics></math> contains similar texture and background as the original image, but the human body anatomy is rectified.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Finetuning Human Mesh Recovery on challenging domains using synthetic data</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Training a single-view Human Mesh Recovery (HMR) model end-to-end would require large amounts of images with paired 3D ground truths. Collecting such training sets requires burdensome motion capturing systems and is often limited to indoor laboratories. As a result, previous works such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> have focused on finetuning HMR model to a particular challenging domain using weak supervision (image paired with 2D keypoints). In this section, we introduce how image-mesh pairs from Diffusion-HPC can be used to finetuning HMR models in challenging domains.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.6" class="ltx_p">Given a pre-trained HMR model that predicts pose <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\theta</annotation></semantics></math>, shape <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">Œ≤</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">ùõΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\beta</annotation></semantics></math> and camera matrix <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\Pi" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi mathvariant="normal" id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">Œ†</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">Œ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\Pi</annotation></semantics></math> from an image <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">‚Ñê</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">‚Ñê</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\mathcal{I}</annotation></semantics></math> (i.e.
<math id="S3.SS3.p2.5.m5.3" class="ltx_Math" alttext="f:\mathcal{I}\rightarrow(\theta,\beta,\Pi)" display="inline"><semantics id="S3.SS3.p2.5.m5.3a"><mrow id="S3.SS3.p2.5.m5.3.4" xref="S3.SS3.p2.5.m5.3.4.cmml"><mi id="S3.SS3.p2.5.m5.3.4.2" xref="S3.SS3.p2.5.m5.3.4.2.cmml">f</mi><mo lspace="0.278em" rspace="0.278em" id="S3.SS3.p2.5.m5.3.4.1" xref="S3.SS3.p2.5.m5.3.4.1.cmml">:</mo><mrow id="S3.SS3.p2.5.m5.3.4.3" xref="S3.SS3.p2.5.m5.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.5.m5.3.4.3.2" xref="S3.SS3.p2.5.m5.3.4.3.2.cmml">‚Ñê</mi><mo stretchy="false" id="S3.SS3.p2.5.m5.3.4.3.1" xref="S3.SS3.p2.5.m5.3.4.3.1.cmml">‚Üí</mo><mrow id="S3.SS3.p2.5.m5.3.4.3.3.2" xref="S3.SS3.p2.5.m5.3.4.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.5.m5.3.4.3.3.2.1" xref="S3.SS3.p2.5.m5.3.4.3.3.1.cmml">(</mo><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">Œ∏</mi><mo id="S3.SS3.p2.5.m5.3.4.3.3.2.2" xref="S3.SS3.p2.5.m5.3.4.3.3.1.cmml">,</mo><mi id="S3.SS3.p2.5.m5.2.2" xref="S3.SS3.p2.5.m5.2.2.cmml">Œ≤</mi><mo id="S3.SS3.p2.5.m5.3.4.3.3.2.3" xref="S3.SS3.p2.5.m5.3.4.3.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p2.5.m5.3.3" xref="S3.SS3.p2.5.m5.3.3.cmml">Œ†</mi><mo stretchy="false" id="S3.SS3.p2.5.m5.3.4.3.3.2.4" xref="S3.SS3.p2.5.m5.3.4.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.3b"><apply id="S3.SS3.p2.5.m5.3.4.cmml" xref="S3.SS3.p2.5.m5.3.4"><ci id="S3.SS3.p2.5.m5.3.4.1.cmml" xref="S3.SS3.p2.5.m5.3.4.1">:</ci><ci id="S3.SS3.p2.5.m5.3.4.2.cmml" xref="S3.SS3.p2.5.m5.3.4.2">ùëì</ci><apply id="S3.SS3.p2.5.m5.3.4.3.cmml" xref="S3.SS3.p2.5.m5.3.4.3"><ci id="S3.SS3.p2.5.m5.3.4.3.1.cmml" xref="S3.SS3.p2.5.m5.3.4.3.1">‚Üí</ci><ci id="S3.SS3.p2.5.m5.3.4.3.2.cmml" xref="S3.SS3.p2.5.m5.3.4.3.2">‚Ñê</ci><vector id="S3.SS3.p2.5.m5.3.4.3.3.1.cmml" xref="S3.SS3.p2.5.m5.3.4.3.3.2"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">ùúÉ</ci><ci id="S3.SS3.p2.5.m5.2.2.cmml" xref="S3.SS3.p2.5.m5.2.2">ùõΩ</ci><ci id="S3.SS3.p2.5.m5.3.3.cmml" xref="S3.SS3.p2.5.m5.3.3">Œ†</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.3c">f:\mathcal{I}\rightarrow(\theta,\beta,\Pi)</annotation></semantics></math>), we aim to
adapt the model to a new target-domain by finetuning <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><mi id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><ci id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">ùëì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">f</annotation></semantics></math> on a small set of target images.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.6" class="ltx_p">In a typical finetuning setup where only 2D keypoints from the target are available as supervision, 2D reprojection loss can be minimized to encourage the consistency between predicted and ground truth keypoints. Formally, for an image from the target training set, let the ground truth 2D keypoints be <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="j\in\mathbb{R}^{k\times 2}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">j</mi><mo id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">‚àà</mo><msup id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml"><mi id="S3.SS3.p3.1.m1.1.1.3.2" xref="S3.SS3.p3.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS3.p3.1.m1.1.1.3.3" xref="S3.SS3.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS3.p3.1.m1.1.1.3.3.2" xref="S3.SS3.p3.1.m1.1.1.3.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.1.m1.1.1.3.3.1" xref="S3.SS3.p3.1.m1.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS3.p3.1.m1.1.1.3.3.3" xref="S3.SS3.p3.1.m1.1.1.3.3.3.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><in id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></in><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">ùëó</ci><apply id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.3.1.cmml" xref="S3.SS3.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.3.2.cmml" xref="S3.SS3.p3.1.m1.1.1.3.2">‚Ñù</ci><apply id="S3.SS3.p3.1.m1.1.1.3.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3"><times id="S3.SS3.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS3.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.2">ùëò</ci><cn type="integer" id="S3.SS3.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">j\in\mathbb{R}^{k\times 2}</annotation></semantics></math> with <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">k</annotation></semantics></math> annotated keypoints per person, we would want to minimize <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{2D}^{real}=||\hat{j}-j||_{2}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mrow id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><msubsup id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.3.m3.1.1.3.2.2" xref="S3.SS3.p3.3.m3.1.1.3.2.2.cmml">‚Ñí</mi><mrow id="S3.SS3.p3.3.m3.1.1.3.2.3" xref="S3.SS3.p3.3.m3.1.1.3.2.3.cmml"><mn id="S3.SS3.p3.3.m3.1.1.3.2.3.2" xref="S3.SS3.p3.3.m3.1.1.3.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p3.3.m3.1.1.3.2.3.1" xref="S3.SS3.p3.3.m3.1.1.3.2.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p3.3.m3.1.1.3.2.3.3" xref="S3.SS3.p3.3.m3.1.1.3.2.3.3.cmml">D</mi></mrow><mrow id="S3.SS3.p3.3.m3.1.1.3.3" xref="S3.SS3.p3.3.m3.1.1.3.3.cmml"><mi id="S3.SS3.p3.3.m3.1.1.3.3.2" xref="S3.SS3.p3.3.m3.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.3.m3.1.1.3.3.1" xref="S3.SS3.p3.3.m3.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p3.3.m3.1.1.3.3.3" xref="S3.SS3.p3.3.m3.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.3.m3.1.1.3.3.1a" xref="S3.SS3.p3.3.m3.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p3.3.m3.1.1.3.3.4" xref="S3.SS3.p3.3.m3.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.3.m3.1.1.3.3.1b" xref="S3.SS3.p3.3.m3.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p3.3.m3.1.1.3.3.5" xref="S3.SS3.p3.3.m3.1.1.3.3.5.cmml">l</mi></mrow></msubsup><mo id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">=</mo><msub id="S3.SS3.p3.3.m3.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.cmml"><mrow id="S3.SS3.p3.3.m3.1.1.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p3.3.m3.1.1.1.1.1.2" xref="S3.SS3.p3.3.m3.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S3.SS3.p3.3.m3.1.1.1.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS3.p3.3.m3.1.1.1.1.1.1.2" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.2.cmml"><mi id="S3.SS3.p3.3.m3.1.1.1.1.1.1.2.2" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.2.2.cmml">j</mi><mo id="S3.SS3.p3.3.m3.1.1.1.1.1.1.2.1" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS3.p3.3.m3.1.1.1.1.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.1.cmml">‚àí</mo><mi id="S3.SS3.p3.3.m3.1.1.1.1.1.1.3" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.3.cmml">j</mi></mrow><mo stretchy="false" id="S3.SS3.p3.3.m3.1.1.1.1.1.3" xref="S3.SS3.p3.3.m3.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.SS3.p3.3.m3.1.1.1.3" xref="S3.SS3.p3.3.m3.1.1.1.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><eq id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2"></eq><apply id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.3.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3">superscript</csymbol><apply id="S3.SS3.p3.3.m3.1.1.3.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.3.2.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.3.2.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3.2.2">‚Ñí</ci><apply id="S3.SS3.p3.3.m3.1.1.3.2.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3.2.3"><times id="S3.SS3.p3.3.m3.1.1.3.2.3.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3.2.3.1"></times><cn type="integer" id="S3.SS3.p3.3.m3.1.1.3.2.3.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3.2.3.2">2</cn><ci id="S3.SS3.p3.3.m3.1.1.3.2.3.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3.2.3.3">ùê∑</ci></apply></apply><apply id="S3.SS3.p3.3.m3.1.1.3.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3"><times id="S3.SS3.p3.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.1"></times><ci id="S3.SS3.p3.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.2">ùëü</ci><ci id="S3.SS3.p3.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.3">ùëí</ci><ci id="S3.SS3.p3.3.m3.1.1.3.3.4.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.4">ùëé</ci><ci id="S3.SS3.p3.3.m3.1.1.3.3.5.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.5">ùëô</ci></apply></apply><apply id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.1">subscript</csymbol><apply id="S3.SS3.p3.3.m3.1.1.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS3.p3.3.m3.1.1.1.1.2.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.1.2">norm</csymbol><apply id="S3.SS3.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1"><minus id="S3.SS3.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.1"></minus><apply id="S3.SS3.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.2"><ci id="S3.SS3.p3.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.2.1">^</ci><ci id="S3.SS3.p3.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.2.2">ùëó</ci></apply><ci id="S3.SS3.p3.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.3">ùëó</ci></apply></apply><cn type="integer" id="S3.SS3.p3.3.m3.1.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\mathcal{L}_{2D}^{real}=||\hat{j}-j||_{2}</annotation></semantics></math>
where <math id="S3.SS3.p3.4.m4.3" class="ltx_Math" alttext="\hat{j}=\Pi(\mathcal{W}\mathcal{M}(\hat{\theta},\hat{\beta}))" display="inline"><semantics id="S3.SS3.p3.4.m4.3a"><mrow id="S3.SS3.p3.4.m4.3.3" xref="S3.SS3.p3.4.m4.3.3.cmml"><mover accent="true" id="S3.SS3.p3.4.m4.3.3.3" xref="S3.SS3.p3.4.m4.3.3.3.cmml"><mi id="S3.SS3.p3.4.m4.3.3.3.2" xref="S3.SS3.p3.4.m4.3.3.3.2.cmml">j</mi><mo id="S3.SS3.p3.4.m4.3.3.3.1" xref="S3.SS3.p3.4.m4.3.3.3.1.cmml">^</mo></mover><mo id="S3.SS3.p3.4.m4.3.3.2" xref="S3.SS3.p3.4.m4.3.3.2.cmml">=</mo><mrow id="S3.SS3.p3.4.m4.3.3.1" xref="S3.SS3.p3.4.m4.3.3.1.cmml"><mi mathvariant="normal" id="S3.SS3.p3.4.m4.3.3.1.3" xref="S3.SS3.p3.4.m4.3.3.1.3.cmml">Œ†</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.4.m4.3.3.1.2" xref="S3.SS3.p3.4.m4.3.3.1.2.cmml">‚Äã</mo><mrow id="S3.SS3.p3.4.m4.3.3.1.1.1" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p3.4.m4.3.3.1.1.1.2" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p3.4.m4.3.3.1.1.1.1" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.4.m4.3.3.1.1.1.1.2" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.2.cmml">ùí≤</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.4.m4.3.3.1.1.1.1.1" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.1.cmml">‚Äã</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.4.m4.3.3.1.1.1.1.3" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.3.cmml">‚Ñ≥</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.4.m4.3.3.1.1.1.1.1a" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.1.cmml">‚Äã</mo><mrow id="S3.SS3.p3.4.m4.3.3.1.1.1.1.4.2" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.SS3.p3.4.m4.3.3.1.1.1.1.4.2.1" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.4.1.cmml">(</mo><mover accent="true" id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.2" xref="S3.SS3.p3.4.m4.1.1.2.cmml">Œ∏</mi><mo id="S3.SS3.p3.4.m4.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.cmml">^</mo></mover><mo id="S3.SS3.p3.4.m4.3.3.1.1.1.1.4.2.2" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.4.1.cmml">,</mo><mover accent="true" id="S3.SS3.p3.4.m4.2.2" xref="S3.SS3.p3.4.m4.2.2.cmml"><mi id="S3.SS3.p3.4.m4.2.2.2" xref="S3.SS3.p3.4.m4.2.2.2.cmml">Œ≤</mi><mo id="S3.SS3.p3.4.m4.2.2.1" xref="S3.SS3.p3.4.m4.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.SS3.p3.4.m4.3.3.1.1.1.1.4.2.3" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.4.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS3.p3.4.m4.3.3.1.1.1.3" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.3b"><apply id="S3.SS3.p3.4.m4.3.3.cmml" xref="S3.SS3.p3.4.m4.3.3"><eq id="S3.SS3.p3.4.m4.3.3.2.cmml" xref="S3.SS3.p3.4.m4.3.3.2"></eq><apply id="S3.SS3.p3.4.m4.3.3.3.cmml" xref="S3.SS3.p3.4.m4.3.3.3"><ci id="S3.SS3.p3.4.m4.3.3.3.1.cmml" xref="S3.SS3.p3.4.m4.3.3.3.1">^</ci><ci id="S3.SS3.p3.4.m4.3.3.3.2.cmml" xref="S3.SS3.p3.4.m4.3.3.3.2">ùëó</ci></apply><apply id="S3.SS3.p3.4.m4.3.3.1.cmml" xref="S3.SS3.p3.4.m4.3.3.1"><times id="S3.SS3.p3.4.m4.3.3.1.2.cmml" xref="S3.SS3.p3.4.m4.3.3.1.2"></times><ci id="S3.SS3.p3.4.m4.3.3.1.3.cmml" xref="S3.SS3.p3.4.m4.3.3.1.3">Œ†</ci><apply id="S3.SS3.p3.4.m4.3.3.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.3.3.1.1.1"><times id="S3.SS3.p3.4.m4.3.3.1.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.1"></times><ci id="S3.SS3.p3.4.m4.3.3.1.1.1.1.2.cmml" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.2">ùí≤</ci><ci id="S3.SS3.p3.4.m4.3.3.1.1.1.1.3.cmml" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.3">‚Ñ≥</ci><interval closure="open" id="S3.SS3.p3.4.m4.3.3.1.1.1.1.4.1.cmml" xref="S3.SS3.p3.4.m4.3.3.1.1.1.1.4.2"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><ci id="S3.SS3.p3.4.m4.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1">^</ci><ci id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2">ùúÉ</ci></apply><apply id="S3.SS3.p3.4.m4.2.2.cmml" xref="S3.SS3.p3.4.m4.2.2"><ci id="S3.SS3.p3.4.m4.2.2.1.cmml" xref="S3.SS3.p3.4.m4.2.2.1">^</ci><ci id="S3.SS3.p3.4.m4.2.2.2.cmml" xref="S3.SS3.p3.4.m4.2.2.2">ùõΩ</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.3c">\hat{j}=\Pi(\mathcal{W}\mathcal{M}(\hat{\theta},\hat{\beta}))</annotation></semantics></math> are the predicted 2D keypoints. Recall that <math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml">ùí≤</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><ci id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">ùí≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">\mathcal{W}</annotation></semantics></math> is the SMPL joint regressor, and <math id="S3.SS3.p3.6.m6.1" class="ltx_Math" alttext="\Pi" display="inline"><semantics id="S3.SS3.p3.6.m6.1a"><mi mathvariant="normal" id="S3.SS3.p3.6.m6.1.1" xref="S3.SS3.p3.6.m6.1.1.cmml">Œ†</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.1b"><ci id="S3.SS3.p3.6.m6.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1">Œ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.1c">\Pi</annotation></semantics></math> is the projection matrix of a weak perspective camera.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.2" class="ltx_p">Given this task setting, Diffusion-HPC can be used to generate synthetic data that has image-mesh pairs (<math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{I}^{*}" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><msup id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">‚Ñê</mi><mo id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">superscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">‚Ñê</ci><times id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\mathcal{I}^{*}</annotation></semantics></math> and <math id="S3.SS3.p4.2.m2.3" class="ltx_Math" alttext="\{\theta,\beta,\Pi\}" display="inline"><semantics id="S3.SS3.p4.2.m2.3a"><mrow id="S3.SS3.p4.2.m2.3.4.2" xref="S3.SS3.p4.2.m2.3.4.1.cmml"><mo stretchy="false" id="S3.SS3.p4.2.m2.3.4.2.1" xref="S3.SS3.p4.2.m2.3.4.1.cmml">{</mo><mi id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">Œ∏</mi><mo id="S3.SS3.p4.2.m2.3.4.2.2" xref="S3.SS3.p4.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS3.p4.2.m2.2.2" xref="S3.SS3.p4.2.m2.2.2.cmml">Œ≤</mi><mo id="S3.SS3.p4.2.m2.3.4.2.3" xref="S3.SS3.p4.2.m2.3.4.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p4.2.m2.3.3" xref="S3.SS3.p4.2.m2.3.3.cmml">Œ†</mi><mo stretchy="false" id="S3.SS3.p4.2.m2.3.4.2.4" xref="S3.SS3.p4.2.m2.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.3b"><set id="S3.SS3.p4.2.m2.3.4.1.cmml" xref="S3.SS3.p4.2.m2.3.4.2"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">ùúÉ</ci><ci id="S3.SS3.p4.2.m2.2.2.cmml" xref="S3.SS3.p4.2.m2.2.2">ùõΩ</ci><ci id="S3.SS3.p4.2.m2.3.3.cmml" xref="S3.SS3.p4.2.m2.3.3">Œ†</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.3c">\{\theta,\beta,\Pi\}</annotation></semantics></math> in Equations <a href="#S3.E1" title="Equation 1 ‚Ä£ 3.2 Data generation process ‚Ä£ 3 Diffusion-HPC ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S3.E6" title="Equation 6 ‚Ä£ 3.2 Data generation process ‚Ä£ 3 Diffusion-HPC ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). Then, on those synthetic image-mesh pairs, we can supervise the model with ground truth body parameters, which provide stronger form of supervision as compared to 2D keypoints.</p>
<table id="S5.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E7.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{3D}^{syn}" display="inline"><semantics id="S3.E7.m1.1a"><msubsup id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.1.1.2.2" xref="S3.E7.m1.1.1.2.2.cmml">‚Ñí</mi><mrow id="S3.E7.m1.1.1.2.3" xref="S3.E7.m1.1.1.2.3.cmml"><mn id="S3.E7.m1.1.1.2.3.2" xref="S3.E7.m1.1.1.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.2.3.1" xref="S3.E7.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E7.m1.1.1.2.3.3" xref="S3.E7.m1.1.1.2.3.3.cmml">D</mi></mrow><mrow id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.3.1" xref="S3.E7.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E7.m1.1.1.3.3" xref="S3.E7.m1.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.3.1a" xref="S3.E7.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E7.m1.1.1.3.4" xref="S3.E7.m1.1.1.3.4.cmml">n</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1">superscript</csymbol><apply id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.2.1.cmml" xref="S3.E7.m1.1.1">subscript</csymbol><ci id="S3.E7.m1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.2.2">‚Ñí</ci><apply id="S3.E7.m1.1.1.2.3.cmml" xref="S3.E7.m1.1.1.2.3"><times id="S3.E7.m1.1.1.2.3.1.cmml" xref="S3.E7.m1.1.1.2.3.1"></times><cn type="integer" id="S3.E7.m1.1.1.2.3.2.cmml" xref="S3.E7.m1.1.1.2.3.2">3</cn><ci id="S3.E7.m1.1.1.2.3.3.cmml" xref="S3.E7.m1.1.1.2.3.3">ùê∑</ci></apply></apply><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><times id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3.1"></times><ci id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2">ùë†</ci><ci id="S3.E7.m1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.3.3">ùë¶</ci><ci id="S3.E7.m1.1.1.3.4.cmml" xref="S3.E7.m1.1.1.3.4">ùëõ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\displaystyle\mathcal{L}_{3D}^{syn}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E7.m2.2" class="ltx_Math" alttext="\displaystyle=||\hat{\beta}-\beta||_{2}+||\hat{\theta}-\theta||_{2}" display="inline"><semantics id="S3.E7.m2.2a"><mrow id="S3.E7.m2.2.2" xref="S3.E7.m2.2.2.cmml"><mi id="S3.E7.m2.2.2.4" xref="S3.E7.m2.2.2.4.cmml"></mi><mo id="S3.E7.m2.2.2.3" xref="S3.E7.m2.2.2.3.cmml">=</mo><mrow id="S3.E7.m2.2.2.2" xref="S3.E7.m2.2.2.2.cmml"><msub id="S3.E7.m2.1.1.1.1" xref="S3.E7.m2.1.1.1.1.cmml"><mrow id="S3.E7.m2.1.1.1.1.1.1" xref="S3.E7.m2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E7.m2.1.1.1.1.1.1.2" xref="S3.E7.m2.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E7.m2.1.1.1.1.1.1.1" xref="S3.E7.m2.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E7.m2.1.1.1.1.1.1.1.2" xref="S3.E7.m2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E7.m2.1.1.1.1.1.1.1.2.2" xref="S3.E7.m2.1.1.1.1.1.1.1.2.2.cmml">Œ≤</mi><mo id="S3.E7.m2.1.1.1.1.1.1.1.2.1" xref="S3.E7.m2.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S3.E7.m2.1.1.1.1.1.1.1.1" xref="S3.E7.m2.1.1.1.1.1.1.1.1.cmml">‚àí</mo><mi id="S3.E7.m2.1.1.1.1.1.1.1.3" xref="S3.E7.m2.1.1.1.1.1.1.1.3.cmml">Œ≤</mi></mrow><mo stretchy="false" id="S3.E7.m2.1.1.1.1.1.1.3" xref="S3.E7.m2.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E7.m2.1.1.1.1.3" xref="S3.E7.m2.1.1.1.1.3.cmml">2</mn></msub><mo id="S3.E7.m2.2.2.2.3" xref="S3.E7.m2.2.2.2.3.cmml">+</mo><msub id="S3.E7.m2.2.2.2.2" xref="S3.E7.m2.2.2.2.2.cmml"><mrow id="S3.E7.m2.2.2.2.2.1.1" xref="S3.E7.m2.2.2.2.2.1.2.cmml"><mo stretchy="false" id="S3.E7.m2.2.2.2.2.1.1.2" xref="S3.E7.m2.2.2.2.2.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E7.m2.2.2.2.2.1.1.1" xref="S3.E7.m2.2.2.2.2.1.1.1.cmml"><mover accent="true" id="S3.E7.m2.2.2.2.2.1.1.1.2" xref="S3.E7.m2.2.2.2.2.1.1.1.2.cmml"><mi id="S3.E7.m2.2.2.2.2.1.1.1.2.2" xref="S3.E7.m2.2.2.2.2.1.1.1.2.2.cmml">Œ∏</mi><mo id="S3.E7.m2.2.2.2.2.1.1.1.2.1" xref="S3.E7.m2.2.2.2.2.1.1.1.2.1.cmml">^</mo></mover><mo id="S3.E7.m2.2.2.2.2.1.1.1.1" xref="S3.E7.m2.2.2.2.2.1.1.1.1.cmml">‚àí</mo><mi id="S3.E7.m2.2.2.2.2.1.1.1.3" xref="S3.E7.m2.2.2.2.2.1.1.1.3.cmml">Œ∏</mi></mrow><mo stretchy="false" id="S3.E7.m2.2.2.2.2.1.1.3" xref="S3.E7.m2.2.2.2.2.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E7.m2.2.2.2.2.3" xref="S3.E7.m2.2.2.2.2.3.cmml">2</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m2.2b"><apply id="S3.E7.m2.2.2.cmml" xref="S3.E7.m2.2.2"><eq id="S3.E7.m2.2.2.3.cmml" xref="S3.E7.m2.2.2.3"></eq><csymbol cd="latexml" id="S3.E7.m2.2.2.4.cmml" xref="S3.E7.m2.2.2.4">absent</csymbol><apply id="S3.E7.m2.2.2.2.cmml" xref="S3.E7.m2.2.2.2"><plus id="S3.E7.m2.2.2.2.3.cmml" xref="S3.E7.m2.2.2.2.3"></plus><apply id="S3.E7.m2.1.1.1.1.cmml" xref="S3.E7.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m2.1.1.1.1.2.cmml" xref="S3.E7.m2.1.1.1.1">subscript</csymbol><apply id="S3.E7.m2.1.1.1.1.1.2.cmml" xref="S3.E7.m2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E7.m2.1.1.1.1.1.2.1.cmml" xref="S3.E7.m2.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E7.m2.1.1.1.1.1.1.1.cmml" xref="S3.E7.m2.1.1.1.1.1.1.1"><minus id="S3.E7.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m2.1.1.1.1.1.1.1.1"></minus><apply id="S3.E7.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m2.1.1.1.1.1.1.1.2"><ci id="S3.E7.m2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E7.m2.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E7.m2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E7.m2.1.1.1.1.1.1.1.2.2">ùõΩ</ci></apply><ci id="S3.E7.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m2.1.1.1.1.1.1.1.3">ùõΩ</ci></apply></apply><cn type="integer" id="S3.E7.m2.1.1.1.1.3.cmml" xref="S3.E7.m2.1.1.1.1.3">2</cn></apply><apply id="S3.E7.m2.2.2.2.2.cmml" xref="S3.E7.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m2.2.2.2.2.2.cmml" xref="S3.E7.m2.2.2.2.2">subscript</csymbol><apply id="S3.E7.m2.2.2.2.2.1.2.cmml" xref="S3.E7.m2.2.2.2.2.1.1"><csymbol cd="latexml" id="S3.E7.m2.2.2.2.2.1.2.1.cmml" xref="S3.E7.m2.2.2.2.2.1.1.2">norm</csymbol><apply id="S3.E7.m2.2.2.2.2.1.1.1.cmml" xref="S3.E7.m2.2.2.2.2.1.1.1"><minus id="S3.E7.m2.2.2.2.2.1.1.1.1.cmml" xref="S3.E7.m2.2.2.2.2.1.1.1.1"></minus><apply id="S3.E7.m2.2.2.2.2.1.1.1.2.cmml" xref="S3.E7.m2.2.2.2.2.1.1.1.2"><ci id="S3.E7.m2.2.2.2.2.1.1.1.2.1.cmml" xref="S3.E7.m2.2.2.2.2.1.1.1.2.1">^</ci><ci id="S3.E7.m2.2.2.2.2.1.1.1.2.2.cmml" xref="S3.E7.m2.2.2.2.2.1.1.1.2.2">ùúÉ</ci></apply><ci id="S3.E7.m2.2.2.2.2.1.1.1.3.cmml" xref="S3.E7.m2.2.2.2.2.1.1.1.3">ùúÉ</ci></apply></apply><cn type="integer" id="S3.E7.m2.2.2.2.2.3.cmml" xref="S3.E7.m2.2.2.2.2.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m2.2c">\displaystyle=||\hat{\beta}-\beta||_{2}+||\hat{\theta}-\theta||_{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p4.3" class="ltx_p">Overall, loss during finetuning is <math id="S3.SS3.p4.3.m1.1" class="ltx_Math" alttext="\mathcal{L}=\mathcal{L}_{2D}^{real}+\mathcal{L}_{3D}^{syn}" display="inline"><semantics id="S3.SS3.p4.3.m1.1a"><mrow id="S3.SS3.p4.3.m1.1.1" xref="S3.SS3.p4.3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.3.m1.1.1.2" xref="S3.SS3.p4.3.m1.1.1.2.cmml">‚Ñí</mi><mo id="S3.SS3.p4.3.m1.1.1.1" xref="S3.SS3.p4.3.m1.1.1.1.cmml">=</mo><mrow id="S3.SS3.p4.3.m1.1.1.3" xref="S3.SS3.p4.3.m1.1.1.3.cmml"><msubsup id="S3.SS3.p4.3.m1.1.1.3.2" xref="S3.SS3.p4.3.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.3.m1.1.1.3.2.2.2" xref="S3.SS3.p4.3.m1.1.1.3.2.2.2.cmml">‚Ñí</mi><mrow id="S3.SS3.p4.3.m1.1.1.3.2.2.3" xref="S3.SS3.p4.3.m1.1.1.3.2.2.3.cmml"><mn id="S3.SS3.p4.3.m1.1.1.3.2.2.3.2" xref="S3.SS3.p4.3.m1.1.1.3.2.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m1.1.1.3.2.2.3.1" xref="S3.SS3.p4.3.m1.1.1.3.2.2.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.3.m1.1.1.3.2.2.3.3" xref="S3.SS3.p4.3.m1.1.1.3.2.2.3.3.cmml">D</mi></mrow><mrow id="S3.SS3.p4.3.m1.1.1.3.2.3" xref="S3.SS3.p4.3.m1.1.1.3.2.3.cmml"><mi id="S3.SS3.p4.3.m1.1.1.3.2.3.2" xref="S3.SS3.p4.3.m1.1.1.3.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m1.1.1.3.2.3.1" xref="S3.SS3.p4.3.m1.1.1.3.2.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.3.m1.1.1.3.2.3.3" xref="S3.SS3.p4.3.m1.1.1.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m1.1.1.3.2.3.1a" xref="S3.SS3.p4.3.m1.1.1.3.2.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.3.m1.1.1.3.2.3.4" xref="S3.SS3.p4.3.m1.1.1.3.2.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m1.1.1.3.2.3.1b" xref="S3.SS3.p4.3.m1.1.1.3.2.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.3.m1.1.1.3.2.3.5" xref="S3.SS3.p4.3.m1.1.1.3.2.3.5.cmml">l</mi></mrow></msubsup><mo id="S3.SS3.p4.3.m1.1.1.3.1" xref="S3.SS3.p4.3.m1.1.1.3.1.cmml">+</mo><msubsup id="S3.SS3.p4.3.m1.1.1.3.3" xref="S3.SS3.p4.3.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.3.m1.1.1.3.3.2.2" xref="S3.SS3.p4.3.m1.1.1.3.3.2.2.cmml">‚Ñí</mi><mrow id="S3.SS3.p4.3.m1.1.1.3.3.2.3" xref="S3.SS3.p4.3.m1.1.1.3.3.2.3.cmml"><mn id="S3.SS3.p4.3.m1.1.1.3.3.2.3.2" xref="S3.SS3.p4.3.m1.1.1.3.3.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m1.1.1.3.3.2.3.1" xref="S3.SS3.p4.3.m1.1.1.3.3.2.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.3.m1.1.1.3.3.2.3.3" xref="S3.SS3.p4.3.m1.1.1.3.3.2.3.3.cmml">D</mi></mrow><mrow id="S3.SS3.p4.3.m1.1.1.3.3.3" xref="S3.SS3.p4.3.m1.1.1.3.3.3.cmml"><mi id="S3.SS3.p4.3.m1.1.1.3.3.3.2" xref="S3.SS3.p4.3.m1.1.1.3.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m1.1.1.3.3.3.1" xref="S3.SS3.p4.3.m1.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.3.m1.1.1.3.3.3.3" xref="S3.SS3.p4.3.m1.1.1.3.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m1.1.1.3.3.3.1a" xref="S3.SS3.p4.3.m1.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.3.m1.1.1.3.3.3.4" xref="S3.SS3.p4.3.m1.1.1.3.3.3.4.cmml">n</mi></mrow></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m1.1b"><apply id="S3.SS3.p4.3.m1.1.1.cmml" xref="S3.SS3.p4.3.m1.1.1"><eq id="S3.SS3.p4.3.m1.1.1.1.cmml" xref="S3.SS3.p4.3.m1.1.1.1"></eq><ci id="S3.SS3.p4.3.m1.1.1.2.cmml" xref="S3.SS3.p4.3.m1.1.1.2">‚Ñí</ci><apply id="S3.SS3.p4.3.m1.1.1.3.cmml" xref="S3.SS3.p4.3.m1.1.1.3"><plus id="S3.SS3.p4.3.m1.1.1.3.1.cmml" xref="S3.SS3.p4.3.m1.1.1.3.1"></plus><apply id="S3.SS3.p4.3.m1.1.1.3.2.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m1.1.1.3.2.1.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2">superscript</csymbol><apply id="S3.SS3.p4.3.m1.1.1.3.2.2.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m1.1.1.3.2.2.1.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS3.p4.3.m1.1.1.3.2.2.2.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.2.2">‚Ñí</ci><apply id="S3.SS3.p4.3.m1.1.1.3.2.2.3.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.2.3"><times id="S3.SS3.p4.3.m1.1.1.3.2.2.3.1.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.2.3.1"></times><cn type="integer" id="S3.SS3.p4.3.m1.1.1.3.2.2.3.2.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.2.3.2">2</cn><ci id="S3.SS3.p4.3.m1.1.1.3.2.2.3.3.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.2.3.3">ùê∑</ci></apply></apply><apply id="S3.SS3.p4.3.m1.1.1.3.2.3.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.3"><times id="S3.SS3.p4.3.m1.1.1.3.2.3.1.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.3.1"></times><ci id="S3.SS3.p4.3.m1.1.1.3.2.3.2.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.3.2">ùëü</ci><ci id="S3.SS3.p4.3.m1.1.1.3.2.3.3.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.3.3">ùëí</ci><ci id="S3.SS3.p4.3.m1.1.1.3.2.3.4.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.3.4">ùëé</ci><ci id="S3.SS3.p4.3.m1.1.1.3.2.3.5.cmml" xref="S3.SS3.p4.3.m1.1.1.3.2.3.5">ùëô</ci></apply></apply><apply id="S3.SS3.p4.3.m1.1.1.3.3.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m1.1.1.3.3.1.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3">superscript</csymbol><apply id="S3.SS3.p4.3.m1.1.1.3.3.2.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m1.1.1.3.3.2.1.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS3.p4.3.m1.1.1.3.3.2.2.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3.2.2">‚Ñí</ci><apply id="S3.SS3.p4.3.m1.1.1.3.3.2.3.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3.2.3"><times id="S3.SS3.p4.3.m1.1.1.3.3.2.3.1.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3.2.3.1"></times><cn type="integer" id="S3.SS3.p4.3.m1.1.1.3.3.2.3.2.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3.2.3.2">3</cn><ci id="S3.SS3.p4.3.m1.1.1.3.3.2.3.3.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3.2.3.3">ùê∑</ci></apply></apply><apply id="S3.SS3.p4.3.m1.1.1.3.3.3.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3.3"><times id="S3.SS3.p4.3.m1.1.1.3.3.3.1.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3.3.1"></times><ci id="S3.SS3.p4.3.m1.1.1.3.3.3.2.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3.3.2">ùë†</ci><ci id="S3.SS3.p4.3.m1.1.1.3.3.3.3.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3.3.3">ùë¶</ci><ci id="S3.SS3.p4.3.m1.1.1.3.3.3.4.cmml" xref="S3.SS3.p4.3.m1.1.1.3.3.3.4">ùëõ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m1.1c">\mathcal{L}=\mathcal{L}_{2D}^{real}+\mathcal{L}_{3D}^{syn}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Guidance from real images.</span>
In the case of a clear target data domain for the HMR task, it can be useful to produce training data that have similar appearances to the target training set. Specifically, instead of using T2I diffusion model to generate the initial <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">‚Ñê</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">‚Ñê</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\mathcal{I}</annotation></semantics></math>, we use real images from the training set. This guidance helps reduce the domain gap between the generated and real images because the appearances and poses will be more similar to the expected ones.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p"><span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">Pose augmentation.</span>
Finally, we can further enhance the diversity of the generated poses, by applying pose augmentations to the predicted poses. Specifically, after Equation <a href="#S3.E1" title="Equation 1 ‚Ä£ 3.2 Data generation process ‚Ä£ 3 Diffusion-HPC ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can augment <math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><mi id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><ci id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">\theta</annotation></semantics></math> before proceeding to Equation <a href="#S3.E2" title="Equation 2 ‚Ä£ 3.2 Data generation process ‚Ä£ 3 Diffusion-HPC ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Formally, we apply pose augmentation in the embedding space of VPoser as in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Weng et¬†al.</span> [<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>,
</p>
<table id="S5.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E8.m1.2" class="ltx_Math" alttext="\displaystyle\mu,\sigma=" display="inline"><semantics id="S3.E8.m1.2a"><mrow id="S3.E8.m1.2.3" xref="S3.E8.m1.2.3.cmml"><mrow id="S3.E8.m1.2.3.2.2" xref="S3.E8.m1.2.3.2.1.cmml"><mi id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml">Œº</mi><mo id="S3.E8.m1.2.3.2.2.1" xref="S3.E8.m1.2.3.2.1.cmml">,</mo><mi id="S3.E8.m1.2.2" xref="S3.E8.m1.2.2.cmml">œÉ</mi></mrow><mo id="S3.E8.m1.2.3.1" xref="S3.E8.m1.2.3.1.cmml">=</mo><mi id="S3.E8.m1.2.3.3" xref="S3.E8.m1.2.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.2b"><apply id="S3.E8.m1.2.3.cmml" xref="S3.E8.m1.2.3"><eq id="S3.E8.m1.2.3.1.cmml" xref="S3.E8.m1.2.3.1"></eq><list id="S3.E8.m1.2.3.2.1.cmml" xref="S3.E8.m1.2.3.2.2"><ci id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1">ùúá</ci><ci id="S3.E8.m1.2.2.cmml" xref="S3.E8.m1.2.2">ùúé</ci></list><csymbol cd="latexml" id="S3.E8.m1.2.3.3.cmml" xref="S3.E8.m1.2.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.2c">\displaystyle\mu,\sigma=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E8.m2.1" class="ltx_Math" alttext="\displaystyle\mathcal{E}_{v}(\theta)" display="inline"><semantics id="S3.E8.m2.1a"><mrow id="S3.E8.m2.1.2" xref="S3.E8.m2.1.2.cmml"><msub id="S3.E8.m2.1.2.2" xref="S3.E8.m2.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E8.m2.1.2.2.2" xref="S3.E8.m2.1.2.2.2.cmml">‚Ñ∞</mi><mi id="S3.E8.m2.1.2.2.3" xref="S3.E8.m2.1.2.2.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E8.m2.1.2.1" xref="S3.E8.m2.1.2.1.cmml">‚Äã</mo><mrow id="S3.E8.m2.1.2.3.2" xref="S3.E8.m2.1.2.cmml"><mo stretchy="false" id="S3.E8.m2.1.2.3.2.1" xref="S3.E8.m2.1.2.cmml">(</mo><mi id="S3.E8.m2.1.1" xref="S3.E8.m2.1.1.cmml">Œ∏</mi><mo stretchy="false" id="S3.E8.m2.1.2.3.2.2" xref="S3.E8.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m2.1b"><apply id="S3.E8.m2.1.2.cmml" xref="S3.E8.m2.1.2"><times id="S3.E8.m2.1.2.1.cmml" xref="S3.E8.m2.1.2.1"></times><apply id="S3.E8.m2.1.2.2.cmml" xref="S3.E8.m2.1.2.2"><csymbol cd="ambiguous" id="S3.E8.m2.1.2.2.1.cmml" xref="S3.E8.m2.1.2.2">subscript</csymbol><ci id="S3.E8.m2.1.2.2.2.cmml" xref="S3.E8.m2.1.2.2.2">‚Ñ∞</ci><ci id="S3.E8.m2.1.2.2.3.cmml" xref="S3.E8.m2.1.2.2.3">ùë£</ci></apply><ci id="S3.E8.m2.1.1.cmml" xref="S3.E8.m2.1.1">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m2.1c">\displaystyle\mathcal{E}_{v}(\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E9.m1.1" class="ltx_Math" alttext="\displaystyle z_{\theta}^{aug}=" display="inline"><semantics id="S3.E9.m1.1a"><mrow id="S3.E9.m1.1.1" xref="S3.E9.m1.1.1.cmml"><msubsup id="S3.E9.m1.1.1.2" xref="S3.E9.m1.1.1.2.cmml"><mi id="S3.E9.m1.1.1.2.2.2" xref="S3.E9.m1.1.1.2.2.2.cmml">z</mi><mi id="S3.E9.m1.1.1.2.2.3" xref="S3.E9.m1.1.1.2.2.3.cmml">Œ∏</mi><mrow id="S3.E9.m1.1.1.2.3" xref="S3.E9.m1.1.1.2.3.cmml"><mi id="S3.E9.m1.1.1.2.3.2" xref="S3.E9.m1.1.1.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.1.2.3.1" xref="S3.E9.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E9.m1.1.1.2.3.3" xref="S3.E9.m1.1.1.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.1.2.3.1a" xref="S3.E9.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E9.m1.1.1.2.3.4" xref="S3.E9.m1.1.1.2.3.4.cmml">g</mi></mrow></msubsup><mo id="S3.E9.m1.1.1.1" xref="S3.E9.m1.1.1.1.cmml">=</mo><mi id="S3.E9.m1.1.1.3" xref="S3.E9.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.1b"><apply id="S3.E9.m1.1.1.cmml" xref="S3.E9.m1.1.1"><eq id="S3.E9.m1.1.1.1.cmml" xref="S3.E9.m1.1.1.1"></eq><apply id="S3.E9.m1.1.1.2.cmml" xref="S3.E9.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.2.1.cmml" xref="S3.E9.m1.1.1.2">superscript</csymbol><apply id="S3.E9.m1.1.1.2.2.cmml" xref="S3.E9.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.2.2.1.cmml" xref="S3.E9.m1.1.1.2">subscript</csymbol><ci id="S3.E9.m1.1.1.2.2.2.cmml" xref="S3.E9.m1.1.1.2.2.2">ùëß</ci><ci id="S3.E9.m1.1.1.2.2.3.cmml" xref="S3.E9.m1.1.1.2.2.3">ùúÉ</ci></apply><apply id="S3.E9.m1.1.1.2.3.cmml" xref="S3.E9.m1.1.1.2.3"><times id="S3.E9.m1.1.1.2.3.1.cmml" xref="S3.E9.m1.1.1.2.3.1"></times><ci id="S3.E9.m1.1.1.2.3.2.cmml" xref="S3.E9.m1.1.1.2.3.2">ùëé</ci><ci id="S3.E9.m1.1.1.2.3.3.cmml" xref="S3.E9.m1.1.1.2.3.3">ùë¢</ci><ci id="S3.E9.m1.1.1.2.3.4.cmml" xref="S3.E9.m1.1.1.2.3.4">ùëî</ci></apply></apply><csymbol cd="latexml" id="S3.E9.m1.1.1.3.cmml" xref="S3.E9.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.1c">\displaystyle z_{\theta}^{aug}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E9.m2.4" class="ltx_Math" alttext="\displaystyle z_{\theta}\odot(1+s\epsilon),z_{\theta}\sim\mathcal{N}(\mu,\sigma)" display="inline"><semantics id="S3.E9.m2.4a"><mrow id="S3.E9.m2.4.4" xref="S3.E9.m2.4.4.cmml"><mrow id="S3.E9.m2.4.4.2.2" xref="S3.E9.m2.4.4.2.3.cmml"><mrow id="S3.E9.m2.3.3.1.1.1" xref="S3.E9.m2.3.3.1.1.1.cmml"><msub id="S3.E9.m2.3.3.1.1.1.3" xref="S3.E9.m2.3.3.1.1.1.3.cmml"><mi id="S3.E9.m2.3.3.1.1.1.3.2" xref="S3.E9.m2.3.3.1.1.1.3.2.cmml">z</mi><mi id="S3.E9.m2.3.3.1.1.1.3.3" xref="S3.E9.m2.3.3.1.1.1.3.3.cmml">Œ∏</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E9.m2.3.3.1.1.1.2" xref="S3.E9.m2.3.3.1.1.1.2.cmml">‚äô</mo><mrow id="S3.E9.m2.3.3.1.1.1.1.1" xref="S3.E9.m2.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E9.m2.3.3.1.1.1.1.1.2" xref="S3.E9.m2.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E9.m2.3.3.1.1.1.1.1.1" xref="S3.E9.m2.3.3.1.1.1.1.1.1.cmml"><mn id="S3.E9.m2.3.3.1.1.1.1.1.1.2" xref="S3.E9.m2.3.3.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E9.m2.3.3.1.1.1.1.1.1.1" xref="S3.E9.m2.3.3.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E9.m2.3.3.1.1.1.1.1.1.3" xref="S3.E9.m2.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E9.m2.3.3.1.1.1.1.1.1.3.2" xref="S3.E9.m2.3.3.1.1.1.1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E9.m2.3.3.1.1.1.1.1.1.3.1" xref="S3.E9.m2.3.3.1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E9.m2.3.3.1.1.1.1.1.1.3.3" xref="S3.E9.m2.3.3.1.1.1.1.1.1.3.3.cmml">œµ</mi></mrow></mrow><mo stretchy="false" id="S3.E9.m2.3.3.1.1.1.1.1.3" xref="S3.E9.m2.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E9.m2.4.4.2.2.3" xref="S3.E9.m2.4.4.2.3.cmml">,</mo><msub id="S3.E9.m2.4.4.2.2.2" xref="S3.E9.m2.4.4.2.2.2.cmml"><mi id="S3.E9.m2.4.4.2.2.2.2" xref="S3.E9.m2.4.4.2.2.2.2.cmml">z</mi><mi id="S3.E9.m2.4.4.2.2.2.3" xref="S3.E9.m2.4.4.2.2.2.3.cmml">Œ∏</mi></msub></mrow><mo id="S3.E9.m2.4.4.3" xref="S3.E9.m2.4.4.3.cmml">‚àº</mo><mrow id="S3.E9.m2.4.4.4" xref="S3.E9.m2.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E9.m2.4.4.4.2" xref="S3.E9.m2.4.4.4.2.cmml">ùí©</mi><mo lspace="0em" rspace="0em" id="S3.E9.m2.4.4.4.1" xref="S3.E9.m2.4.4.4.1.cmml">‚Äã</mo><mrow id="S3.E9.m2.4.4.4.3.2" xref="S3.E9.m2.4.4.4.3.1.cmml"><mo stretchy="false" id="S3.E9.m2.4.4.4.3.2.1" xref="S3.E9.m2.4.4.4.3.1.cmml">(</mo><mi id="S3.E9.m2.1.1" xref="S3.E9.m2.1.1.cmml">Œº</mi><mo id="S3.E9.m2.4.4.4.3.2.2" xref="S3.E9.m2.4.4.4.3.1.cmml">,</mo><mi id="S3.E9.m2.2.2" xref="S3.E9.m2.2.2.cmml">œÉ</mi><mo stretchy="false" id="S3.E9.m2.4.4.4.3.2.3" xref="S3.E9.m2.4.4.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m2.4b"><apply id="S3.E9.m2.4.4.cmml" xref="S3.E9.m2.4.4"><csymbol cd="latexml" id="S3.E9.m2.4.4.3.cmml" xref="S3.E9.m2.4.4.3">similar-to</csymbol><list id="S3.E9.m2.4.4.2.3.cmml" xref="S3.E9.m2.4.4.2.2"><apply id="S3.E9.m2.3.3.1.1.1.cmml" xref="S3.E9.m2.3.3.1.1.1"><csymbol cd="latexml" id="S3.E9.m2.3.3.1.1.1.2.cmml" xref="S3.E9.m2.3.3.1.1.1.2">direct-product</csymbol><apply id="S3.E9.m2.3.3.1.1.1.3.cmml" xref="S3.E9.m2.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m2.3.3.1.1.1.3.1.cmml" xref="S3.E9.m2.3.3.1.1.1.3">subscript</csymbol><ci id="S3.E9.m2.3.3.1.1.1.3.2.cmml" xref="S3.E9.m2.3.3.1.1.1.3.2">ùëß</ci><ci id="S3.E9.m2.3.3.1.1.1.3.3.cmml" xref="S3.E9.m2.3.3.1.1.1.3.3">ùúÉ</ci></apply><apply id="S3.E9.m2.3.3.1.1.1.1.1.1.cmml" xref="S3.E9.m2.3.3.1.1.1.1.1"><plus id="S3.E9.m2.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E9.m2.3.3.1.1.1.1.1.1.1"></plus><cn type="integer" id="S3.E9.m2.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E9.m2.3.3.1.1.1.1.1.1.2">1</cn><apply id="S3.E9.m2.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E9.m2.3.3.1.1.1.1.1.1.3"><times id="S3.E9.m2.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E9.m2.3.3.1.1.1.1.1.1.3.1"></times><ci id="S3.E9.m2.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E9.m2.3.3.1.1.1.1.1.1.3.2">ùë†</ci><ci id="S3.E9.m2.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E9.m2.3.3.1.1.1.1.1.1.3.3">italic-œµ</ci></apply></apply></apply><apply id="S3.E9.m2.4.4.2.2.2.cmml" xref="S3.E9.m2.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.E9.m2.4.4.2.2.2.1.cmml" xref="S3.E9.m2.4.4.2.2.2">subscript</csymbol><ci id="S3.E9.m2.4.4.2.2.2.2.cmml" xref="S3.E9.m2.4.4.2.2.2.2">ùëß</ci><ci id="S3.E9.m2.4.4.2.2.2.3.cmml" xref="S3.E9.m2.4.4.2.2.2.3">ùúÉ</ci></apply></list><apply id="S3.E9.m2.4.4.4.cmml" xref="S3.E9.m2.4.4.4"><times id="S3.E9.m2.4.4.4.1.cmml" xref="S3.E9.m2.4.4.4.1"></times><ci id="S3.E9.m2.4.4.4.2.cmml" xref="S3.E9.m2.4.4.4.2">ùí©</ci><interval closure="open" id="S3.E9.m2.4.4.4.3.1.cmml" xref="S3.E9.m2.4.4.4.3.2"><ci id="S3.E9.m2.1.1.cmml" xref="S3.E9.m2.1.1">ùúá</ci><ci id="S3.E9.m2.2.2.cmml" xref="S3.E9.m2.2.2">ùúé</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m2.4c">\displaystyle z_{\theta}\odot(1+s\epsilon),z_{\theta}\sim\mathcal{N}(\mu,\sigma)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
<tbody id="S3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E10.m1.1" class="ltx_Math" alttext="\displaystyle\theta^{aug}=" display="inline"><semantics id="S3.E10.m1.1a"><mrow id="S3.E10.m1.1.1" xref="S3.E10.m1.1.1.cmml"><msup id="S3.E10.m1.1.1.2" xref="S3.E10.m1.1.1.2.cmml"><mi id="S3.E10.m1.1.1.2.2" xref="S3.E10.m1.1.1.2.2.cmml">Œ∏</mi><mrow id="S3.E10.m1.1.1.2.3" xref="S3.E10.m1.1.1.2.3.cmml"><mi id="S3.E10.m1.1.1.2.3.2" xref="S3.E10.m1.1.1.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.1.1.2.3.1" xref="S3.E10.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E10.m1.1.1.2.3.3" xref="S3.E10.m1.1.1.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.1.1.2.3.1a" xref="S3.E10.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E10.m1.1.1.2.3.4" xref="S3.E10.m1.1.1.2.3.4.cmml">g</mi></mrow></msup><mo id="S3.E10.m1.1.1.1" xref="S3.E10.m1.1.1.1.cmml">=</mo><mi id="S3.E10.m1.1.1.3" xref="S3.E10.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m1.1b"><apply id="S3.E10.m1.1.1.cmml" xref="S3.E10.m1.1.1"><eq id="S3.E10.m1.1.1.1.cmml" xref="S3.E10.m1.1.1.1"></eq><apply id="S3.E10.m1.1.1.2.cmml" xref="S3.E10.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.2.1.cmml" xref="S3.E10.m1.1.1.2">superscript</csymbol><ci id="S3.E10.m1.1.1.2.2.cmml" xref="S3.E10.m1.1.1.2.2">ùúÉ</ci><apply id="S3.E10.m1.1.1.2.3.cmml" xref="S3.E10.m1.1.1.2.3"><times id="S3.E10.m1.1.1.2.3.1.cmml" xref="S3.E10.m1.1.1.2.3.1"></times><ci id="S3.E10.m1.1.1.2.3.2.cmml" xref="S3.E10.m1.1.1.2.3.2">ùëé</ci><ci id="S3.E10.m1.1.1.2.3.3.cmml" xref="S3.E10.m1.1.1.2.3.3">ùë¢</ci><ci id="S3.E10.m1.1.1.2.3.4.cmml" xref="S3.E10.m1.1.1.2.3.4">ùëî</ci></apply></apply><csymbol cd="latexml" id="S3.E10.m1.1.1.3.cmml" xref="S3.E10.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.1c">\displaystyle\theta^{aug}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E10.m2.1" class="ltx_Math" alttext="\displaystyle\mathcal{D}_{v}(z_{\theta}^{aug})" display="inline"><semantics id="S3.E10.m2.1a"><mrow id="S3.E10.m2.1.1" xref="S3.E10.m2.1.1.cmml"><msub id="S3.E10.m2.1.1.3" xref="S3.E10.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E10.m2.1.1.3.2" xref="S3.E10.m2.1.1.3.2.cmml">ùíü</mi><mi id="S3.E10.m2.1.1.3.3" xref="S3.E10.m2.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E10.m2.1.1.2" xref="S3.E10.m2.1.1.2.cmml">‚Äã</mo><mrow id="S3.E10.m2.1.1.1.1" xref="S3.E10.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E10.m2.1.1.1.1.2" xref="S3.E10.m2.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E10.m2.1.1.1.1.1" xref="S3.E10.m2.1.1.1.1.1.cmml"><mi id="S3.E10.m2.1.1.1.1.1.2.2" xref="S3.E10.m2.1.1.1.1.1.2.2.cmml">z</mi><mi id="S3.E10.m2.1.1.1.1.1.2.3" xref="S3.E10.m2.1.1.1.1.1.2.3.cmml">Œ∏</mi><mrow id="S3.E10.m2.1.1.1.1.1.3" xref="S3.E10.m2.1.1.1.1.1.3.cmml"><mi id="S3.E10.m2.1.1.1.1.1.3.2" xref="S3.E10.m2.1.1.1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E10.m2.1.1.1.1.1.3.1" xref="S3.E10.m2.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E10.m2.1.1.1.1.1.3.3" xref="S3.E10.m2.1.1.1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E10.m2.1.1.1.1.1.3.1a" xref="S3.E10.m2.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E10.m2.1.1.1.1.1.3.4" xref="S3.E10.m2.1.1.1.1.1.3.4.cmml">g</mi></mrow></msubsup><mo stretchy="false" id="S3.E10.m2.1.1.1.1.3" xref="S3.E10.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m2.1b"><apply id="S3.E10.m2.1.1.cmml" xref="S3.E10.m2.1.1"><times id="S3.E10.m2.1.1.2.cmml" xref="S3.E10.m2.1.1.2"></times><apply id="S3.E10.m2.1.1.3.cmml" xref="S3.E10.m2.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m2.1.1.3.1.cmml" xref="S3.E10.m2.1.1.3">subscript</csymbol><ci id="S3.E10.m2.1.1.3.2.cmml" xref="S3.E10.m2.1.1.3.2">ùíü</ci><ci id="S3.E10.m2.1.1.3.3.cmml" xref="S3.E10.m2.1.1.3.3">ùë£</ci></apply><apply id="S3.E10.m2.1.1.1.1.1.cmml" xref="S3.E10.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E10.m2.1.1.1.1.1.1.cmml" xref="S3.E10.m2.1.1.1.1">superscript</csymbol><apply id="S3.E10.m2.1.1.1.1.1.2.cmml" xref="S3.E10.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E10.m2.1.1.1.1.1.2.1.cmml" xref="S3.E10.m2.1.1.1.1">subscript</csymbol><ci id="S3.E10.m2.1.1.1.1.1.2.2.cmml" xref="S3.E10.m2.1.1.1.1.1.2.2">ùëß</ci><ci id="S3.E10.m2.1.1.1.1.1.2.3.cmml" xref="S3.E10.m2.1.1.1.1.1.2.3">ùúÉ</ci></apply><apply id="S3.E10.m2.1.1.1.1.1.3.cmml" xref="S3.E10.m2.1.1.1.1.1.3"><times id="S3.E10.m2.1.1.1.1.1.3.1.cmml" xref="S3.E10.m2.1.1.1.1.1.3.1"></times><ci id="S3.E10.m2.1.1.1.1.1.3.2.cmml" xref="S3.E10.m2.1.1.1.1.1.3.2">ùëé</ci><ci id="S3.E10.m2.1.1.1.1.1.3.3.cmml" xref="S3.E10.m2.1.1.1.1.1.3.3">ùë¢</ci><ci id="S3.E10.m2.1.1.1.1.1.3.4.cmml" xref="S3.E10.m2.1.1.1.1.1.3.4">ùëî</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m2.1c">\displaystyle\mathcal{D}_{v}(z_{\theta}^{aug})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p6.5" class="ltx_p">where <math id="S3.SS3.p6.2.m1.1" class="ltx_Math" alttext="\mathcal{E}_{v}" display="inline"><semantics id="S3.SS3.p6.2.m1.1a"><msub id="S3.SS3.p6.2.m1.1.1" xref="S3.SS3.p6.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p6.2.m1.1.1.2" xref="S3.SS3.p6.2.m1.1.1.2.cmml">‚Ñ∞</mi><mi id="S3.SS3.p6.2.m1.1.1.3" xref="S3.SS3.p6.2.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m1.1b"><apply id="S3.SS3.p6.2.m1.1.1.cmml" xref="S3.SS3.p6.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.2.m1.1.1.1.cmml" xref="S3.SS3.p6.2.m1.1.1">subscript</csymbol><ci id="S3.SS3.p6.2.m1.1.1.2.cmml" xref="S3.SS3.p6.2.m1.1.1.2">‚Ñ∞</ci><ci id="S3.SS3.p6.2.m1.1.1.3.cmml" xref="S3.SS3.p6.2.m1.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m1.1c">\mathcal{E}_{v}</annotation></semantics></math> and <math id="S3.SS3.p6.3.m2.1" class="ltx_Math" alttext="\mathcal{D}_{v}" display="inline"><semantics id="S3.SS3.p6.3.m2.1a"><msub id="S3.SS3.p6.3.m2.1.1" xref="S3.SS3.p6.3.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p6.3.m2.1.1.2" xref="S3.SS3.p6.3.m2.1.1.2.cmml">ùíü</mi><mi id="S3.SS3.p6.3.m2.1.1.3" xref="S3.SS3.p6.3.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.3.m2.1b"><apply id="S3.SS3.p6.3.m2.1.1.cmml" xref="S3.SS3.p6.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m2.1.1.1.cmml" xref="S3.SS3.p6.3.m2.1.1">subscript</csymbol><ci id="S3.SS3.p6.3.m2.1.1.2.cmml" xref="S3.SS3.p6.3.m2.1.1.2">ùíü</ci><ci id="S3.SS3.p6.3.m2.1.1.3.cmml" xref="S3.SS3.p6.3.m2.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.3.m2.1c">\mathcal{D}_{v}</annotation></semantics></math> are the encoder and decoder of VPoser, <math id="S3.SS3.p6.4.m3.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS3.p6.4.m3.1a"><mi id="S3.SS3.p6.4.m3.1.1" xref="S3.SS3.p6.4.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.4.m3.1b"><ci id="S3.SS3.p6.4.m3.1.1.cmml" xref="S3.SS3.p6.4.m3.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.4.m3.1c">s</annotation></semantics></math> is a constant scalar, and <math id="S3.SS3.p6.5.m4.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS3.p6.5.m4.1a"><mi id="S3.SS3.p6.5.m4.1.1" xref="S3.SS3.p6.5.m4.1.1.cmml">œµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.5.m4.1b"><ci id="S3.SS3.p6.5.m4.1.1.cmml" xref="S3.SS3.p6.5.m4.1.1">italic-œµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.5.m4.1c">\epsilon</annotation></semantics></math> is from a multivariate uniform distribution of the same dimension as the VPoser latent space.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We first show the effectiveness of Diffusion-HPC for improving HMR performance in challenging domains in Sec.¬†<a href="#S4.SS1" title="4.1 Finetuning on challenging HMR settings ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. Then, in Sec.¬†<a href="#S4.SS2" title="4.2 Image generation quality ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> we present comparisons on the synthetic data generation quality of Diffusion-HPC.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Finetuning on challenging HMR settings</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We demonstrate the potential of Diffusion-HPC through the task of few-shot adaptation of human mesh recovery models. We consider the setting where a small set of real images with 2D keypoints are available. This represents a typical scenario where we want to deploy a pre-trained HMR model on a new domain but there is limited ground truth annotations on the target domain. Through our experiments, we show that training with synthetic data from Diffusion-HPC improves HMR on challenging target domains as compared to previous adaptation methods.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We use the following sports datasets as they contain much more challenging poses than common HMR benchmarks. As a result, there is a large domain gap when applying pre-trained HMR models on those datasets, and finetuning is necessarily to close the domain gap. Pre-processing details are in the <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">Supplementary Material</span>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2303.09541/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Qualitative HMR results on SMART and Ski-Pose datasets. Finetuning with data from Diffusion-HPC (rightmost) helps HMR models learn novel poses from challenging domains.</span></figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Ski-Pose <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite></span> includes 3D and 2D keypoints labels from 5 professional ski athletes in motion. There is a significant domain gap between ski poses and poses from other human pose estimation datasets, therefore Ski-Pose has been used as a benchmark in evaluating pose domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Sports Motion and Recognition Tasks (SMART) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite></span> contains videos with per-frame 2D keypoints for various competitive sports. We consider 6 publicly released categories except for ‚Äúbadminton‚Äù, which only contains one clip.
We sample enough clips so that the training set contains roughly <math id="S4.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.I1.i2.p1.1.m1.1a"><mn id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><cn type="integer" id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">100</annotation></semantics></math> images per category, and evaluate our finetuned models on the remaining images.</p>
</div>
</li>
</ul>
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Evaluation metrics.</span> For Ski-Pose, we use Mean Per Joint Position Error (MPJPE) and Procrustes-Aligned MPJPE (PA-MPJPE) as our evaluation metrics. PA-MPJPE measures MPJPE after performing Procrustes alignment of the predicted and ground truth keypoints. SMART does not have ground truth 3D keypoints, so we report Percentage of Correct Keypoint (PCK) determined by distance between predicted and ground truth keypoints in pixels.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:397.5pt;height:102.7pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-151.5pt,38.9pt) scale(0.567386563204273,0.567386563204273) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_top">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.1.2.1" class="ltx_text">Method</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.1.3.1" class="ltx_text">Ft.</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.1.4.1" class="ltx_text">Ft. with <span id="S4.T1.1.1.1.4.1.1" class="ltx_text ltx_font_italic">syn</span></span></th>
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="6">PCK (<math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>) per Action</th>
<th id="S4.T1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.1.5.1" class="ltx_text">Mean</span></th>
</tr>
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Diving</th>
<th id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Pole Vault</th>
<th id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">High Jump</th>
<th id="S4.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Uneven Bars</th>
<th id="S4.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Balance Beam</th>
<th id="S4.T1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Vault</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.3.1" class="ltx_tr">
<th id="S4.T1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SPIN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>
</th>
<th id="S4.T1.1.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.1.3.1.2.1" class="ltx_text" style="color:#AD0000;">‚úó</span></th>
<td id="S4.T1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">63.1</td>
<td id="S4.T1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">60.0</td>
<td id="S4.T1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">73.3</td>
<td id="S4.T1.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">36.2</td>
<td id="S4.T1.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">74.2</td>
<td id="S4.T1.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">61.4</td>
<td id="S4.T1.1.1.3.1.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">50.4</td>
</tr>
<tr id="S4.T1.1.1.4.2" class="ltx_tr">
<th id="S4.T1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>
</th>
<th id="S4.T1.1.1.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.4.2.2.1" class="ltx_text" style="color:#AD0000;">‚úó</span></th>
<td id="S4.T1.1.1.4.2.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.1.4.2.4" class="ltx_td ltx_align_center">55.9</td>
<td id="S4.T1.1.1.4.2.5" class="ltx_td ltx_align_center">52.5</td>
<td id="S4.T1.1.1.4.2.6" class="ltx_td ltx_align_center">68.8</td>
<td id="S4.T1.1.1.4.2.7" class="ltx_td ltx_align_center">12.9</td>
<td id="S4.T1.1.1.4.2.8" class="ltx_td ltx_align_center">62.0</td>
<td id="S4.T1.1.1.4.2.9" class="ltx_td ltx_align_center">38.9</td>
<td id="S4.T1.1.1.4.2.10" class="ltx_td ltx_nopad_r ltx_align_center">48.5</td>
</tr>
<tr id="S4.T1.1.1.5.3" class="ltx_tr">
<th id="S4.T1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PARE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</th>
<th id="S4.T1.1.1.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.5.3.2.1" class="ltx_text" style="color:#AD0000;">‚úó</span></th>
<td id="S4.T1.1.1.5.3.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.1.5.3.4" class="ltx_td ltx_align_center">63.3</td>
<td id="S4.T1.1.1.5.3.5" class="ltx_td ltx_align_center">65.2</td>
<td id="S4.T1.1.1.5.3.6" class="ltx_td ltx_align_center">77.9</td>
<td id="S4.T1.1.1.5.3.7" class="ltx_td ltx_align_center">31.8</td>
<td id="S4.T1.1.1.5.3.8" class="ltx_td ltx_align_center">71.2</td>
<td id="S4.T1.1.1.5.3.9" class="ltx_td ltx_align_center">53.9</td>
<td id="S4.T1.1.1.5.3.10" class="ltx_td ltx_nopad_r ltx_align_center">60.5</td>
</tr>
<tr id="S4.T1.1.1.6.4" class="ltx_tr">
<th id="S4.T1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BEDLAM-CLIFF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>
</th>
<th id="S4.T1.1.1.6.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.6.4.2.1" class="ltx_text" style="color:#AD0000;">‚úó</span></th>
<td id="S4.T1.1.1.6.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.1.6.4.4" class="ltx_td ltx_align_center">30.4</td>
<td id="S4.T1.1.1.6.4.5" class="ltx_td ltx_align_center">57.3</td>
<td id="S4.T1.1.1.6.4.6" class="ltx_td ltx_align_center">67.4</td>
<td id="S4.T1.1.1.6.4.7" class="ltx_td ltx_align_center">31.1</td>
<td id="S4.T1.1.1.6.4.8" class="ltx_td ltx_align_center">55.7</td>
<td id="S4.T1.1.1.6.4.9" class="ltx_td ltx_align_center">48.3</td>
<td id="S4.T1.1.1.6.4.10" class="ltx_td ltx_nopad_r ltx_align_center">48.4</td>
</tr>
<tr id="S4.T1.1.1.7.5" class="ltx_tr">
<th id="S4.T1.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SPIN-ft <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>
</th>
<th id="S4.T1.1.1.7.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.1.7.5.2.1" class="ltx_text" style="color:#008000;">‚úì</span></th>
<td id="S4.T1.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.7.5.3.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T1.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_t">74.3</td>
<td id="S4.T1.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_t">73.5</td>
<td id="S4.T1.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_t">78.1</td>
<td id="S4.T1.1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_t">41.9</td>
<td id="S4.T1.1.1.7.5.8" class="ltx_td ltx_align_center ltx_border_t">84.1</td>
<td id="S4.T1.1.1.7.5.9" class="ltx_td ltx_align_center ltx_border_t">64.3</td>
<td id="S4.T1.1.1.7.5.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">69.3</td>
</tr>
<tr id="S4.T1.1.1.8.6" class="ltx_tr">
<th id="S4.T1.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DAPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</th>
<th id="S4.T1.1.1.8.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.8.6.2.1" class="ltx_text" style="color:#008000;">‚úì</span></th>
<td id="S4.T1.1.1.8.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.8.6.3.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T1.1.1.8.6.4" class="ltx_td ltx_align_center">70.9</td>
<td id="S4.T1.1.1.8.6.5" class="ltx_td ltx_align_center">64.8</td>
<td id="S4.T1.1.1.8.6.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.8.6.6.1" class="ltx_text ltx_font_bold">79.4</span></td>
<td id="S4.T1.1.1.8.6.7" class="ltx_td ltx_align_center">42.0</td>
<td id="S4.T1.1.1.8.6.8" class="ltx_td ltx_align_center">79.4</td>
<td id="S4.T1.1.1.8.6.9" class="ltx_td ltx_align_center">64.5</td>
<td id="S4.T1.1.1.8.6.10" class="ltx_td ltx_nopad_r ltx_align_center">66.8</td>
</tr>
<tr id="S4.T1.1.1.9.7" class="ltx_tr">
<th id="S4.T1.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>
</th>
<th id="S4.T1.1.1.9.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.9.7.2.1" class="ltx_text" style="color:#008000;">‚úì</span></th>
<td id="S4.T1.1.1.9.7.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.9.7.3.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T1.1.1.9.7.4" class="ltx_td ltx_align_center">70.6</td>
<td id="S4.T1.1.1.9.7.5" class="ltx_td ltx_align_center">65.3</td>
<td id="S4.T1.1.1.9.7.6" class="ltx_td ltx_align_center">74.2</td>
<td id="S4.T1.1.1.9.7.7" class="ltx_td ltx_align_center">43.4</td>
<td id="S4.T1.1.1.9.7.8" class="ltx_td ltx_align_center">83.6</td>
<td id="S4.T1.1.1.9.7.9" class="ltx_td ltx_align_center">62.3</td>
<td id="S4.T1.1.1.9.7.10" class="ltx_td ltx_nopad_r ltx_align_center">66.6</td>
</tr>
<tr id="S4.T1.1.1.10.8" class="ltx_tr">
<th id="S4.T1.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Ours</th>
<th id="S4.T1.1.1.10.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T1.1.1.10.8.2.1" class="ltx_text" style="color:#008000;">‚úì</span></th>
<td id="S4.T1.1.1.10.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.10.8.3.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T1.1.1.10.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.10.8.4.1" class="ltx_text ltx_font_bold">79.2</span></td>
<td id="S4.T1.1.1.10.8.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.10.8.5.1" class="ltx_text ltx_font_bold">77.7</span></td>
<td id="S4.T1.1.1.10.8.6" class="ltx_td ltx_align_center ltx_border_bb">78.1</td>
<td id="S4.T1.1.1.10.8.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.10.8.7.1" class="ltx_text ltx_font_bold">44.1</span></td>
<td id="S4.T1.1.1.10.8.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.10.8.8.1" class="ltx_text ltx_font_bold">85.1</span></td>
<td id="S4.T1.1.1.10.8.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.10.8.9.1" class="ltx_text ltx_font_bold">66.9</span></td>
<td id="S4.T1.1.1.10.8.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.10.8.10.1" class="ltx_text ltx_font_bold">71.9</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.4.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.5.2" class="ltx_text" style="font-size:90%;">Quantitative results (PCK) on SMART. Ft indicates fine
tuning on test data. Best numbers are in <span id="S4.T1.5.2.1" class="ltx_text ltx_font_bold">bold</span>.</span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.8" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:397.5pt;height:159.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-48.2pt,19.3pt) scale(0.804734256452752,0.804734256452752) ;">
<table id="S4.T2.8.8" class="ltx_tabular ltx_guessed_headers ltx_align_top">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.2.3.1" class="ltx_text" style="font-size:70%;">Method</span></th>
<th id="S4.T2.2.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.2.4.1" class="ltx_text" style="font-size:70%;">Ft.</span></th>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.2.5.1" class="ltx_text" style="font-size:70%;">Ft. with <span id="S4.T2.2.2.2.5.1.1" class="ltx_text ltx_font_italic">syn</span></span></td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">
<span id="S4.T2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">MPJPE (</span><math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo mathsize="70%" stretchy="false" id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math><span id="S4.T2.2.2.2.2.2" class="ltx_text" style="font-size:70%;">) / PA-MPJPE (</span><math id="S4.T2.2.2.2.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.2.2.2.2.m2.1a"><mo mathsize="70%" stretchy="false" id="S4.T2.2.2.2.2.m2.1.1" xref="S4.T2.2.2.2.2.m2.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m2.1b"><ci id="S4.T2.2.2.2.2.m2.1.1.cmml" xref="S4.T2.2.2.2.2.m2.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m2.1c">\downarrow</annotation></semantics></math><span id="S4.T2.2.2.2.2.3" class="ltx_text" style="font-size:70%;">)</span>
</td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S4.T2.7.7.7" class="ltx_tr">
<td id="S4.T2.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.3.3.3.1.1" class="ltx_text" style="font-size:70%;">0</span><math id="S4.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T2.3.3.3.1.m1.1a"><mo mathsize="70%" id="S4.T2.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\%</annotation></semantics></math><span id="S4.T2.3.3.3.1.2" class="ltx_text" style="font-size:70%;"> train</span>
</td>
<td id="S4.T2.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.4.4.4.2.1" class="ltx_text" style="font-size:70%;">1</span><math id="S4.T2.4.4.4.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T2.4.4.4.2.m1.1a"><mo mathsize="70%" id="S4.T2.4.4.4.2.m1.1.1" xref="S4.T2.4.4.4.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.2.m1.1b"><csymbol cd="latexml" id="S4.T2.4.4.4.2.m1.1.1.cmml" xref="S4.T2.4.4.4.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.2.m1.1c">\%</annotation></semantics></math><span id="S4.T2.4.4.4.2.2" class="ltx_text" style="font-size:70%;"> train</span>
</td>
<td id="S4.T2.5.5.5.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.5.5.5.3.1" class="ltx_text" style="font-size:70%;">5</span><math id="S4.T2.5.5.5.3.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T2.5.5.5.3.m1.1a"><mo mathsize="70%" id="S4.T2.5.5.5.3.m1.1.1" xref="S4.T2.5.5.5.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.3.m1.1b"><csymbol cd="latexml" id="S4.T2.5.5.5.3.m1.1.1.cmml" xref="S4.T2.5.5.5.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.3.m1.1c">\%</annotation></semantics></math><span id="S4.T2.5.5.5.3.2" class="ltx_text" style="font-size:70%;"> train</span>
</td>
<td id="S4.T2.6.6.6.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.6.6.6.4.1" class="ltx_text" style="font-size:70%;">50</span><math id="S4.T2.6.6.6.4.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T2.6.6.6.4.m1.1a"><mo mathsize="70%" id="S4.T2.6.6.6.4.m1.1.1" xref="S4.T2.6.6.6.4.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.4.m1.1b"><csymbol cd="latexml" id="S4.T2.6.6.6.4.m1.1.1.cmml" xref="S4.T2.6.6.6.4.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.4.m1.1c">\%</annotation></semantics></math><span id="S4.T2.6.6.6.4.2" class="ltx_text" style="font-size:70%;"> train</span>
</td>
<td id="S4.T2.7.7.7.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">
<span id="S4.T2.7.7.7.5.1" class="ltx_text" style="font-size:70%;">100</span><math id="S4.T2.7.7.7.5.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T2.7.7.7.5.m1.1a"><mo mathsize="70%" id="S4.T2.7.7.7.5.m1.1.1" xref="S4.T2.7.7.7.5.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.5.m1.1b"><csymbol cd="latexml" id="S4.T2.7.7.7.5.m1.1.1.cmml" xref="S4.T2.7.7.7.5.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.5.m1.1c">\%</annotation></semantics></math><span id="S4.T2.7.7.7.5.2" class="ltx_text" style="font-size:70%;"> train</span>
</td>
</tr>
<tr id="S4.T2.8.8.9.1" class="ltx_tr">
<th id="S4.T2.8.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T2.8.8.9.1.1.1" class="ltx_text" style="font-size:70%;">SPIN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.8.8.9.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a><span id="S4.T2.8.8.9.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S4.T2.8.8.9.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.8.8.9.1.2.1" class="ltx_text" style="font-size:70%;color:#AD0000;">‚úó</span></th>
<td id="S4.T2.8.8.9.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.8.8.9.1.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.9.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.8.8.9.1.4.1" class="ltx_text" style="font-size:70%;">225.1 / 120.2</span></td>
<td id="S4.T2.8.8.9.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.8.8.9.1.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.9.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.8.8.9.1.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.9.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.8.8.9.1.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.9.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.8.8.9.1.8.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T2.8.8.10.2" class="ltx_tr">
<th id="S4.T2.8.8.10.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.8.8.10.2.1.1" class="ltx_text" style="font-size:70%;">BEV </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.8.8.10.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a><span id="S4.T2.8.8.10.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S4.T2.8.8.10.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.8.8.10.2.2.1" class="ltx_text" style="font-size:70%;color:#AD0000;">‚úó</span></th>
<td id="S4.T2.8.8.10.2.3" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.10.2.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.10.2.4" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.10.2.4.1" class="ltx_text" style="font-size:70%;">313.5 / 125.1</span></td>
<td id="S4.T2.8.8.10.2.5" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.10.2.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.10.2.6" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.10.2.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.10.2.7" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.10.2.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.10.2.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.8.8.10.2.8.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T2.8.8.11.3" class="ltx_tr">
<th id="S4.T2.8.8.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.8.8.11.3.1.1" class="ltx_text" style="font-size:70%;">PARE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.8.8.11.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a><span id="S4.T2.8.8.11.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S4.T2.8.8.11.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.8.8.11.3.2.1" class="ltx_text" style="font-size:70%;color:#AD0000;">‚úó</span></th>
<td id="S4.T2.8.8.11.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.11.3.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.11.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.11.3.4.1" class="ltx_text" style="font-size:70%;">234.9 / 113.6</span></td>
<td id="S4.T2.8.8.11.3.5" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.11.3.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.11.3.6" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.11.3.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.11.3.7" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.11.3.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.11.3.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.8.8.11.3.8.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T2.8.8.8" class="ltx_tr">
<th id="S4.T2.8.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.8.8.8.2.1" class="ltx_text" style="font-size:70%;">ProHMR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.8.8.8.2.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.T2.8.8.8.2.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S4.T2.8.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.8.8.8.3.1" class="ltx_text" style="font-size:70%;color:#AD0000;">‚úó</span></th>
<td id="S4.T2.8.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.8.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.8.1" class="ltx_td ltx_align_center">
<span id="S4.T2.8.8.8.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">122.7</span><span id="S4.T2.8.8.8.1.3" class="ltx_text" style="font-size:70%;"> / </span><span id="S4.T2.8.8.8.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">
82.6<sup id="S4.T2.8.8.8.1.1.1" class="ltx_sup"><span id="S4.T2.8.8.8.1.1.1.1" class="ltx_text ltx_font_medium">‚àó</span></sup></span>
</td>
<td id="S4.T2.8.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.8.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.8.6" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.8.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.8.7" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.8.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.8.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.8.8.8.8.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T2.8.8.12.4" class="ltx_tr">
<th id="S4.T2.8.8.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.8.8.12.4.1.1" class="ltx_text" style="font-size:70%;">BEDLAM-CLIFF </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.8.8.12.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a><span id="S4.T2.8.8.12.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S4.T2.8.8.12.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.8.8.12.4.2.1" class="ltx_text" style="font-size:70%;color:#AD0000;">‚úó</span></th>
<td id="S4.T2.8.8.12.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.12.4.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.12.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.12.4.4.1" class="ltx_text" style="font-size:70%;">363.5 / 136.5</span></td>
<td id="S4.T2.8.8.12.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.12.4.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.12.4.6" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.12.4.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.12.4.7" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.12.4.7.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.12.4.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.8.8.12.4.8.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T2.8.8.13.5" class="ltx_tr">
<th id="S4.T2.8.8.13.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T2.8.8.13.5.1.1" class="ltx_text" style="font-size:70%;">SPIN-ft </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.8.8.13.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a><span id="S4.T2.8.8.13.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S4.T2.8.8.13.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.8.8.13.5.2.1" class="ltx_text" style="font-size:70%;color:#008000;">‚úì</span></th>
<td id="S4.T2.8.8.13.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.8.8.13.5.3.1" class="ltx_text" style="font-size:70%;color:#AD0000;">‚úó</span></td>
<td id="S4.T2.8.8.13.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.8.8.13.5.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.13.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.8.8.13.5.5.1" class="ltx_text" style="font-size:70%;">206.9 / 115.6</span></td>
<td id="S4.T2.8.8.13.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.8.8.13.5.6.1" class="ltx_text" style="font-size:70%;">161.3 / 103.6</span></td>
<td id="S4.T2.8.8.13.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.8.8.13.5.7.1" class="ltx_text" style="font-size:70%;">127.8 / 91.7</span></td>
<td id="S4.T2.8.8.13.5.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.8.8.13.5.8.1" class="ltx_text" style="font-size:70%;">133.7 / 92.3</span></td>
</tr>
<tr id="S4.T2.8.8.14.6" class="ltx_tr">
<th id="S4.T2.8.8.14.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.8.8.14.6.1.1" class="ltx_text" style="font-size:70%;">DAPA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.8.8.14.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a><span id="S4.T2.8.8.14.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S4.T2.8.8.14.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.8.8.14.6.2.1" class="ltx_text" style="font-size:70%;color:#008000;">‚úì</span></th>
<td id="S4.T2.8.8.14.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.14.6.3.1" class="ltx_text" style="font-size:70%;color:#008000;">‚úì</span></td>
<td id="S4.T2.8.8.14.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.14.6.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.14.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.14.6.5.1" class="ltx_text" style="font-size:70%;">222.2 / 123.6</span></td>
<td id="S4.T2.8.8.14.6.6" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.14.6.6.1" class="ltx_text" style="font-size:70%;">180.2 / 108.7</span></td>
<td id="S4.T2.8.8.14.6.7" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.14.6.7.1" class="ltx_text" style="font-size:70%;">128.7 / 90.5</span></td>
<td id="S4.T2.8.8.14.6.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.8.8.14.6.8.1" class="ltx_text" style="font-size:70%;">126.9 / 86.1</span></td>
</tr>
<tr id="S4.T2.8.8.15.7" class="ltx_tr">
<th id="S4.T2.8.8.15.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.8.8.15.7.1.1" class="ltx_text" style="font-size:70%;">ControlNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.8.8.15.7.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a><span id="S4.T2.8.8.15.7.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<th id="S4.T2.8.8.15.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.8.8.15.7.2.1" class="ltx_text" style="font-size:70%;color:#008000;">‚úì</span></th>
<td id="S4.T2.8.8.15.7.3" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.15.7.3.1" class="ltx_text" style="font-size:70%;color:#008000;">‚úì</span></td>
<td id="S4.T2.8.8.15.7.4" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.15.7.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.15.7.5" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.15.7.5.1" class="ltx_text" style="font-size:70%;">194.6 / 106.2</span></td>
<td id="S4.T2.8.8.15.7.6" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.15.7.6.1" class="ltx_text" style="font-size:70%;">144.1 / 94.0</span></td>
<td id="S4.T2.8.8.15.7.7" class="ltx_td ltx_align_center"><span id="S4.T2.8.8.15.7.7.1" class="ltx_text" style="font-size:70%;">118.2 / 85.3</span></td>
<td id="S4.T2.8.8.15.7.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.8.8.15.7.8.1" class="ltx_text" style="font-size:70%;">114.2 / 83.4</span></td>
</tr>
<tr id="S4.T2.8.8.16.8" class="ltx_tr">
<th id="S4.T2.8.8.16.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T2.8.8.16.8.1.1" class="ltx_text" style="font-size:70%;">Ours</span></th>
<th id="S4.T2.8.8.16.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T2.8.8.16.8.2.1" class="ltx_text" style="font-size:70%;color:#008000;">‚úì</span></th>
<td id="S4.T2.8.8.16.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.8.8.16.8.3.1" class="ltx_text" style="font-size:70%;color:#008000;">‚úì</span></td>
<td id="S4.T2.8.8.16.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.8.8.16.8.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T2.8.8.16.8.5" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T2.8.8.16.8.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">182.3</span><span id="S4.T2.8.8.16.8.5.2" class="ltx_text" style="font-size:70%;"> / </span><span id="S4.T2.8.8.16.8.5.3" class="ltx_text ltx_font_bold" style="font-size:70%;">105.9</span>
</td>
<td id="S4.T2.8.8.16.8.6" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T2.8.8.16.8.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">143.4</span><span id="S4.T2.8.8.16.8.6.2" class="ltx_text" style="font-size:70%;"> / ‚ÄÑ</span><span id="S4.T2.8.8.16.8.6.3" class="ltx_text ltx_font_bold" style="font-size:70%;">90.7</span>
</td>
<td id="S4.T2.8.8.16.8.7" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T2.8.8.16.8.7.1" class="ltx_text ltx_font_bold" style="font-size:70%;">116.5</span><span id="S4.T2.8.8.16.8.7.2" class="ltx_text" style="font-size:70%;"> / </span><span id="S4.T2.8.8.16.8.7.3" class="ltx_text ltx_font_bold" style="font-size:70%;">83.5</span>
</td>
<td id="S4.T2.8.8.16.8.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">
<span id="S4.T2.8.8.16.8.8.1" class="ltx_text ltx_font_bold" style="font-size:70%;">111.3</span><span id="S4.T2.8.8.16.8.8.2" class="ltx_text" style="font-size:70%;"> / </span><span id="S4.T2.8.8.16.8.8.3" class="ltx_text ltx_font_bold" style="font-size:70%;">81.5</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.14.1.1" class="ltx_text" style="font-size:129%;">Table 2</span>: </span><span id="S4.T2.15.2" class="ltx_text" style="font-size:129%;">Quantitative comparisons on Ski-Pose. We report MPJPE/PA-MPJPE on the test set. (*: Note that ProHMR uses ground truth 2D keypoints for test-time optimization and therefore has an unfair advantage for this experiment. All other models (including Ours) perform direct inference on test set). The best number per configuration is in <span id="S4.T2.15.2.1" class="ltx_text ltx_font_bold">bold</span>.</span></figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2303.09541/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="401" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Comparison with Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> on text-conditioned generations. Red arrows point out implausible body parts in Stable Diffusion generations. To show a spectrum of varying pose difficulty levels, we present generations from the 5%, 50%, 95% quantiles (i.e. from easy to hard) in terms of VPoser score. Rendered depths are included to show correct pose guidance.
</span></figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Implementation details.</span>
We use the backbone of SPIN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> to estimate the human mesh, since the backbone is shared by both SPIN and DAPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, which enables fair comparison to these two. For each real image in the few-shot training set, we create <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mn id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><cn type="integer" id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">3</annotation></semantics></math> synthetic images, where each one has a slightly different pose due to pose augmentation. We finetune and update the entire HMR model with batch size of 64, learning rate of 1e-4. All hyperparameters are the same as in SPIN. The models are trained until the loss curves plateau and on average each finetuning experiment takes about 6 hours on a single NVIDIA TITAN V GPU.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Results.</span>
We compare to recent HMR models BEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> and PARE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> that are pre-trained on MoCap datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> as well as in-the-wild pose estimation datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>. We also compare to BEDLAM-CLIFF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, a state-of-the-art HMR model that is trained with a large synthetic dataset BEDLAM with realistic humans. In addition, we compare to finetuning methods SPIN-ft <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, DAPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, as well as finetuning with synthetic data generated with ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite> and Diffusion-HPC. The finetuning of these methods and ours minimizes 2D keypoint reprojection error by using 2D keypoints from the target training set. In addition, SPIN-ft uses in-the-loop model fitting to provide additional model-based supervision. DAPA generates synthetic data with paired 3D ground truths on the fly as additional supervision, while Ours uses data from Diffusion-HPC.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">In Table <a href="#S4.T2" title="Table 2 ‚Ä£ 4.1 Finetuning on challenging HMR settings ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we report PCK on sports categories from SMART. Although the off-the-shelf models (SPIN-pt, BEV, PARE) were pre-trained on 2D datasets that include sports poses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, there is still a significant domain gap between the training sets and SMART. BEDLAM-CLIFF was trained with a massive synthetic dataset BEDLAM, but the data generation was not tailored for the specific target domains, and therefore their training does not improve the model performance on the target dataset. As shown in the lower half of Table <a href="#S4.T2" title="Table 2 ‚Ä£ 4.1 Finetuning on challenging HMR settings ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, finetuning on a small set of target images is helpful in closing the domain gap. Among those methods, we achieve better performance in general.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">In Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.1 Finetuning on challenging HMR settings ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we report MPJPE/PA-MPJPE on Ski-Pose testset. We vary the size of the real training set during adaptation, and observe that with the same mount of real data, models trained with our synthetic data attain best performance. Further, with the help of synthetic data generated by Diffusion-HPC, we attain better performance than SPIN-ft and DAPA using much smaller amount of real data. We achieve best performance when using the entire training set. Notably, our best performance (111.3 MPJPE, 81.5 PA-MPJPE) is better than ProHMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> (122.7 MPJPE, 82.6 PA-MPJPE), which uses ground truth 2D keypoints from the testset as additional information. Finally, as qualitatively demonstrated in Figure <a href="#S4.F3" title="Figure 3 ‚Ä£ 4.1 Finetuning on challenging HMR settings ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our method produces more accurate human mesh estimations on challenging poses, and in general have better alignment with 2D images.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Image generation quality</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Data generation details.</span> We use a text-to-image Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> model pre-trained on LAION-5B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> and a CLIP ViT-L/14 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> as text encoder. To condition the generation on depth maps we employ the depth-to-image Stable Diffusion model that was resumed from the text-to-image model, and finetuned for 200k steps. The denoising model has an extra input channel to process the (relative) depth prediction produced by MiDaS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> which is used as added conditioning. As our segmentation model we use Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> pre-trained on MS-COCO <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>. For the qualitative examples in Figure <a href="#S4.F4" title="Figure 4 ‚Ä£ 4.1 Finetuning on challenging HMR settings ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and experiments in Section <a href="#S4.SS2" title="4.2 Image generation quality ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we use BEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> as the HMR model, due to its capacity of recovering people of all age groups and better empirical performance at localizing implausible synthetic humans, whereas two-stage HMR models that rely on a human detector often treat these erroneous generations as false negatives. With 50 inference steps, it takes about 6 seconds to create an image starting from text, and in the setting when a real image is used as guidance, the time is halved.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Comparison on text-conditioned generation</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We assess the quality of the text-only conditioned images generated by Diffusion-HPC by comparing them to off-the-shelf Stable Diffusion.
In order to span a wide taxonomy of human activities we compose text prompts from the category labels available in the MPII <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> dataset.
In addition, to assess the generation quality regarding extremely challenging human poses, we use the publicly released sports categories from SMART <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> (further introduced in Sec. <a href="#S4.SS1" title="4.1 Finetuning on challenging HMR settings ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) as text prompts.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:48.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-94.3pt,20.9pt) scale(0.534745757481283,0.534745757481283) ;">
<table id="S4.T3.7.7" class="ltx_tabular ltx_guessed_headers ltx_align_top">
<thead class="ltx_thead">
<tr id="S4.T3.3.3.3" class="ltx_tr">
<th id="S4.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S4.T3.3.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Dataset</th>
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">User Preference (<math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</th>
<th id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FID / H-FID (<math id="S4.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>)</th>
<th id="S4.T3.3.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">KID / H-KID (<math id="S4.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T3.3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.4.4" class="ltx_tr">
<td id="S4.T3.4.4.4.2" class="ltx_td ltx_align_left ltx_border_t">Stable Diffusion</td>
<td id="S4.T3.4.4.4.3" class="ltx_td ltx_align_center ltx_border_t">MPII</td>
<td id="S4.T3.4.4.4.1" class="ltx_td ltx_align_center ltx_border_t">0.45 <math id="S4.T3.4.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T3.4.4.4.1.m1.1a"><mo id="S4.T3.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><csymbol cd="latexml" id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">\pm</annotation></semantics></math> 0.23</td>
<td id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T3.4.4.4.4.1" class="ltx_text ltx_font_bold">75.6</span> / 70.5</td>
<td id="S4.T3.4.4.4.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">
<span id="S4.T3.4.4.4.5.1" class="ltx_text ltx_font_bold">0.03</span> / 0.11</td>
</tr>
<tr id="S4.T3.5.5.5" class="ltx_tr">
<td id="S4.T3.5.5.5.2" class="ltx_td ltx_align_left">Diffusion-HPC</td>
<td id="S4.T3.5.5.5.3" class="ltx_td ltx_align_center">MPII</td>
<td id="S4.T3.5.5.5.1" class="ltx_td ltx_align_center"><span id="S4.T3.5.5.5.1.1" class="ltx_text ltx_font_bold">0.55 <math id="S4.T3.5.5.5.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T3.5.5.5.1.1.m1.1a"><mo id="S4.T3.5.5.5.1.1.m1.1.1" xref="S4.T3.5.5.5.1.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.1.m1.1b"><csymbol cd="latexml" id="S4.T3.5.5.5.1.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.1.m1.1c">\pm</annotation></semantics></math> 0.23</span></td>
<td id="S4.T3.5.5.5.4" class="ltx_td ltx_align_center">
<span id="S4.T3.5.5.5.4.1" class="ltx_text ltx_font_bold">75.6</span> / <span id="S4.T3.5.5.5.4.2" class="ltx_text ltx_font_bold">68.1</span>
</td>
<td id="S4.T3.5.5.5.5" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S4.T3.5.5.5.5.1" class="ltx_text ltx_font_bold">0.03</span> / <span id="S4.T3.5.5.5.5.2" class="ltx_text ltx_font_bold">0.04</span>
</td>
</tr>
<tr id="S4.T3.6.6.6" class="ltx_tr">
<td id="S4.T3.6.6.6.2" class="ltx_td ltx_align_left ltx_border_t">Stable Diffusion</td>
<td id="S4.T3.6.6.6.3" class="ltx_td ltx_align_center ltx_border_t">SMART</td>
<td id="S4.T3.6.6.6.1" class="ltx_td ltx_align_center ltx_border_t">0.23 <math id="S4.T3.6.6.6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T3.6.6.6.1.m1.1a"><mo id="S4.T3.6.6.6.1.m1.1.1" xref="S4.T3.6.6.6.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.1.m1.1b"><csymbol cd="latexml" id="S4.T3.6.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.1.m1.1c">\pm</annotation></semantics></math> 0.08</td>
<td id="S4.T3.6.6.6.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T3.6.6.6.4.1" class="ltx_text ltx_font_bold">66.3</span> / 91.4</td>
<td id="S4.T3.6.6.6.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.04 / 0.07</td>
</tr>
<tr id="S4.T3.7.7.7" class="ltx_tr">
<td id="S4.T3.7.7.7.2" class="ltx_td ltx_align_left ltx_border_bb">Diffusion-HPC</td>
<td id="S4.T3.7.7.7.3" class="ltx_td ltx_align_center ltx_border_bb">SMART</td>
<td id="S4.T3.7.7.7.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.7.1.1" class="ltx_text ltx_font_bold">0.77 <math id="S4.T3.7.7.7.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T3.7.7.7.1.1.m1.1a"><mo id="S4.T3.7.7.7.1.1.m1.1.1" xref="S4.T3.7.7.7.1.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.1.1.m1.1b"><csymbol cd="latexml" id="S4.T3.7.7.7.1.1.m1.1.1.cmml" xref="S4.T3.7.7.7.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.1.1.m1.1c">\pm</annotation></semantics></math> 0.09</span></td>
<td id="S4.T3.7.7.7.4" class="ltx_td ltx_align_center ltx_border_bb">67.7 / <span id="S4.T3.7.7.7.4.1" class="ltx_text ltx_font_bold">89.5</span>
</td>
<td id="S4.T3.7.7.7.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">
<span id="S4.T3.7.7.7.5.1" class="ltx_text ltx_font_bold">0.03</span> / <span id="S4.T3.7.7.7.5.2" class="ltx_text ltx_font_bold">0.06</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.9.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.10.2" class="ltx_text" style="font-size:90%;">Text-conditioned comparisons on activities from MPII.
</span></figcaption>
</figure>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">For both datasets, we report the standard evaluation metric Fr√©chet Inception Distance (FID) and Kernel Inception Distance (KID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. Since the focus of our method is on human generation, we report H-FID / H-KID, which is FID / KID computed with only foreground humans (segmented by Mask R-CNN). Note that FID/KID are computed using image-level features, and therefore do not focus on human generation quality in particular. Thus, we deem H-FID/H-KID more suitable metrics for our work.</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">Furthermore, we perform a user study where 6 independent blinded users were shown a randomly sampled set of 100 side-by-side images each generated by Stable Diffusion and Diffusion-HPC. The users were given the task of selecting the image with the most plausible human pose and anatomy. If the images were comparable, the user could select a ‚Äúno preference‚Äù option.</p>
</div>
<div id="S4.SS2.SSS1.p4" class="ltx_para">
<p id="S4.SS2.SSS1.p4.1" class="ltx_p"><span id="S4.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Results.</span> Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.2.1 Comparison on text-conditioned generation ‚Ä£ 4.2 Image generation quality ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents comparisons on text-conditioned generations. While FID/KID values are roughly the same, we highlight that humans generated by Diffusion-HPC have lower H-FID/H-KID to humans from real images. User study suggests that users prefer our generations most of the time. Qualitative results in Figure <a href="#S4.T3" title="Table 3 ‚Ä£ 4.2.1 Comparison on text-conditioned generation ‚Ä£ 4.2 Image generation quality ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> suggest that our generations, while preserving the textures of the original images (hence similar FID/KID), effectively corrects the human anatomy (hence lower H-FID/H-KID).</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Comparison on pose-conditioned generation</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Most previous pose-conditioned generative models focus on the task of ‚Äúreposing‚Äù <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, where the goal is to repose the reference person using the target pose. These models are trained on fashion catalog images with clean background, therefore they are too simplistic to be effective baselines for our purpose. The only fair baseline, to our knowledge, is <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Brooks and Efros</span> [<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, a recent StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>-based generative model that takes 2D keypoints of a posed person and generates images with compatible background. We benchmark their pre-trained model (trained on 18 million images sourced from 10 existing human pose estimation and action recognition datasets) on MPII for in-domain assessment.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p"><span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Results.</span> Table <a href="#S4.T4" title="Table 4 ‚Ä£ 4.2.2 Comparison on pose-conditioned generation ‚Ä£ 4.2 Image generation quality ‚Ä£ 4 Experiments ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows quantitative comparisons of image quality.
Notably, even though <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Brooks and Efros</span> [<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> was trained with paired data (keypoint-image pairs) and therefore has an advantage over Diffusion-HPC where the underlying models are trained/finetuned only with images, Diffusion-HPC consistently achieves better performance. Moreover, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Brooks and Efros</span> [<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> has poor generalization capability to novel pose distributions as in SMART, whereas our method powered by Stable Diffusion has a better zero-shot capability. Additional details, qualitative comparisons and limitations are in the <span id="S4.SS2.SSS2.p2.1.2" class="ltx_text ltx_font_italic">Supplementary Material</span>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.2" class="ltx_inline-block ltx_transformed_outer" style="width:216.8pt;height:206.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-62.0pt,58.9pt) scale(0.636271865987618,0.636271865987618) ;">
<table id="S4.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_top">
<tbody class="ltx_tbody">
<tr id="S4.T4.2.1.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S4.T4.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T4.2.1.1.1.2.1" class="ltx_text ltx_font_bold">Results on MPII</span></td>
<td id="S4.T4.2.1.1.1.3" class="ltx_td ltx_border_tt"></td>
<td id="S4.T4.2.1.1.1.4" class="ltx_td ltx_border_tt"></td>
<td id="S4.T4.2.1.1.1.5" class="ltx_td ltx_border_tt"></td>
<td id="S4.T4.2.1.1.1.6" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S4.T4.2.1.2.2" class="ltx_tr">
<th id="S4.T4.2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T4.2.1.2.2.1.1" class="ltx_text">Method</span></th>
<td id="S4.T4.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">Trained with</td>
<td id="S4.T4.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T4.2.1.2.2.3.1" class="ltx_text">T</span></td>
<td id="S4.T4.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T4.2.1.2.2.4.1" class="ltx_text">R</span></td>
<td id="S4.T4.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T4.2.1.2.2.5.1" class="ltx_text">D</span></td>
<td id="S4.T4.2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">FID/</td>
<td id="S4.T4.2.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">KID/</td>
</tr>
<tr id="S4.T4.2.1.3.3" class="ltx_tr">
<td id="S4.T4.2.1.3.3.1" class="ltx_td ltx_align_center">paired data</td>
<td id="S4.T4.2.1.3.3.2" class="ltx_td ltx_align_center">H-FID</td>
<td id="S4.T4.2.1.3.3.3" class="ltx_td ltx_align_center">H-KID</td>
</tr>
<tr id="S4.T4.2.1.4.4" class="ltx_tr">
<th id="S4.T4.2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Brooks et al.</th>
<td id="S4.T4.2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.4.2.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.4.3.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.4.4.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.4.4.5.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">109.0 / 75.5</td>
<td id="S4.T4.2.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">0.10 / 0.07</td>
</tr>
<tr id="S4.T4.2.1.5.5" class="ltx_tr">
<th id="S4.T4.2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="6"><span id="S4.T4.2.1.5.5.1.1" class="ltx_text">Diffusion-HPC</span></th>
<td id="S4.T4.2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.5.5.2.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.5.5.3.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.5.5.4.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.5.5.5.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">95.6 / 59.3</td>
<td id="S4.T4.2.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">0.07 / 0.05</td>
</tr>
<tr id="S4.T4.2.1.6.6" class="ltx_tr">
<td id="S4.T4.2.1.6.6.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.6.6.1.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.6.6.2.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.6.6.3.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.6.6.4.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.6.6.5" class="ltx_td ltx_align_center">54.6 / <span id="S4.T4.2.1.6.6.5.1" class="ltx_text ltx_framed ltx_framed_underline">41.3</span>
</td>
<td id="S4.T4.2.1.6.6.6" class="ltx_td ltx_align_center">0.03 / 0.03</td>
</tr>
<tr id="S4.T4.2.1.7.7" class="ltx_tr">
<td id="S4.T4.2.1.7.7.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.7.7.1.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.7.7.2.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.7.7.3.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.7.7.4.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.7.7.5" class="ltx_td ltx_align_center">
<span id="S4.T4.2.1.7.7.5.1" class="ltx_text ltx_framed ltx_framed_underline">44.6</span> / 41.5</td>
<td id="S4.T4.2.1.7.7.6" class="ltx_td ltx_align_center">
<span id="S4.T4.2.1.7.7.6.1" class="ltx_text ltx_framed ltx_framed_underline">0.02</span> / <span id="S4.T4.2.1.7.7.6.2" class="ltx_text ltx_framed ltx_framed_underline">0.03</span>
</td>
</tr>
<tr id="S4.T4.2.1.8.8" class="ltx_tr">
<td id="S4.T4.2.1.8.8.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.8.8.1.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.8.8.2.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.8.8.3.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.8.8.4.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.8.8.5" class="ltx_td ltx_align_center">95.3 / 58.1</td>
<td id="S4.T4.2.1.8.8.6" class="ltx_td ltx_align_center">0.07 / 0.04</td>
</tr>
<tr id="S4.T4.2.1.9.9" class="ltx_tr">
<td id="S4.T4.2.1.9.9.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.9.9.1.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.9.9.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.9.9.2.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.9.9.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.9.9.3.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.9.9.4.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.9.9.5" class="ltx_td ltx_align_center">72.8 / 136.2</td>
<td id="S4.T4.2.1.9.9.6" class="ltx_td ltx_align_center">0.03 / 0.11</td>
</tr>
<tr id="S4.T4.2.1.10.10" class="ltx_tr">
<td id="S4.T4.2.1.10.10.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.10.10.1.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.10.10.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.10.10.2.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.10.10.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.10.10.3.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.10.10.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.10.10.4.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.10.10.5" class="ltx_td ltx_align_center">
<span id="S4.T4.2.1.10.10.5.1" class="ltx_text ltx_font_bold">42.6</span> / <span id="S4.T4.2.1.10.10.5.2" class="ltx_text ltx_font_bold">38.6</span>
</td>
<td id="S4.T4.2.1.10.10.6" class="ltx_td ltx_align_center">
<span id="S4.T4.2.1.10.10.6.1" class="ltx_text ltx_font_bold">0.02</span> / <span id="S4.T4.2.1.10.10.6.2" class="ltx_text ltx_font_bold">0.02</span>
</td>
</tr>
<tr id="S4.T4.2.1.11.11" class="ltx_tr">
<th id="S4.T4.2.1.11.11.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T4.2.1.11.11.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T4.2.1.11.11.2.1" class="ltx_text ltx_font_bold">Results on SMART</span></td>
<td id="S4.T4.2.1.11.11.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T4.2.1.11.11.4" class="ltx_td ltx_border_t"></td>
<td id="S4.T4.2.1.11.11.5" class="ltx_td ltx_border_t"></td>
<td id="S4.T4.2.1.11.11.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T4.2.1.12.12" class="ltx_tr">
<th id="S4.T4.2.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Brooks et al.</th>
<td id="S4.T4.2.1.12.12.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.12.12.2.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.12.12.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.12.12.3.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.12.12.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.12.12.4.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.12.12.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.12.12.5.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.12.12.6" class="ltx_td ltx_align_center ltx_border_t">175.8 / 114.1</td>
<td id="S4.T4.2.1.12.12.7" class="ltx_td ltx_align_center ltx_border_t">0.14 / 0.06</td>
</tr>
<tr id="S4.T4.2.1.13.13" class="ltx_tr">
<th id="S4.T4.2.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="6"><span id="S4.T4.2.1.13.13.1.1" class="ltx_text">Diffusion-HPC</span></th>
<td id="S4.T4.2.1.13.13.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.13.13.2.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.13.13.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.13.13.3.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.13.13.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.13.13.4.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.13.13.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.13.13.5.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.13.13.6" class="ltx_td ltx_align_center ltx_border_t">85.9 / 121.5</td>
<td id="S4.T4.2.1.13.13.7" class="ltx_td ltx_align_center ltx_border_t">0.06 / 0.07</td>
</tr>
<tr id="S4.T4.2.1.14.14" class="ltx_tr">
<td id="S4.T4.2.1.14.14.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.14.14.1.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.14.14.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.14.14.2.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.14.14.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.14.14.3.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.14.14.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.14.14.4.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.14.14.5" class="ltx_td ltx_align_center">162.3 / 113.9</td>
<td id="S4.T4.2.1.14.14.6" class="ltx_td ltx_align_center">0.13 / 0.08</td>
</tr>
<tr id="S4.T4.2.1.15.15" class="ltx_tr">
<td id="S4.T4.2.1.15.15.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.15.15.1.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.15.15.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.15.15.2.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.15.15.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.15.15.3.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.15.15.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.15.15.4.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.15.15.5" class="ltx_td ltx_align_center">94.8 / <span id="S4.T4.2.1.15.15.5.1" class="ltx_text ltx_framed ltx_framed_underline">99.8</span>
</td>
<td id="S4.T4.2.1.15.15.6" class="ltx_td ltx_align_center">
<span id="S4.T4.2.1.15.15.6.1" class="ltx_text ltx_framed ltx_framed_underline">0.06</span> / <span id="S4.T4.2.1.15.15.6.2" class="ltx_text ltx_framed ltx_framed_underline">0.05</span>
</td>
</tr>
<tr id="S4.T4.2.1.16.16" class="ltx_tr">
<td id="S4.T4.2.1.16.16.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.16.16.1.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.16.16.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.16.16.2.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.16.16.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.16.16.3.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.16.16.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.16.16.4.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.16.16.5" class="ltx_td ltx_align_center">
<span id="S4.T4.2.1.16.16.5.1" class="ltx_text ltx_font_bold">85.5</span> / 122.2</td>
<td id="S4.T4.2.1.16.16.6" class="ltx_td ltx_align_center">0.06 / 0.07</td>
</tr>
<tr id="S4.T4.2.1.17.17" class="ltx_tr">
<td id="S4.T4.2.1.17.17.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.17.17.1.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.17.17.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.17.17.2.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.17.17.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.17.17.3.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.17.17.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.17.17.4.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.17.17.5" class="ltx_td ltx_align_center">145.9 / 131.5</td>
<td id="S4.T4.2.1.17.17.6" class="ltx_td ltx_align_center">0.10 / 0.07</td>
</tr>
<tr id="S4.T4.2.1.18.18" class="ltx_tr">
<td id="S4.T4.2.1.18.18.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.18.18.1.1" class="ltx_text" style="color:#AD0000;">‚úó</span></td>
<td id="S4.T4.2.1.18.18.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.18.18.2.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.18.18.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.18.18.3.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.18.18.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.1.18.18.4.1" class="ltx_text" style="color:#008000;">‚úì</span></td>
<td id="S4.T4.2.1.18.18.5" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T4.2.1.18.18.5.1" class="ltx_text ltx_framed ltx_framed_underline">92.4</span> / <span id="S4.T4.2.1.18.18.5.2" class="ltx_text ltx_font_bold">44.5</span>
</td>
<td id="S4.T4.2.1.18.18.6" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T4.2.1.18.18.6.1" class="ltx_text ltx_font_bold">0.06</span> / <span id="S4.T4.2.1.18.18.6.2" class="ltx_text ltx_font_bold">0.03</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.4.2" class="ltx_text" style="font-size:90%;">Pose-conditioned generation quality.
Note that <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Brooks and Efros</span> [<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> was trained with paired data (images with corresponding 2D keypoints), whereas Diffusion-HPC is trained/finetuned only with images.
Diffusion-HPC can take the background of text (‚ÄúT‚Äù) and/or a real image (‚ÄúR‚Äù) as conditioning information.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We proposed Diffusion-HPC, a text-conditioned and training-free method that injects model-based human body prior to improve human-centric generation of state-of-the-art text-conditioned and pose-conditioned generative models. Further, Diffusion-HPC demonstrates excellent utility in a challenging downstream task, single-view HMR.
For future work, we anticipate further investigation into the obstacles associated with human generation in foundation generative models, as well as exploring innovative ways of using generative models to tackle the challenges in 3D human perception tasks.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Acknowledgments.</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">This material is based upon work supported by the National Science Foundation under Grant No. 2026498.
L. Bravo-S√°nchez acknowledges financial support for this work by the Fulbright U.S. Student Program, which is sponsored by the U.S. Department of State and Fulbright Colombia. In addition, she was partially supported by an educational grant from IBM Research.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S5.SS0.SSS0.Px1.p2" class="ltx_para ltx_align_center">
<span id="S5.SS0.SSS0.Px1.p2.1" class="ltx_ERROR undefined">\thetitle</span>
<br class="ltx_break">
<p id="S5.SS0.SSS0.Px1.p2.2" class="ltx_p"><span id="S5.SS0.SSS0.Px1.p2.2.1" class="ltx_text" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"></span></p>
</div>
</section>
<section id="S5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">A. Code</h3>

<div id="S5.SSx1.p1" class="ltx_para">
<p id="S5.SSx1.p1.1" class="ltx_p"><span id="S5.SSx1.p1.1.1" class="ltx_text" style="font-size:144%;">Code and trained models can be found at </span><a target="_blank" href="https://github.com/ZZWENG/Diffusion_HPC" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:144%;">https://github.com/ZZWENG/Diffusion_HPC</a><span id="S5.SSx1.p1.1.2" class="ltx_text" style="font-size:144%;">.</span></p>
</div>
</section>
<section id="S5.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">B. Additional Implementation Details</h3>

<div id="S5.SSx2.p1" class="ltx_para">
<p id="S5.SSx2.p1.1" class="ltx_p"><span id="S5.SSx2.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Quantitative evaluation in Table 1.</span><span id="S5.SSx2.p1.1.2" class="ltx_text" style="font-size:144%;"> We compute quantitative metrics using roughly 10,000 images generated from MPII </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SSx2.p1.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a><span id="S5.SSx2.p1.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx2.p1.1.5" class="ltx_text" style="font-size:144%;"> and SMART </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SSx2.p1.1.6.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a><span id="S5.SSx2.p1.1.7.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx2.p1.1.8" class="ltx_text" style="font-size:144%;"> prompts, respectively.</span></p>
</div>
<div id="S5.SSx2.p2" class="ltx_para">
<p id="S5.SSx2.p2.1" class="ltx_p"><span id="S5.SSx2.p2.1.1" class="ltx_text" style="font-size:144%;">For MPII, we use ‚Äú{image description} of {person} doing {action}‚Äù as the text prompts, where ‚Äú{person}‚Äù can be single- or multi-person descriptions of person(s) of interest, and ‚Äú{action}‚Äù are the activity categories from MPII. (We exclude categories ‚Äúinactivity, quite/light‚Äù and miscellaneous‚Äù because they do not describe a specific activity.) For example, resulting prompts could be ‚Äúa nice photo of a man doing water activities.‚Äù or ‚Äúa high-resolution photo of a group of people doing conditioning exercises‚Äù.</span></p>
</div>
<div id="S5.SSx2.p3" class="ltx_para">
<p id="S5.SSx2.p3.1" class="ltx_p"><span id="S5.SSx2.p3.1.1" class="ltx_text" style="font-size:144%;">Text prompts for SMART are constructed using the template ‚Äúa photo of an athlete doing {action}‚Äù where action is one of ‚Äúhigh jump‚Äù, ‚Äúvault‚Äù, ‚Äúpole vault‚Äù, ‚Äúdiving‚Äù, ‚Äúgymnastics on uneven bars‚Äù, and ‚Äúgymnastics on a balance beam‚Äù.</span></p>
</div>
<div id="S5.SSx2.p4" class="ltx_para">
<p id="S5.SSx2.p4.1" class="ltx_p"><span id="S5.SSx2.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Data processing for downstream experiments.</span><span id="S5.SSx2.p4.1.2" class="ltx_text" style="font-size:144%;">
Following previous works </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SSx2.p4.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a><span id="S5.SSx2.p4.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx2.p4.1.5" class="ltx_text" style="font-size:144%;">, we crop the images such that the persons (localized by ground truth 2D keypoints) are centered in the crop. In addition, the persons are scaled such that the torso (i.e. mean distance between left/right shoulder and hip) are roughly one third of the crop size (</span><math id="S5.SSx2.p4.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S5.SSx2.p4.1.m1.1a"><mrow id="S5.SSx2.p4.1.m1.1.1" xref="S5.SSx2.p4.1.m1.1.1.cmml"><mn mathsize="144%" id="S5.SSx2.p4.1.m1.1.1.2" xref="S5.SSx2.p4.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" mathsize="144%" rspace="0.222em" id="S5.SSx2.p4.1.m1.1.1.1" xref="S5.SSx2.p4.1.m1.1.1.1.cmml">√ó</mo><mn mathsize="144%" id="S5.SSx2.p4.1.m1.1.1.3" xref="S5.SSx2.p4.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SSx2.p4.1.m1.1b"><apply id="S5.SSx2.p4.1.m1.1.1.cmml" xref="S5.SSx2.p4.1.m1.1.1"><times id="S5.SSx2.p4.1.m1.1.1.1.cmml" xref="S5.SSx2.p4.1.m1.1.1.1"></times><cn type="integer" id="S5.SSx2.p4.1.m1.1.1.2.cmml" xref="S5.SSx2.p4.1.m1.1.1.2">224</cn><cn type="integer" id="S5.SSx2.p4.1.m1.1.1.3.cmml" xref="S5.SSx2.p4.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SSx2.p4.1.m1.1c">224\times 224</annotation></semantics></math><span id="S5.SSx2.p4.1.6" class="ltx_text" style="font-size:144%;">).</span></p>
</div>
</section>
<section id="S5.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">C. Additional Comparisons on Pose-Conditioned Generation</h3>

<figure id="S5.F5" class="ltx_figure"><img src="/html/2303.09541/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="475" height="270" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.7.1.1" class="ltx_text" style="font-size:63%;">Figure S5</span>: </span><span id="S5.F5.8.2" class="ltx_text" style="font-size:63%;">Qualitative comparisons to <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Brooks and Efros</span> [<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> (input 2D keypoints are overlaid on the bottom left). Our generations conditioned on text (T), real images (R), and in-domain (D).
</span></figcaption>
</figure>
<div id="S5.SSx3.p1" class="ltx_para">
<p id="S5.SSx3.p1.1" class="ltx_p"><span id="S5.SSx3.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Effect of text and real guidance.</span><span id="S5.SSx3.p1.1.2" class="ltx_text" style="font-size:144%;"> Figure </span><a href="#S5.F5" title="Figure S5 ‚Ä£ C. Additional Comparisons on Pose-Conditioned Generation ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S5</span></a><span id="S5.SSx3.p1.1.3" class="ltx_text" style="font-size:144%;"> demonstrates qualitative comparisons between different versions of our model and </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Brooks and Efros</span> <span id="S5.SSx3.p1.1.4.1.1.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a><span id="S5.SSx3.p1.1.5.2.2.1" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx3.p1.1.6" class="ltx_text" style="font-size:144%;"> As shown, text guidance (T) is essential in capturing the context of the human action. Guidance from real images (R) provides overall texture information such as background colors. While guidance from real images alone is not sufficient in preserving the action of the human (</span><math id="S5.SSx3.p1.1.m1.1" class="ltx_Math" alttext="3_{rd}" display="inline"><semantics id="S5.SSx3.p1.1.m1.1a"><msub id="S5.SSx3.p1.1.m1.1.1" xref="S5.SSx3.p1.1.m1.1.1.cmml"><mn mathsize="144%" id="S5.SSx3.p1.1.m1.1.1.2" xref="S5.SSx3.p1.1.m1.1.1.2.cmml">3</mn><mrow id="S5.SSx3.p1.1.m1.1.1.3" xref="S5.SSx3.p1.1.m1.1.1.3.cmml"><mi mathsize="144%" id="S5.SSx3.p1.1.m1.1.1.3.2" xref="S5.SSx3.p1.1.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SSx3.p1.1.m1.1.1.3.1" xref="S5.SSx3.p1.1.m1.1.1.3.1.cmml">‚Äã</mo><mi mathsize="144%" id="S5.SSx3.p1.1.m1.1.1.3.3" xref="S5.SSx3.p1.1.m1.1.1.3.3.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SSx3.p1.1.m1.1b"><apply id="S5.SSx3.p1.1.m1.1.1.cmml" xref="S5.SSx3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SSx3.p1.1.m1.1.1.1.cmml" xref="S5.SSx3.p1.1.m1.1.1">subscript</csymbol><cn type="integer" id="S5.SSx3.p1.1.m1.1.1.2.cmml" xref="S5.SSx3.p1.1.m1.1.1.2">3</cn><apply id="S5.SSx3.p1.1.m1.1.1.3.cmml" xref="S5.SSx3.p1.1.m1.1.1.3"><times id="S5.SSx3.p1.1.m1.1.1.3.1.cmml" xref="S5.SSx3.p1.1.m1.1.1.3.1"></times><ci id="S5.SSx3.p1.1.m1.1.1.3.2.cmml" xref="S5.SSx3.p1.1.m1.1.1.3.2">ùëü</ci><ci id="S5.SSx3.p1.1.m1.1.1.3.3.cmml" xref="S5.SSx3.p1.1.m1.1.1.3.3">ùëë</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SSx3.p1.1.m1.1c">3_{rd}</annotation></semantics></math><span id="S5.SSx3.p1.1.7" class="ltx_text" style="font-size:144%;"> row in Ours R), it adds to text guidance, and further improves the realism of the image generations (Ours T+R).</span></p>
</div>
<div id="S5.SSx3.p2" class="ltx_para">
<p id="S5.SSx3.p2.1" class="ltx_p"><span id="S5.SSx3.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Effect of finetuning.</span><span id="S5.SSx3.p2.1.2" class="ltx_text" style="font-size:144%;"> To see whether a finetuned diffusion model could further help improve generation quality, we finetune Stable Diffusion on the target dataset (MPII and SMART respectively) for 10 epochs. Generations with finetuned diffusion models is noted with ‚ÄúD‚Äù. As shown in Figure </span><a href="#S5.F5" title="Figure S5 ‚Ä£ C. Additional Comparisons on Pose-Conditioned Generation ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S5</span></a><span id="S5.SSx3.p2.1.3" class="ltx_text" style="font-size:144%;">, although finetuned diffusion model generates images with better background when there is no real guidance (Ours T vs. T+D), the foreground often loses the texture of humans, which is likely due to the ‚Äúcatastrophic forgetting‚Äù as sometimes observed in finetuning large pretrained models. Qualitatively, with both text and real guidance, the effect of finetuning is barely noticeable (Ours T+R vs. T+R+D). Quantitatively, when using real guidance (with or without text guidance), finetuning slightly improves FID, and significantly improves H-FID and H-KID. Further, consistent to what is observed in qualitative results, 41.3 H-FID (with T) vs. 136.2 H-FID (with T+D) suggests that finetuning worsens performance without real guidance. This suggests that for text-conditioned generations, it is optimal to utilize an off-the-shelf diffusion model without finetuning.</span></p>
</div>
</section>
<section id="S5.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">D. Additional Qualitative Results</h3>

<div id="S5.SSx4.p1" class="ltx_para">
<p id="S5.SSx4.p1.1" class="ltx_p"><span id="S5.SSx4.p1.1.1" class="ltx_text" style="font-size:144%;">Here we include additional qualitative results as well as failure cases for the text-conditioned and pose-conditioned generations (Section 4.2).</span></p>
</div>
<section id="S5.SSx4.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:144%;">Text-Conditioned Generation</h4>

<div id="S5.SSx4.SSSx1.p1" class="ltx_para">
<p id="S5.SSx4.SSSx1.p1.1" class="ltx_p"><span id="S5.SSx4.SSSx1.p1.1.1" class="ltx_text" style="font-size:144%;">Figure¬†</span><a href="#S5.F6" title="Figure S6 ‚Ä£ Text-Conditioned Generation ‚Ä£ D. Additional Qualitative Results ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S6</span></a><span id="S5.SSx4.SSSx1.p1.1.2" class="ltx_text" style="font-size:144%;"> shows qualitative comparisons of Stable Diffusion </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.SSx4.SSSx1.p1.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a><span id="S5.SSx4.SSSx1.p1.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx4.SSSx1.p1.1.5" class="ltx_text" style="font-size:144%;"> and Diffusion-HPC on text-conditioned generations. The images were selected from those sampled for the user study.</span></p>
</div>
<div id="S5.SSx4.SSSx1.p2" class="ltx_para">
<p id="S5.SSx4.SSSx1.p2.1" class="ltx_p"><span id="S5.SSx4.SSSx1.p2.1.1" class="ltx_text" style="font-size:144%;">In Figure </span><a href="#S5.F7" title="Figure S7 ‚Ä£ Text-Conditioned Generation ‚Ä£ D. Additional Qualitative Results ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S7</span></a><span id="S5.SSx4.SSSx1.p2.1.2" class="ltx_text" style="font-size:144%;"> we include typical failure cases of text-conditioned generations. In left and middle columns, the body structures are not sufficiently rectified. This is likely because that resolution of depth maps (used for conditioning) is limited (</span><math id="S5.SSx4.SSSx1.p2.1.m1.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S5.SSx4.SSSx1.p2.1.m1.1a"><mrow id="S5.SSx4.SSSx1.p2.1.m1.1.1" xref="S5.SSx4.SSSx1.p2.1.m1.1.1.cmml"><mn mathsize="144%" id="S5.SSx4.SSSx1.p2.1.m1.1.1.2" xref="S5.SSx4.SSSx1.p2.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" mathsize="144%" rspace="0.222em" id="S5.SSx4.SSSx1.p2.1.m1.1.1.1" xref="S5.SSx4.SSSx1.p2.1.m1.1.1.1.cmml">√ó</mo><mn mathsize="144%" id="S5.SSx4.SSSx1.p2.1.m1.1.1.3" xref="S5.SSx4.SSSx1.p2.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SSx4.SSSx1.p2.1.m1.1b"><apply id="S5.SSx4.SSSx1.p2.1.m1.1.1.cmml" xref="S5.SSx4.SSSx1.p2.1.m1.1.1"><times id="S5.SSx4.SSSx1.p2.1.m1.1.1.1.cmml" xref="S5.SSx4.SSSx1.p2.1.m1.1.1.1"></times><cn type="integer" id="S5.SSx4.SSSx1.p2.1.m1.1.1.2.cmml" xref="S5.SSx4.SSSx1.p2.1.m1.1.1.2">64</cn><cn type="integer" id="S5.SSx4.SSSx1.p2.1.m1.1.1.3.cmml" xref="S5.SSx4.SSSx1.p2.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SSx4.SSSx1.p2.1.m1.1c">64\times 64</annotation></semantics></math><span id="S5.SSx4.SSSx1.p2.1.3" class="ltx_text" style="font-size:144%;">), so consequently small humans with out-of-distribution poses are challenging to rectify. In the right column, we show a failure scenario when the HMR model (i.e. BEV </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SSx4.SSSx1.p2.1.4.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a><span id="S5.SSx4.SSSx1.p2.1.5.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx4.SSSx1.p2.1.6" class="ltx_text" style="font-size:144%;">) fails to reconstruct the humans in close-up shots. We could consider filtering out close-up shots, as they are not the primary intended use cases for Diffusion-HPC.</span></p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2303.09541/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="475" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.13.1.1" class="ltx_text" style="font-size:63%;">Figure S6</span>: </span><span id="S5.F6.14.2" class="ltx_text" style="font-size:63%;">Comparison with Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> on text-conditioned generations. Row 1 and rows 2-3 are generated with MPII <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> and SMART <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> prompts, respectively. Red arrows point out implausible body parts in Stable Diffusion generations.</span></figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2303.09541/assets/x7.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="475" height="91" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.4.1.1" class="ltx_text" style="font-size:63%;">Figure S7</span>: </span><span id="S5.F7.5.2" class="ltx_text" style="font-size:63%;">Failure cases on text-conditioned generations. </span></figcaption>
</figure>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2303.09541/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="502" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.4.1.1" class="ltx_text" style="font-size:63%;">Figure S8</span>: </span><span id="S5.F8.5.2" class="ltx_text" style="font-size:63%;">Comparison to images generated by ControlNet. Top: Pose-to-image ControlNet often results in misaligned limbs (e.g. child legs in top left; front person‚Äôs legs in top right), and does not generalize to challenging domains such as sports poses in general. Bottom: Example synthetic images generated by pose-conditioned ControlNet and Diffusion-HPC. We highlight Diffusion-HPC‚Äôs superior generalization capability, even on extremely challenging domains such as sports. Such generalization capability is key for it to be effective for downstream tasks, such as HMR.</span></figcaption>
</figure>
</section>
<section id="S5.SSx4.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:144%;">Pose-Conditioned Generation</h4>

<div id="S5.SSx4.SSSx2.p1" class="ltx_para">
<p id="S5.SSx4.SSSx2.p1.1" class="ltx_p"><span id="S5.SSx4.SSSx2.p1.1.1" class="ltx_text" style="font-size:144%;">Figure¬†</span><a href="#S5.F9" title="Figure S9 ‚Ä£ Pose-Conditioned Generation ‚Ä£ D. Additional Qualitative Results ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S9</span></a><span id="S5.SSx4.SSSx2.p1.1.2" class="ltx_text" style="font-size:144%;"> shows qualitative comparisons of </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Brooks and Efros</span> <span id="S5.SSx4.SSSx2.p1.1.3.1.1.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a><span id="S5.SSx4.SSSx2.p1.1.4.2.2.1" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx4.SSSx2.p1.1.5" class="ltx_text" style="font-size:144%;"> and Diffusion-HPC on pose-conditioned generations, and
Figure¬†</span><a href="#S5.F10" title="Figure S10 ‚Ä£ Pose-Conditioned Generation ‚Ä£ D. Additional Qualitative Results ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S10</span></a><span id="S5.SSx4.SSSx2.p1.1.6" class="ltx_text" style="font-size:144%;"> shows failures cases of pose-conditione generations. As seen from ‚ÄúOurs T+R‚Äù and ‚ÄúOurs T+R+D‚Äù, human-object interactions are sometimes not preserved.</span></p>
</div>
<div id="S5.SSx4.SSSx2.p2" class="ltx_para">
<p id="S5.SSx4.SSSx2.p2.1" class="ltx_p"><span id="S5.SSx4.SSSx2.p2.1.1" class="ltx_text" style="font-size:144%;">Note that in Diffusion-HPC, human-object interactions are considered but not modelled in an explicit way. Specifically, when we construct the depth map, we use Mask R-CNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SSx4.SSSx2.p2.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a><span id="S5.SSx4.SSSx2.p2.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx4.SSSx2.p2.1.4" class="ltx_text" style="font-size:144%;"> to segment out the occluded body part, which helps with scenarios when, for instance, the person is riding the horse (row 1 of Figure </span><a href="#S5.F9" title="Figure S9 ‚Ä£ Pose-Conditioned Generation ‚Ä£ D. Additional Qualitative Results ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S9</span></a><span id="S5.SSx4.SSSx2.p2.1.5" class="ltx_text" style="font-size:144%;">). However, row 2 of figure¬†</span><a href="#S5.F10" title="Figure S10 ‚Ä£ Pose-Conditioned Generation ‚Ä£ D. Additional Qualitative Results ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S10</span></a><span id="S5.SSx4.SSSx2.p2.1.6" class="ltx_text" style="font-size:144%;"> shows a failure case where the boat is not detected by Mask R-CNN.</span></p>
</div>
<div id="S5.SSx4.SSSx2.p3" class="ltx_para">
<p id="S5.SSx4.SSSx2.p3.1" class="ltx_p"><span id="S5.SSx4.SSSx2.p3.1.1" class="ltx_text" style="font-size:144%;">In addition, in Diffusion-HPC, latents from the initial generations help preserve the objects and context in the final generated scenes. For the pose-conditioned generations here, latents of real images help capture the background objects such as horse and surfboard (row 1 and 3 of Figure¬†</span><a href="#S5.F9" title="Figure S9 ‚Ä£ Pose-Conditioned Generation ‚Ä£ D. Additional Qualitative Results ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S9</span></a><span id="S5.SSx4.SSSx2.p3.1.2" class="ltx_text" style="font-size:144%;">). However, when the background object is occluded or small (row 2 in Figure¬†</span><a href="#S5.F9" title="Figure S9 ‚Ä£ Pose-Conditioned Generation ‚Ä£ D. Additional Qualitative Results ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S9</span></a><span id="S5.SSx4.SSSx2.p3.1.3" class="ltx_text" style="font-size:144%;"> and row 1 in Figure¬†</span><a href="#S5.F10" title="Figure S10 ‚Ä£ Pose-Conditioned Generation ‚Ä£ D. Additional Qualitative Results ‚Ä£ 5 Conclusion ‚Ä£ Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">S10</span></a><span id="S5.SSx4.SSSx2.p3.1.4" class="ltx_text" style="font-size:144%;">), the latents are not sufficient in preserving the object in the final generations. Future work could consider extending Diffusion-HPC by explicitly modelling the human-object/scene interaction.</span></p>
</div>
<div id="S5.SSx4.SSSx2.p4" class="ltx_para">
<p id="S5.SSx4.SSSx2.p4.1" class="ltx_p"><span id="S5.SSx4.SSSx2.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Comparison to ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>.</span><span id="S5.SSx4.SSSx2.p4.1.2" class="ltx_text" style="font-size:144%;">
Adding control to pre-trained generative models has received increasing attention. While it is possible to use works such as ControlNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SSx4.SSSx2.p4.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a><span id="S5.SSx4.SSSx2.p4.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx4.SSSx2.p4.1.5" class="ltx_text" style="font-size:144%;"> to generate pose-conditioned images, we note that ControlNet needs additional finetuning, requiring greater computing and annotation resources. In particular, training pose-to-image ControlNet requires paired data (i.e. 2D keypoints and images) which also limits the training data distribution to easy poses. 2D keypoints inherently provide less information compared to 3D body pose, and solely relying on them for 3D pose understanding tasks is insufficient. As a comparison, we train HMR models on SMART and SkiPose using the synthetic data generated with ControlNet, and on both evaluation sets, PCK and PA-MPJPE are worse than training with Diffusion-HPC (Table 1 and 2). Notably, for SMART, training with ControlNet data is worse than SPIN-ft which does not use synthetic data at all. Regardless of the possibility of re-training ControlNet using 3D human representations as conditioning, it is impractical when considering the substantial number of images with paired 3D GTs demanded. For reference, ControlNet pose-to-image model was trained on 200K keypoint-image pairs. Note that the scarcity of such 3D data was the primary motivation behind our work. Thus, rather than perceiving ControlNet as a direct alternative to our approach, it is more fair to consider it as a promising avenue to enhance our work, as the two methods can be synergistically combined - we could use Diffusion-HPC to bootstrap large amounts of image data with paired 3D pseudo ground truths and then use ControlNet to finetune a diffusion model.</span></p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2303.09541/assets/x9.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="325" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.10.1.1" class="ltx_text" style="font-size:63%;">Figure S9</span>: </span><span id="S5.F9.11.2" class="ltx_text" style="font-size:63%;">Additional qualitative comparisons to <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Brooks and Efros</span> [<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> on the MPII dataset. Input 2D keypoints to <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Brooks and Efros</span> [<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> are overlayed on the bottom left in column 2. Top 3 rows are from MPII, and bottom 3 rows are from SMART. Our generations conditioned on text (T), real images (R). ‚Äú(D)‚Äù means the diffusion model is finetuned on the target dataset (MPII and SMART respectively).</span></figcaption>
</figure>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2303.09541/assets/x10.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.4.1.1" class="ltx_text" style="font-size:63%;">Figure S10</span>: </span><span id="S5.F10.5.2" class="ltx_text" style="font-size:63%;">Failure cases on pose-conditioned generations.</span></figcaption>
</figure>
</section>
</section>
<section id="S5.SSx5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">E. Limitations</h3>

<div id="S5.SSx5.p1" class="ltx_para">
<p id="S5.SSx5.p1.1" class="ltx_p"><span id="S5.SSx5.p1.1.1" class="ltx_text" style="font-size:144%;">As we rely on large pre-trained models </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.SSx5.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a><span id="S5.SSx5.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx5.p1.1.4" class="ltx_text" style="font-size:144%;">, any biases in these models or datasets that they were trained on will be replicated onto our generated images. Due to the resolution of depth maps (</span><math id="S5.SSx5.p1.1.m1.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S5.SSx5.p1.1.m1.1a"><mrow id="S5.SSx5.p1.1.m1.1.1" xref="S5.SSx5.p1.1.m1.1.1.cmml"><mn mathsize="144%" id="S5.SSx5.p1.1.m1.1.1.2" xref="S5.SSx5.p1.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" mathsize="144%" rspace="0.222em" id="S5.SSx5.p1.1.m1.1.1.1" xref="S5.SSx5.p1.1.m1.1.1.1.cmml">√ó</mo><mn mathsize="144%" id="S5.SSx5.p1.1.m1.1.1.3" xref="S5.SSx5.p1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SSx5.p1.1.m1.1b"><apply id="S5.SSx5.p1.1.m1.1.1.cmml" xref="S5.SSx5.p1.1.m1.1.1"><times id="S5.SSx5.p1.1.m1.1.1.1.cmml" xref="S5.SSx5.p1.1.m1.1.1.1"></times><cn type="integer" id="S5.SSx5.p1.1.m1.1.1.2.cmml" xref="S5.SSx5.p1.1.m1.1.1.2">64</cn><cn type="integer" id="S5.SSx5.p1.1.m1.1.1.3.cmml" xref="S5.SSx5.p1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SSx5.p1.1.m1.1c">64\times 64</annotation></semantics></math><span id="S5.SSx5.p1.1.5" class="ltx_text" style="font-size:144%;">), fine details such as fingers and facial expressions are challenging to synthesize. Besides, since we only render person depth maps, human-object/human-scene interactions may not be well-preserved in the final generation (e.g. the person and yoga mat in column 3, row 2 of Figure 3).
While these limitations do not affect downstream tasks where we only care about the body pose, there is large room to improve the photo-realism of human-centric image synthesis, and for the synthetic data to be useful for a wider variety of downstream tasks such as expressive HMR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SSx5.p1.1.6.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a><span id="S5.SSx5.p1.1.7.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx5.p1.1.8" class="ltx_text" style="font-size:144%;"> and recovering human-object/scene interaction </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SSx5.p1.1.9.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a><span id="S5.SSx5.p1.1.10.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S5.SSx5.p1.1.11" class="ltx_text" style="font-size:144%;">. Lastly, as we use SMPL body representation, our method does not consider people with limb losses, but it can be adapted to do so.</span></p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">AlBahar et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Badour AlBahar, Jingwan Lu, Jimei Yang, Zhixin Shu, Eli Shechtman, and Jia-Bin
Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Pose with style: Detail-preserving pose-guided image synthesis with
conditional stylegan.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics (TOG)</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 40(6):1‚Äì11, 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Andriluka et¬†al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">2d human pose estimation: New benchmark and state of the art
analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on computer Vision and
Pattern Recognition</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, pages 3686‚Äì3693, 2014.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Avrahami et¬†al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Omri Avrahami, Ohad Fried, and Dani Lischinski.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Blended latent diffusion.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib3.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.02779</em><span id="bib.bib3.10.2" class="ltx_text" style="font-size:90%;">, 2022a.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Avrahami et¬†al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Omri Avrahami, Dani Lischinski, and Ohad Fried.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Blended diffusion for text-driven editing of natural images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, pages 18208‚Äì18218, 2022b.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Bhatnagar et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Bharat¬†Lal Bhatnagar, Xianghui Xie, Ilya¬†A Petrov, Cristian Sminchisescu,
Christian Theobalt, and Gerard Pons-Moll.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Behave: Dataset and method for tracking human object interactions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, pages 15935‚Äì15946, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Black et¬†al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Michael¬†J. Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">BEDLAM: A synthetic dataset of bodies exhibiting detailed lifelike
animated motion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings IEEE/CVF Conf.¬†on Computer Vision and Pattern
Recognition (CVPR)</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 8726‚Äì8737, 2023.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.4.4.1" class="ltx_text" style="font-size:90%;">Brooks and Efros [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text" style="font-size:90%;">
Tim Brooks and Alexei¬†A Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">Hallucinating pose-compatible scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XVI</em><span id="bib.bib7.10.3" class="ltx_text" style="font-size:90%;">, pages 510‚Äì528.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Cao et¬†al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee¬†K Wong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Dreamavatar: Text-and-shape guided 3d human avatar generation via
diffusion models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.00916</em><span id="bib.bib8.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Chen et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Xin Chen, Anqi Pang, Wei Yang, Yuexin Ma, Lan Xu, and Jingyi Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Sportscap: Monocular 3d human motion capture and fine-grained
understanding in challenging sports videos.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 129:2846‚Äì2864, 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Delmas et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Ginger Delmas, Philippe Weinzaepfel, Thomas Lucas, Francesc Moreno-Noguer, and
Gr√©gory Rogez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Posescript: 3d human poses from natural language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib10.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part VI</em><span id="bib.bib10.11.3" class="ltx_text" style="font-size:90%;">, pages 346‚Äì362.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.4.4.1" class="ltx_text" style="font-size:90%;">Dhariwal and Nichol [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">
Prafulla Dhariwal and Alexander Nichol.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">Diffusion models beat gans on image synthesis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib11.9.2" class="ltx_text" style="font-size:90%;">,
34:8780‚Äì8794, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.4.4.1" class="ltx_text" style="font-size:90%;">Doersch and Zisserman [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text" style="font-size:90%;">
Carl Doersch and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">Sim2real transfer learning for 3d human pose estimation: motion to
the rescue.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib12.9.2" class="ltx_text" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Dong et¬†al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Zijian Dong, Xu Chen, Jinlong Yang, Michael¬†J Black, Otmar Hilliges, and
Andreas Geiger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Ag3d: Learning to generate 3d avatars from 2d image collections.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.02312</em><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><em id="bib.bib14.6.6.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="bib.bib14.7.7.2" class="ltx_text" style="font-size:90%;">[cite][</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">.\hbox{}</span><span id="bib.bib14.8.8.3" class="ltx_text" style="font-size:90%;">] [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.10.1" class="ltx_text" style="font-size:90%;">
Rombach </span><em id="bib.bib14.11.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="bib.bib14.12.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.13.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.14.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.15.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib14.16.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Gholami et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Mohsen Gholami, Bastian Wandt, Helge Rhodin, Rabab Ward, and Z¬†Jane Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Adaptpose: Cross-dataset adaptation for 3d human pose estimation by
learnable motion generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, pages 13075‚Äì13085, 2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Guo et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Generating diverse and natural 3d human motions from text.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, pages 5152‚Äì5161, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">He et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, pages 2961‚Äì2969, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">He et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song
Bai, and Xiaojuan Qi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Is synthetic data from generative models ready for image recognition?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib18.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.07574</em><span id="bib.bib18.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Hertz et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel
Cohen-Or.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Prompt-to-prompt image editing with cross attention control.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2208.01626</em><span id="bib.bib19.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Heusel et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
Hochreiter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Gans trained by a two time-scale update rule converge to a local nash
equilibrium.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib20.10.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Ho et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Denoising diffusion probabilistic models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib21.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib21.10.2" class="ltx_text" style="font-size:90%;">,
33:6840‚Äì6851, 2020.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Hu et¬†al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Animate anyone: Consistent and controllable image-to-video synthesis
for character animation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.17117</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.4.4.1" class="ltx_text" style="font-size:90%;">HuggingFace [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text" style="font-size:90%;">
HuggingFace, 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Ionescu et¬†al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Human3. 6m: Large scale datasets and predictive methods for 3d human
sensing in natural environments.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine
intelligence</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 36(7):1325‚Äì1339, 2013.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.4.4.1" class="ltx_text" style="font-size:90%;">Johnson and Everingham [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">
Sam Johnson and Mark Everingham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">Clustered pose and nonlinear appearance models for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">bmvc</em><span id="bib.bib25.10.3" class="ltx_text" style="font-size:90%;">, page¬†5. Aberystwyth, UK, 2010.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Karras et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and
Timo Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Analyzing and improving the image quality of stylegan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, pages 8110‚Äì8119, 2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Knoche et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Markus Knoche, Istv√°n S√°r√°ndi, and Bastian Leibe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Reposing humans by warping 3d features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition Workshops</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, pages 1044‚Äì1045, 2020.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Kocabas et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Muhammed Kocabas, Chun-Hao¬†P Huang, Otmar Hilliges, and Michael¬†J Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Pare: Part attention regressor for 3d human body estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, pages 11127‚Äì11137, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Kolotouros et¬†al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Nikos Kolotouros, Georgios Pavlakos, Michael¬†J Black, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Learning to reconstruct 3d human pose and shape via model-fitting in
the loop.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, pages 2252‚Äì2261, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Kolotouros et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Probabilistic modeling for human mesh recovery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, pages 11605‚Äì11614, 2021.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Lin et¬†al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Doll√°r, and C¬†Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib31.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision‚ÄìECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em><span id="bib.bib31.11.3" class="ltx_text" style="font-size:90%;">, pages
740‚Äì755. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Loper et¬†al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael¬†J
Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Smpl: A skinned multi-person linear model.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM transactions on graphics (TOG)</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 34(6):1‚Äì16, 2015.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Mahmood et¬†al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Naureen Mahmood, Nima Ghorbani, Nikolaus¬†F Troje, Gerard Pons-Moll, and
Michael¬†J Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Amass: Archive of motion capture as surface shapes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</em><span id="bib.bib33.11.3" class="ltx_text" style="font-size:90%;">, pages 5442‚Äì5451, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Men et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Yifang Men, Yiming Mao, Yuning Jiang, Wei-Ying Ma, and Zhouhui Lian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Controllable person image synthesis with attribute-decomposed gan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib34.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em><span id="bib.bib34.11.3" class="ltx_text" style="font-size:90%;">, pages 5084‚Äì5093, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Nichol et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
Bob McGrew, Ilya Sutskever, and Mark Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Glide: Towards photorealistic image generation and editing with
text-guided diffusion models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib35.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.10741</em><span id="bib.bib35.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Parmar et¬†al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Gaurav Parmar, Krishna¬†Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and
Jun-Yan Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Zero-shot image-to-image translation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib36.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.03027</em><span id="bib.bib36.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Patel et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Priyanka Patel, Chun-Hao¬†P Huang, Joachim Tesch, David¬†T Hoffmann, Shashank
Tripathi, and Michael¬†J Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Agora: Avatars in geography optimized for regression analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib37.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em><span id="bib.bib37.11.3" class="ltx_text" style="font-size:90%;">, pages 13468‚Äì13478, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Pavlakos et¬†al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A.¬†A.
Osman, Dimitrios Tzionas, and Michael¬†J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Expressive body capture: 3d hands, face, and body from a single
image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR)</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Radford et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong¬†Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine learning</em><span id="bib.bib39.11.3" class="ltx_text" style="font-size:90%;">, pages
8748‚Äì8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Ranftl et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Ren√© Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen
Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Towards robust monocular depth estimation: Mixing datasets for
zero-shot cross-dataset transfer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib40.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em><span id="bib.bib40.10.2" class="ltx_text" style="font-size:90%;">, 44(3), 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Rhodin et¬†al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Helge Rhodin, J√∂rg Sp√∂rri, Isinsu Katircioglu, Victor Constantin,
Fr√©d√©ric Meyer, Erich M√ºller, Mathieu Salzmann, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Learning monocular 3d human pose estimation from multi-view images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</em><span id="bib.bib41.11.3" class="ltx_text" style="font-size:90%;">, pages 8437‚Äì8446, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Ronneberger et¬†al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Medical Image Computing and Computer-Assisted
Intervention‚ÄìMICCAI 2015: 18th International Conference, Munich, Germany,
October 5-9, 2015, Proceedings, Part III 18</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, pages 234‚Äì241. Springer, 2015.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Saharia et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
Denton, Seyed Kamyar¬†Seyed Ghasemipour, Burcu¬†Karagol Ayan, S¬†Sara Mahdavi,
Rapha¬†Gontijo Lopes, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Photorealistic text-to-image diffusion models with deep language
understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib43.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.11487</em><span id="bib.bib43.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Schuhmann et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
Wortsman, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Laion-5b: An open large-scale dataset for training next generation
image-text models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.08402</em><span id="bib.bib44.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Sengupta et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Akash Sengupta, Ignas Budvytis, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Synthetic training for accurate 3d human pose and shape estimation in
the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">British Machine Vision Conference (BMVC)</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.4.4.1" class="ltx_text" style="font-size:90%;">Sp√∂rri [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text" style="font-size:90%;">
J√∂rg Sp√∂rri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">Reasearch dedicated to sports injury prevention-the‚Äôsequence of
prevention‚Äôon the example of alpine ski racing.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib46.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Habilitation with Venia Docendi in Biomechanics</em><span id="bib.bib46.9.2" class="ltx_text" style="font-size:90%;">, 1(2):7, 2016.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Sun et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael¬†J Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Putting people in their place: Monocular regression of 3d people in
depth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib47.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib47.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Varol et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael¬†J Black, Ivan
Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib48.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</em><span id="bib.bib48.11.3" class="ltx_text" style="font-size:90%;">, pages 109‚Äì117, 2017.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Wang et¬†al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang,
Zicheng Liu, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Disco: Disentangled control for referring human dance generation in
real world.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib49.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2307.00040</em><span id="bib.bib49.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.4.4.1" class="ltx_text" style="font-size:90%;">Weng and Yeung [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.6.1" class="ltx_text" style="font-size:90%;">
Zhenzhen Weng and Serena Yeung.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">Holistic 3d human and scene mesh estimation from single view images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib50.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em><span id="bib.bib50.10.3" class="ltx_text" style="font-size:90%;">, pages 334‚Äì343, 2021.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Weng et¬†al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Zhenzhen Weng, Kuan-Chieh Wang, Angjoo Kanazawa, and Serena Yeung.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">Domain adaptive 3d pose augmentation for in-the-wild human mesh
recovery.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on 3D Vision (3DV)</em><span id="bib.bib51.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Weng et¬†al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Zhenzhen Weng, Zeyu Wang, and Serena Yeung.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Zeroavatar: Zero-shot 3d avatar generation from a single image.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib52.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.16411</em><span id="bib.bib52.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Wu et¬†al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Detectron2.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/facebookresearch/detectron2" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/facebookresearch/detectron2</a><span id="bib.bib53.9.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Xu et¬†al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Zhongcong Xu, Jianfeng Zhang, Jun¬†Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu
Zhang, Jiashi Feng, and Mike¬†Zheng Shou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">Magicanimate: Temporally consistent human image animation using
diffusion model.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib54.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.16498</em><span id="bib.bib54.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
Jason¬†Y Zhang, Sam Pepose, Hanbyul Joo, Deva Ramanan, Jitendra Malik, and
Angjoo Kanazawa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">Perceiving 3d human-object spatial arrangements from a single image
in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib55.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision‚ÄìECCV 2020: 16th European Conference,
Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part XII 16</em><span id="bib.bib55.11.3" class="ltx_text" style="font-size:90%;">, pages 34‚Äì51.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.4.4.1" class="ltx_text" style="font-size:90%;">Zhang and Agrawala [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.6.1" class="ltx_text" style="font-size:90%;">
Lvmin Zhang and Maneesh Agrawala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">Adding conditional control to text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib56.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.05543</em><span id="bib.bib56.9.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela
Barriuso, Antonio Torralba, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">Datasetgan: Efficient labeled data factory with minimal human effort.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib57.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em><span id="bib.bib57.11.3" class="ltx_text" style="font-size:90%;">, pages 10145‚Äì10155, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.09540" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.09541" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.09541">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.09541" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.09542" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 19:57:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
