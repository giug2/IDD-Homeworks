<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.14584] DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning</title><meta property="og:description" content="Simple data augmentation techniques, such as rotations and flips, are widely used to enhance the generalization power of computer vision models. However, these techniques often fail to modify high-level semantic attribâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.14584">

<!--Generated on Thu Sep  5 14:20:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Image Augmentation Diffusion Models Few-Shot Classification Dataset Diversity.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span><sup id="id1.1" class="ltx_sup">1</sup>Department of Computer Science, Technical University of Darmstadt, Germany
<br class="ltx_break"><sup id="id1.2" class="ltx_sup">2</sup>Hessian Center for AI (hessian.AI)
<br class="ltx_break"><span id="id1.3" class="ltx_text ltx_font_typewriter">{tobias.lingenberg, markus.reuter}@stud.tu-darmstadt.de</span> 
<br class="ltx_break"><span id="id1.4" class="ltx_text ltx_font_typewriter">{gopika.sudhakaran, stefan.roth, simone.schaub}@visinf.tu-darmstadt.de</span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tobias Lingenberg<sup id="id1.1.id1" class="ltx_sup">*,1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0000-7061-9210" title="ORCID identifier" class="ltx_ref">0009-0000-7061-9210</a></span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Markus Reuter<sup id="id2.1.id1" class="ltx_sup">*,1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0000-2179-4901" title="ORCID identifier" class="ltx_ref">0009-0000-2179-4901</a></span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gopika Sudhakaran<sup id="id3.1.id1" class="ltx_sup">â€ ,1,2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0007-3721-5602" title="ORCID identifier" class="ltx_ref">0009-0007-3721-5602</a></span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dominik Gojny<sup id="id4.1.id1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-1965-8286" title="ORCID identifier" class="ltx_ref">0000-0003-1965-8286</a></span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stefan Roth<sup id="id5.1.id1" class="ltx_sup">1,2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-9002-9832" title="ORCID identifier" class="ltx_ref">0000-0001-9002-9832</a></span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simone Schaub-Meyer<sup id="id6.1.id1" class="ltx_sup">1,2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-8644-1074" title="ORCID identifier" class="ltx_ref">0000-0001-8644-1074</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Simple data augmentation techniques, such as rotations and flips, are widely used to enhance the generalization power of computer vision models. However, these techniques often fail to modify high-level semantic attributes of a class. To address this limitation, researchers have explored generative augmentation methods like the recently proposed DA-Fusion. Despite some progress, the variations are still largely limited to textural changes, thus falling short on aspects like varied viewpoints, environment, weather conditions, or even class-level semantic attributes (<em id="id7.id1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="id7.id1.2" class="ltx_text"></span>, variations in a dogâ€™s breed). To overcome this challenge, we propose DIAGen, building upon DA-Fusion.
First, we apply Gaussian noise to the embeddings of an object learned with Textual Inversion to diversify generations using a pre-trained diffusion modelâ€™s knowledge.
Second, we exploit the general knowledge of a text-to-text generative model to guide the image generation of the diffusion model with varied class-specific prompts.
Finally, We introduce a weighting mechanism to mitigate the impact of poorly generated samples. Experimental results across various datasets show that DIAGen not only enhances semantic diversity but also improves the performance of subsequent classifiers. The advantages of DIAGen over standard augmentations and the DA-Fusion baseline are particularly pronounced with out-of-distribution samples.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code is available at <a target="_blank" href="https://github.com/visinf/DIAGen" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://github.com/visinf/DIAGen</a>, <sup id="footnote1.1" class="ltx_sup">*</sup>Equal contribution, <sup id="footnote1.2" class="ltx_sup">â€ </sup>Corresponding author.</span></span></span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Image Augmentation Diffusion Models Few-Shot Classification Dataset Diversity.
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2408.14584/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="124" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.5.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.6.2" class="ltx_text" style="font-size:90%;">Comparison of augmentation results between the baseline method DA-Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> <em id="S0.F1.6.2.1" class="ltx_emph ltx_font_italic">(left)</em> and our proposed approach DIAGen <em id="S0.F1.6.2.2" class="ltx_emph ltx_font_italic">(right)</em>, utilizing the same guiding image <em id="S0.F1.6.2.3" class="ltx_emph ltx_font_italic">(middle)</em> for the augmentation process.
DIAGen demonstrates superior, semantically diverse image augmentations, as evidenced through more variation of object appearance and settings.
This observation is supported by improvements in classification accuracy and recall as a diversity metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A common problem in the field of computer vision is the insufficient amount of real-world training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Collecting and annotating data at scale can be difficult, expensive, and time-consuming <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
To address this issue, data augmentation techniques are crucial in scenarios with very few labelled samples (few-shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, as they support generalization and robustness by introducing data variation. While offering valuable benefits, standard augmentation methods like rotations, flips, and scaling often fall short in providing semantic diversity beyond that of the original data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
This lack of diversity in training data negatively influences downstream applications, <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p1.1.2" class="ltx_text"></span>, for objects that typically occur in certain environments, such as a cow being correctly classified on a grassy background but not on a beach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
In response to the bottleneck of real data and its lack of diversity, researchers have turned to synthetic data generation as a promising alternative <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Recent advancements, such as DA-Fusion by Trabucco <em id="S1.p1.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p1.1.4" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, demonstrate the potential of synthetic data augmentation by using an off-the-shelf diffusion model.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, the lack of diversity in synthetic data generation is still a known issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
Upon inspecting the images generated by DA-Fusion (see <a href="#S0.F1" title="In DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> andÂ <a href="#S4.F6" title="Figure 6 â€£ 4.5 Diversity Analysis â€£ 4 Experiments â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), it is evident that they appear to be very similar, primarily altering textural details and minor structural elements with no noticeable change in viewpoint, limiting its effect as an augmentation technique.
We observe that DA-Fusion is constrained in achieving sufficient diversity due to a lack of control over how an image is augmented.
To address this issue, we propose DIAGen (<span id="S1.p2.1.1" class="ltx_text ltx_font_bold">D</span>iverse <span id="S1.p2.1.2" class="ltx_text ltx_font_bold">I</span>mage <span id="S1.p2.1.3" class="ltx_text ltx_font_bold">A</span>ugmentation with <span id="S1.p2.1.4" class="ltx_text ltx_font_bold">Gen</span>erative Models), which builds on DA-Fusion and adds three components to it.
Thereby, DIAGen enhances the semantic diversity of synthetic images while maintaining high quality, making it an effective augmentation technique to generate diverse training data that simulates a wide range of environments for applications such as autonomous vehicles and robotics. Thus, improving model robustness by enabling safer, more reliable behavior in real-world scenarios. Additionally, DIAGen can be applied to downstream tasks like relation detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> to enhance the augmentation diversity of rare relations to mitigate biased prediction.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">main contributions</span> of our work can be summarized as follows:
<em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">(i)</em> We introduce variations in the embedding space of learned class concepts by adding Gaussian noise, taking advantage of the semantic richness inherent in vector representations within the embedding space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
<em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">(ii)</em> Inspired by the idea of He <em id="S1.p3.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p3.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, we guide the generation process of the diffusion model with varied class-specific text prompts.
In contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, we obtain meaningful prompts by leveraging the world-knowledge of a text-to-text generative model, here <em id="S1.p3.1.6" class="ltx_emph ltx_font_italic">GPT-4</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, increasing the diversity may result in a reduced quality of some of the generated images, a challenge referred to as the fidelity-diversity trade-off <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
<em id="S1.p3.1.7" class="ltx_emph ltx_font_italic">(iii)</em> To tackle this potential issue, we use a weighting mechanism for synthetic images, which was previously considered in the context of Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.
<em id="S1.p3.1.8" class="ltx_emph ltx_font_italic">(iv)</em> We show the effectiveness of our model by comparing the accuracy of a downstream classifier to DA-Fusion and standard augmentations across multiple datasets in few-shot settings.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.1 </span>Synthetic Data for Few-Shot Learning.</h4>

<div id="S2.SS0.SSS1.p1" class="ltx_para">
<p id="S2.SS0.SSS1.p1.1" class="ltx_p">Numerous works have explored synthetic image generation using GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. In few-shot learning, the small number of labelled images presents a challenge due to the inherently scarce class sampling. Collecting more real-world data is resource intensive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, but synthetic data can utilize the knowledge of pre-trained generative model. While GANs have already been used for few-shot learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, diffusion models offer better results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> due to their stability, high image quality, and flexibility during image generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S2.SS0.SSS1.p2" class="ltx_para">
<p id="S2.SS0.SSS1.p2.1" class="ltx_p">Recently, Trabucco <em id="S2.SS0.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS0.SSS1.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> attempted to generalize their pipeline based on a diffusion model, DA-Fusion, to unseen concepts by integrating Textual Inversion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
This method uses three to five real images to learn new visual concepts, creating pseudo word vectors for a text-to-image model. Textual Inversion is ideal for few-shot learning, enhancing the text encoderâ€™s vocabulary with new concepts. DA-Fusion then uses these embeddings to generate synthetic images with Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. The denoising process of the diffusion model is conditioned on the text prompt and guided by a real training image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
DIAGen builds upon the work of Trabucco <em id="S2.SS0.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS0.SSS1.p2.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> to increase the semantic diversity of the synthetic images further.</p>
</div>
</section>
<section id="S2.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.2 </span>Diversity in Datasets.</h4>

<div id="S2.SS0.SSS2.p1" class="ltx_para">
<p id="S2.SS0.SSS2.p1.1" class="ltx_p">The quality of machine learning models heavily relies on the diversity of their training data.
A lack of diversity can lead to biases and poor performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, particularly in few-shot scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
Creating synthetic data comes with a challenging trade-off: balancing fidelity for accurate representation and diversity for increased coverage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
While previous methods have improved synthetic data quality, they only address coverage implicitly. That said, there have been attempts to explicitly focus on the aspect of diversity.
Wang <em id="S2.SS0.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS0.SSS2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> demonstrated promising results by using a diversity measurement-based meta-learner.
He <em id="S2.SS0.SSS2.p1.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS0.SSS2.p1.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> utilized a text-to-image diffusion model and inserted different descriptive image prompts to achieve a higher coverage for zero- and few-shot learning.
Although we use a similar idea, our method can generalize to unseen concepts and provide explicit control over how image prompts are generated, by instructing our LLM with a meta prompt.</p>
</div>
<div id="S2.SS0.SSS2.p2" class="ltx_para">
<p id="S2.SS0.SSS2.p2.1" class="ltx_p">Due to the importance of a high visual quality and coverage of synthetic datasets, many metrics have been proposed to assess these properties <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
The most common metrics are the FrÃ©chet Inception Distance (FID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and the Inception Score (IS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, which rely on a pre-existing classifier (InceptionNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>).
However, both summarise the comparison of the two distributions (real and synthetic) into a single number, overlooking the distinction between fidelity and diversity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. To address this, more refined metrics like precision and recall <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, density and coverage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and the Vendi score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> have been developed.
In our work, we use an improved version of precision and recall <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> due to its wide acceptance in the text-to-image community and its high agreement with human perception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.3 </span>Out-of-Distribution Generalization.</h4>

<div id="S2.SS0.SSS3.p1" class="ltx_para">
<p id="S2.SS0.SSS3.p1.1" class="ltx_p">Conventional machine learning algorithms often assume that training and test data come from the same distribution. However, in real-world applications, this assumption often fails to hold due to unforeseen distributional shifts. This can lead to a drastic decline in real-world performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Especially in safety-critical applications, these out-of-distribution (OOD) scenarios need to be handled with the same quality and confidence as identically distributed data. While there have been advancements in OOD detection to reject these samples or hand them over to human users (<em id="S2.SS0.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS0.SSS3.p1.1.2" class="ltx_text"></span>, in the case of autonomous driving) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, our goal is to investigate whether an increased dataset diversity allows for the implicit handling of these cases.</p>
</div>
<div id="S2.SS0.SSS3.p2" class="ltx_para">
<p id="S2.SS0.SSS3.p2.1" class="ltx_p">Recent advancements in large-scale models designed to encapsulate extensive world knowledge have enabled a broader coverage of OOD examples. Tong and Dai <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> demonstrate the promising performance of pre-trained text-to-image diffusion models for OOD generalization. However, despite advancements, studies indicate that large-scale text-generation models are still not as effective at handling OOD cases compared to identically distributed data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
Building on these developments, we leverage two distinct large-scale models trained on text and image modalities, harnessing their comprehensive world knowledge.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.5" class="ltx_p">The input to our model DIAGen is a small dataset, <math id="S3.p1.1.m1.5" class="ltx_Math" alttext="R=\{R_{n}\mid 1,\ldots,N\}" display="inline"><semantics id="S3.p1.1.m1.5a"><mrow id="S3.p1.1.m1.5.5" xref="S3.p1.1.m1.5.5.cmml"><mi id="S3.p1.1.m1.5.5.4" xref="S3.p1.1.m1.5.5.4.cmml">R</mi><mo id="S3.p1.1.m1.5.5.3" xref="S3.p1.1.m1.5.5.3.cmml">=</mo><mrow id="S3.p1.1.m1.5.5.2.2" xref="S3.p1.1.m1.5.5.2.3.cmml"><mo stretchy="false" id="S3.p1.1.m1.5.5.2.2.3" xref="S3.p1.1.m1.5.5.2.3.1.cmml">{</mo><msub id="S3.p1.1.m1.4.4.1.1.1" xref="S3.p1.1.m1.4.4.1.1.1.cmml"><mi id="S3.p1.1.m1.4.4.1.1.1.2" xref="S3.p1.1.m1.4.4.1.1.1.2.cmml">R</mi><mi id="S3.p1.1.m1.4.4.1.1.1.3" xref="S3.p1.1.m1.4.4.1.1.1.3.cmml">n</mi></msub><mo fence="true" lspace="0em" rspace="0em" id="S3.p1.1.m1.5.5.2.2.4" xref="S3.p1.1.m1.5.5.2.3.1.cmml">âˆ£</mo><mrow id="S3.p1.1.m1.5.5.2.2.2.2" xref="S3.p1.1.m1.5.5.2.2.2.1.cmml"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">1</mn><mo id="S3.p1.1.m1.5.5.2.2.2.2.1" xref="S3.p1.1.m1.5.5.2.2.2.1.cmml">,</mo><mi mathvariant="normal" id="S3.p1.1.m1.2.2" xref="S3.p1.1.m1.2.2.cmml">â€¦</mi><mo id="S3.p1.1.m1.5.5.2.2.2.2.2" xref="S3.p1.1.m1.5.5.2.2.2.1.cmml">,</mo><mi id="S3.p1.1.m1.3.3" xref="S3.p1.1.m1.3.3.cmml">N</mi></mrow><mo stretchy="false" id="S3.p1.1.m1.5.5.2.2.5" xref="S3.p1.1.m1.5.5.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.5b"><apply id="S3.p1.1.m1.5.5.cmml" xref="S3.p1.1.m1.5.5"><eq id="S3.p1.1.m1.5.5.3.cmml" xref="S3.p1.1.m1.5.5.3"></eq><ci id="S3.p1.1.m1.5.5.4.cmml" xref="S3.p1.1.m1.5.5.4">ğ‘…</ci><apply id="S3.p1.1.m1.5.5.2.3.cmml" xref="S3.p1.1.m1.5.5.2.2"><csymbol cd="latexml" id="S3.p1.1.m1.5.5.2.3.1.cmml" xref="S3.p1.1.m1.5.5.2.2.3">conditional-set</csymbol><apply id="S3.p1.1.m1.4.4.1.1.1.cmml" xref="S3.p1.1.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.4.4.1.1.1.1.cmml" xref="S3.p1.1.m1.4.4.1.1.1">subscript</csymbol><ci id="S3.p1.1.m1.4.4.1.1.1.2.cmml" xref="S3.p1.1.m1.4.4.1.1.1.2">ğ‘…</ci><ci id="S3.p1.1.m1.4.4.1.1.1.3.cmml" xref="S3.p1.1.m1.4.4.1.1.1.3">ğ‘›</ci></apply><list id="S3.p1.1.m1.5.5.2.2.2.1.cmml" xref="S3.p1.1.m1.5.5.2.2.2.2"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">1</cn><ci id="S3.p1.1.m1.2.2.cmml" xref="S3.p1.1.m1.2.2">â€¦</ci><ci id="S3.p1.1.m1.3.3.cmml" xref="S3.p1.1.m1.3.3">ğ‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.5c">R=\{R_{n}\mid 1,\ldots,N\}</annotation></semantics></math> of <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">N</annotation></semantics></math> real images, containing only a few images per class.
The output of the pipeline (see <a href="#S3.F2" title="In 3 Methodology â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>) is an expanded labelled dataset that includes both the real images as well as <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">M</annotation></semantics></math> corresponding synthetic images <math id="S3.p1.4.m4.2" class="ltx_Math" alttext="S_{n,m}" display="inline"><semantics id="S3.p1.4.m4.2a"><msub id="S3.p1.4.m4.2.3" xref="S3.p1.4.m4.2.3.cmml"><mi id="S3.p1.4.m4.2.3.2" xref="S3.p1.4.m4.2.3.2.cmml">S</mi><mrow id="S3.p1.4.m4.2.2.2.4" xref="S3.p1.4.m4.2.2.2.3.cmml"><mi id="S3.p1.4.m4.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.cmml">n</mi><mo id="S3.p1.4.m4.2.2.2.4.1" xref="S3.p1.4.m4.2.2.2.3.cmml">,</mo><mi id="S3.p1.4.m4.2.2.2.2" xref="S3.p1.4.m4.2.2.2.2.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.2b"><apply id="S3.p1.4.m4.2.3.cmml" xref="S3.p1.4.m4.2.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.2.3.1.cmml" xref="S3.p1.4.m4.2.3">subscript</csymbol><ci id="S3.p1.4.m4.2.3.2.cmml" xref="S3.p1.4.m4.2.3.2">ğ‘†</ci><list id="S3.p1.4.m4.2.2.2.3.cmml" xref="S3.p1.4.m4.2.2.2.4"><ci id="S3.p1.4.m4.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1.1">ğ‘›</ci><ci id="S3.p1.4.m4.2.2.2.2.cmml" xref="S3.p1.4.m4.2.2.2.2">ğ‘š</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.2c">S_{n,m}</annotation></semantics></math> for each real image <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="R_{n}" display="inline"><semantics id="S3.p1.5.m5.1a"><msub id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">R</mi><mi id="S3.p1.5.m5.1.1.3" xref="S3.p1.5.m5.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">ğ‘…</ci><ci id="S3.p1.5.m5.1.1.3.cmml" xref="S3.p1.5.m5.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">R_{n}</annotation></semantics></math>.
The goal is to augment the small given dataset in a semantically diverse way to enable the training of a downstream application with better generalization.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Before detailing our method to increase semantic diversity, we first lay out, how we define diversity.
We focus on improving the intra-class diversity that represents the variance within the data points of a class.
We aim to improve two different aspects of diversity: First, we address the different semantically meaningful contexts in which the class can occur. Here we use the three categories of diverse settings as proposed by Kattakinda and Feizi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, namely weather conditions, time of day, and environment.
Second, we enhance the diversity of object appearance itself, <em id="S3.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p2.1.2" class="ltx_text"></span>, by changing the type of a motorcycle (<em id="S3.p2.1.3" class="ltx_emph ltx_font_italic">cf</em>.<span id="S3.p2.1.4" class="ltx_text"></span> <a href="#S0.F1" title="In DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2408.14584/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="240" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.7.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.2.1" class="ltx_text" style="font-size:90%;">DIAGenâ€™s image generation pipeline based on DA-Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
Our contributions include: <em id="S3.F2.2.1.1" class="ltx_emph ltx_font_italic">a)</em> Varying the learned class concept in the embedding space by applying Gaussian noise. <em id="S3.F2.2.1.2" class="ltx_emph ltx_font_italic">b)</em> Using varied class-specific prompts generated by an LLM. <em id="S3.F2.2.1.3" class="ltx_emph ltx_font_italic">c)</em> Training and utilizing a classifier trained on real images as a weighting mechanism. All real images combined with the generated synthetic ones are then used to train an arbitrary downstream model. The ratio of real to synthetic images can be controlled by the synthetic probability hyperparameter <math id="S3.F2.2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F2.2.1.m1.1b"><mi id="S3.F2.2.1.m1.1.1" xref="S3.F2.2.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.F2.2.1.m1.1c"><ci id="S3.F2.2.1.m1.1.1.cmml" xref="S3.F2.2.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.1.m1.1d">\alpha</annotation></semantics></math>.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Embedding Noise</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.5" class="ltx_p">The diffusion model that DIAGen builds upon is conditioned on a text prompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> containing the learned pseudo word vector for a specific class. The word vector of a class, <em id="S3.SS1.p1.5.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.p1.5.2" class="ltx_text"></span>, <em id="S3.SS1.p1.5.3" class="ltx_emph ltx_font_italic">car</em> to give a concrete example, is learned with Textual Inversion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and the resulting embedding vector <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{S_{*}}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">ğ’®</mi><mo id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">âˆ—</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ’®</ci><times id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathcal{S_{*}}</annotation></semantics></math> is inserted into the prompt â€œa photo of a <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{S}_{*}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">ğ’®</mi><mo id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">âˆ—</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ’®</ci><times id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathcal{S}_{*}</annotation></semantics></math>â€. Following Mikolov <em id="S3.SS1.p1.5.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.p1.5.5" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, who observed that directions in embedding spaces represent semantic meaning, <em id="S3.SS1.p1.5.6" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.p1.5.7" class="ltx_text"></span>, <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\text{\emph{king}}-\text{\emph{man}}+\text{\emph{woman}}=\text{\emph{queen}}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml"><mtext class="ltx_mathvariant_italic" id="S3.SS1.p1.3.m3.1.1.2.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.2b.cmml"><em id="S3.SS1.p1.3.m3.1.1.2.2.2.1nest" class="ltx_emph ltx_font_italic">king</em></mtext><mo id="S3.SS1.p1.3.m3.1.1.2.2.1" xref="S3.SS1.p1.3.m3.1.1.2.2.1.cmml">âˆ’</mo><mtext class="ltx_mathvariant_italic" id="S3.SS1.p1.3.m3.1.1.2.2.3" xref="S3.SS1.p1.3.m3.1.1.2.2.3b.cmml"><em id="S3.SS1.p1.3.m3.1.1.2.2.3.1nest" class="ltx_emph ltx_font_italic">man</em></mtext></mrow><mo id="S3.SS1.p1.3.m3.1.1.2.1" xref="S3.SS1.p1.3.m3.1.1.2.1.cmml">+</mo><mtext class="ltx_mathvariant_italic" id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3b.cmml"><em id="S3.SS1.p1.3.m3.1.1.2.3.1nest" class="ltx_emph ltx_font_italic">woman</em></mtext></mrow><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">=</mo><mtext class="ltx_mathvariant_italic" id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3b.cmml"><em id="S3.SS1.p1.3.m3.1.1.3.1nest" class="ltx_emph ltx_font_italic">queen</em></mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><eq id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></eq><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><plus id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2.1"></plus><apply id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2"><minus id="S3.SS1.p1.3.m3.1.1.2.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2.1"></minus><ci id="S3.SS1.p1.3.m3.1.1.2.2.2b.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2.2"><mtext class="ltx_mathvariant_italic" id="S3.SS1.p1.3.m3.1.1.2.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2.2"><em id="S3.SS1.p1.3.m3.1.1.2.2.2.1anest" class="ltx_emph ltx_font_italic">king</em></mtext></ci><ci id="S3.SS1.p1.3.m3.1.1.2.2.3b.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2.3"><mtext class="ltx_mathvariant_italic" id="S3.SS1.p1.3.m3.1.1.2.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2.3"><em id="S3.SS1.p1.3.m3.1.1.2.2.3.1anest" class="ltx_emph ltx_font_italic">man</em></mtext></ci></apply><ci id="S3.SS1.p1.3.m3.1.1.2.3b.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3"><mtext class="ltx_mathvariant_italic" id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3"><em id="S3.SS1.p1.3.m3.1.1.2.3.1anest" class="ltx_emph ltx_font_italic">woman</em></mtext></ci></apply><ci id="S3.SS1.p1.3.m3.1.1.3b.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><mtext class="ltx_mathvariant_italic" id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><em id="S3.SS1.p1.3.m3.1.1.3.1anest" class="ltx_emph ltx_font_italic">queen</em></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\text{\emph{king}}-\text{\emph{man}}+\text{\emph{woman}}=\text{\emph{queen}}</annotation></semantics></math>, and that vectors that are very close to each other also have very similar meaning, we propose adding noise on top of the learned class concept vectors (see <a href="#S3.F2" title="In 3 Methodology â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>, contribution a). We hypothesize that varying <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{S}_{*}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">ğ’®</mi><mo id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">âˆ—</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ’®</ci><times id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathcal{S}_{*}</annotation></semantics></math> of an object yields images of similar object types, since their representations are likely to be close together in the embedding space.
This may result in scenarios where the representation of <em id="S3.SS1.p1.5.8" class="ltx_emph ltx_font_italic">oldtimer</em> is next to the embedding vector of our learned representation of <em id="S3.SS1.p1.5.9" class="ltx_emph ltx_font_italic">car</em>. The generation of a noisy embedding vector <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{S}_{v}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">ğ’®</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">ğ’®</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\mathcal{S}_{v}</annotation></semantics></math> can be formulated as</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\mathcal{S}_{v}=\mathcal{S}_{*}+\mathcal{N}(0,\sigma^{2})\,," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.3.2" xref="S3.E1.m1.2.2.1.1.3.2.cmml">ğ’®</mi><mi id="S3.E1.m1.2.2.1.1.3.3" xref="S3.E1.m1.2.2.1.1.3.3.cmml">v</mi></msub><mo id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.3.2.cmml">ğ’®</mi><mo id="S3.E1.m1.2.2.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.1.3.3.cmml">âˆ—</mo></msub><mo id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">+</mo><mrow id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml">(</mo><mn id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">0</mn><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml">,</mo><msup id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">Ïƒ</mi><mn id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">2</mn></msup><mo rspace="0.170em" stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.4" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"></eq><apply id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2">ğ’®</ci><ci id="S3.E1.m1.2.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3">ğ‘£</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><plus id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2"></plus><apply id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2">ğ’®</ci><times id="S3.E1.m1.2.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3"></times></apply><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2"></times><ci id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3">ğ’©</ci><interval closure="open" id="S3.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><cn type="integer" id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">0</cn><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2">ğœ</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3">2</cn></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\mathcal{S}_{v}=\mathcal{S}_{*}+\mathcal{N}(0,\sigma^{2})\,,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.9" class="ltx_p">where <math id="S3.SS1.p1.6.m1.1" class="ltx_Math" alttext="\mathcal{S}_{*}" display="inline"><semantics id="S3.SS1.p1.6.m1.1a"><msub id="S3.SS1.p1.6.m1.1.1" xref="S3.SS1.p1.6.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.6.m1.1.1.2" xref="S3.SS1.p1.6.m1.1.1.2.cmml">ğ’®</mi><mo id="S3.SS1.p1.6.m1.1.1.3" xref="S3.SS1.p1.6.m1.1.1.3.cmml">âˆ—</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m1.1b"><apply id="S3.SS1.p1.6.m1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m1.1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m1.1.1.2.cmml" xref="S3.SS1.p1.6.m1.1.1.2">ğ’®</ci><times id="S3.SS1.p1.6.m1.1.1.3.cmml" xref="S3.SS1.p1.6.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m1.1c">\mathcal{S}_{*}</annotation></semantics></math> is the original word embedding vector obtained with Textual Inversion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and <math id="S3.SS1.p1.7.m2.2" class="ltx_Math" alttext="\mathcal{N}(0,\sigma^{2})" display="inline"><semantics id="S3.SS1.p1.7.m2.2a"><mrow id="S3.SS1.p1.7.m2.2.2" xref="S3.SS1.p1.7.m2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.7.m2.2.2.3" xref="S3.SS1.p1.7.m2.2.2.3.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.7.m2.2.2.2" xref="S3.SS1.p1.7.m2.2.2.2.cmml">â€‹</mo><mrow id="S3.SS1.p1.7.m2.2.2.1.1" xref="S3.SS1.p1.7.m2.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.7.m2.2.2.1.1.2" xref="S3.SS1.p1.7.m2.2.2.1.2.cmml">(</mo><mn id="S3.SS1.p1.7.m2.1.1" xref="S3.SS1.p1.7.m2.1.1.cmml">0</mn><mo id="S3.SS1.p1.7.m2.2.2.1.1.3" xref="S3.SS1.p1.7.m2.2.2.1.2.cmml">,</mo><msup id="S3.SS1.p1.7.m2.2.2.1.1.1" xref="S3.SS1.p1.7.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.7.m2.2.2.1.1.1.2" xref="S3.SS1.p1.7.m2.2.2.1.1.1.2.cmml">Ïƒ</mi><mn id="S3.SS1.p1.7.m2.2.2.1.1.1.3" xref="S3.SS1.p1.7.m2.2.2.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S3.SS1.p1.7.m2.2.2.1.1.4" xref="S3.SS1.p1.7.m2.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m2.2b"><apply id="S3.SS1.p1.7.m2.2.2.cmml" xref="S3.SS1.p1.7.m2.2.2"><times id="S3.SS1.p1.7.m2.2.2.2.cmml" xref="S3.SS1.p1.7.m2.2.2.2"></times><ci id="S3.SS1.p1.7.m2.2.2.3.cmml" xref="S3.SS1.p1.7.m2.2.2.3">ğ’©</ci><interval closure="open" id="S3.SS1.p1.7.m2.2.2.1.2.cmml" xref="S3.SS1.p1.7.m2.2.2.1.1"><cn type="integer" id="S3.SS1.p1.7.m2.1.1.cmml" xref="S3.SS1.p1.7.m2.1.1">0</cn><apply id="S3.SS1.p1.7.m2.2.2.1.1.1.cmml" xref="S3.SS1.p1.7.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.7.m2.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.p1.7.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.7.m2.2.2.1.1.1.2">ğœ</ci><cn type="integer" id="S3.SS1.p1.7.m2.2.2.1.1.1.3.cmml" xref="S3.SS1.p1.7.m2.2.2.1.1.1.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m2.2c">\mathcal{N}(0,\sigma^{2})</annotation></semantics></math> is a noise sample of the same dimensions from a Gaussian distribution with zero mean and variance <math id="S3.SS1.p1.8.m3.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S3.SS1.p1.8.m3.1a"><msup id="S3.SS1.p1.8.m3.1.1" xref="S3.SS1.p1.8.m3.1.1.cmml"><mi id="S3.SS1.p1.8.m3.1.1.2" xref="S3.SS1.p1.8.m3.1.1.2.cmml">Ïƒ</mi><mn id="S3.SS1.p1.8.m3.1.1.3" xref="S3.SS1.p1.8.m3.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m3.1b"><apply id="S3.SS1.p1.8.m3.1.1.cmml" xref="S3.SS1.p1.8.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m3.1.1.1.cmml" xref="S3.SS1.p1.8.m3.1.1">superscript</csymbol><ci id="S3.SS1.p1.8.m3.1.1.2.cmml" xref="S3.SS1.p1.8.m3.1.1.2">ğœ</ci><cn type="integer" id="S3.SS1.p1.8.m3.1.1.3.cmml" xref="S3.SS1.p1.8.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m3.1c">\sigma^{2}</annotation></semantics></math>. Further details on the visual impact of this noise and the selection of the hyperparameter <math id="S3.SS1.p1.9.m4.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S3.SS1.p1.9.m4.1a"><msup id="S3.SS1.p1.9.m4.1.1" xref="S3.SS1.p1.9.m4.1.1.cmml"><mi id="S3.SS1.p1.9.m4.1.1.2" xref="S3.SS1.p1.9.m4.1.1.2.cmml">Ïƒ</mi><mn id="S3.SS1.p1.9.m4.1.1.3" xref="S3.SS1.p1.9.m4.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m4.1b"><apply id="S3.SS1.p1.9.m4.1.1.cmml" xref="S3.SS1.p1.9.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m4.1.1.1.cmml" xref="S3.SS1.p1.9.m4.1.1">superscript</csymbol><ci id="S3.SS1.p1.9.m4.1.1.2.cmml" xref="S3.SS1.p1.9.m4.1.1.2">ğœ</ci><cn type="integer" id="S3.SS1.p1.9.m4.1.1.3.cmml" xref="S3.SS1.p1.9.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m4.1c">\sigma^{2}</annotation></semantics></math> are in Appendix D.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>LLM Prompting</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To achieve more explicit control over image generation beyond simply adding noise to the class embedding, we utilise a large language model (LLM) to provide textual guidance for the diffusion model (see <a href="#S3.F2" title="In 3 Methodology â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>, contribution b).
Specifically, we employ <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">GPT-4</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, known for its robustness and extensive knowledge acquired from internet-scale data. We also tried the smaller model Llama2 (7B) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and observed a similar performance.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Due to the different functioning and training data of language and image models, the covered knowledge also differs. This is beneficial in scenarios where the diffusion model has rarely seen a concept and hence has no contextual knowledge of the concept. An LLM such as GPT-4 can provide additional meaningful context so that the resulting synthetic images exhibit high semantic diversity.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We instructed GPT-4 to dynamically generate a certain number of prompts in the following style:</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<span id="S3.SS2.p4.9" class="ltx_inline-block ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<span id="S3.SS2.p4.9.9" class="ltx_p">a photo of a <math id="S3.SS2.p4.1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S3.SS2.p4.1.1.m1.1a"><mo stretchy="false" id="S3.SS2.p4.1.1.m1.1.1" xref="S3.SS2.p4.1.1.m1.1.1.cmml">âŸ¨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.1.m1.1b"><ci id="S3.SS2.p4.1.1.m1.1.1.cmml" xref="S3.SS2.p4.1.1.m1.1.1">âŸ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.1.m1.1c">\langle</annotation></semantics></math>adjective<math id="S3.SS2.p4.2.2.m2.1" class="ltx_math_unparsed" alttext="\rangle\mbox{ }\mathcal{S}_{v}\mbox{ }\langle" display="inline"><semantics id="S3.SS2.p4.2.2.m2.1a"><mrow id="S3.SS2.p4.2.2.m2.1b"><mo stretchy="false" id="S3.SS2.p4.2.2.m2.1.1">âŸ©</mo><mtext id="S3.SS2.p4.2.2.m2.1.2">Â </mtext><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p4.2.2.m2.1.3">ğ’®</mi><msub id="S3.SS2.p4.2.2.m2.1.4"><mi id="S3.SS2.p4.2.2.m2.1.4a"></mi><mi id="S3.SS2.p4.2.2.m2.1.4.1">v</mi></msub><mtext id="S3.SS2.p4.2.2.m2.1.5">Â </mtext><mo stretchy="false" id="S3.SS2.p4.2.2.m2.1.6">âŸ¨</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p4.2.2.m2.1c">\rangle\mbox{ }\mathcal{S}_{v}\mbox{ }\langle</annotation></semantics></math>location and/or
weather preposition<math id="S3.SS2.p4.3.3.m3.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S3.SS2.p4.3.3.m3.1a"><mo stretchy="false" id="S3.SS2.p4.3.3.m3.1.1" xref="S3.SS2.p4.3.3.m3.1.1.cmml">âŸ©</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.3.m3.1b"><ci id="S3.SS2.p4.3.3.m3.1.1.cmml" xref="S3.SS2.p4.3.3.m3.1.1">âŸ©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.3.m3.1c">\rangle</annotation></semantics></math> <math id="S3.SS2.p4.4.4.m4.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S3.SS2.p4.4.4.m4.1a"><mo stretchy="false" id="S3.SS2.p4.4.4.m4.1.1" xref="S3.SS2.p4.4.4.m4.1.1.cmml">âŸ¨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.4.m4.1b"><ci id="S3.SS2.p4.4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.4.m4.1.1">âŸ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.4.m4.1c">\langle</annotation></semantics></math>weather<math id="S3.SS2.p4.5.5.m5.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S3.SS2.p4.5.5.m5.1a"><mo stretchy="false" id="S3.SS2.p4.5.5.m5.1.1" xref="S3.SS2.p4.5.5.m5.1.1.cmml">âŸ©</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.5.m5.1b"><ci id="S3.SS2.p4.5.5.m5.1.1.cmml" xref="S3.SS2.p4.5.5.m5.1.1">âŸ©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.5.m5.1c">\rangle</annotation></semantics></math> <math id="S3.SS2.p4.6.6.m6.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S3.SS2.p4.6.6.m6.1a"><mo stretchy="false" id="S3.SS2.p4.6.6.m6.1.1" xref="S3.SS2.p4.6.6.m6.1.1.cmml">âŸ¨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.6.m6.1b"><ci id="S3.SS2.p4.6.6.m6.1.1.cmml" xref="S3.SS2.p4.6.6.m6.1.1">âŸ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.6.m6.1c">\langle</annotation></semantics></math>location<math id="S3.SS2.p4.7.7.m7.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S3.SS2.p4.7.7.m7.1a"><mo stretchy="false" id="S3.SS2.p4.7.7.m7.1.1" xref="S3.SS2.p4.7.7.m7.1.1.cmml">âŸ©</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.7.m7.1b"><ci id="S3.SS2.p4.7.7.m7.1.1.cmml" xref="S3.SS2.p4.7.7.m7.1.1">âŸ©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.7.m7.1c">\rangle</annotation></semantics></math> <math id="S3.SS2.p4.8.8.m8.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S3.SS2.p4.8.8.m8.1a"><mo stretchy="false" id="S3.SS2.p4.8.8.m8.1.1" xref="S3.SS2.p4.8.8.m8.1.1.cmml">âŸ¨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.8.m8.1b"><ci id="S3.SS2.p4.8.8.m8.1.1.cmml" xref="S3.SS2.p4.8.8.m8.1.1">âŸ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.8.m8.1c">\langle</annotation></semantics></math>time of day
with preposition<math id="S3.SS2.p4.9.9.m9.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S3.SS2.p4.9.9.m9.1a"><mo stretchy="false" id="S3.SS2.p4.9.9.m9.1.1" xref="S3.SS2.p4.9.9.m9.1.1.cmml">âŸ©</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.9.9.m9.1b"><ci id="S3.SS2.p4.9.9.m9.1.1.cmml" xref="S3.SS2.p4.9.9.m9.1.1">âŸ©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.9.9.m9.1c">\rangle</annotation></semantics></math></span>
</span>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.8" class="ltx_p">As mentioned earlier, <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{S}_{v}" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><msub id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">ğ’®</mi><mi id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">ğ’®</ci><ci id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\mathcal{S}_{v}</annotation></semantics></math> denotes the learned embedding vector of a <em id="S3.SS2.p5.8.1" class="ltx_emph ltx_font_italic">class</em> after adding noise, which can be treated as a new pseudo-word.
Every placeholder enclosed in brackets is optional and may be completed by GPT-4 to generate prompts of varying lengths and complexity.
For instance, the final prompt of <em id="S3.SS2.p5.8.2" class="ltx_emph ltx_font_italic">class: dog</em> could be â€œa photo of a <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><mo stretchy="false" id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">âŸ¨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">âŸ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\langle</annotation></semantics></math>fluffy<math id="S3.SS2.p5.3.m3.1" class="ltx_math_unparsed" alttext="\rangle\mbox{ }\mathcal{S}_{v}" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><mrow id="S3.SS2.p5.3.m3.1b"><mo stretchy="false" id="S3.SS2.p5.3.m3.1.1">âŸ©</mo><mtext id="S3.SS2.p5.3.m3.1.2">Â </mtext><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.3.m3.1.3">ğ’®</mi><msub id="S3.SS2.p5.3.m3.1.4"><mi id="S3.SS2.p5.3.m3.1.4a"></mi><mi id="S3.SS2.p5.3.m3.1.4.1">v</mi></msub></mrow><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">\rangle\mbox{ }\mathcal{S}_{v}</annotation></semantics></math>â€, for <em id="S3.SS2.p5.8.3" class="ltx_emph ltx_font_italic">class: plane</em> â€œa photo of a <math id="S3.SS2.p5.4.m4.1" class="ltx_math_unparsed" alttext="\mathcal{S}_{v}\mbox{ }\langle" display="inline"><semantics id="S3.SS2.p5.4.m4.1a"><mrow id="S3.SS2.p5.4.m4.1b"><msub id="S3.SS2.p5.4.m4.1.1"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.4.m4.1.1.2">ğ’®</mi><mi id="S3.SS2.p5.4.m4.1.1.3">v</mi></msub><mtext id="S3.SS2.p5.4.m4.1.2">Â </mtext><mo stretchy="false" id="S3.SS2.p5.4.m4.1.3">âŸ¨</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">\mathcal{S}_{v}\mbox{ }\langle</annotation></semantics></math>flying above a city at night<math id="S3.SS2.p5.5.m5.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S3.SS2.p5.5.m5.1a"><mo stretchy="false" id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml">âŸ©</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><ci id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1">âŸ©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">\rangle</annotation></semantics></math>â€, and for <em id="S3.SS2.p5.8.4" class="ltx_emph ltx_font_italic">class: spoon</em> â€œa photo of an <math id="S3.SS2.p5.6.m6.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S3.SS2.p5.6.m6.1a"><mo stretchy="false" id="S3.SS2.p5.6.m6.1.1" xref="S3.SS2.p5.6.m6.1.1.cmml">âŸ¨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m6.1b"><ci id="S3.SS2.p5.6.m6.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1">âŸ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m6.1c">\langle</annotation></semantics></math>antique<math id="S3.SS2.p5.7.m7.1" class="ltx_math_unparsed" alttext="\rangle\mbox{ }\mathcal{S}_{v}\mbox{ }\langle" display="inline"><semantics id="S3.SS2.p5.7.m7.1a"><mrow id="S3.SS2.p5.7.m7.1b"><mo stretchy="false" id="S3.SS2.p5.7.m7.1.1">âŸ©</mo><mtext id="S3.SS2.p5.7.m7.1.2">Â </mtext><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.7.m7.1.3">ğ’®</mi><msub id="S3.SS2.p5.7.m7.1.4"><mi id="S3.SS2.p5.7.m7.1.4a"></mi><mi id="S3.SS2.p5.7.m7.1.4.1">v</mi></msub><mtext id="S3.SS2.p5.7.m7.1.5">Â </mtext><mo stretchy="false" id="S3.SS2.p5.7.m7.1.6">âŸ¨</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m7.1c">\rangle\mbox{ }\mathcal{S}_{v}\mbox{ }\langle</annotation></semantics></math>on a wooden table<math id="S3.SS2.p5.8.m8.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S3.SS2.p5.8.m8.1a"><mo stretchy="false" id="S3.SS2.p5.8.m8.1.1" xref="S3.SS2.p5.8.m8.1.1.cmml">âŸ©</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m8.1b"><ci id="S3.SS2.p5.8.m8.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1">âŸ©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m8.1c">\rangle</annotation></semantics></math>â€. For more details see Appendix E.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Weighting Mechanism</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.6" class="ltx_p">Similar to DA-FusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, the extent to which the generated images can deviate from the guiding image is controlled by a strength hyperparameter <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="t_{0}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">t</mi><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">ğ‘¡</ci><cn type="integer" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">t_{0}</annotation></semantics></math>. This parameter, ranging from <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mn id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><cn type="integer" id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">0</cn></annotation-xml></semantics></math> to <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mn id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><cn type="integer" id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">1</annotation></semantics></math>, relates to the time step of inserting the guiding image during the diffusion modelâ€™s denoising process. When <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="t_{0}\to 0" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><msub id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2.2" xref="S3.SS3.p1.4.m4.1.1.2.2.cmml">t</mi><mn id="S3.SS3.p1.4.m4.1.1.2.3" xref="S3.SS3.p1.4.m4.1.1.2.3.cmml">0</mn></msub><mo stretchy="false" id="S3.SS3.p1.4.m4.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.cmml">â†’</mo><mn id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><ci id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1">â†’</ci><apply id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.2.1.cmml" xref="S3.SS3.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.2.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2.2">ğ‘¡</ci><cn type="integer" id="S3.SS3.p1.4.m4.1.1.2.3.cmml" xref="S3.SS3.p1.4.m4.1.1.2.3">0</cn></apply><cn type="integer" id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">t_{0}\to 0</annotation></semantics></math>, the generated images closely resemble the guiding image. While increasing <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="t_{0}" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><msub id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml">t</mi><mn id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2">ğ‘¡</ci><cn type="integer" id="S3.SS3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">t_{0}</annotation></semantics></math> enhances the image diversity, this increased freedom also leads to a higher probability of generating synthetic images that do not match the intended class label, which can result in either distorted class representations or entirely unrelated concepts. We select a higher value for <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="t_{0}" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><msub id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><mi id="S3.SS3.p1.6.m6.1.1.2" xref="S3.SS3.p1.6.m6.1.1.2.cmml">t</mi><mn id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p1.6.m6.1.1.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2">ğ‘¡</ci><cn type="integer" id="S3.SS3.p1.6.m6.1.1.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">t_{0}</annotation></semantics></math> (see Appendix B) than Trabucco <em id="S3.SS3.p1.6.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS3.p1.6.2" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> to encourage diversity at the potential cost of class fidelity.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.7" class="ltx_p">To address the issue of poorly matching synthetic images and thus increase the class fidelity, we implement a weighting mechanism (see <a href="#S3.F2" title="In 3 Methodology â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>, contribution c). This module operates by estimating a class confidence score <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">q</annotation></semantics></math> for each generated synthetic image using a classifier trained on the original data.
To enhance the significance of these confidence scores, we apply temperature scaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, a well-established method for calibrating probabilistic models.
The temperature <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">T</annotation></semantics></math> scales the logit output vector of the classifier <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{z}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">ğ³</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">ğ³</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\mathbf{z}</annotation></semantics></math> before calculating the softmax function
<math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{q}=\text{softmax}\left(\frac{\mathbf{z}}{T}\right)" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mrow id="S3.SS3.p2.4.m4.1.2" xref="S3.SS3.p2.4.m4.1.2.cmml"><mi id="S3.SS3.p2.4.m4.1.2.2" xref="S3.SS3.p2.4.m4.1.2.2.cmml">ğª</mi><mo id="S3.SS3.p2.4.m4.1.2.1" xref="S3.SS3.p2.4.m4.1.2.1.cmml">=</mo><mrow id="S3.SS3.p2.4.m4.1.2.3" xref="S3.SS3.p2.4.m4.1.2.3.cmml"><mtext id="S3.SS3.p2.4.m4.1.2.3.2" xref="S3.SS3.p2.4.m4.1.2.3.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m4.1.2.3.1" xref="S3.SS3.p2.4.m4.1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS3.p2.4.m4.1.2.3.3.2" xref="S3.SS3.p2.4.m4.1.1.cmml"><mo id="S3.SS3.p2.4.m4.1.2.3.3.2.1" xref="S3.SS3.p2.4.m4.1.1.cmml">(</mo><mfrac id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">ğ³</mi><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">T</mi></mfrac><mo id="S3.SS3.p2.4.m4.1.2.3.3.2.2" xref="S3.SS3.p2.4.m4.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.2.cmml" xref="S3.SS3.p2.4.m4.1.2"><eq id="S3.SS3.p2.4.m4.1.2.1.cmml" xref="S3.SS3.p2.4.m4.1.2.1"></eq><ci id="S3.SS3.p2.4.m4.1.2.2.cmml" xref="S3.SS3.p2.4.m4.1.2.2">ğª</ci><apply id="S3.SS3.p2.4.m4.1.2.3.cmml" xref="S3.SS3.p2.4.m4.1.2.3"><times id="S3.SS3.p2.4.m4.1.2.3.1.cmml" xref="S3.SS3.p2.4.m4.1.2.3.1"></times><ci id="S3.SS3.p2.4.m4.1.2.3.2a.cmml" xref="S3.SS3.p2.4.m4.1.2.3.2"><mtext id="S3.SS3.p2.4.m4.1.2.3.2.cmml" xref="S3.SS3.p2.4.m4.1.2.3.2">softmax</mtext></ci><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.2.3.3.2"><divide id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.2.3.3.2"></divide><ci id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">ğ³</ci><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">ğ‘‡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\mathbf{q}=\text{softmax}\left(\frac{\mathbf{z}}{T}\right)</annotation></semantics></math>.
From the scaled class scores <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">ğª</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">ğª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">\mathbf{q}</annotation></semantics></math>, we pick the entry <math id="S3.SS3.p2.6.m6.2" class="ltx_Math" alttext="q\in[0,1]" display="inline"><semantics id="S3.SS3.p2.6.m6.2a"><mrow id="S3.SS3.p2.6.m6.2.3" xref="S3.SS3.p2.6.m6.2.3.cmml"><mi id="S3.SS3.p2.6.m6.2.3.2" xref="S3.SS3.p2.6.m6.2.3.2.cmml">q</mi><mo id="S3.SS3.p2.6.m6.2.3.1" xref="S3.SS3.p2.6.m6.2.3.1.cmml">âˆˆ</mo><mrow id="S3.SS3.p2.6.m6.2.3.3.2" xref="S3.SS3.p2.6.m6.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.6.m6.2.3.3.2.1" xref="S3.SS3.p2.6.m6.2.3.3.1.cmml">[</mo><mn id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">0</mn><mo id="S3.SS3.p2.6.m6.2.3.3.2.2" xref="S3.SS3.p2.6.m6.2.3.3.1.cmml">,</mo><mn id="S3.SS3.p2.6.m6.2.2" xref="S3.SS3.p2.6.m6.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS3.p2.6.m6.2.3.3.2.3" xref="S3.SS3.p2.6.m6.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.2b"><apply id="S3.SS3.p2.6.m6.2.3.cmml" xref="S3.SS3.p2.6.m6.2.3"><in id="S3.SS3.p2.6.m6.2.3.1.cmml" xref="S3.SS3.p2.6.m6.2.3.1"></in><ci id="S3.SS3.p2.6.m6.2.3.2.cmml" xref="S3.SS3.p2.6.m6.2.3.2">ğ‘</ci><interval closure="closed" id="S3.SS3.p2.6.m6.2.3.3.1.cmml" xref="S3.SS3.p2.6.m6.2.3.3.2"><cn type="integer" id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">0</cn><cn type="integer" id="S3.SS3.p2.6.m6.2.2.cmml" xref="S3.SS3.p2.6.m6.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.2c">q\in[0,1]</annotation></semantics></math> corresponding to the class of the guiding image, which serves as confidence score. The value of <math id="S3.SS3.p2.7.m7.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS3.p2.7.m7.1a"><mi id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><ci id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">T</annotation></semantics></math> is optimized with respect to the cross-entropy loss on the validation set.
This process does not affect the overall accuracy of the classifier but refines the confidence estimates.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Instead of using a binary threshold on the confidence score <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">q</annotation></semantics></math> and filtering out images whose confidence is below that threshold, we use a weighting scheme following Rebbapragada and BrodleyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, since this retains the full dataset and reduces the impact of filtering errors.
Moreover, it avoids having to optimize the threshold as a sensitive hyperparameter.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.9" class="ltx_p">The probability to select a specific real or synthetic image is defined as</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.12" class="ltx_Math" alttext="P_{\text{real},n}=\frac{1}{N}(1-\alpha)\qquad\text{and}\qquad P_{\text{syn},n,m}=\frac{1}{N}\left(\alpha\frac{q_{n,m}}{\sum_{j=1}^{M}q_{n,j}}\right)\,\text{.}" display="block"><semantics id="S3.E2.m1.12a"><mrow id="S3.E2.m1.12.12.2" xref="S3.E2.m1.12.12.3.cmml"><mrow id="S3.E2.m1.11.11.1.1" xref="S3.E2.m1.11.11.1.1.cmml"><msub id="S3.E2.m1.11.11.1.1.3" xref="S3.E2.m1.11.11.1.1.3.cmml"><mi id="S3.E2.m1.11.11.1.1.3.2" xref="S3.E2.m1.11.11.1.1.3.2.cmml">P</mi><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mtext id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1a.cmml">real</mtext><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">n</mi></mrow></msub><mo id="S3.E2.m1.11.11.1.1.2" xref="S3.E2.m1.11.11.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.11.11.1.1.1.1" xref="S3.E2.m1.11.11.1.1.1.2.cmml"><mrow id="S3.E2.m1.11.11.1.1.1.1.1" xref="S3.E2.m1.11.11.1.1.1.1.1.cmml"><mfrac id="S3.E2.m1.11.11.1.1.1.1.1.3" xref="S3.E2.m1.11.11.1.1.1.1.1.3.cmml"><mn id="S3.E2.m1.11.11.1.1.1.1.1.3.2" xref="S3.E2.m1.11.11.1.1.1.1.1.3.2.cmml">1</mn><mi id="S3.E2.m1.11.11.1.1.1.1.1.3.3" xref="S3.E2.m1.11.11.1.1.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.11.11.1.1.1.1.1.2" xref="S3.E2.m1.11.11.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.11.11.1.1.1.1.1.1.1" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.11.11.1.1.1.1.1.1.1.2" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.11.11.1.1.1.1.1.1.1.1" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.3.cmml">Î±</mi></mrow><mo stretchy="false" id="S3.E2.m1.11.11.1.1.1.1.1.1.1.3" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mspace width="2em" id="S3.E2.m1.11.11.1.1.1.1.2" xref="S3.E2.m1.11.11.1.1.1.2.cmml"></mspace><mtext id="S3.E2.m1.10.10" xref="S3.E2.m1.10.10a.cmml">and</mtext></mrow></mrow><mspace width="2em" id="S3.E2.m1.12.12.2.3" xref="S3.E2.m1.12.12.3a.cmml"></mspace><mrow id="S3.E2.m1.12.12.2.2" xref="S3.E2.m1.12.12.2.2.cmml"><msub id="S3.E2.m1.12.12.2.2.3" xref="S3.E2.m1.12.12.2.2.3.cmml"><mi id="S3.E2.m1.12.12.2.2.3.2" xref="S3.E2.m1.12.12.2.2.3.2.cmml">P</mi><mrow id="S3.E2.m1.5.5.3.5" xref="S3.E2.m1.5.5.3.4.cmml"><mtext id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1a.cmml">syn</mtext><mo id="S3.E2.m1.5.5.3.5.1" xref="S3.E2.m1.5.5.3.4.cmml">,</mo><mi id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.2.cmml">n</mi><mo id="S3.E2.m1.5.5.3.5.2" xref="S3.E2.m1.5.5.3.4.cmml">,</mo><mi id="S3.E2.m1.5.5.3.3" xref="S3.E2.m1.5.5.3.3.cmml">m</mi></mrow></msub><mo id="S3.E2.m1.12.12.2.2.2" xref="S3.E2.m1.12.12.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.12.12.2.2.1" xref="S3.E2.m1.12.12.2.2.1.cmml"><mfrac id="S3.E2.m1.12.12.2.2.1.3" xref="S3.E2.m1.12.12.2.2.1.3.cmml"><mn id="S3.E2.m1.12.12.2.2.1.3.2" xref="S3.E2.m1.12.12.2.2.1.3.2.cmml">1</mn><mi id="S3.E2.m1.12.12.2.2.1.3.3" xref="S3.E2.m1.12.12.2.2.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.12.12.2.2.1.2" xref="S3.E2.m1.12.12.2.2.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.12.12.2.2.1.1.1" xref="S3.E2.m1.12.12.2.2.1.1.1.1.cmml"><mo id="S3.E2.m1.12.12.2.2.1.1.1.2" xref="S3.E2.m1.12.12.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.12.12.2.2.1.1.1.1" xref="S3.E2.m1.12.12.2.2.1.1.1.1.cmml"><mi id="S3.E2.m1.12.12.2.2.1.1.1.1.2" xref="S3.E2.m1.12.12.2.2.1.1.1.1.2.cmml">Î±</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.12.12.2.2.1.1.1.1.1" xref="S3.E2.m1.12.12.2.2.1.1.1.1.1.cmml">â€‹</mo><mfrac id="S3.E2.m1.9.9" xref="S3.E2.m1.9.9.cmml"><msub id="S3.E2.m1.7.7.2" xref="S3.E2.m1.7.7.2.cmml"><mi id="S3.E2.m1.7.7.2.4" xref="S3.E2.m1.7.7.2.4.cmml">q</mi><mrow id="S3.E2.m1.7.7.2.2.2.4" xref="S3.E2.m1.7.7.2.2.2.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.cmml">n</mi><mo id="S3.E2.m1.7.7.2.2.2.4.1" xref="S3.E2.m1.7.7.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.7.7.2.2.2.2" xref="S3.E2.m1.7.7.2.2.2.2.cmml">m</mi></mrow></msub><mrow id="S3.E2.m1.9.9.4" xref="S3.E2.m1.9.9.4.cmml"><msubsup id="S3.E2.m1.9.9.4.3" xref="S3.E2.m1.9.9.4.3.cmml"><mo id="S3.E2.m1.9.9.4.3.2.2" xref="S3.E2.m1.9.9.4.3.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.9.9.4.3.2.3" xref="S3.E2.m1.9.9.4.3.2.3.cmml"><mi id="S3.E2.m1.9.9.4.3.2.3.2" xref="S3.E2.m1.9.9.4.3.2.3.2.cmml">j</mi><mo id="S3.E2.m1.9.9.4.3.2.3.1" xref="S3.E2.m1.9.9.4.3.2.3.1.cmml">=</mo><mn id="S3.E2.m1.9.9.4.3.2.3.3" xref="S3.E2.m1.9.9.4.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.9.9.4.3.3" xref="S3.E2.m1.9.9.4.3.3.cmml">M</mi></msubsup><msub id="S3.E2.m1.9.9.4.4" xref="S3.E2.m1.9.9.4.4.cmml"><mi id="S3.E2.m1.9.9.4.4.2" xref="S3.E2.m1.9.9.4.4.2.cmml">q</mi><mrow id="S3.E2.m1.9.9.4.2.2.4" xref="S3.E2.m1.9.9.4.2.2.3.cmml"><mi id="S3.E2.m1.8.8.3.1.1.1" xref="S3.E2.m1.8.8.3.1.1.1.cmml">n</mi><mo id="S3.E2.m1.9.9.4.2.2.4.1" xref="S3.E2.m1.9.9.4.2.2.3.cmml">,</mo><mi id="S3.E2.m1.9.9.4.2.2.2" xref="S3.E2.m1.9.9.4.2.2.2.cmml">j</mi></mrow></msub></mrow></mfrac></mrow><mo id="S3.E2.m1.12.12.2.2.1.1.1.3" xref="S3.E2.m1.12.12.2.2.1.1.1.1.cmml">)</mo></mrow><mo lspace="0.170em" rspace="0em" id="S3.E2.m1.12.12.2.2.1.2a" xref="S3.E2.m1.12.12.2.2.1.2.cmml">â€‹</mo><mtext id="S3.E2.m1.12.12.2.2.1.4" xref="S3.E2.m1.12.12.2.2.1.4a.cmml">.</mtext></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.12b"><apply id="S3.E2.m1.12.12.3.cmml" xref="S3.E2.m1.12.12.2"><csymbol cd="ambiguous" id="S3.E2.m1.12.12.3a.cmml" xref="S3.E2.m1.12.12.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.11.11.1.1.cmml" xref="S3.E2.m1.11.11.1.1"><eq id="S3.E2.m1.11.11.1.1.2.cmml" xref="S3.E2.m1.11.11.1.1.2"></eq><apply id="S3.E2.m1.11.11.1.1.3.cmml" xref="S3.E2.m1.11.11.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.11.11.1.1.3.1.cmml" xref="S3.E2.m1.11.11.1.1.3">subscript</csymbol><ci id="S3.E2.m1.11.11.1.1.3.2.cmml" xref="S3.E2.m1.11.11.1.1.3.2">ğ‘ƒ</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><ci id="S3.E2.m1.1.1.1.1a.cmml" xref="S3.E2.m1.1.1.1.1"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">real</mtext></ci><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">ğ‘›</ci></list></apply><list id="S3.E2.m1.11.11.1.1.1.2.cmml" xref="S3.E2.m1.11.11.1.1.1.1"><apply id="S3.E2.m1.11.11.1.1.1.1.1.cmml" xref="S3.E2.m1.11.11.1.1.1.1.1"><times id="S3.E2.m1.11.11.1.1.1.1.1.2.cmml" xref="S3.E2.m1.11.11.1.1.1.1.1.2"></times><apply id="S3.E2.m1.11.11.1.1.1.1.1.3.cmml" xref="S3.E2.m1.11.11.1.1.1.1.1.3"><divide id="S3.E2.m1.11.11.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.11.11.1.1.1.1.1.3"></divide><cn type="integer" id="S3.E2.m1.11.11.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.11.11.1.1.1.1.1.3.2">1</cn><ci id="S3.E2.m1.11.11.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.11.11.1.1.1.1.1.3.3">ğ‘</ci></apply><apply id="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1"><minus id="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.11.11.1.1.1.1.1.1.1.1.3">ğ›¼</ci></apply></apply><ci id="S3.E2.m1.10.10a.cmml" xref="S3.E2.m1.10.10"><mtext id="S3.E2.m1.10.10.cmml" xref="S3.E2.m1.10.10">and</mtext></ci></list></apply><apply id="S3.E2.m1.12.12.2.2.cmml" xref="S3.E2.m1.12.12.2.2"><eq id="S3.E2.m1.12.12.2.2.2.cmml" xref="S3.E2.m1.12.12.2.2.2"></eq><apply id="S3.E2.m1.12.12.2.2.3.cmml" xref="S3.E2.m1.12.12.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.12.12.2.2.3.1.cmml" xref="S3.E2.m1.12.12.2.2.3">subscript</csymbol><ci id="S3.E2.m1.12.12.2.2.3.2.cmml" xref="S3.E2.m1.12.12.2.2.3.2">ğ‘ƒ</ci><list id="S3.E2.m1.5.5.3.4.cmml" xref="S3.E2.m1.5.5.3.5"><ci id="S3.E2.m1.3.3.1.1a.cmml" xref="S3.E2.m1.3.3.1.1"><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1">syn</mtext></ci><ci id="S3.E2.m1.4.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2">ğ‘›</ci><ci id="S3.E2.m1.5.5.3.3.cmml" xref="S3.E2.m1.5.5.3.3">ğ‘š</ci></list></apply><apply id="S3.E2.m1.12.12.2.2.1.cmml" xref="S3.E2.m1.12.12.2.2.1"><times id="S3.E2.m1.12.12.2.2.1.2.cmml" xref="S3.E2.m1.12.12.2.2.1.2"></times><apply id="S3.E2.m1.12.12.2.2.1.3.cmml" xref="S3.E2.m1.12.12.2.2.1.3"><divide id="S3.E2.m1.12.12.2.2.1.3.1.cmml" xref="S3.E2.m1.12.12.2.2.1.3"></divide><cn type="integer" id="S3.E2.m1.12.12.2.2.1.3.2.cmml" xref="S3.E2.m1.12.12.2.2.1.3.2">1</cn><ci id="S3.E2.m1.12.12.2.2.1.3.3.cmml" xref="S3.E2.m1.12.12.2.2.1.3.3">ğ‘</ci></apply><apply id="S3.E2.m1.12.12.2.2.1.1.1.1.cmml" xref="S3.E2.m1.12.12.2.2.1.1.1"><times id="S3.E2.m1.12.12.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.12.12.2.2.1.1.1.1.1"></times><ci id="S3.E2.m1.12.12.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.12.12.2.2.1.1.1.1.2">ğ›¼</ci><apply id="S3.E2.m1.9.9.cmml" xref="S3.E2.m1.9.9"><divide id="S3.E2.m1.9.9.5.cmml" xref="S3.E2.m1.9.9"></divide><apply id="S3.E2.m1.7.7.2.cmml" xref="S3.E2.m1.7.7.2"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.2.3.cmml" xref="S3.E2.m1.7.7.2">subscript</csymbol><ci id="S3.E2.m1.7.7.2.4.cmml" xref="S3.E2.m1.7.7.2.4">ğ‘</ci><list id="S3.E2.m1.7.7.2.2.2.3.cmml" xref="S3.E2.m1.7.7.2.2.2.4"><ci id="S3.E2.m1.6.6.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1">ğ‘›</ci><ci id="S3.E2.m1.7.7.2.2.2.2.cmml" xref="S3.E2.m1.7.7.2.2.2.2">ğ‘š</ci></list></apply><apply id="S3.E2.m1.9.9.4.cmml" xref="S3.E2.m1.9.9.4"><apply id="S3.E2.m1.9.9.4.3.cmml" xref="S3.E2.m1.9.9.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.9.9.4.3.1.cmml" xref="S3.E2.m1.9.9.4.3">superscript</csymbol><apply id="S3.E2.m1.9.9.4.3.2.cmml" xref="S3.E2.m1.9.9.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.9.9.4.3.2.1.cmml" xref="S3.E2.m1.9.9.4.3">subscript</csymbol><sum id="S3.E2.m1.9.9.4.3.2.2.cmml" xref="S3.E2.m1.9.9.4.3.2.2"></sum><apply id="S3.E2.m1.9.9.4.3.2.3.cmml" xref="S3.E2.m1.9.9.4.3.2.3"><eq id="S3.E2.m1.9.9.4.3.2.3.1.cmml" xref="S3.E2.m1.9.9.4.3.2.3.1"></eq><ci id="S3.E2.m1.9.9.4.3.2.3.2.cmml" xref="S3.E2.m1.9.9.4.3.2.3.2">ğ‘—</ci><cn type="integer" id="S3.E2.m1.9.9.4.3.2.3.3.cmml" xref="S3.E2.m1.9.9.4.3.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.9.9.4.3.3.cmml" xref="S3.E2.m1.9.9.4.3.3">ğ‘€</ci></apply><apply id="S3.E2.m1.9.9.4.4.cmml" xref="S3.E2.m1.9.9.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.9.9.4.4.1.cmml" xref="S3.E2.m1.9.9.4.4">subscript</csymbol><ci id="S3.E2.m1.9.9.4.4.2.cmml" xref="S3.E2.m1.9.9.4.4.2">ğ‘</ci><list id="S3.E2.m1.9.9.4.2.2.3.cmml" xref="S3.E2.m1.9.9.4.2.2.4"><ci id="S3.E2.m1.8.8.3.1.1.1.cmml" xref="S3.E2.m1.8.8.3.1.1.1">ğ‘›</ci><ci id="S3.E2.m1.9.9.4.2.2.2.cmml" xref="S3.E2.m1.9.9.4.2.2.2">ğ‘—</ci></list></apply></apply></apply></apply><ci id="S3.E2.m1.12.12.2.2.1.4a.cmml" xref="S3.E2.m1.12.12.2.2.1.4"><mtext id="S3.E2.m1.12.12.2.2.1.4.cmml" xref="S3.E2.m1.12.12.2.2.1.4">.</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.12c">P_{\text{real},n}=\frac{1}{N}(1-\alpha)\qquad\text{and}\qquad P_{\text{syn},n,m}=\frac{1}{N}\left(\alpha\frac{q_{n,m}}{\sum_{j=1}^{M}q_{n,j}}\right)\,\text{.}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p4.8" class="ltx_p">That is, each real image <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="R_{n}" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><msub id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">R</mi><mi id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">ğ‘…</ci><ci id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">R_{n}</annotation></semantics></math> with <math id="S3.SS3.p4.2.m2.3" class="ltx_Math" alttext="n\in\{1,\ldots,N\}" display="inline"><semantics id="S3.SS3.p4.2.m2.3a"><mrow id="S3.SS3.p4.2.m2.3.4" xref="S3.SS3.p4.2.m2.3.4.cmml"><mi id="S3.SS3.p4.2.m2.3.4.2" xref="S3.SS3.p4.2.m2.3.4.2.cmml">n</mi><mo id="S3.SS3.p4.2.m2.3.4.1" xref="S3.SS3.p4.2.m2.3.4.1.cmml">âˆˆ</mo><mrow id="S3.SS3.p4.2.m2.3.4.3.2" xref="S3.SS3.p4.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS3.p4.2.m2.3.4.3.2.1" xref="S3.SS3.p4.2.m2.3.4.3.1.cmml">{</mo><mn id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">1</mn><mo id="S3.SS3.p4.2.m2.3.4.3.2.2" xref="S3.SS3.p4.2.m2.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p4.2.m2.2.2" xref="S3.SS3.p4.2.m2.2.2.cmml">â€¦</mi><mo id="S3.SS3.p4.2.m2.3.4.3.2.3" xref="S3.SS3.p4.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p4.2.m2.3.3" xref="S3.SS3.p4.2.m2.3.3.cmml">N</mi><mo stretchy="false" id="S3.SS3.p4.2.m2.3.4.3.2.4" xref="S3.SS3.p4.2.m2.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.3b"><apply id="S3.SS3.p4.2.m2.3.4.cmml" xref="S3.SS3.p4.2.m2.3.4"><in id="S3.SS3.p4.2.m2.3.4.1.cmml" xref="S3.SS3.p4.2.m2.3.4.1"></in><ci id="S3.SS3.p4.2.m2.3.4.2.cmml" xref="S3.SS3.p4.2.m2.3.4.2">ğ‘›</ci><set id="S3.SS3.p4.2.m2.3.4.3.1.cmml" xref="S3.SS3.p4.2.m2.3.4.3.2"><cn type="integer" id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">1</cn><ci id="S3.SS3.p4.2.m2.2.2.cmml" xref="S3.SS3.p4.2.m2.2.2">â€¦</ci><ci id="S3.SS3.p4.2.m2.3.3.cmml" xref="S3.SS3.p4.2.m2.3.3">ğ‘</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.3c">n\in\{1,\ldots,N\}</annotation></semantics></math> is chosen with probability <math id="S3.SS3.p4.3.m3.2" class="ltx_Math" alttext="P_{\text{real},n}" display="inline"><semantics id="S3.SS3.p4.3.m3.2a"><msub id="S3.SS3.p4.3.m3.2.3" xref="S3.SS3.p4.3.m3.2.3.cmml"><mi id="S3.SS3.p4.3.m3.2.3.2" xref="S3.SS3.p4.3.m3.2.3.2.cmml">P</mi><mrow id="S3.SS3.p4.3.m3.2.2.2.4" xref="S3.SS3.p4.3.m3.2.2.2.3.cmml"><mtext id="S3.SS3.p4.3.m3.1.1.1.1" xref="S3.SS3.p4.3.m3.1.1.1.1a.cmml">real</mtext><mo id="S3.SS3.p4.3.m3.2.2.2.4.1" xref="S3.SS3.p4.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p4.3.m3.2.2.2.2" xref="S3.SS3.p4.3.m3.2.2.2.2.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.2b"><apply id="S3.SS3.p4.3.m3.2.3.cmml" xref="S3.SS3.p4.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.2.3.1.cmml" xref="S3.SS3.p4.3.m3.2.3">subscript</csymbol><ci id="S3.SS3.p4.3.m3.2.3.2.cmml" xref="S3.SS3.p4.3.m3.2.3.2">ğ‘ƒ</ci><list id="S3.SS3.p4.3.m3.2.2.2.3.cmml" xref="S3.SS3.p4.3.m3.2.2.2.4"><ci id="S3.SS3.p4.3.m3.1.1.1.1a.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1"><mtext mathsize="70%" id="S3.SS3.p4.3.m3.1.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1">real</mtext></ci><ci id="S3.SS3.p4.3.m3.2.2.2.2.cmml" xref="S3.SS3.p4.3.m3.2.2.2.2">ğ‘›</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.2c">P_{\text{real},n}</annotation></semantics></math> during training, where <math id="S3.SS3.p4.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><mi id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><ci id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">\alpha</annotation></semantics></math> is a hyperparameter called synthetic probability. A synthetic image <math id="S3.SS3.p4.5.m5.2" class="ltx_Math" alttext="S_{n,m}" display="inline"><semantics id="S3.SS3.p4.5.m5.2a"><msub id="S3.SS3.p4.5.m5.2.3" xref="S3.SS3.p4.5.m5.2.3.cmml"><mi id="S3.SS3.p4.5.m5.2.3.2" xref="S3.SS3.p4.5.m5.2.3.2.cmml">S</mi><mrow id="S3.SS3.p4.5.m5.2.2.2.4" xref="S3.SS3.p4.5.m5.2.2.2.3.cmml"><mi id="S3.SS3.p4.5.m5.1.1.1.1" xref="S3.SS3.p4.5.m5.1.1.1.1.cmml">n</mi><mo id="S3.SS3.p4.5.m5.2.2.2.4.1" xref="S3.SS3.p4.5.m5.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p4.5.m5.2.2.2.2" xref="S3.SS3.p4.5.m5.2.2.2.2.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m5.2b"><apply id="S3.SS3.p4.5.m5.2.3.cmml" xref="S3.SS3.p4.5.m5.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m5.2.3.1.cmml" xref="S3.SS3.p4.5.m5.2.3">subscript</csymbol><ci id="S3.SS3.p4.5.m5.2.3.2.cmml" xref="S3.SS3.p4.5.m5.2.3.2">ğ‘†</ci><list id="S3.SS3.p4.5.m5.2.2.2.3.cmml" xref="S3.SS3.p4.5.m5.2.2.2.4"><ci id="S3.SS3.p4.5.m5.1.1.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1.1.1">ğ‘›</ci><ci id="S3.SS3.p4.5.m5.2.2.2.2.cmml" xref="S3.SS3.p4.5.m5.2.2.2.2">ğ‘š</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m5.2c">S_{n,m}</annotation></semantics></math> with <math id="S3.SS3.p4.6.m6.3" class="ltx_Math" alttext="m\in\{1,\ldots,M\}" display="inline"><semantics id="S3.SS3.p4.6.m6.3a"><mrow id="S3.SS3.p4.6.m6.3.4" xref="S3.SS3.p4.6.m6.3.4.cmml"><mi id="S3.SS3.p4.6.m6.3.4.2" xref="S3.SS3.p4.6.m6.3.4.2.cmml">m</mi><mo id="S3.SS3.p4.6.m6.3.4.1" xref="S3.SS3.p4.6.m6.3.4.1.cmml">âˆˆ</mo><mrow id="S3.SS3.p4.6.m6.3.4.3.2" xref="S3.SS3.p4.6.m6.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS3.p4.6.m6.3.4.3.2.1" xref="S3.SS3.p4.6.m6.3.4.3.1.cmml">{</mo><mn id="S3.SS3.p4.6.m6.1.1" xref="S3.SS3.p4.6.m6.1.1.cmml">1</mn><mo id="S3.SS3.p4.6.m6.3.4.3.2.2" xref="S3.SS3.p4.6.m6.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p4.6.m6.2.2" xref="S3.SS3.p4.6.m6.2.2.cmml">â€¦</mi><mo id="S3.SS3.p4.6.m6.3.4.3.2.3" xref="S3.SS3.p4.6.m6.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p4.6.m6.3.3" xref="S3.SS3.p4.6.m6.3.3.cmml">M</mi><mo stretchy="false" id="S3.SS3.p4.6.m6.3.4.3.2.4" xref="S3.SS3.p4.6.m6.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m6.3b"><apply id="S3.SS3.p4.6.m6.3.4.cmml" xref="S3.SS3.p4.6.m6.3.4"><in id="S3.SS3.p4.6.m6.3.4.1.cmml" xref="S3.SS3.p4.6.m6.3.4.1"></in><ci id="S3.SS3.p4.6.m6.3.4.2.cmml" xref="S3.SS3.p4.6.m6.3.4.2">ğ‘š</ci><set id="S3.SS3.p4.6.m6.3.4.3.1.cmml" xref="S3.SS3.p4.6.m6.3.4.3.2"><cn type="integer" id="S3.SS3.p4.6.m6.1.1.cmml" xref="S3.SS3.p4.6.m6.1.1">1</cn><ci id="S3.SS3.p4.6.m6.2.2.cmml" xref="S3.SS3.p4.6.m6.2.2">â€¦</ci><ci id="S3.SS3.p4.6.m6.3.3.cmml" xref="S3.SS3.p4.6.m6.3.3">ğ‘€</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m6.3c">m\in\{1,\ldots,M\}</annotation></semantics></math>, which was generated based on the real image <math id="S3.SS3.p4.7.m7.1" class="ltx_Math" alttext="R_{n}" display="inline"><semantics id="S3.SS3.p4.7.m7.1a"><msub id="S3.SS3.p4.7.m7.1.1" xref="S3.SS3.p4.7.m7.1.1.cmml"><mi id="S3.SS3.p4.7.m7.1.1.2" xref="S3.SS3.p4.7.m7.1.1.2.cmml">R</mi><mi id="S3.SS3.p4.7.m7.1.1.3" xref="S3.SS3.p4.7.m7.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m7.1b"><apply id="S3.SS3.p4.7.m7.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.1.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.p4.7.m7.1.1.2.cmml" xref="S3.SS3.p4.7.m7.1.1.2">ğ‘…</ci><ci id="S3.SS3.p4.7.m7.1.1.3.cmml" xref="S3.SS3.p4.7.m7.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m7.1c">R_{n}</annotation></semantics></math>, is selected with probability <math id="S3.SS3.p4.8.m8.3" class="ltx_Math" alttext="P_{\text{syn},n,m}" display="inline"><semantics id="S3.SS3.p4.8.m8.3a"><msub id="S3.SS3.p4.8.m8.3.4" xref="S3.SS3.p4.8.m8.3.4.cmml"><mi id="S3.SS3.p4.8.m8.3.4.2" xref="S3.SS3.p4.8.m8.3.4.2.cmml">P</mi><mrow id="S3.SS3.p4.8.m8.3.3.3.5" xref="S3.SS3.p4.8.m8.3.3.3.4.cmml"><mtext id="S3.SS3.p4.8.m8.1.1.1.1" xref="S3.SS3.p4.8.m8.1.1.1.1a.cmml">syn</mtext><mo id="S3.SS3.p4.8.m8.3.3.3.5.1" xref="S3.SS3.p4.8.m8.3.3.3.4.cmml">,</mo><mi id="S3.SS3.p4.8.m8.2.2.2.2" xref="S3.SS3.p4.8.m8.2.2.2.2.cmml">n</mi><mo id="S3.SS3.p4.8.m8.3.3.3.5.2" xref="S3.SS3.p4.8.m8.3.3.3.4.cmml">,</mo><mi id="S3.SS3.p4.8.m8.3.3.3.3" xref="S3.SS3.p4.8.m8.3.3.3.3.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m8.3b"><apply id="S3.SS3.p4.8.m8.3.4.cmml" xref="S3.SS3.p4.8.m8.3.4"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m8.3.4.1.cmml" xref="S3.SS3.p4.8.m8.3.4">subscript</csymbol><ci id="S3.SS3.p4.8.m8.3.4.2.cmml" xref="S3.SS3.p4.8.m8.3.4.2">ğ‘ƒ</ci><list id="S3.SS3.p4.8.m8.3.3.3.4.cmml" xref="S3.SS3.p4.8.m8.3.3.3.5"><ci id="S3.SS3.p4.8.m8.1.1.1.1a.cmml" xref="S3.SS3.p4.8.m8.1.1.1.1"><mtext mathsize="70%" id="S3.SS3.p4.8.m8.1.1.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1.1.1">syn</mtext></ci><ci id="S3.SS3.p4.8.m8.2.2.2.2.cmml" xref="S3.SS3.p4.8.m8.2.2.2.2">ğ‘›</ci><ci id="S3.SS3.p4.8.m8.3.3.3.3.cmml" xref="S3.SS3.p4.8.m8.3.3.3.3">ğ‘š</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m8.3c">P_{\text{syn},n,m}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To evaluate the effectiveness of our method, four datasets were utilized. A consistent set of hyperparameters was used across all datasets to maintain the modelâ€™s off-the-shelf property and to allow for direct comparison.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">First, the <em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">FOCUS</em> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> was chosen, which contains 21K images of 10 different classes in common and uncommon settings, altering the time of day, weather condition, and location. This broad distribution makes FOCUS a well-suited dataset for our experiments, enabling the evaluation of DIAGenâ€™s ability to reproduce the distribution of the data only knowing very few images per class.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Second, we test our model on the <em id="S4.SS1.p3.1.1" class="ltx_emph ltx_font_italic">MS COCO</em> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. This dataset comprises common objects in context, which implies that objects occur in different settings and have a variety of appearances. This dataset allows for a direct comparison to the baseline DA-Fusion, as it was also used in Trabucco <em id="S4.SS1.p3.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.SS1.p3.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Third, we introduce our own dataset, <em id="S4.SS1.p4.1.1" class="ltx_emph ltx_font_italic">Custom COCO</em>, which is based on a subset of 23 classes from MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
In contrast to MS COCO, our dataset ensures that each image contains only one selected class, making it more suitable for single-label classification. The primary motivation for creating an alternative to MS COCO is to address the common data leakage issues in publicly available datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
Large pre-trained generative models, such as Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> utilized by DIAGen, are likely to be trained on instances from benchmark datasets such as MS COCO. Therefore, the diffusion model may have already observed validation and test images. To mitigate this, our Custom COCO dataset is based on custom-collected images, ensuring that none of them exists elsewhere on the internet. Thus, guaranteeing that the model is exposed to entirely novel images, eliminating the risk of prior exposure during training.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Fourth, to better evaluate the diversity of synthetic images produced by DIAGen, we generated an additional test set <em id="S4.SS1.p5.1.1" class="ltx_emph ltx_font_italic">Uncommon Settings</em> for the same classes as in Custom COCO, however in uncommon settings. This test set aims to measure the ability to classify out-of-distribution (OOD) samples.
Our test set includes 247 uncommon scenarios, like <span id="S4.SS1.p5.1.2" class="ltx_text ltx_font_italic">a chair in space</span> or <span id="S4.SS1.p5.1.3" class="ltx_text ltx_font_italic">a bicycle at the bottom of the sea</span>. These test images were collected by conducting internet searches using a number of unusual locations that we had previously compiled.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Setup</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We compare DIAGenâ€™s results against two baselines: DA-Fusion was chosen as the first baseline since DIAGen is built upon this model. We use the original experimental setup of Trabucco <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.SS2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. Secondly, we compare DIAGen to standard augmentations, given their widespread use for data augmentation tasks. For this, we used a combination of rotations, flips, scale adjustments, and crops.
More details on the experimental setup including all values for the hyperparameters can be found in the supplemental material.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The modelâ€™s effectiveness was evaluated on a downstream classifier, comparing its behaviour on four datasets: FOCUS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, Custom COCO, and Uncommon Settings. The downstream classifier accuracy serves as the primary metric for our studies, following the work of Ravuri and Oriol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.3" class="ltx_p">To ensure relevance for few-shot learning, we trained on small, varying dataset sizes containing <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn type="integer" id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">2</annotation></semantics></math>, <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><cn type="integer" id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">4</annotation></semantics></math>, and <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mn id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><cn type="integer" id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">8</annotation></semantics></math> examples per class. Furthermore, to increase the reproducibility and reliability of our findings, we used 3 different seeds to alter the selection of the images in the training split and calculated the mean.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Classification Accuracy</h3>

<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.14584/assets/x3.png" id="S4.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.14584/assets/x4.png" id="S4.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.14584/assets/x5.png" id="S4.F3.3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.14584/assets/x6.png" id="S4.F3.4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.12.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.13.2" class="ltx_text" style="font-size:90%;">Downstream classification accuracy of DIAGen, DA-Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, and standard augmentations on four datasets: <em id="S4.F3.13.2.1" class="ltx_emph ltx_font_italic">(a)</em> FOCUS, <em id="S4.F3.13.2.2" class="ltx_emph ltx_font_italic">(b)</em> MS COCO, <em id="S4.F3.13.2.3" class="ltx_emph ltx_font_italic">(c)</em> Custom COCO dataset, and <em id="S4.F3.13.2.4" class="ltx_emph ltx_font_italic">(d)</em> training on Custom COCO with evaluation on Uncommon Settings test set. Runs marked with * are taken from Trabucco <em id="S4.F3.13.2.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.F3.13.2.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</span></figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.3" class="ltx_p"><a href="#S4.F3" title="In 4.3 Classification Accuracy â€£ 4 Experiments â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> shows the downstream classifier accuracy for DIAGen, the baseline DA-Fusion, and standard augmentations.
We plot the accuracy over different few-shot dataset sizes, limiting the size to <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="integer" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">2</annotation></semantics></math>, <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn type="integer" id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">4</annotation></semantics></math>, and <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mn id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><cn type="integer" id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">8</annotation></semantics></math> examples for each class.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.2" class="ltx_p">We observe a consistent improvement in validation and test accuracy, by as much as <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="+5\%" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mo id="S4.SS3.p2.1.m1.1.1a" xref="S4.SS3.p2.1.m1.1.1.cmml">+</mo><mrow id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2.2" xref="S4.SS3.p2.1.m1.1.1.2.2.cmml">5</mn><mo id="S4.SS3.p2.1.m1.1.1.2.1" xref="S4.SS3.p2.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><plus id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"></plus><apply id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS3.p2.1.m1.1.1.2.1.cmml" xref="S4.SS3.p2.1.m1.1.1.2.1">percent</csymbol><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">+5\%</annotation></semantics></math> points across the four datasets when compared to DA-Fusion. The gain of DIAGen against standard augmentations is even more evident, reaching up to <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="+10.5\%" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mo id="S4.SS3.p2.2.m2.1.1a" xref="S4.SS3.p2.2.m2.1.1.cmml">+</mo><mrow id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2.2" xref="S4.SS3.p2.2.m2.1.1.2.2.cmml">10.5</mn><mo id="S4.SS3.p2.2.m2.1.1.2.1" xref="S4.SS3.p2.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><plus id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"></plus><apply id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.2.1.cmml" xref="S4.SS3.p2.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p2.2.m2.1.1.2.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2.2">10.5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">+10.5\%</annotation></semantics></math> points.
These results highlight the effectiveness of DIAGen, especially in limited data scenarios. In few-shot learning situations where training examples are scarce, DIAGen introduces additional semantic diversity as we further analyse below, thereby strengthening the modelâ€™s generalization ability.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.2" class="ltx_p">By using Uncommon Settings, which targets edge cases of real-world object occurrences, we measure how effectively each method can cover a broad range of real-world scenarios. An analysis of the results on the Uncommon Setting test set (see <a href="#S4.F3" title="In 4.3 Classification Accuracy â€£ 4 Experiments â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>, bottom-right) reveals a significantly higher accuracy of our DIAGen, with gains of approximately <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="+2\%" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mo id="S4.SS3.p3.1.m1.1.1a" xref="S4.SS3.p3.1.m1.1.1.cmml">+</mo><mrow id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml"><mn id="S4.SS3.p3.1.m1.1.1.2.2" xref="S4.SS3.p3.1.m1.1.1.2.2.cmml">2</mn><mo id="S4.SS3.p3.1.m1.1.1.2.1" xref="S4.SS3.p3.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><plus id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"></plus><apply id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.2.1.cmml" xref="S4.SS3.p3.1.m1.1.1.2.1">percent</csymbol><cn type="integer" id="S4.SS3.p3.1.m1.1.1.2.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2.2">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">+2\%</annotation></semantics></math> points compared to DA-Fusion and <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="+3\%" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mo id="S4.SS3.p3.2.m2.1.1a" xref="S4.SS3.p3.2.m2.1.1.cmml">+</mo><mrow id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml"><mn id="S4.SS3.p3.2.m2.1.1.2.2" xref="S4.SS3.p3.2.m2.1.1.2.2.cmml">3</mn><mo id="S4.SS3.p3.2.m2.1.1.2.1" xref="S4.SS3.p3.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><plus id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"></plus><apply id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS3.p3.2.m2.1.1.2.1.cmml" xref="S4.SS3.p3.2.m2.1.1.2.1">percent</csymbol><cn type="integer" id="S4.SS3.p3.2.m2.1.1.2.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">+3\%</annotation></semantics></math> points compared to standard augmentations, across all dataset sizes. While the training dataset remains identical to Custom COCO, the test set now includes samples from a distribution entirely different from the training data. This supports the hypothesis that our augmentation technique improves semantic diversity, particularly in generalizing to edge cases and uncommon scenarios.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We now analyze the contributions of the three components in our DIAGen pipeline: embedding noise, LLM prompts, and weighting mechanism. We conduct an ablation study by running each module independently to assess their individual impact. <a href="#S4.F4" title="In 4.4 Ablation Study â€£ 4 Experiments â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> illustrates the accuracy gains attributed to each component relative to the DA-Fusion baseline.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.1" class="ltx_figure ltx_figure_panel"><img src="/html/2408.14584/assets/x7.png" id="S4.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="368" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.2" class="ltx_figure ltx_figure_panel"><img src="/html/2408.14584/assets/x8.png" id="S4.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="368" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.3" class="ltx_figure ltx_figure_panel"><img src="/html/2408.14584/assets/x9.png" id="S4.F4.3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="368" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.4" class="ltx_figure ltx_figure_panel"><img src="/html/2408.14584/assets/x10.png" id="S4.F4.4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="368" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.10.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.11.2" class="ltx_text" style="font-size:90%;">Ablation study of the three proposed components, showcasing their distinct contribution to the classification accuracy. We illustrate the accuracy gains over DA-Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> solely utilizing embedding noise <em id="S4.F4.11.2.1" class="ltx_emph ltx_font_italic">(top left)</em>, employing only the LLM prompt module <em id="S4.F4.11.2.2" class="ltx_emph ltx_font_italic">(top right)</em>, and combining both <em id="S4.F4.11.2.3" class="ltx_emph ltx_font_italic">(bottom left)</em>. Also shown are the improvements by adding the weighting mechanism <em id="S4.F4.11.2.4" class="ltx_emph ltx_font_italic">(bottom right)</em>. Latter corresponds to the full DIAGen method.</span></figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Embedding noise leads to major improvements when only <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mn id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><cn type="integer" id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">2</annotation></semantics></math> examples per class are used for training.
Although the positive effect of adding noise on its own decreases with more examples per class, its combination with the other components yields significant benefits.
We attribute the synergy of the combined method to the ability of the embedding noise and LLM to increase diversity at the expense of class fidelity, a trade-off that the weighting mechanism mitigates by assigning a lower weight to low-quality images.
Weighting is effective in refining the augmentation process, as visualized by comparing its results with the runs only utilizing noise and LLM prompts in <a href="#S4.F4" title="In 4.4 Ablation Study â€£ 4 Experiments â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">In contrast to embedding noise, using LLM prompts alone yields promising results, significantly improving the accuracy in case of 2 and 4 examples per class.
Interestingly, although DIAGen proves effective across all tasks and dataset sizes, the use of LLM prompts alone outperformed the combined application in specific scenarios (4 examples per class with Custom COCO).
This observation suggests that DIAGen holds the potential to achieve even better results through task-specific fine-tuning by activating different components of its pipeline.
Our findings show that while each component can independently improve the accuracy, their true strength emerges in combination.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">To further verify that the observed improvements in DIAGen are not merely due to hyperparameter adjustments (see Appendix C), we conducted an experiment directly comparing DIAGen with DA-Fusion using an identical set of hyperparameters (see <a href="#S4.F5" title="In 4.4 Ablation Study â€£ 4 Experiments â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>). The results clearly show that DIAGenâ€™s performance gains stem from our contributions, rather than from changes in hyperparameters alone. In fact, relaxing the hyperparameters within the DA-Fusion model proves counterproductive, often resulting in reduced performance.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.14584/assets/x11.png" id="S4.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.14584/assets/x12.png" id="S4.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.6.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.7.2" class="ltx_text" style="font-size:90%;">Direct comparison of downstream classifier accuracy between DIAGen (ours) and the baseline method DA-Fusion (our parameters), using the same hyperparameters for a fair evaluation. For reference, the original DA-Fusion method with its parameters from Trabucco <em id="S4.F5.7.2.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.F5.7.2.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> is also included.</span></figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Diversity Analysis</h3>

<figure id="S4.F6" class="ltx_figure"><img src="/html/2408.14584/assets/x13.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="227" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.4.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.5.2" class="ltx_text" style="font-size:90%;">Qualitative comparison of synthetic images generated by the DA-Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> baseline <em id="S4.F6.5.2.1" class="ltx_emph ltx_font_italic">(left)</em> and our model DIAGen <em id="S4.F6.5.2.2" class="ltx_emph ltx_font_italic">(right)</em>. Each row was created using the same guiding image.</span></figcaption>
</figure>
<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">While it is important to evaluate the results of the downstream application, we also consider the overall quality and especially the semantic diversity of the synthetic dataset by exploring alternative metrics. If we visually compare the two datasets generated by DA-Fusion and DIAGen, we clearly observe a higher level of diversity with our method (see <a href="#S4.F6" title="In 4.5 Diversity Analysis â€£ 4 Experiments â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>).
At first glance, the DA-Fusion images all look very similar. Small changes can only be observed in fine textural details, such as the imprint on the bottle in <a href="#S4.F6" title="In 4.5 Diversity Analysis â€£ 4 Experiments â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a> (left).
DIAGen, on the other hand, achieves a higher degree of diversity in its images. As shown in <a href="#S4.F6" title="In 4.5 Diversity Analysis â€£ 4 Experiments â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>(right), it generates varied cars, like an oldtimer and a silver car, in different styles and settings, whereas DA-Fusion produces nearly identical cars.
DIAGenâ€™s images are both accurately labeled and semantically diverse.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.8.4.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.6.3" class="ltx_text" style="font-size:90%;">Averaged precision and recall <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> between the two distributions of the real images and the synthetic ones generated by the baseline DA-FusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> respectively our model DIAGen. Training of the models was done using dataset sizes of <math id="S4.T1.4.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.T1.4.1.m1.1b"><mn id="S4.T1.4.1.m1.1.1" xref="S4.T1.4.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.T1.4.1.m1.1c"><cn type="integer" id="S4.T1.4.1.m1.1.1.cmml" xref="S4.T1.4.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.1.m1.1d">2</annotation></semantics></math>, <math id="S4.T1.5.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.T1.5.2.m2.1b"><mn id="S4.T1.5.2.m2.1.1" xref="S4.T1.5.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.T1.5.2.m2.1c"><cn type="integer" id="S4.T1.5.2.m2.1.1.cmml" xref="S4.T1.5.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.2.m2.1d">4</annotation></semantics></math>, and <math id="S4.T1.6.3.m3.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.T1.6.3.m3.1b"><mn id="S4.T1.6.3.m3.1.1" xref="S4.T1.6.3.m3.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.T1.6.3.m3.1c"><cn type="integer" id="S4.T1.6.3.m3.1.1.cmml" xref="S4.T1.6.3.m3.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.3.m3.1d">8</annotation></semantics></math> examples per class for each of the three datasets.</span></figcaption>
<table id="S4.T1.9" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.9.1.1" class="ltx_tr">
<td id="S4.T1.9.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T1.9.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T1.9.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.1.1.2.1.1" class="ltx_p" style="width:29.9pt;"></span>
</span>
</th>
<th id="S4.T1.9.1.1.3" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="3">FOCUS</th>
<th id="S4.T1.9.1.1.4" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="3">Custom COCO</th>
<th id="S4.T1.9.1.1.5" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="3">MS COCO</th>
</tr>
<tr id="S4.T1.9.2.2" class="ltx_tr">
<td id="S4.T1.9.2.2.1" class="ltx_td"></td>
<th id="S4.T1.9.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S4.T1.9.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.2.2.2.1.1" class="ltx_p" style="width:29.9pt;"></span>
</span>
</th>
<th id="S4.T1.9.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.9.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.2.2.3.1.1" class="ltx_p" style="width:25.3pt;">(2)</span>
</span>
</th>
<th id="S4.T1.9.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.9.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.2.2.4.1.1" class="ltx_p" style="width:25.3pt;">(4)</span>
</span>
</th>
<th id="S4.T1.9.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.9.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.2.2.5.1.1" class="ltx_p" style="width:25.3pt;">(8)</span>
</span>
</th>
<th id="S4.T1.9.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.9.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.2.2.6.1.1" class="ltx_p" style="width:25.3pt;">(2)</span>
</span>
</th>
<th id="S4.T1.9.2.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.9.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.2.2.7.1.1" class="ltx_p" style="width:25.3pt;">(4)</span>
</span>
</th>
<th id="S4.T1.9.2.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.9.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.2.2.8.1.1" class="ltx_p" style="width:25.3pt;">(8)</span>
</span>
</th>
<th id="S4.T1.9.2.2.9" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.9.2.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.2.2.9.1.1" class="ltx_p" style="width:25.3pt;">(2)</span>
</span>
</th>
<th id="S4.T1.9.2.2.10" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.9.2.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.2.2.10.1.1" class="ltx_p" style="width:25.3pt;">(4)</span>
</span>
</th>
<th id="S4.T1.9.2.2.11" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.9.2.2.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.2.2.11.1.1" class="ltx_p" style="width:25.3pt;">(8)</span>
</span>
</th>
</tr>
<tr id="S4.T1.9.3.3" class="ltx_tr">
<td id="S4.T1.9.3.3.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.9.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.1.1.1" class="ltx_p">DA-FusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></span>
</span>
</td>
<td id="S4.T1.9.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.2.1.1" class="ltx_p" style="width:29.9pt;">Prec.</span>
</span>
</td>
<td id="S4.T1.9.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.3.1.1" class="ltx_p" style="width:25.3pt;">98.67</span>
</span>
</td>
<td id="S4.T1.9.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.4.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.3.3.4.1.1.1" class="ltx_text ltx_font_bold">97.67</span></span>
</span>
</td>
<td id="S4.T1.9.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.5.1.1" class="ltx_p" style="width:25.3pt;">93.67</span>
</span>
</td>
<td id="S4.T1.9.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.6.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.3.3.6.1.1.1" class="ltx_text ltx_font_bold">89.33</span></span>
</span>
</td>
<td id="S4.T1.9.3.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.7.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.3.3.7.1.1.1" class="ltx_text ltx_font_bold">78.00</span></span>
</span>
</td>
<td id="S4.T1.9.3.3.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.3.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.8.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.3.3.8.1.1.1" class="ltx_text ltx_font_bold">82.67</span></span>
</span>
</td>
<td id="S4.T1.9.3.3.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.3.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.9.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.3.3.9.1.1.1" class="ltx_text ltx_font_bold">89.31</span></span>
</span>
</td>
<td id="S4.T1.9.3.3.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.3.3.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.10.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.3.3.10.1.1.1" class="ltx_text ltx_font_bold">89.69</span></span>
</span>
</td>
<td id="S4.T1.9.3.3.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.3.3.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.3.3.11.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.3.3.11.1.1.1" class="ltx_text ltx_font_bold">86.08</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.9.4.4" class="ltx_tr">
<td id="S4.T1.9.4.4.1" class="ltx_td ltx_align_justify">
<span id="S4.T1.9.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.1.1.1" class="ltx_p">DIAGen (ours)</span>
</span>
</td>
<td id="S4.T1.9.4.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.9.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.2.1.1" class="ltx_p" style="width:29.9pt;">(%)</span>
</span>
</td>
<td id="S4.T1.9.4.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.9.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.3.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.4.4.3.1.1.1" class="ltx_text ltx_font_bold">99.33</span></span>
</span>
</td>
<td id="S4.T1.9.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.9.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.4.1.1" class="ltx_p" style="width:25.3pt;">96.00</span>
</span>
</td>
<td id="S4.T1.9.4.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.9.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.5.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.4.4.5.1.1.1" class="ltx_text ltx_font_bold">95.33</span></span>
</span>
</td>
<td id="S4.T1.9.4.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.9.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.6.1.1" class="ltx_p" style="width:25.3pt;">71.33</span>
</span>
</td>
<td id="S4.T1.9.4.4.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.9.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.7.1.1" class="ltx_p" style="width:25.3pt;">60.67</span>
</span>
</td>
<td id="S4.T1.9.4.4.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.9.4.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.8.1.1" class="ltx_p" style="width:25.3pt;">58.00</span>
</span>
</td>
<td id="S4.T1.9.4.4.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.9.4.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.9.1.1" class="ltx_p" style="width:25.3pt;">83.75</span>
</span>
</td>
<td id="S4.T1.9.4.4.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.9.4.4.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.10.1.1" class="ltx_p" style="width:25.3pt;">81.56</span>
</span>
</td>
<td id="S4.T1.9.4.4.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.9.4.4.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.4.4.11.1.1" class="ltx_p" style="width:25.3pt;">81.03</span>
</span>
</td>
</tr>
<tr id="S4.T1.9.5.5" class="ltx_tr">
<td id="S4.T1.9.5.5.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.9.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.1.1.1" class="ltx_p">DA-FusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></span>
</span>
</td>
<td id="S4.T1.9.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.2.1.1" class="ltx_p" style="width:29.9pt;">Rec.</span>
</span>
</td>
<td id="S4.T1.9.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.3.1.1" class="ltx_p" style="width:25.3pt;">8.00</span>
</span>
</td>
<td id="S4.T1.9.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.4.1.1" class="ltx_p" style="width:25.3pt;">9.33</span>
</span>
</td>
<td id="S4.T1.9.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.5.1.1" class="ltx_p" style="width:25.3pt;">7.00</span>
</span>
</td>
<td id="S4.T1.9.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.6.1.1" class="ltx_p" style="width:25.3pt;">20.67</span>
</span>
</td>
<td id="S4.T1.9.5.5.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.5.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.7.1.1" class="ltx_p" style="width:25.3pt;">21.67</span>
</span>
</td>
<td id="S4.T1.9.5.5.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.5.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.8.1.1" class="ltx_p" style="width:25.3pt;">47.33</span>
</span>
</td>
<td id="S4.T1.9.5.5.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.5.5.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.9.1.1" class="ltx_p" style="width:25.3pt;">3.69</span>
</span>
</td>
<td id="S4.T1.9.5.5.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.5.5.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.10.1.1" class="ltx_p" style="width:25.3pt;">8.03</span>
</span>
</td>
<td id="S4.T1.9.5.5.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.9.5.5.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.5.5.11.1.1" class="ltx_p" style="width:25.3pt;">15.19</span>
</span>
</td>
</tr>
<tr id="S4.T1.9.6.6" class="ltx_tr">
<td id="S4.T1.9.6.6.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S4.T1.9.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.1.1.1" class="ltx_p">DIAGen (ours)</span>
</span>
</td>
<td id="S4.T1.9.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.9.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.2.1.1" class="ltx_p" style="width:29.9pt;">(%)</span>
</span>
</td>
<td id="S4.T1.9.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.9.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.3.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.6.6.3.1.1.1" class="ltx_text ltx_font_bold">25.67</span></span>
</span>
</td>
<td id="S4.T1.9.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.9.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.4.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.6.6.4.1.1.1" class="ltx_text ltx_font_bold">26.00</span></span>
</span>
</td>
<td id="S4.T1.9.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.9.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.5.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.6.6.5.1.1.1" class="ltx_text ltx_font_bold">20.00</span></span>
</span>
</td>
<td id="S4.T1.9.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.9.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.6.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.6.6.6.1.1.1" class="ltx_text ltx_font_bold">58.00</span></span>
</span>
</td>
<td id="S4.T1.9.6.6.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.9.6.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.7.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.6.6.7.1.1.1" class="ltx_text ltx_font_bold">57.67</span></span>
</span>
</td>
<td id="S4.T1.9.6.6.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.9.6.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.8.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.6.6.8.1.1.1" class="ltx_text ltx_font_bold">59.67</span></span>
</span>
</td>
<td id="S4.T1.9.6.6.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.9.6.6.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.9.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.6.6.9.1.1.1" class="ltx_text ltx_font_bold">36.06</span></span>
</span>
</td>
<td id="S4.T1.9.6.6.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.9.6.6.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.10.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.6.6.10.1.1.1" class="ltx_text ltx_font_bold">39.25</span></span>
</span>
</td>
<td id="S4.T1.9.6.6.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.9.6.6.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.6.6.11.1.1" class="ltx_p" style="width:25.3pt;"><span id="S4.T1.9.6.6.11.1.1.1" class="ltx_text ltx_font_bold">41.39</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">To objectively quantify the diversity enhancement, we use the precision and recall metrics for the real and synthetic dataset distributions as defined by KynkÃ¤Ã¤nniemi <em id="S4.SS5.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.SS5.p2.1.2" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> (see Appendix F).
When interpreting the results, it is important to consider the dataset sizes. Our Custom COCO dataset is relatively small, with less than 50 images per class collected by us, leading to a high data bias. In contrast, the MS COCO and FOCUS datasets contain significantly more images per class. Notably, the FOCUS dataset was collected with an emphasis on including uncommon settings. As a result, the real image distributions are likely to differ significantly among these three datasets.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">The results in <a href="#S4.T1" title="In 4.5 Diversity Analysis â€£ 4 Experiments â€£ DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> show a significant recall improvement, with up to a 37.3% increase across all datasets and training samples, reflecting greater image diversity. These results align with observed increases in image diversity. For precision, the FOCUS dataset shows only minor differences, indicating that DIAGen generates diverse yet class-consistent images. However, Custom COCO exhibits a notable precision drop.
This can be attributed to the small size of the Custom COCO dataset, which does not adequately represent the real-world data distribution of its classes, as stated above.
For instance, DIAGen produces an oldtimer as a <em id="S4.SS5.p3.1.1" class="ltx_emph ltx_font_italic">car</em>, which is a valid real-world representation of <em id="S4.SS5.p3.1.2" class="ltx_emph ltx_font_italic">cars</em>, but Custom COCO does not contain any oldtimer images. While Custom COCO addresses data leakage, its distribution is not fully representative of each class and this â€œgoodâ€ sample is considered to be outside the real distribution, which lowers the precision score.
For this reason, the precision score for Custom COCO has limited significance.
This argumentation is backed by the precision results for MS COCO, where we observe a smaller drop in precision compared to DA-Fusion. This difference is attributed to the fidelity-diversity trade-off.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">Overall, these results underline that DIAGen notably enhances diversity in synthetic images.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We introduced DIAGen, an off-the-shelf image augmentation technique designed to increase semantic diversity in datasets with few labeled examples per class. DIAGen expands the DA-Fusion frameworkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> by incorporating three key components: The first two modules of DIAGen focus on increasing diversity in the augmentation process by <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">(i)</em> introducing noise to the class representations in the embedding space, and <em id="S5.p1.1.2" class="ltx_emph ltx_font_italic">(ii)</em> enriching text prompts with semantically meaningful content, leveraging the capabilities of an LLM. The last module is designed to complement these strategies to keep a high class fidelity by <em id="S5.p1.1.3" class="ltx_emph ltx_font_italic">(iii)</em> using a weighting mechanism to reduce the influence of suboptimal generated images using a classifier. These components help balance fidelity and diversity in synthesized images. The resulting model improves classification accuracy across various datasets and enhances recall as a diversity metric. It is particularly effective in enabling downstream models to generalize to uncommon scenarios and edge cases, making it valuable for augmenting data in few-shot settings.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgements.</span> This project is partially funded by the European Research Council (ERC) under the EU Horizon 2020 programme (grant agreement No. 866008) and the State of Hesse, Germany, through the cluster project â€œThe Third Wave of Artificial Intelligence (3AI)â€.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., etÂ al.: GPT-4 Technical report. arXiv:2303.08774 [cs.CL] (2023)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Alaa, A., VanÂ Breugel, B., Saveliev, E.S., vanÂ der Schaar, M.: How faithful is your synthetic data? Sample-level metrics for evaluating and auditing generative models. In: ICML. pp. 290â€“306 (2022)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Antoniou, A., Storkey, A., Edwards, H.: Data augmentation generative adversarial networks. arXiv:1711.04340 [stat.ML] (2017)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Beery, S., VanÂ Horn, G., Perona, P.: Recognition in terra incognita. In: Proceedings of the European conference on computer vision (ECCV). pp. 456â€“473 (2018)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Besnier, V., Jain, H., Bursuc, A., Cord, M., PÃ©rez, P.: This dataset does not exist: training models from generated images. In: ICASSP. pp.Â 1â€“5 (2020)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Dhariwal, P., Nichol, A.: Diffusion models beat GANs on image synthesis. NeurIPS <span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">34</span>, 8780â€“8794 (2021)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Friedman, D., Dieng, A.B.: The Vendi Score: A diversity evaluation metric for machine learning. TMLR (2023)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: An image is worth one word: Personalizing text-to-image generation using textual inversion. In: ICLR (2023)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neural networks. In: ICML. pp. 1321â€“1330 (2017)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
He, R., Sun, S., Yu, X., Xue, C., Zhang, W., Torr, P., Bai, S., Qi, X.: Is synthetic data from generative models ready for image recognition? In: ICLR (2023)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Henriksson, J., Berger, C., Ursing, S.: Understanding the impact of edge cases from occluded pedestrians for ML systems. In: SEAA. pp. 316â€“325 (2021)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: GANs trained by a two time-scale update rule converge to a local Nash equilibrium. NeurIPS <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">30</span> (2017)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS <span id="bib.bib13.1.1" class="ltx_text ltx_font_bold">33</span>, 6840â€“6851 (2020)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ho, J., Saharia, C., Chan, W., Fleet, D.J., Norouzi, M., Salimans, T.: Cascaded diffusion models for high fidelity image generation. JMLR <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">23</span>(47), 1â€“33 (2022)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jahanian, A., Puig, X., Tian, Y., Isola, P.: Generative models as a data source for multiview representation learning. In: ICLR (2022)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jiang, S., Zhu, Y., Liu, C., Song, X., Li, X., Min, W.: Dataset bias in few-shot image recognition. TPAMI <span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">45</span>(1), 229â€“246 (2022)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Kattakinda, P., Feizi, S.: Focus: Familiar objects in common and uncommon settings. In: ICML. pp. 10825â€“10847 (2022)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
KynkÃ¤Ã¤nniemi, T., Karras, T., Laine, S., Lehtinen, J., Aila, T.: Improved precision and recall metric for assessing generative models. NeurIPS <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">32</span> (2019)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., DollÃ¡r, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV. pp. 740â€“755 (2014)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Liu, J., Shen, Z., He, Y., Zhang, X., Xu, R., Yu, H., Cui, P.: Towards out-of-distribution generalization: A survey. arXiv:2108.13624 [cs.LG] (2021)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Man, K., Chahl, J.: A review of synthetic image data and its use in computer vision. Journal of Imaging <span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">8</span>(11), Â 310 (2022)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: SDEdit: Guided image synthesis and editing with stochastic differential equations. In: ICLR (2022)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
MikoÅ‚ajczyk, A., Grochowski, M.: Data augmentation for improving deep learning in image classification problem. In: IIPhDW. pp. 117â€“122 (2018)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed representations of words and phrases and their compositionality. NeurIPS <span id="bib.bib24.1.1" class="ltx_text ltx_font_bold">26</span> (2013)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Murphy, K.P.: Probabilistic machine learning: Advanced topics. MIT press (2023)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Naeem, M.F., Oh, S.J., Uh, Y., Choi, Y., Yoo, J.: Reliable fidelity and diversity metrics for generative models. In: ICML. pp. 7176â€“7185 (2020)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Nagarajan, V., Andreassen, A., Neyshabur, B.: Understanding the failure modes of out-of-distribution generalization. In: ICLR (2021)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Nichol, A.Q., Dhariwal, P.: Improved denoising diffusion probabilistic models. In: ICML. pp. 8162â€“8171 (2021)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Perez, L., Wang, J.: The effectiveness of data augmentation in image classification using deep learning. arXiv:1712.04621 [cs.CV] (2017)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Ravuri, S., Vinyals, O.: Classification accuracy score for conditional generative models. NeurIPS <span id="bib.bib30.1.1" class="ltx_text ltx_font_bold">32</span> (2019)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Rebbapragada, U., Brodley, C.E.: Class noise mitigation through instance weighting. In: ECML. pp. 708â€“715 (2007)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Ren, J., Luo, J., Zhao, Y., Krishna, K., Saleh, M., Lakshminarayanan, B., Liu, P.J.: Out-of-distribution detection and selective generation for conditional language models. In: ICLR (2023)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR. pp. 10684â€“10695 (2022)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D., Norouzi, M.: Palette: Image-to-image diffusion models. In: ACM SIGGRAPH. pp. 1â€“10 (2022)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Sajjadi, M.S., Bachem, O., Lucic, M., Bousquet, O., Gelly, S.: Assessing generative models via precision and recall. NeurIPS <span id="bib.bib35.1.1" class="ltx_text ltx_font_bold">31</span> (2018)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.: Improved techniques for training GANs. NeurIPS <span id="bib.bib36.1.1" class="ltx_text ltx_font_bold">29</span> (2016)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Shijie, J., Ping, W., Peiyi, J., Siping, H.: Research on data augmentation for image classification based on convolution neural networks. In: CAC. pp. 4165â€“4170 (2017)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Singh, K., Navaratnam, T., Holmer, J., Schaub-Meyer, S., Roth, S.: Is synthetic data all we need? Benchmarking the robustness of models trained with synthetic images. In: CVPR Workshops (2024)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Stein, G., Cresswell, J., Hosseinzadeh, R., Sui, Y., Ross, B., Villecroze, V., Liu, Z., Caterini, A.L., Taylor, E., Loaiza-Ganem, G.: Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. NeurIPS <span id="bib.bib39.1.1" class="ltx_text ltx_font_bold">36</span> (2024)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Sudhakaran, G., Dhami, D.S., Kersting, K., Roth, S.: Vision relation transformer for unbiased scene graph generation. In: ICCV. pp. 21882â€“21893 (2023)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Sushko, V., SchÃ¶nfeld, E., Zhang, D., Gall, J., Schiele, B., Khoreva, A.: Oasis: only adversarial supervision for semantic image synthesis. International Journal of Computer Vision pp. 2903â€“2923 (2022)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR. pp.Â 1â€“9 (2015)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Tang, K., Niu, Y., Huang, J., Shi, J., Zhang, H.: Unbiased scene graph generation from biased training. In: CVPR. pp. 3716â€“3725 (2020)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Tong, J., Dai, L.: Out-of-distribution with text-to-image diffusion models. In: PRCV. pp. 276â€“288 (2023)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., etÂ al.: Llama: Open and efficient foundation language models. arXiv.2302.13971[cs.CL] (2023)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Trabucco, B., Doherty, K., Gurinas, M., Salakhutdinov, R.: Effective data augmentation with diffusion models. In: ICLR Workshop: Mathematical and Empirical Understanding of Foundation Models (2024)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Wang, J., Hu, X., Hou, W., Chen, H., Zheng, R., Wang, Y., Yang, L., Huang, H., Ye, W., Geng, X., etÂ al.: On the robustness of ChatGPT: An adversarial and out-of-distribution perspective. In: ICLR Workshop: Trustworthy and Reliable Large-Scale Machine Learning Models (2023)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Wang, L., Zhang, S., Han, Z., Feng, Y., Wei, J., Mei, S.: Diversity measurement-based meta-learning for few-shot object detection of remote sensing images. In: IGARSS. pp. 3087â€“3090 (2022)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Xue, Y., Zhou, Q., Ye, J., Long, L.R., Antani, S., Cornwell, C., Xue, Z., Huang, X.: Synthetic augmentation and feature-based filtering for improved cervical histopathology image classification. In: MICCAI. pp. 387â€“396 (2019)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Yang, J., Zhou, K., Li, Y., Liu, Z.: Generalized out-of-distribution detection: A survey. arXiv:2110.11334 [cs.CV] (2021)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Yang, L., Song, Y., Ren, X., Lyu, C., Wang, Y., Liu, L., Wang, J., Foster, J., Zhang, Y.: Out-of-distribution generalization in text classification: Past, present, and future. arXiv:2305.14104 [cs.CL] (2023)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Zhang, Y., Ling, H., Gao, J., Yin, K., Lafleche, J.F., Barriuso, A., Torralba, A., Fidler, S.: DatasetGAN: Efficient labeled data factory with minimal human effort. In: CVPR. pp. 10145â€“10155 (2021)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.14583" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.14584" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.14584">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.14584" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.14585" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 14:20:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
