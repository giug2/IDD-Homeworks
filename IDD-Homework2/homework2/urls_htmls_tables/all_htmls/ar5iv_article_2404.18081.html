<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  ComposerX: Multi-Agent Symbolic Music Composition with LLMs
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Qixin Deng
    <sup class="ltx_sup" id="id30.30.id1">
     <span class="ltx_text ltx_font_italic" id="id30.30.id1.1">
      5
     </span>
    </sup>
    , Qikai Yang
    <sup class="ltx_sup" id="id31.31.id2">
     <span class="ltx_text ltx_font_italic" id="id31.31.id2.1">
      6
     </span>
    </sup>
    , Ruibin Yuan
    <sup class="ltx_sup" id="id32.32.id3">
     <span class="ltx_text ltx_font_italic" id="id32.32.id3.1">
      1
     </span>
    </sup>
    , Yipeng Huang
    <sup class="ltx_sup" id="id33.33.id4">
     <span class="ltx_text ltx_font_italic" id="id33.33.id4.1">
      2
     </span>
    </sup>
    ,
    <br class="ltx_break"/>
    Yi Wang
    <sup class="ltx_sup" id="id34.34.id5">
     <span class="ltx_text ltx_font_italic" id="id34.34.id5.1">
      2
     </span>
    </sup>
    , Xubo Liu
    <sup class="ltx_sup" id="id35.35.id6">
     <span class="ltx_text ltx_font_italic" id="id35.35.id6.1">
      8
     </span>
    </sup>
    , Zeyue Tian
    <sup class="ltx_sup" id="id36.36.id7">
     <span class="ltx_text ltx_font_italic" id="id36.36.id7.1">
      1
     </span>
    </sup>
    , Jiahao Pan
    <sup class="ltx_sup" id="id37.37.id8">
     <span class="ltx_text ltx_font_italic" id="id37.37.id8.1">
      1
     </span>
    </sup>
    , Ge Zhang
    <sup class="ltx_sup" id="id38.38.id9">
     <span class="ltx_text ltx_font_italic" id="id38.38.id9.1">
      9
     </span>
    </sup>
    , Hanfeng Lin
    <sup class="ltx_sup" id="id39.39.id10">
     <span class="ltx_text ltx_font_italic" id="id39.39.id10.1">
      2
     </span>
    </sup>
    , Yizhi Li
    <sup class="ltx_sup" id="id40.40.id11">
     <span class="ltx_text ltx_font_italic" id="id40.40.id11.1">
      4
     </span>
    </sup>
    , Yinghao Ma
    <sup class="ltx_sup" id="id41.41.id12">
     <span class="ltx_text ltx_font_italic" id="id41.41.id12.1">
      3
     </span>
    </sup>
    ,
    <br class="ltx_break"/>
    Jie Fu
    <sup class="ltx_sup" id="id42.42.id13">
     <span class="ltx_text ltx_font_italic" id="id42.42.id13.1">
      1
     </span>
    </sup>
    , Chenghua Lin
    <sup class="ltx_sup" id="id43.43.id14">
     <span class="ltx_text ltx_font_italic" id="id43.43.id14.1">
      4
     </span>
    </sup>
    , Emmanouil Benetos
    <sup class="ltx_sup" id="id44.44.id15">
     <span class="ltx_text ltx_font_italic" id="id44.44.id15.1">
      3
     </span>
    </sup>
    , Wenwu Wang
    <sup class="ltx_sup" id="id45.45.id16">
     <span class="ltx_text ltx_font_italic" id="id45.45.id16.1">
      8
     </span>
    </sup>
    , Guangyu Xia
    <sup class="ltx_sup" id="id46.46.id17">
     <span class="ltx_text ltx_font_italic" id="id46.46.id17.1">
      7
     </span>
    </sup>
    , Wei Xue
    <sup class="ltx_sup" id="id47.47.id18">
     <span class="ltx_text ltx_font_italic" id="id47.47.id18.1">
      1
     </span>
    </sup>
    , Yike Guo
    <sup class="ltx_sup" id="id48.48.id19">
     <span class="ltx_text ltx_font_italic" id="id48.48.id19.1">
      1
     </span>
    </sup>
    <br class="ltx_break"/>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id49.49.id20">
     <span class="ltx_text ltx_font_italic" id="id49.49.id20.1">
      1
     </span>
    </sup>
    <span class="ltx_text ltx_font_italic" id="id50.50.id21">
     Hong Kong University of Science and Technology
    </span>
    <br class="ltx_break"/>
    <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="id21.21.g1" src="/html/2404.18081/assets/icon.png" width="14"/>
    <sup class="ltx_sup" id="id51.51.id22">
     <span class="ltx_text ltx_font_italic" id="id51.51.id22.1">
      2
     </span>
    </sup>
    <span class="ltx_text ltx_font_italic" id="id52.52.id23">
     Multimodal Art Projection Research Community
    </span>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id53.53.id24">
     <span class="ltx_text ltx_font_italic" id="id53.53.id24.1">
      3
     </span>
    </sup>
    <span class="ltx_text ltx_font_italic" id="id54.54.id25">
     Queen Mary University of London
    </span>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id55.55.id26">
     <span class="ltx_text ltx_font_italic" id="id55.55.id26.1">
      4
     </span>
    </sup>
    <span class="ltx_text ltx_font_italic" id="id25.25.1">
     The University of Manchester
     <br class="ltx_break"/>
     <sup class="ltx_sup" id="id25.25.1.1">
      5
     </sup>
     University of Rochester
    </span>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id56.56.id27">
     <span class="ltx_text ltx_font_italic" id="id56.56.id27.1">
      6
     </span>
    </sup>
    <span class="ltx_text ltx_font_italic" id="id28.28.3">
     University of Illinois at Urbana-Champaign
     <br class="ltx_break"/>
     <sup class="ltx_sup" id="id28.28.3.1">
      7
     </sup>
     Mohamed bin Zayed University of Artificial Intelligence
     <br class="ltx_break"/>
     <sup class="ltx_sup" id="id28.28.3.2">
      8
     </sup>
     University of Surrey
    </span>
    <sup class="ltx_sup" id="id57.57.id28">
     <span class="ltx_text ltx_font_italic" id="id57.57.id28.1">
      9
     </span>
    </sup>
    <span class="ltx_text ltx_font_italic" id="id58.58.id29">
     01.AI
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id59.id1">
   Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. While demonstrating impressive capabilities in STEM subjects, current LLMs easily fail in this task, generating ill-written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs’ potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose
   <span class="ltx_text ltx_font_bold" id="id59.id1.1">
    ComposerX
    <span class="ltx_note ltx_role_footnote" id="footnote1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        <span class="ltx_text ltx_font_medium" id="footnote1.1.1.1">
         1
        </span>
       </span>
       <span class="ltx_text ltx_font_medium" id="footnote1.4">
        Demo page: https://lllindsey0615.github.io/ComposerX_demo/
       </span>
      </span>
     </span>
    </span>
   </span>
   , an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Music shares many structural similarities with language
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ]
    </cite>
    , prompting researchers to explore the application of language models (LMs) in music generation
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib11" title="">
      11
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib13" title="">
      13
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ]
    </cite>
    . Recent advances in large language models (LLMs) have opened potential pathways towards achieving Artificial General Intelligence (AGI). While much of the research emphasis has been on the STEM aspects of AGI
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib17" title="">
      17
     </a>
     ]
    </cite>
    , there is comparatively less focus on its creative dimensions, particularly in music creation. Current methodologies primarily involve training LMs from scratch, as seen with initiatives like MusicLM
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ]
    </cite>
    and MusicGen
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    , with a predominant focus on audio generation. However, these models often struggle with processing advanced musical instructions and typically offer only limited control options, such as genre and instrument selection. Enhancing controllability in these systems requires neural architectural engineering and extensive computational resources
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib18" title="">
      18
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib19" title="">
      19
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib20" title="">
      20
     </a>
     ]
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Recent research, influenced by Bubeck et al.
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib17" title="">
      17
     </a>
     ]
    </cite>
    , has revealed that pretrained large language models (LLMs) might inherently possess emergent musical capabilities. Inspired by these findings, subsequent studies
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib21" title="">
      21
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib22" title="">
      22
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib23" title="">
      23
     </a>
     ]
    </cite>
    have explored leveraging pretrained LLM checkpoints for handling symbolic music in an end-to-end manner, aiming to tap into the extensive knowledge and reasoning abilities embedded in these LLMs.
However, these unified approaches are not without limitations. They depend heavily on hand-crafted datasets tailored for specific musical tasks and often require both a phase of continual pretraining and subsequent supervised fine-tuning. Furthermore, while training on symbolic music data is generally less computationally intensive than processing raw audio data, the costs remain prohibitive for many researchers. For example, renting an 8xGPU machine (such as a p4d.24xlarge spot instance on AWS) for one month can exceed $8,000 USD
    <span class="ltx_note ltx_role_footnote" id="footnote2">
     <sup class="ltx_note_mark">
      2
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        2
       </sup>
       <span class="ltx_tag ltx_tag_note">
        2
       </span>
       https://instances.vantage.sh/aws/ec2/p4d.24xlarge
      </span>
     </span>
    </span>
    , posing a significant financial barrier.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    In this paper, we introduce a novel multi-agent-based methodology, ComposerX
    <span class="ltx_note ltx_role_footnote" id="footnote3">
     <sup class="ltx_note_mark">
      3
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        3
       </sup>
       <span class="ltx_tag ltx_tag_note">
        3
       </span>
       https://github.com/lllindsey0615/ComposerX
      </span>
     </span>
    </span>
    , which is training-free, cheap, and unified. Leveraging the internal musical capabilities of the state-of-the-art GPT-4-turbo, ComposerX can generate polyphonic music pieces of comparable, if not superior, quality to those produced by dedicated symbolic music generation systems
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib24" title="">
      24
     </a>
     ]
    </cite>
    that require extensive computational resources and data. ComposerX utilizes approximately 26k tokens per song, incurring a cost of less than $0.8 USD per piece. Throughout the development phase of ComposerX, the total expenditure on the OpenAI API was under $1k USD. We achieved a good case rate of 18.4%, as assessed by music experts, which translates to an average cost of approximately $4.34 USD for each musically interesting piece. Furthermore, experimental results demonstrate that the multi-agent strategy substantially enhances composition quality over single-agent baselines. In Turing tests, approximately 32.2% of the pieces identified as ‘good’ by ComposerX were indistinguishable from those composed by humans, as indicated in Table
    <a class="ltx_ref" href="#S3.T7" title="Table 7 ‣ 3.3 Results ‣ 3 Experiments ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
     <span class="ltx_text ltx_ref_tag">
      7
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    While there is existing research on musical LLM agents
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib25" title="">
      25
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib26" title="">
      26
     </a>
     ]
    </cite>
    , our approach distinctively diverges from these precedents. Prior studies primarily focus on single-agent systems. In contrast, our work introduces a multi-agent framework, emphasizing collaborative aspects of music creation. Furthermore, we concentrate on symbolic music generation, leveraging the intrinsic musical understanding of LLMs without the need for external computational resources or tools. Previous methodologies typically depend on GPU servers for deploying local inference services, treating the LLMs more as tool-use agents rather than harnessing their inherent capabilities to process and generate musical content. In sum, the contributions of our paper are as follows:
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    (1) We propose the first multi-agent polyphonic symbolic music composition system, ComposerX. It elicits the internal musical capabilities inside LLMs without the need for external tools.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    (2) Through extensive subjective evaluations, we demonstrate that our multi-agent approach substantially enhances the quality of music composition compared to single-agent systems and specialized music generation models. Our method also offers cost-efficiency advantages by obviating the need for dedicated training or local inference services.
   </p>
  </div>
  <div class="ltx_para" id="S1.p7">
   <p class="ltx_p" id="S1.p7.1">
    (3) We commit to the advancement of this research area by open-sourcing our code, prompt-set, and experimental results, facilitating further investigation and development by the community.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Method
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    We first construct a set of user prompts for music composition, which is used for evaluation. Then we demonstrate how we implement our single-agent and multi-agent LLM composition systems.
   </p>
  </div>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    User Prompt Set Curation
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     To understand how the users, typically those with substantial musical backgrounds, would prompt a text-to-music generation system, a user prompt set is collected by asking humans with music backgrounds to manually write high-quality prompts. These prompts typically include essential musical attributes such as genre, tempo, key, chord progression, melody, rhythm, number of bars, number of voices, instruments, style, feeling, emotion, title, and motif of the music piece. Based on the human-written samples, more prompt samples are generated using Self-instruct by GPT-4
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ]
     </cite>
     . This results in a set of 163 prompts, which is used in the later agent testing and system evaluation.
An example prompt is given below.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p2">
    <svg class="ltx_picture" height="97.44" id="S2.SS1.p2.pic1" overflow="visible" version="1.1" width="600">
     <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,97.44) matrix(1 0 0 -1 0 0)">
      <g fill="#BFBFBF" fill-opacity="1.0">
       <path d="M 0 5.91 L 0 91.54 C 0 94.8 2.64 97.44 5.91 97.44 L 594.09 97.44 C 597.36 97.44 600 94.8 600 91.54 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#FFFFFF" fill-opacity="1.0">
       <path d="M 1.97 5.91 L 1.97 73.49 L 598.03 73.49 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#E6E6E6" fill-opacity="1.0">
       <path d="M 1.97 75.46 L 1.97 91.54 C 1.97 93.71 3.73 95.48 5.91 95.48 L 594.09 95.48 C 596.27 95.48 598.03 93.71 598.03 91.54 L 598.03 75.46 Z" style="stroke:none">
       </path>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 79.39)">
       <foreignobject color="#000000" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.32">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS1.p2.pic1.1.1.1.1.1" style="width:419.4pt;">
         <span class="ltx_p" id="S2.SS1.p2.pic1.1.1.1.1.1.1">
          <span class="ltx_text ltx_font_bold" id="S2.SS1.p2.pic1.1.1.1.1.1.1.1">
           Prompt
          </span>
         </span>
        </span>
       </foreignobject>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 9.84)">
       <foreignobject color="#000000" height="55.77" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.32">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS1.p2.pic1.2.2.2.1.1" style="width:419.4pt;">
         <span class="ltx_p" id="S2.SS1.p2.pic1.2.2.2.1.1.1">
          <span class="ltx_text ltx_font_bold" id="S2.SS1.p2.pic1.2.2.2.1.1.1.1">
           Vintage French Chanson:
          </span>
          A nostalgic chanson in C major with a slow tempo, featuring accordion, violin, and upright bass over 16 bars with chords C, Am, Dm, G. The accordion leads with expressive sound, violin adds romance, and the upright bass supports, evoking vintage French charm.
         </span>
        </span>
       </foreignobject>
      </g>
     </g>
    </svg>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p3">
    <svg class="ltx_picture" height="84.64" id="S2.SS1.p3.pic1" overflow="visible" version="1.1" width="600">
     <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,84.64) matrix(1 0 0 -1 0 0)">
      <g fill="#BFBFBF" fill-opacity="1.0">
       <path d="M 0 5.91 L 0 78.74 C 0 82 2.64 84.64 5.91 84.64 L 594.09 84.64 C 597.36 84.64 600 82 600 78.74 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#FFFFFF" fill-opacity="1.0">
       <path d="M 1.97 5.91 L 1.97 63.22 L 598.03 63.22 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#E6E6E6" fill-opacity="1.0">
       <path d="M 1.97 65.19 L 1.97 78.74 C 1.97 80.91 3.73 82.68 5.91 82.68 L 594.09 82.68 C 596.27 82.68 598.03 80.91 598.03 78.74 L 598.03 65.19 Z" style="stroke:none">
       </path>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 69.13)">
       <foreignobject color="#000000" height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.32">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS1.p3.pic1.1.1.1.1.1" style="width:419.4pt;">
         <span class="ltx_p" id="S2.SS1.p3.pic1.1.1.1.1.1.1">
          <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.pic1.1.1.1.1.1.1.1">
           Attributes
          </span>
         </span>
        </span>
       </foreignobject>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 9.84)">
       <foreignobject color="#000000" height="45.51" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.32">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS1.p3.pic1.2.2.2.1.1" style="width:419.4pt;">
         <span class="ltx_p" id="S2.SS1.p3.pic1.2.2.2.1.1.1">
          <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.pic1.2.2.2.1.1.1.1">
           Name:
          </span>
          Vintage French Chanson
          <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.pic1.2.2.2.1.1.1.2">
           Tempo:
          </span>
          Slow
         </span>
         <span class="ltx_p" id="S2.SS1.p3.pic1.2.2.2.1.1.2">
          <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.pic1.2.2.2.1.1.2.1">
           Feeling:
          </span>
          Nostalgic
          <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.pic1.2.2.2.1.1.2.2">
           Chord Progression:
          </span>
          C, Am, Dm, G
         </span>
         <span class="ltx_p" id="S2.SS1.p3.pic1.2.2.2.1.1.3">
          <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.pic1.2.2.2.1.1.3.1">
           Key:
          </span>
          C major
          <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.pic1.2.2.2.1.1.3.2">
           Bars:
          </span>
          16
          <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.pic1.2.2.2.1.1.3.3">
           Instruments:
          </span>
          Accordion, violin, upright bass
         </span>
        </span>
       </foreignobject>
      </g>
     </g>
    </svg>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Single-Agent
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     We apply various prompt engineering techniques, including In Context Learning (ICL), Chain of Thought (CoT), and Role-play to guide a single GPT acting as the composer. Additionally, we have refined the prompt template by incorporating specific instructions that ensure the correctness of the ABC notation format.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p2">
    <p class="ltx_p" id="S2.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">
      Original GPT with Simple Role-play (Ori):
     </span>
     To investigate the inherent capabilities of the original GPT model in interpreting user prompts and generating ABC notation, we instructed GPT to act in the role of a professional composer, with user prompts directly input into the system. This method aims to assess the model’s basic performance in music composition without the integration of additional complex prompting techniques.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p3">
    <p class="ltx_p" id="S2.SS2.p3.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">
      Role-Play with Additional Instruction (Role):
     </span>
     Inspired by classical rule-based computer music generation, we equipped GPT with enhanced musical knowledge focusing on phrase management and melody line construction, detailed in Table
     <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ 2.2 Single-Agent ‣ 2 Method ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     . For instance, in composing melodies, we instructed the model to ensure that each generated melody possesses distinct phrase divisions. Additionally, each phrase is required to conclude with a prominent ending note, which serves as the last note of the phrase. By incorporating these additional instructions, we aim to elevate the music quality and structural coherence of the music, aligning the generated compositions more closely with traditional musical standards.
    </p>
   </div>
   <figure class="ltx_table" id="S2.T1">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.2" style="width:409.1pt;height:167.7pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-10.8pt,4.4pt) scale(0.95,0.95) ;">
      <table class="ltx_tabular ltx_align_middle" id="S2.T1.2.1">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S2.T1.2.1.1.1">
         <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.1.1.1.1">
          <span class="ltx_inline-block ltx_align_top" id="S2.T1.2.1.1.1.1.1">
           <span class="ltx_p" id="S2.T1.2.1.1.1.1.1.1" style="width:411.9pt;">
            <span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.1.1.1.1.1">
             Role-play Prompting with Additional Music Knowledge
            </span>
           </span>
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S2.T1.2.1.2.2">
         <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.1.2.2.1">
          <span class="ltx_inline-block ltx_align_top" id="S2.T1.2.1.2.2.1.1">
           <span class="ltx_p" id="S2.T1.2.1.2.2.1.1.1" style="width:411.9pt;">
            <span class="ltx_text ltx_font_typewriter" id="S2.T1.2.1.2.2.1.1.1.1">
             <span class="ltx_text" id="S2.T1.2.1.2.2.1.1.1.1.1" style="color:#0000FF;">
              You are a talented musician.
             </span>
             Here are some tips for generating melodies:
            </span>
           </span>
           <span class="ltx_p" id="S2.T1.2.1.2.2.1.1.2">
            <span class="ltx_text ltx_font_typewriter" id="S2.T1.2.1.2.2.1.1.2.1">
             1. The generated melody should have clear phrase divisions, and it’s preferable to avoid more than two consecutive measures within one phrase to prevent an uncomfortable listening experience. There should be a certain amount of space between phrases, allowing the audience to clearly distinguish between them.
            </span>
           </span>
           <span class="ltx_p" id="S2.T1.2.1.2.2.1.1.3">
            <span class="ltx_text ltx_font_typewriter" id="S2.T1.2.1.2.2.1.1.3.1">
             2. A phrase usually has a prominent ending note, which is the last note of the entire phrase. It typically has a longer duration, or it might be followed by a rest. This ending note is usually within the key or the chord, e.g., phrases ending with a Cmaj chord usually terminate on one of the three chord tones, C, E, or G, ensuring a stable listening experience.
            </span>
           </span>
           <span class="ltx_p" id="S2.T1.2.1.2.2.1.1.4">
            <span class="ltx_text ltx_font_typewriter" id="S2.T1.2.1.2.2.1.1.4.1">
             3. When generating melodies, the movement of the notes should primarily consist of stable intervals such as whole steps, thirds, and fifths, while avoiding excessive large leaps. This will help maintain a sense of logic and coherence throughout the composition.
            </span>
           </span>
           <span class="ltx_p" id="S2.T1.2.1.2.2.1.1.5">
            <span class="ltx_text ltx_font_typewriter" id="S2.T1.2.1.2.2.1.1.5.1">
             4. The rhythm of the phrases should be rich and harmonious. Try using different rhythmic patterns to build the melody, such as combining eighth notes with sixteenth notes, syncopated rhythms, or triplets.
            </span>
           </span>
          </span>
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      <span class="ltx_text ltx_font_bold" id="S2.T1.3.1.1">
       Table 1
      </span>
      :
     </span>
     Single-agent role-play(indicated in the blue text) prompting with additional tips given by human composer on melody construction.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S2.SS2.p4">
    <p class="ltx_p" id="S2.SS2.p4.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1">
      Chain-of-Thought (CoT):
     </span>
     As proven in other fields of research, CoT improves the ability of LLMs on complex reasoning by encouraging them to write down intermediate reasoning steps
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     . Within the context of music composition, we deconstruct the music generation process into several distinct stages. These stages include specifying initial music information, such as title, key, tempo, and speed, followed by the development of chord progressions and melody composition as detailed in Table
     <a class="ltx_ref" href="#S2.T2" title="Table 2 ‣ 2.2 Single-Agent ‣ 2 Method ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_table" id="S2.T2">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T2.2" style="width:409.1pt;height:122.6pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-10.8pt,3.2pt) scale(0.95,0.95) ;">
      <table class="ltx_tabular ltx_align_middle" id="S2.T2.2.1">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S2.T2.2.1.1.1">
         <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.1.1.1">
          <span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.1.1.1.1">
           <span class="ltx_p" id="S2.T2.2.1.1.1.1.1.1" style="width:411.9pt;">
            <span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.1.1.1.1">
             Chain of Thought prompting with three steps
            </span>
           </span>
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S2.T2.2.1.2.2">
         <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.2.2.1">
          <span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.2.2.1.1">
           <span class="ltx_p" id="S2.T2.2.1.2.2.1.1.1" style="width:411.9pt;">
            <span class="ltx_text ltx_font_typewriter" id="S2.T2.2.1.2.2.1.1.1.1">
             First, you need to determine all the information related to the piece in the ABC notation format, such as the name,tune, speed, mode, and anything other than the notes.
This forms the basis of the piece’s style.***Note that only return the music information in ABC notation format without any notes or text or Additional note.***
            </span>
           </span>
           <span class="ltx_p" id="S2.T2.2.1.2.2.1.1.2">
            <span class="ltx_text ltx_font_typewriter" id="S2.T2.2.1.2.2.1.1.2.1">
             Second,Based on the song information in the ABC notation format provided earlier, generate a ***16-bar long*** chord progression and return it in text form, with each bar separated by a "|" symbol. The generated chord progression should be consistent with the song’s key and as closely aligned with the song’s theme and characteristics as possible.
            </span>
           </span>
           <span class="ltx_p" id="S2.T2.2.1.2.2.1.1.3">
            <span class="ltx_text ltx_font_typewriter" id="S2.T2.2.1.2.2.1.1.3.1">
             Now the chord progression and other information are provided,you are required to create a ***16-bar long*** piece of music based on these information.
            </span>
           </span>
          </span>
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      <span class="ltx_text ltx_font_bold" id="S2.T2.3.1.1">
       Table 2
      </span>
      :
     </span>
     Single-agent CoT prompting method with three steps.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S2.SS2.p5">
    <p class="ltx_p" id="S2.SS2.p5.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p5.1.1">
      In Context Learning (ICL):
     </span>
     ICL leverages a few input-output examples to enhance an LLM’s understanding of a specific task. In this method, we use pairs of user prompts and corresponding ABC notations from ChatMusician
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib21" title="">
       21
      </a>
      ]
     </cite>
     as demonstrative examples for prompting as detailed in Table
     <a class="ltx_ref" href="#S2.T3" title="Table 3 ‣ 2.2 Single-Agent ‣ 2 Method ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_table" id="S2.T3">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T3.2" style="width:409.1pt;height:87.9pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-10.8pt,2.3pt) scale(0.95,0.95) ;">
      <table class="ltx_tabular ltx_align_middle" id="S2.T3.2.1">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S2.T3.2.1.1.1">
         <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.2.1.1.1.1">
          <span class="ltx_inline-block ltx_align_top" id="S2.T3.2.1.1.1.1.1">
           <span class="ltx_p" id="S2.T3.2.1.1.1.1.1.1" style="width:411.9pt;">
            <span class="ltx_text ltx_font_bold" id="S2.T3.2.1.1.1.1.1.1.1">
             Single-agent In-context learning prompting method
            </span>
           </span>
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S2.T3.2.1.2.2">
         <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.2.1.2.2.1">
          <span class="ltx_inline-block ltx_align_top" id="S2.T3.2.1.2.2.1.1">
           <span class="ltx_p" id="S2.T3.2.1.2.2.1.1.1" style="width:411.9pt;">
            <span class="ltx_text ltx_font_typewriter" id="S2.T3.2.1.2.2.1.1.1.1">
             You are an intelligent agent with musical intelligence, and your goal is to create music that meets the relevant needs and human listening habits.In this task, use ABC as the format for outputting sheet music.***Only return the ABC notation without any other description or text,and only return one piece that follow the music description given this time.***Below are the requirements for the music,it contains music elements like title,genre,key and more,and some composition examples are listed after the requirements.
            </span>
           </span>
           <span class="ltx_p" id="S2.T3.2.1.2.2.1.1.2">
            <span class="ltx_text ltx_font_typewriter" id="S2.T3.2.1.2.2.1.1.2.1" style="color:#FF0000;">
             ABC Notation of the selected sample
            </span>
           </span>
          </span>
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      <span class="ltx_text ltx_font_bold" id="S2.T3.3.1.1">
       Table 3
      </span>
      :
     </span>
     Single-agent In-context learning prompting method
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Multi-Agent Music Composition: ComposerX
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     To enhance the music generation capabilities of GPT-4, we developed a collaborative music creation framework, ComposerX, that draws inspiration from key elements inherent in real-world music composition processes, such as melody construction, harmony or counterpoint development, and instrumentation. This framework facilitates the music creation process through a structured conversation chain between agents role-played by GPT-4.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S2.SS3.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.3.1
     </span>
     Agent Role Assignment
    </h4>
    <div class="ltx_para" id="S2.SS3.SSS1.p1">
     <p class="ltx_p" id="S2.SS3.SSS1.p1.1">
      In the collaborative music creation framework designed to augment GPT-4’s music generation capabilities, roles are assigned to ensure a structured and efficient composition process. The assignment of roles is as follows:
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS1.p2">
     <p class="ltx_p" id="S2.SS3.SSS1.p2.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p2.1.1">
       Group Leader
      </span>
      : Tasked with interpreting user inputs, decomposing these inputs into granular tasks, and assigning these tasks to specialized agents in the group.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS1.p3">
     <p class="ltx_p" id="S2.SS3.SSS1.p3.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p3.1.1">
       Melody Agent
      </span>
      : Responsible for generating single-line melodies under the guidance of the group leader.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS1.p4">
     <p class="ltx_p" id="S2.SS3.SSS1.p4.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p4.1.1">
       Harmony Agent:
      </span>
      This agent is tasked with enriching the musical piece, and adds harmonic and contrapuntal elements to the melody.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS1.p5">
     <p class="ltx_p" id="S2.SS3.SSS1.p5.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p5.1.1">
       Instrument Agent:
      </span>
      This agent selects and assigns instruments to each voice.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS1.p6">
     <p class="ltx_p" id="S2.SS3.SSS1.p6.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p6.1.1">
       Reviewer Agent:
      </span>
      Performing a quality assurance role, this agent evaluates the outputs of the melody, harmony, and instrumentation agents across four critical dimensions. (1)Melodic Structure: Evaluation of melody’s narrative flow, thematic development, and variation in pitch and rhythm.
Harmony and Counterpoint: Assessment of how harmonies complement the melody, counterpoint effectiveness, and chord progression quality. (2)Rhythmic Complexity: Analysis of rhythm’s role in sustaining interest, its synergy with melody, and the incorporation of dynamic variations. (3)Instrumentation and Timbre: Review of instrument selection, timbral blending, and dynamic usage to achieve an optimal auditory experience. (4)Form and Structure: Examination of the composition’s overarching structure, transitional elements, connectivity between sections, and conclusion efficacy.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS1.p7">
     <p class="ltx_p" id="S2.SS3.SSS1.p7.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p7.1.1">
       Arrangement Agent:
      </span>
      Concluding the collaborative process, this agent is responsible for compiling and formatting the collective output into standardized ABC notation, ensuring the music is documented in a universally readable format.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S2.SS3.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.3.2
     </span>
     Agent Communication Pattern
    </h4>
    <div class="ltx_para" id="S2.SS3.SSS2.p1">
     <p class="ltx_p" id="S2.SS3.SSS2.p1.1">
      The collaborative framework employs a structured communication pattern to ensure an orderly and efficient flow of information between the different agents involved in the composition process. This pattern is crucial for maintaining the integrity and coherence of the musical piece being generated. The communication process unfolds as follows:
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS2.p2">
     <p class="ltx_p" id="S2.SS3.SSS2.p2.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS2.p2.1.1">
       Initial Composition Round
      </span>
      : The composition process begins with the Group Leader Agent initiating the sequence by analyzing the user input and breaking it down into specific tasks assigned to the Melody, Harmony, and Instrument Agents respectively. This step sets the foundation for the composition based on the user’s requirements.
Following the leader’s instructions, the Melody Agent then generates the initial melody line, adhering to the thematic direction and stylistic guidelines provided by the Group Leader.
Subsequently, the Harmony Agent enriches the melody by adding harmonic layers and counterpoints.
The Instrument Agent assigns appropriate instruments to the generated melody and harmony lines by selecting timbres that complement the overall composition.
     </p>
    </div>
    <div class="ltx_para" id="S2.SS3.SSS2.p3">
     <p class="ltx_p" id="S2.SS3.SSS2.p3.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS2.p3.1.1">
       Iterative Review and Feedback Cycle
      </span>
      : Upon completion of the initial composition round, the Reviewer Agent steps in to evaluate the work produced by the Melody, Harmony, and Instrument Agents. This agent provides comprehensive feedback across several critical dimensions, including melodic structure, harmony and counterpoint, rhythmic complexity, and instrumentation.
     </p>
    </div>
    <div class="ltx_para" id="S2.SS3.SSS2.p4">
     <p class="ltx_p" id="S2.SS3.SSS2.p4.1">
      Based on the feedback from the Reviewer Agent, the Melody, Harmony, and Instrument Agents proceed to refine their respective components of the composition. This iterative refinement process typically follows the order: Melody, Harmony, and then Instrument, allowing for modifications to be made in response to the feedback provided.
     </p>
    </div>
    <div class="ltx_para" id="S2.SS3.SSS2.p5">
     <p class="ltx_p" id="S2.SS3.SSS2.p5.1">
      The composition undergoes several rounds of review and refinement, with the Reviewer Agent continuously providing feedback to ensure the musical piece evolves toward a coherent and high-quality final product. This iterative process allows for dynamic adjustments and enhancements to be made, enriching the overall composition.
     </p>
    </div>
    <div class="ltx_para" id="S2.SS3.SSS2.p6">
     <p class="ltx_p" id="S2.SS3.SSS2.p6.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS2.p6.1.1">
       Final Arrangement and Notation
      </span>
      : Once the composition has reached a satisfactory level of polish and coherence, the Arrangement Agent takes over to compile and format the collective output into the standardized ABC notation. This final step ensures that the music is documented in a format that is readable and can be interpreted by musicians and software alike.
     </p>
    </div>
    <div class="ltx_para" id="S2.SS3.SSS2.p7">
     <p class="ltx_p" id="S2.SS3.SSS2.p7.1">
      This communication pattern enables a collaborative and adaptive approach to music generation. The primary advantage of this communication pattern lies in its ability to simulate a real-world collaborative music creation environment, where each participant’s input is valued and considered in the development of the final piece. Integrating this structured approach with a multi-agent system significantly reduces the likelihood of LLM hallucination, a notable challenge where models may generate inaccurate or nonsensical outputs. By assigning specific roles to specialized agents, the framework ensures that each segment of the music composition undergoes rigorous scrutiny and refinement. This division of labor not only enhances the precision and relevance of the generated content but also leverages the collective intelligence of the agents to cross-verify information, thereby mitigating the risk of incorporating erroneous elements into the composition. An example of the music composition process is given in Figure 1 and Figure 2 with a comparison made between
     </p>
    </div>
    <figure class="ltx_figure" id="S2.F2">
     <div class="ltx_flex_figure">
      <div class="ltx_flex_cell ltx_flex_size_1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="325" id="S2.F2.g1" src="/html/2404.18081/assets/figs/fig1.png" width="540"/>
      </div>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text ltx_font_bold" id="S2.F2.2.1.1">
        Figure 1
       </span>
       :
      </span>
      The Leader Agent will distribute the tasks among the Melody Agent, Harmony Agent, Instrumentation Agent when it is requested a "Breezy Caribbean Calypso" piece. This figure demonstrates the work of the three agents with changes in the same four bar opening.
     </figcaption>
     <div class="ltx_flex_figure">
      <div class="ltx_flex_cell ltx_flex_size_1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="342" id="S2.F2.g2" src="/html/2404.18081/assets/figs/fig2.png" width="540"/>
      </div>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text ltx_font_bold" id="S2.F2.3.1.1">
        Figure 2
       </span>
       :
      </span>
      The Reviewer Agent then analyze the collective effort of the three agents in the first stage (shown in Figure
      <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ 2.3.2 Agent Communication Pattern ‣ 2.3 Multi-Agent Music Composition: ComposerX ‣ 2 Method ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      ), and give advice for agents to work on. This figure demonstrates the work of the three agents after incorporating the advice given by the Reviewer Agent in the same four-bar opening.
     </figcaption>
    </figure>
    <figure class="ltx_figure" id="S2.F3">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="375" id="S2.F3.g1" src="/html/2404.18081/assets/x1.png" width="468"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text ltx_font_bold" id="S2.F3.2.1.1">
        Figure 3
       </span>
       :
      </span>
      Agent Communication Pattern of ComposerX.The system is given with a user prompt. In the Planning stage, the Leader analyzes the user prompt and decomposes it into subtasks that can be assigned to other musician agents. In the Composing stage, the musician agents, including Melody Agent, Harmony Agent, and Instrument Agent compose in ABC notation according to their assigned tasks. In the Reviewing stage, the Review Agent provides constructive feedback to the musician agents and the musician agents revise their work according to the feedback they received. In the arrangement stage, the Arrangement Agent arranges the work of the musicians agent to standardized ABC notation.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S2.SS3.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.3.3
     </span>
     Agent Prompt Engineering
    </h4>
    <div class="ltx_para" id="S2.SS3.SSS3.p1">
     <p class="ltx_p" id="S2.SS3.SSS3.p1.1">
      Agent prompt engineering emerges as a crucial technique for optimizing the performance of each specialized agent and the quality of the generated music. This process involves the meticulous design of role-specific instructions and guidelines that encapsulate both the musicality and technicality of ABC notation generation. The framework incorporates In-Context Learning for ABC notations to ensure agents can effectively communicate and document their contributions. An example of the agent prompt is given in Table
      <a class="ltx_ref" href="#S2.T4" title="Table 4 ‣ 2.3.3 Agent Prompt Engineering ‣ 2.3 Multi-Agent Music Composition: ComposerX ‣ 2 Method ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      . This section elaborates on these components and their significance in fostering collaborative dynamics within the framework.
     </p>
    </div>
    <div class="ltx_para" id="S2.SS3.SSS3.p2">
     <p class="ltx_p" id="S2.SS3.SSS3.p2.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS3.p2.1.1">
       Role-Specific Instructions
      </span>
      : Within the framework, each agent is endowed with a set of instructions tailored to its designated role. These instructions serve to ensure a comprehensive understanding of the agent’s duties, the expectations for its performance, and its role within the larger collaborative ensemble. Agents are briefed on the specific outcomes they are expected to achieve and informed about the dynamics of their interactions with other agents. This detailed prompt design facilitates a cohesive operation among the agents, fostering an environment where each component of the framework is aligned toward the collective goal of generating sophisticated and coherent musical compositions.
     </p>
    </div>
    <div class="ltx_para" id="S2.SS3.SSS3.p3">
     <p class="ltx_p" id="S2.SS3.SSS3.p3.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS3.p3.1.1">
       In-Context Learning for ABC Notation
      </span>
      : In Context Learning for ABC notation ensures accurate format output from each agent. The Melody Agent is shown with an example of a monophonic melody in ABC notation, providing a clear model for representing single-line melodies.
The Harmony Agent receives a polyphonic music piece example in ABC notation, aiding in understanding the notation of harmonies and counterpoints in multiple voices.
The Instrument Agent is given a polyphonic piece with MIDI program information noted, demonstrating how to detail instrumental assignments within the notation.
This approach equips agents with the knowledge to correctly apply ABC notation, essential for the structured and coherent documentation of musical compositions.
     </p>
    </div>
    <figure class="ltx_table" id="S2.T4">
     <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T4.2" style="width:409.1pt;height:174.7pt;vertical-align:-0.0pt;">
      <span class="ltx_transformed_inner" style="transform:translate(-10.8pt,4.6pt) scale(0.95,0.95) ;">
       <table class="ltx_tabular ltx_align_middle" id="S2.T4.2.1">
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S2.T4.2.1.1.1">
          <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T4.2.1.1.1.1">
           <span class="ltx_inline-block ltx_align_top" id="S2.T4.2.1.1.1.1.1">
            <span class="ltx_p" id="S2.T4.2.1.1.1.1.1.1" style="width:411.9pt;">
             <span class="ltx_text ltx_font_bold" id="S2.T4.2.1.1.1.1.1.1.1">
              Melody Agent Prompt
             </span>
            </span>
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S2.T4.2.1.2.2">
          <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T4.2.1.2.2.1">
           <span class="ltx_inline-block ltx_align_top" id="S2.T4.2.1.2.2.1.1">
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.1" style="width:411.9pt;">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.1.1">
              <span class="ltx_text" id="S2.T4.2.1.2.2.1.1.1.1.1" style="color:#0000FF;">
               You are a skillful musician, especially in writing melody.
              </span>
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.2">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.2.1">
              You will compose a single-line melody based on the client’s request
and assigned tasks from the Leader.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.3">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.3.1">
              You must output your work in ABC Notations.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.4">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.4.1" style="color:#FF0000;">
              Here is a template of a music piece in ABC notation, in this template:
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.5">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.5.1" style="color:#FF0000;">
              X:1 is the reference number. You can increment this for each new tune.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.6">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.6.1" style="color:#FF0000;">
              T:Title is where you’ll put the title of your tune.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.7">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.7.1" style="color:#FF0000;">
              C:Composer is where you’ll put the composer’s name.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.8">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.8.1" style="color:#FF0000;">
              M:4/4 sets the meter to 4/4 time, but you can change this as needed.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.9">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.9.1" style="color:#FF0000;">
              L:1/8 sets the default note length to eighth notes.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.10">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.10.1" style="color:#FF0000;">
              K:C sets the key to C Major. Change this to match your desired key.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.11">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.11.1" style="color:#FF0000;">
              The music notation follows, with |: and :| denoting the beginning
and end of repeated sections.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.12">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.12.1" style="color:#FF0000;">
              Markdown your work using ‘‘‘ ‘‘‘ to the client.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.13">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.13.1" style="color:#FF0000;">
              ‘‘‘
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.14">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.14.1" style="color:#FF0000;">
              X:1
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.15">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.15.1" style="color:#FF0000;">
              T:Title
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.16">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.16.1" style="color:#FF0000;">
              C:Composer
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.17">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.17.1" style="color:#FF0000;">
              M:Meter
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.18">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.18.1" style="color:#FF0000;">
              L:Unit note length
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.19">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.19.1" style="color:#FF0000;">
              K:Key
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.20">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.20.1" style="color:#FF0000;">
              |:GABc d2e2|f2d2 e4|g4 f2e2|d6 z2:|
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.21">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.21.1" style="color:#FF0000;">
              |:c2A2 B2G2|A2F2 G4|E2c2 D2B,2|C6 z2:|
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.22">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.22.1" style="color:#FF0000;">
              ‘‘‘
              <span class="ltx_text" id="S2.T4.2.1.2.2.1.1.22.1.1" style="color:#000000;">
              </span>
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.23">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.23.1">
              You will output the melody following this template,
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.24">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.24.1">
              but decide the time signature, key signature, and the
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.25">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.25.1">
              actual musical contents and length yourself.
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.26">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.26.1">
              After you receive the feedback from the Reviewer Agent,
             </span>
            </span>
            <span class="ltx_p" id="S2.T4.2.1.2.2.1.1.27">
             <span class="ltx_text ltx_font_typewriter" id="S2.T4.2.1.2.2.1.1.27.1">
              please improve your work according to the suggestions you were given.
             </span>
            </span>
           </span>
          </td>
         </tr>
        </tbody>
       </table>
      </span>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       <span class="ltx_text ltx_font_bold" id="S2.T4.3.1.1">
        Table 4
       </span>
       :
      </span>
      Prompt for Melody Agent. GPT is prompted with role-specific instructions(indicated in blue text) and In-Context-Learning of ABC notations(indicated in red text)
     </figcaption>
    </figure>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Experiments
  </h2>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Setup
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     Our experiment leverages the multi-agent conversation provided by the AutoGen framework
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib29" title="">
       29
      </a>
      ]
     </cite>
     , utilizing its group chat function to facilitate a customized interaction among pre-defined agents. This setup comprises an ensemble of agents including one leader, three musician agents (melody, harmony, and instrument agents), one review agent, and one arrangement agent. Additionally, a user proxy agent is integrated into the framework to simulate user interaction by inputting prompts from our curated user prompt set.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     We employ the "GroupChatManager" class from AutoGen to ensure seamless coordination and oversight of the conversation’s content and workflow. According to AutoGen, the group manager is also powered by LLMs, functions as the supervisor of the conversation, and implements a structured communication protocol that involves three critical steps: dynamically selecting a speaker from the agents, collecting the response from the chosen agent, and disseminating the collected response to the rest of the group.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.1">
     For the purpose of our experiment, we have predetermined a maximum number of twelve rounds for agent communication. This limitation allows us to observe the effectiveness of the multi-agent system over a defined number of interaction cycles, facilitating one or more rounds of iterative review and refinement within the conversation. This structured experimental design is aimed at evaluating the collaborative dynamics and output quality of the multi-agent conversation in generating cohesive and musically rich compositions based on user prompts.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Evaluation
   </h3>
   <section class="ltx_subsubsection" id="S3.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.1
     </span>
     Automatic Evaluation
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS1.p1">
     <p class="ltx_p" id="S3.SS2.SSS1.p1.1">
      We conducted two experiments to automatically evaluate our system. One experiment assessed the success rate of generating symbolic music in a multi-agent setting, with results presented in Table
      <a class="ltx_ref" href="#S3.T5" title="Table 5 ‣ 3.2.1 Automatic Evaluation ‣ 3.2 Evaluation ‣ 3 Experiments ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      . One experiment compared the sequence lengths of symbolic music generated by multi-agent and single-agent systems, detailed in Table
      <a class="ltx_ref" href="#S3.T6" title="Table 6 ‣ 3.2.1 Automatic Evaluation ‣ 3.2 Evaluation ‣ 3 Experiments ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      . These experiments demonstrate the effectiveness of our approach in generating symbolic music.
     </p>
    </div>
    <figure class="ltx_table" id="S3.T5">
     <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T5.3">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S3.T5.3.4.1">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T5.3.4.1.1">
         Checkpoints
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.3.4.1.2">
         Generation Success Rate
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S3.T5.1.1">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T5.1.1.2">
         GPT-4-Turbo
        </th>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1">
         <math alttext="98.2" class="ltx_Math" display="inline" id="S3.T5.1.1.1.m1.1">
          <semantics id="S3.T5.1.1.1.m1.1a">
           <mn id="S3.T5.1.1.1.m1.1.1" xref="S3.T5.1.1.1.m1.1.1.cmml">
            98.2
           </mn>
           <annotation-xml encoding="MathML-Content" id="S3.T5.1.1.1.m1.1b">
            <cn id="S3.T5.1.1.1.m1.1.1.cmml" type="float" xref="S3.T5.1.1.1.m1.1.1">
             98.2
            </cn>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T5.1.1.1.m1.1c">
            98.2
           </annotation>
          </semantics>
         </math>
         %
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T5.2.2">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T5.2.2.2">
         GPT-4-0314
        </th>
        <td class="ltx_td ltx_align_center" id="S3.T5.2.2.1">
         <math alttext="95.7" class="ltx_Math" display="inline" id="S3.T5.2.2.1.m1.1">
          <semantics id="S3.T5.2.2.1.m1.1a">
           <mn id="S3.T5.2.2.1.m1.1.1" xref="S3.T5.2.2.1.m1.1.1.cmml">
            95.7
           </mn>
           <annotation-xml encoding="MathML-Content" id="S3.T5.2.2.1.m1.1b">
            <cn id="S3.T5.2.2.1.m1.1.1.cmml" type="float" xref="S3.T5.2.2.1.m1.1.1">
             95.7
            </cn>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T5.2.2.1.m1.1c">
            95.7
           </annotation>
          </semantics>
         </math>
         %
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T5.3.3">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T5.3.3.2">
         GPT-3.5-Turbo
        </th>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.3.3.1">
         <math alttext="73.0" class="ltx_Math" display="inline" id="S3.T5.3.3.1.m1.1">
          <semantics id="S3.T5.3.3.1.m1.1a">
           <mn id="S3.T5.3.3.1.m1.1.1" xref="S3.T5.3.3.1.m1.1.1.cmml">
            73.0
           </mn>
           <annotation-xml encoding="MathML-Content" id="S3.T5.3.3.1.m1.1b">
            <cn id="S3.T5.3.3.1.m1.1.1.cmml" type="float" xref="S3.T5.3.3.1.m1.1.1">
             73.0
            </cn>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T5.3.3.1.m1.1c">
            73.0
           </annotation>
          </semantics>
         </math>
         %
        </td>
       </tr>
      </tbody>
     </table>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       <span class="ltx_text ltx_font_bold" id="S3.T5.6.1.1">
        Table 5
       </span>
       :
      </span>
      <span class="ltx_text" id="S3.T5.7.2" style="color:#000000;">
       One-time generation success rate for multi-agent system with different checkpoints
      </span>
     </figcaption>
    </figure>
    <figure class="ltx_table" id="S3.T6">
     <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T6.5">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S3.T6.5.6.1">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T6.5.6.1.1">
         Methods
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.5.6.1.2">
         Average ABC String Length
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S3.T6.1.1">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T6.1.1.2">
         GPT-4-Turbo multi
        </th>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.1.1.1">
         <math alttext="1005.925" class="ltx_Math" display="inline" id="S3.T6.1.1.1.m1.1">
          <semantics id="S3.T6.1.1.1.m1.1a">
           <mn id="S3.T6.1.1.1.m1.1.1" xref="S3.T6.1.1.1.m1.1.1.cmml">
            1005.925
           </mn>
           <annotation-xml encoding="MathML-Content" id="S3.T6.1.1.1.m1.1b">
            <cn id="S3.T6.1.1.1.m1.1.1.cmml" type="float" xref="S3.T6.1.1.1.m1.1.1">
             1005.925
            </cn>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T6.1.1.1.m1.1c">
            1005.925
           </annotation>
          </semantics>
         </math>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T6.2.2">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T6.2.2.2">
         GPT-4-Turbo cot
        </th>
        <td class="ltx_td ltx_align_center" id="S3.T6.2.2.1">
         <math alttext="360.92" class="ltx_Math" display="inline" id="S3.T6.2.2.1.m1.1">
          <semantics id="S3.T6.2.2.1.m1.1a">
           <mn id="S3.T6.2.2.1.m1.1.1" xref="S3.T6.2.2.1.m1.1.1.cmml">
            360.92
           </mn>
           <annotation-xml encoding="MathML-Content" id="S3.T6.2.2.1.m1.1b">
            <cn id="S3.T6.2.2.1.m1.1.1.cmml" type="float" xref="S3.T6.2.2.1.m1.1.1">
             360.92
            </cn>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T6.2.2.1.m1.1c">
            360.92
           </annotation>
          </semantics>
         </math>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T6.3.3">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T6.3.3.2">
         GPT-4-Turbo icl
        </th>
        <td class="ltx_td ltx_align_center" id="S3.T6.3.3.1">
         <math alttext="366.30" class="ltx_Math" display="inline" id="S3.T6.3.3.1.m1.1">
          <semantics id="S3.T6.3.3.1.m1.1a">
           <mn id="S3.T6.3.3.1.m1.1.1" xref="S3.T6.3.3.1.m1.1.1.cmml">
            366.30
           </mn>
           <annotation-xml encoding="MathML-Content" id="S3.T6.3.3.1.m1.1b">
            <cn id="S3.T6.3.3.1.m1.1.1.cmml" type="float" xref="S3.T6.3.3.1.m1.1.1">
             366.30
            </cn>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T6.3.3.1.m1.1c">
            366.30
           </annotation>
          </semantics>
         </math>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T6.4.4">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T6.4.4.2">
         GPT-4-Turbo ori
        </th>
        <td class="ltx_td ltx_align_center" id="S3.T6.4.4.1">
         <math alttext="354.53" class="ltx_Math" display="inline" id="S3.T6.4.4.1.m1.1">
          <semantics id="S3.T6.4.4.1.m1.1a">
           <mn id="S3.T6.4.4.1.m1.1.1" xref="S3.T6.4.4.1.m1.1.1.cmml">
            354.53
           </mn>
           <annotation-xml encoding="MathML-Content" id="S3.T6.4.4.1.m1.1b">
            <cn id="S3.T6.4.4.1.m1.1.1.cmml" type="float" xref="S3.T6.4.4.1.m1.1.1">
             354.53
            </cn>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T6.4.4.1.m1.1c">
            354.53
           </annotation>
          </semantics>
         </math>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T6.5.5">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T6.5.5.2">
         GPT-4-Turbo role
        </th>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.5.5.1">
         <math alttext="337.64" class="ltx_Math" display="inline" id="S3.T6.5.5.1.m1.1">
          <semantics id="S3.T6.5.5.1.m1.1a">
           <mn id="S3.T6.5.5.1.m1.1.1" xref="S3.T6.5.5.1.m1.1.1.cmml">
            337.64
           </mn>
           <annotation-xml encoding="MathML-Content" id="S3.T6.5.5.1.m1.1b">
            <cn id="S3.T6.5.5.1.m1.1.1.cmml" type="float" xref="S3.T6.5.5.1.m1.1.1">
             337.64
            </cn>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T6.5.5.1.m1.1c">
            337.64
           </annotation>
          </semantics>
         </math>
        </td>
       </tr>
      </tbody>
     </table>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       <span class="ltx_text ltx_font_bold" id="S3.T6.8.1.1">
        Table 6
       </span>
       :
      </span>
      <span class="ltx_text" id="S3.T6.9.2" style="color:#000000;">
       The average length of ABC String generated by different methods on GPT-4-Turbo checkpoint
      </span>
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.2
     </span>
     Human Listening Test
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS2.p1">
     <p class="ltx_p" id="S3.SS2.SSS2.p1.1">
      To qualitatively assess our work, we conducted three listening tests. In the first test, we compared music samples generated by single-agent and multi-agent baselines. Similar to the AB-test setting from previous work
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib30" title="">
        30
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib21" title="">
        21
       </a>
       ]
      </cite>
      , participants were presented with pairs of samples: one from a multi-agent baseline with GPT-4 Turbo checkpoints, and the other from a single-agent baseline employing prompting techniques mentioned above: Original, In-Context Learning (ICL), Chain of Thought (CoT), and Role-play, also driven by GPT-4 Turbo checkpoints. Participants were asked to select the sample they preferred. All paired samples were generated using the same prompt; however, participants were not informed about the specific prompt details before making their selections.
     </p>
    </div>
    <figure class="ltx_figure" id="S3.F4">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="253" id="S3.F4.g1" src="/html/2404.18081/assets/exp1.png" width="338"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text ltx_font_bold" id="S3.F4.2.1.1">
        Figure 4
       </span>
       :
      </span>
      Result from the first listening test comparing multi-agent baseline and single-agent baselines with different prompting techniques. Each row indicates the fraction of listeners’ preference for the indicated baseline over other baselines. i.e. 0.77 means raters prefer multi-agent system over CoT single-agent 77% of the times.
     </figcaption>
    </figure>
    <div class="ltx_para" id="S3.SS2.SSS2.p2">
     <p class="ltx_p" id="S3.SS2.SSS2.p2.1">
      In the second listening test, we assess the perceived human-like quality of music generated by the multi-agent baselines. Participants were presented with two types of music samples: those generated by multi-agent baselines and those composed by humans, sourced from Irishman and KernScores
      <span class="ltx_note ltx_role_footnote" id="footnote4">
       <sup class="ltx_note_mark">
        4
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          4
         </sup>
         <span class="ltx_tag ltx_tag_note">
          4
         </span>
         http://kern.ccarh.org/
        </span>
       </span>
      </span>
      , which are ABC notation datasets containing human-composed music pieces from all around the world. Each participant is asked to determine whether each sample was composed by a human or a machine.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS2.p3">
     <p class="ltx_p" id="S3.SS2.SSS2.p3.1">
      In the third listening test, we assessed the performance of our multi-agent baselines, which incorporate GPT-4 Turbo, GPT-4-0314, and GPT-3.5-Turbo checkpoints, against established text-to-music generation models. Specifically, comparisons were made with MuseCoco
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib7" title="">
        7
       </a>
       ]
      </cite>
      , developed by Peiling Lu et al., and a BART-based model fine-tuned on 282,870 English text2music pairs in ABC notation, as proposed by Wu et al
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib24" title="">
        24
       </a>
       ]
      </cite>
      . Participants were presented with music samples alongside their corresponding prompts and asked to select the sample that best matched the prompt in terms of musical structure and content.
     </p>
    </div>
    <figure class="ltx_figure" id="S3.F5">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="298" id="S3.F5.g1" src="/html/2404.18081/assets/exp3.png" width="338"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text ltx_font_bold" id="S3.F5.2.1.1">
        Figure 5
       </span>
       :
      </span>
      Result from listening test comparing multi-agent baselines with GPT-4-Turbo, GPT-4-0314, GPT-3.5-Turbo checkpoints, MuseCoco and text2music Baselines. Each row indicates the fraction of listeners’ preference for the indicated baseline over other baselines. In this case, the strongest multi-agent baseline with GPT-4-Turbo checkpoints outperformed text2music, and received the same score as MuseCoco.
     </figcaption>
    </figure>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Results
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     Results from comparing multi-agent baseline and single-agent baseline appear in Figure 4. The preference score of GPT-4-Turbo multi has 0.77, 0.68, 0.6, and 0.57 on each of other single-agent baselines.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS3.p2">
    <p class="ltx_p" id="S3.SS3.p2.1">
     As indicated by the fractions, the multi-agent baseline outperformed all the single-agent baselines. In addition to the dominant performance of the multi-agent baseline over single-agent baselines, we also observe that the multi-agent baseline has a stronger ability to generate longer music. As indicated by Table
     <a class="ltx_ref" href="#S3.T6" title="Table 6 ‣ 3.2.1 Automatic Evaluation ‣ 3.2 Evaluation ‣ 3 Experiments ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     , GPT-4-Turbo multi demonstrates the capacity to generate music pieces nearly three times longer than those produced by any other single-agent baselines.
    </p>
   </div>
   <figure class="ltx_table" id="S3.T7">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T7.4">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S3.T7.4.5.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T7.4.5.1.1">
        Model
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.4.5.1.2">
        Perceived as Human
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.4.5.1.3">
        Perceived as Machine
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S3.T7.2.2">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T7.2.2.3">
        ComposerX
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.1.1.1">
        <math alttext="32.2" class="ltx_Math" display="inline" id="S3.T7.1.1.1.m1.1">
         <semantics id="S3.T7.1.1.1.m1.1a">
          <mn id="S3.T7.1.1.1.m1.1.1" xref="S3.T7.1.1.1.m1.1.1.cmml">
           32.2
          </mn>
          <annotation-xml encoding="MathML-Content" id="S3.T7.1.1.1.m1.1b">
           <cn id="S3.T7.1.1.1.m1.1.1.cmml" type="float" xref="S3.T7.1.1.1.m1.1.1">
            32.2
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T7.1.1.1.m1.1c">
           32.2
          </annotation>
         </semantics>
        </math>
        %
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.2.2.2">
        <math alttext="67.8" class="ltx_Math" display="inline" id="S3.T7.2.2.2.m1.1">
         <semantics id="S3.T7.2.2.2.m1.1a">
          <mn id="S3.T7.2.2.2.m1.1.1" xref="S3.T7.2.2.2.m1.1.1.cmml">
           67.8
          </mn>
          <annotation-xml encoding="MathML-Content" id="S3.T7.2.2.2.m1.1b">
           <cn id="S3.T7.2.2.2.m1.1.1.cmml" type="float" xref="S3.T7.2.2.2.m1.1.1">
            67.8
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T7.2.2.2.m1.1c">
           67.8
          </annotation>
         </semantics>
        </math>
        %
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T7.4.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T7.4.4.3">
        Ground Truth
       </th>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.3.3.1">
        <math alttext="55.4" class="ltx_Math" display="inline" id="S3.T7.3.3.1.m1.1">
         <semantics id="S3.T7.3.3.1.m1.1a">
          <mn id="S3.T7.3.3.1.m1.1.1" xref="S3.T7.3.3.1.m1.1.1.cmml">
           55.4
          </mn>
          <annotation-xml encoding="MathML-Content" id="S3.T7.3.3.1.m1.1b">
           <cn id="S3.T7.3.3.1.m1.1.1.cmml" type="float" xref="S3.T7.3.3.1.m1.1.1">
            55.4
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T7.3.3.1.m1.1c">
           55.4
          </annotation>
         </semantics>
        </math>
        %
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.4.4.2">
        <math alttext="44.6" class="ltx_Math" display="inline" id="S3.T7.4.4.2.m1.1">
         <semantics id="S3.T7.4.4.2.m1.1a">
          <mn id="S3.T7.4.4.2.m1.1.1" xref="S3.T7.4.4.2.m1.1.1.cmml">
           44.6
          </mn>
          <annotation-xml encoding="MathML-Content" id="S3.T7.4.4.2.m1.1b">
           <cn id="S3.T7.4.4.2.m1.1.1.cmml" type="float" xref="S3.T7.4.4.2.m1.1.1">
            44.6
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T7.4.4.2.m1.1c">
           44.6
          </annotation>
         </semantics>
        </math>
        %
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      <span class="ltx_text ltx_font_bold" id="S3.T7.7.1.1">
       Table 7
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.T7.8.2" style="color:#000000;">
      Result from our second listening test (Turing test).
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS3.p3">
    <p class="ltx_p" id="S3.SS3.p3.1">
     Results from comparing the multi-agent baseline with music composed by humans indicates that ComposerX gets 32.2% perceived as human which is lower than the rate of real human music - 55.4% as indicated in Table
     <a class="ltx_ref" href="#S3.T7" title="Table 7 ‣ 3.3 Results ‣ 3 Experiments ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     . Despite failing the Turing test, ComposerX showcases its capability to closely match human skill in music composition.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS3.p4">
    <p class="ltx_p" id="S3.SS3.p4.1">
     Results from comparing the multi-agent baseline with GPT-4-Turbo, GPT-4-0314, GPT-3.5-Turbo checkpoints, MuseCoco, and text2music are presented in Figure 5. As indicated by the fractional numbers, the multi-agent baseline with GPT-4-Turbo checkpoints is our strongest-performed baseline. It outperformed text2music baseline with 0.56 preference score and received the same score as MuseCoco. GPT-4-Turbo also shows the highest generation success rate among all checkpoints, as indicated in Table
     <a class="ltx_ref" href="#S3.T5" title="Table 5 ‣ 3.2.1 Automatic Evaluation ‣ 3.2 Evaluation ‣ 3 Experiments ‣ ComposerX: Multi-Agent Symbolic Music Composition with LLMs">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     .
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Discussion
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    Overall, we observed that our GPT-powered multi-agent framework significantly enhances the quality of the music generated over solutions utilizing a singular GPT instance. Advantages of our system include:
   </p>
  </div>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    <span class="ltx_text ltx_font_bold" id="S4.p2.1.1">
     Controllability:
    </span>
    Our observations of the collaborative interactions among the agents, with particular emphasis on the contributions of the Group Leader, indicate the system’s competency in comprehending and executing a wide range of musical attributes as delineated by user inputs. Fundamental components such as tempo, key, time signature, chord progression, and instrumentation are adeptly translated into the corresponding ABC notations. This adept interpretation and realization of user directives significantly augment user controllability, enabling the generation of music that closely mirrors their specifications and artistic preferences.
   </p>
  </div>
  <div class="ltx_para" id="S4.p3">
   <p class="ltx_p" id="S4.p3.1">
    <span class="ltx_text ltx_font_bold" id="S4.p3.1.1">
     Training-free and data-free:
    </span>
    In contrast to conventional text-to-music generation models, which predominantly depend on large datasets for training, our system introduces significant benefits by obviating the need for such extensive data. This approach substantially mitigates the challenges associated with the compilation and refinement of large training datasets, including potential biases and the substantial resources often requisite for these processes. Moreover, this strategy enhances the system’s adaptability and accessibility, marking a shift towards more resource-efficient practices in music generation. As a result, this innovation plays a pivotal role in democratizing music generation, making it more attainable across a broader spectrum of users and applications.
   </p>
  </div>
  <div class="ltx_para" id="S4.p4">
   <p class="ltx_p" id="S4.p4.1">
    The system exhibits certain limitations, particularly when engaging with the nuanced aspects of musical composition that are often intrinsic to human-created music. These limitations delineate areas for potential enhancement and further research:
   </p>
  </div>
  <div class="ltx_para" id="S4.p5">
   <p class="ltx_p" id="S4.p5.1">
    <span class="ltx_text ltx_font_bold" id="S4.p5.1.1">
     Subtlety in Musical Expression:
    </span>
    The system is adept at interpreting basic musical elements but faces challenges in generating compositions with the nuanced subtlety characteristic of human composers. This includes aspects such as emotional depth, dynamic contrasts, and intricate phrasing, which are essential for conveying more profound musical narratives and experiences.
   </p>
  </div>
  <div class="ltx_para" id="S4.p6">
   <p class="ltx_p" id="S4.p6.1">
    <span class="ltx_text ltx_font_bold" id="S4.p6.1.1">
     Translation from Natural Language to Musical Notation:
    </span>
    Instructions and feedback given by the Group Leader and Review Agent aiming to facilitate nuanced musical elements are sometimes inadequately translated into ABC notations by the musician agents. This gap between conceptual understanding and practical embodiment in music notation underscores the system’s current limitations in realizing more sophisticated musical ideas.
   </p>
  </div>
  <div class="ltx_para" id="S4.p7">
   <p class="ltx_p" id="S4.p7.1">
    <span class="ltx_text ltx_font_bold" id="S4.p7.1.1">
     Instrumental Note Range Compliance:
    </span>
    The system occasionally generates notes beyond the conventional pitch ranges of certain instruments. For example, despite directives to adhere to instrument-specific pitch ranges, outputs have included notes exceeding the upper limit of a contrabass (C2 to F4), reflecting a discrepancy between the system’s outputs and practical musical performance constraints.
   </p>
  </div>
  <div class="ltx_para" id="S4.p8">
   <p class="ltx_p" id="S4.p8.1">
    <span class="ltx_text ltx_font_bold" id="S4.p8.1.1">
     Inter-Voice Alignment:
    </span>
    Our system faces challenges with aligning multiple musical voices accurately. This challenge primarily arises from the inherent limitations of text-based LLMs in generating polyphonic ABC notations. The linear nature of text-based input and output mechanisms does not naturally accommodate the complexity of polyphonic music, where multiple voices or instruments must be coordinated in time.
   </p>
  </div>
  <div class="ltx_para" id="S4.p9">
   <p class="ltx_p" id="S4.p9.1">
    <span class="ltx_text ltx_font_bold" id="S4.p9.1.1">
     Cadential Resolution:
    </span>
    Certain compositions generated by the system appear to lack a conclusive sense of resolution, resulting in pieces that may feel unfinished or conclude abruptly. This issue affects the listener’s sense of closure and satisfaction, detracting from the overall effectiveness of the musical experience. The challenge in achieving cadential resolution is partly due to the inherent difficulty for GPTs to grasp the concept of musical closure, which the perpetual aspect of its nature is hard for a language model to handle.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In conclusion, our investigation into ComposerX, a multi-agent polyphonic symbolic music composition system, reveals its efficacy in leveraging the inherent musical capabilities of LLMs to compose high-quality music. By introducing a collaborative agent-based approach, ComposerX not only transcends the capabilities of single-agent systems but also offers a cost-effective alternative to traditional music generation models that rely heavily on computational resources.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Contributions and Acknowledgments
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    <span class="ltx_text ltx_font_bold" id="S6.p1.1.1">
     Core
     <br class="ltx_break"/>
    </span>
    Qixin Deng,
    <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">
     qdeng4@u.rochester.edu
     <br class="ltx_break"/>
    </span>
    Qikai Yang,
    <span class="ltx_text ltx_font_italic" id="S6.p1.1.3">
     qikaiy2@illinois.edu
     <br class="ltx_break"/>
    </span>
    Ruibin Yuan,
    <span class="ltx_text ltx_font_italic" id="S6.p1.1.4">
     ryuanab@connect.ust.hk
     <br class="ltx_break"/>
    </span>
    Yipeng Huang,
    <span class="ltx_text ltx_font_italic" id="S6.p1.1.5">
     hyp744009246@163.com
     <br class="ltx_break"/>
    </span>
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S6.p2">
   <p class="ltx_p" id="S6.p2.1">
    <span class="ltx_text ltx_font_bold" id="S6.p2.1.1">
     Contributors
     <br class="ltx_break"/>
    </span>
    Yi Wang
    <br class="ltx_break"/>
    Xubo Liu
    <br class="ltx_break"/>
    Zeyue Tian
    <br class="ltx_break"/>
    Jiahao Pan
    <br class="ltx_break"/>
    Ge Zhang
    <br class="ltx_break"/>
    Hanfeng Lin
    <br class="ltx_break"/>
    Yizhi Li
    <br class="ltx_break"/>
    Yinghao Ma
    <br class="ltx_break"/>
    Jie Fu
    <br class="ltx_break"/>
    Chenghua Lin
    <br class="ltx_break"/>
    Emmanouil Benetos
    <br class="ltx_break"/>
    Wenwu Wang
    <br class="ltx_break"/>
    Gus Xia
    <br class="ltx_break"/>
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S6.p3">
   <p class="ltx_p" id="S6.p3.1">
    <span class="ltx_text ltx_font_bold" id="S6.p3.1.1">
     Correspondence
     <br class="ltx_break"/>
    </span>
    Wei Xue,
    <span class="ltx_text ltx_font_italic" id="S6.p3.1.2">
     weixue@ust.hk
     <br class="ltx_break"/>
    </span>
    Yike Guo,
    <span class="ltx_text ltx_font_italic" id="S6.p3.1.3">
     yikeguo@ust.hk
     <br class="ltx_break"/>
    </span>
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_tag_bibitem">
     [1]
    </span>
    <span class="ltx_bibblock">
     N. Masataka, “The origins of language and the evolution of music: A comparative perspective,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      Physics of Life Reviews
     </em>
     , vol. 6, no. 1, pp. 11–22, 2009.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_tag_bibitem">
     [2]
    </span>
    <span class="ltx_bibblock">
     ——, “Music, evolution and language,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      Developmental science
     </em>
     , vol. 10, no. 1, pp. 35–39, 2007.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_tag_bibitem">
     [3]
    </span>
    <span class="ltx_bibblock">
     M. C. Pino, M. Giancola, and S. D’Amico, “The association between music and language in children: A state-of-the-art review,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      Children
     </em>
     , vol. 10, no. 5, p. 801, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_tag_bibitem">
     [4]
    </span>
    <span class="ltx_bibblock">
     A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      Advances in neural information processing systems
     </em>
     , vol. 30, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_tag_bibitem">
     [5]
    </span>
    <span class="ltx_bibblock">
     C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, I. Simon, C. Hawthorne, A. M. Dai, M. D. Hoffman, M. Dinculescu, and D. Eck, “Music transformer,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      arXiv preprint arXiv:1809.04281
     </em>
     , 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_tag_bibitem">
     [6]
    </span>
    <span class="ltx_bibblock">
     C. Payne, “Musenet,” OpenAI Blog, Apr 2019. [Online]. Available:
     <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">
      https://openai.com/blog/musenet
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_tag_bibitem">
     [7]
    </span>
    <span class="ltx_bibblock">
     P. Lu, X. Xu, C. Kang, B. Yu, C. Xing, X. Tan, and J. Bian, “Musecoco: Generating symbolic music from text,” 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_tag_bibitem">
     [8]
    </span>
    <span class="ltx_bibblock">
     P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever, “Jukebox: A generative model for music,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      arXiv preprint arXiv:2005.00341
     </em>
     , 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_tag_bibitem">
     [9]
    </span>
    <span class="ltx_bibblock">
     A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      et al.
     </em>
     , “Musiclm: Generating music from text,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2">
      arXiv preprint arXiv:2301.11325
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_tag_bibitem">
     [10]
    </span>
    <span class="ltx_bibblock">
     J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Défossez, “Simple and controllable music generation,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      arXiv preprint arXiv:2306.05284
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_tag_bibitem">
     [11]
    </span>
    <span class="ltx_bibblock">
     E. H. Margulis and R. Simchy-Gross, “Repetition enhances the musicality of randomly generated tone sequences,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      Music Perception: An Interdisciplinary Journal
     </em>
     , vol. 33, no. 4, pp. 509–514, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_tag_bibitem">
     [12]
    </span>
    <span class="ltx_bibblock">
     S. Dai, H. Yu, and R. B. Dannenberg, “What is missing in deep music generation? a study of repetition and structure in popular music,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      arXiv preprint arXiv:2209.00182
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_tag_bibitem">
     [13]
    </span>
    <span class="ltx_bibblock">
     H. Jhamtani and T. Berg-Kirkpatrick, “Modeling self-repetition in music generation using generative adversarial networks,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      Machine Learning for Music Discovery Workshop, ICML
     </em>
     , 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_tag_bibitem">
     [14]
    </span>
    <span class="ltx_bibblock">
     X. Qu, Y. Bai, Y. Ma, Z. Zhou, K. M. Lo, J. Liu, R. Yuan, L. Min, X. Liu, T. Zhang
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      et al.
     </em>
     , “Mupt: A generative symbolic music pretrained transformer,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2">
      arXiv preprint arXiv:2404.06393
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_tag_bibitem">
     [15]
    </span>
    <span class="ltx_bibblock">
     X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen, “Mammoth: Building math generalist models through hybrid instruction tuning,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv preprint arXiv:2309.05653
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_tag_bibitem">
     [16]
    </span>
    <span class="ltx_bibblock">
     B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      et al.
     </em>
     , “Code llama: Open foundation models for code,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">
      arXiv preprint arXiv:2308.12950
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_tag_bibitem">
     [17]
    </span>
    <span class="ltx_bibblock">
     S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      et al.
     </em>
     , “Sparks of artificial general intelligence: Early experiments with gpt-4,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2">
      arXiv preprint arXiv:2303.12712
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_tag_bibitem">
     [18]
    </span>
    <span class="ltx_bibblock">
     L. Lin, G. Xia, Y. Zhang, and J. Jiang, “Arrange, inpaint, and refine: Steerable long-term music audio generation and editing via content-based controls,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      arXiv preprint arXiv:2402.09508
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_tag_bibitem">
     [19]
    </span>
    <span class="ltx_bibblock">
     L. Lin, G. Xia, J. Jiang, and Y. Zhang, “Content-based controls for music large language modeling,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      arXiv preprint arXiv:2310.17162
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_tag_bibitem">
     [20]
    </span>
    <span class="ltx_bibblock">
     ——, “Equipping musicgen with chord and rhythm controls,” in
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Ismir 2023 Hybrid Conference
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_tag_bibitem">
     [21]
    </span>
    <span class="ltx_bibblock">
     R. Yuan, H. Lin, Y. Wang, Z. Tian, S. Wu, T. Shen, G. Zhang, Y. Wu, C. Liu, Z. Zhou, Z. Ma, L. Xue, Z. Wang, Q. Liu, T. Zheng, Y. Li, Y. Ma, Y. Liang, X. Chi, R. Liu, Z. Wang, P. Li, J. Wu, C. Lin, Q. Liu, T. Jiang, W. Huang, W. Chen, E. Benetos, J. Fu, G. Xia, R. Dannenberg, W. Xue, S. Kang, and Y. Guo, “Chatmusician: Understanding and generating music intrinsically with llm,” 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_tag_bibitem">
     [22]
    </span>
    <span class="ltx_bibblock">
     S. Ding, Z. Liu, X. Dong, P. Zhang, R. Qian, C. He, D. Lin, and J. Wang, “Songcomposer: A large language model for lyric and melody composition in song generation,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      arXiv preprint arXiv:2402.17645
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_tag_bibitem">
     [23]
    </span>
    <span class="ltx_bibblock">
     X. Liang, J. Lin, and X. Du, “Bytecomposer: a human-like melody composition method based on language model agent,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2402.17785
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_tag_bibitem">
     [24]
    </span>
    <span class="ltx_bibblock">
     S. Wu and M. Sun, “Exploring the efficacy of pre-trained checkpoints in text-to-music generation task,” 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_tag_bibitem">
     [25]
    </span>
    <span class="ltx_bibblock">
     Y. Zhang, A. Maezawa, G. Xia, K. Yamamoto, and S. Dixon, “Loop copilot: Conducting ai ensembles for music generation and iterative editing,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      arXiv preprint arXiv:2310.12404
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_tag_bibitem">
     [26]
    </span>
    <span class="ltx_bibblock">
     D. Yu, K. Song, P. Lu, T. He, X. Tan, W. Ye, S. Zhang, and J. Bian, “Musicagent: An ai agent for music understanding and generation with large language models,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      arXiv preprint arXiv:2310.11954
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_tag_bibitem">
     [27]
    </span>
    <span class="ltx_bibblock">
     Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, “Self-instruct: Aligning language models with self-generated instructions,” 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_tag_bibitem">
     [28]
    </span>
    <span class="ltx_bibblock">
     J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,” 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_tag_bibitem">
     [29]
    </span>
    <span class="ltx_bibblock">
     Q. Wu, G. Bansal, J. Zhang, Y. Wu, B. Li, E. Zhu, L. Jiang, X. Zhang, S. Zhang, J. Liu, A. H. Awadallah, R. W. White, D. Burger, and C. Wang, “Autogen: Enabling next-gen llm applications via multi-agent conversation,” 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_tag_bibitem">
     [30]
    </span>
    <span class="ltx_bibblock">
     C. Donahue, A. Caillon, A. Roberts, E. Manilow, P. Esling, A. Agostinelli, M. Verzetti, I. Simon, O. Pietquin, N. Zeghidour
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      et al.
     </em>
     , “Singsong: Generating musical accompaniments from singing,”
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.2.2">
      arXiv preprint arXiv:2301.12662
     </em>
     , 2023.
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
</article>
