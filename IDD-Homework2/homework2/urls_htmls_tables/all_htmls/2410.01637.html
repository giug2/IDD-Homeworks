<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>On The Adaptation of Unlimiformer for Decoder-Only Transformers</title>
<!--Generated on Wed Oct  2 14:58:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.01637v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S1" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S1.SS0.SSS0.Px1" title="In 1. Introduction ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title">Contributions:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S2" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S3" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S3.SS1" title="In 3. Methodology ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Cross-Attention</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S3.SS2" title="In 3. Methodology ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>kNN Indices</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S3.SS3" title="In 3. Methodology ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Index Staleness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S3.SS4" title="In 3. Methodology ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Chunks Encoding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.SS1" title="In 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets &amp; Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.SS2" title="In 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.SS3" title="In 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Evaluation Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.SS4" title="In 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Prompt Structure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.SS5" title="In 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S5" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S5.SS1" title="In 5. Results ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Summarization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S5.SS2" title="In 5. Results ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Free-Form Q&amp;A</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S6" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Ablations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S6.SS1" title="In 6. Ablations ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Index Staleness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S6.SS2" title="In 6. Ablations ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Contextual Information</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S7" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S8" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Limitations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S8.SS0.SSS0.Px1" title="In 8. Limitations ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title">Query Bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S8.SS0.SSS0.Px2" title="In 8. Limitations ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title">Latency</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S9" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Ethical Considerations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S10" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Bibliographical References</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#A1" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Implementation Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#A2" title="In On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Hyperparameters</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#A2.SS0.SSS0.Px1" title="In Appendix B Hyperparameters ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title">Cross-Attention Layers (<math alttext="\mathcal{L}" class="ltx_Math" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation-xml encoding="MathML-Content"><ci>ℒ</ci></annotation-xml><annotation encoding="application/x-tex">\mathcal{L}</annotation><annotation encoding="application/x-llamapun">caligraphic_L</annotation></semantics></math>)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#A2.SS0.SSS0.Px2" title="In Appendix B Hyperparameters ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_title">Retention Sizes (<math alttext="\alpha_{q},\beta_{q},\beta_{kv},\alpha_{kv}" class="ltx_Math" display="inline"><semantics><mrow><msub><mi>α</mi><mi>q</mi></msub><mo>,</mo><msub><mi>β</mi><mi>q</mi></msub><mo>,</mo><msub><mi>β</mi><mrow><mi>k</mi><mo>⁢</mo><mi>v</mi></mrow></msub><mo>,</mo><msub><mi>α</mi><mrow><mi>k</mi><mo>⁢</mo><mi>v</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content"><list><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝛼</ci><ci>𝑞</ci></apply><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝛽</ci><ci>𝑞</ci></apply><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝛽</ci><apply><times></times><ci>𝑘</ci><ci>𝑣</ci></apply></apply><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝛼</ci><apply><times></times><ci>𝑘</ci><ci>𝑣</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex">\alpha_{q},\beta_{q},\beta_{kv},\alpha_{kv}</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_β start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_β start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT , italic_α start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT</annotation></semantics></math>)</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">On The Adaptation of Unlimiformer for Decoder-Only Transformers</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">One of the prominent issues stifling the current generation of large language models is their limited context length.
Recent proprietary models such as GPT-4 and Claude 2 have introduced longer context lengths, 8k/32k and 100k, respectively; however, despite the efforts in the community, most common models, such as LLama-2, have a context length of 4k or less.
Unlimiformer <cite class="ltx_cite ltx_citemacro_cite">Bertsch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib3" title="">2023</a>)</cite> is a recently popular vector-retrieval augmentation method that offloads cross-attention computations to a kNN index.
However, its main limitation is incompatibility with decoder-only transformers out of the box.
In this work, we explore practical considerations of adapting Unlimiformer to decoder-only transformers and introduce a series of modifications to overcome this limitation.
Moreover, we expand the original experimental setup on summarization to include a new task (i.e., free-form Q&amp;A) and an instruction-tuned model (i.e., a custom 6.7B GPT model).
Our results showcase the effectiveness of these modifications on summarization, performing on par with a model with 2x the context length.
Moreover, we discuss limitations and future directions for free-form Q&amp;A and instruction-tuned models.

<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="id12.id1.1">Keywords: </span>Large Language Models, Decoder-Only Transformers, Retrieval-Augmented Attention</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\newcites</span>
<p class="ltx_p" id="p1.2">languageresource 














</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1"></span></p>
</div>
<div class="ltx_logical-block" id="id11">
<div class="ltx_para" id="id11.p1">
<p class="ltx_p ltx_align_center" id="id11.p1.1"><span class="ltx_text ltx_font_bold" id="id11.p1.1.1" style="font-size:144%;">On The Adaptation of Unlimiformer for Decoder-Only Transformers</span></p>
<br class="ltx_break ltx_centering"/>
<table class="ltx_tabular ltx_centering ltx_align_top" id="id10.10">
<tr class="ltx_tr" id="id5.5.5">
<td class="ltx_td ltx_align_center" id="id5.5.5.5"><span class="ltx_text ltx_font_bold" id="id5.5.5.5.5" style="font-size:120%;">Kian Ahrabian<sup class="ltx_sup" id="id5.5.5.5.5.2">1</sup><sup class="ltx_sup" id="id5.5.5.5.5.3">∗</sup><span class="ltx_note ltx_role_thanks" id="id3.3.3.3.3.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span><sup class="ltx_sup" id="id3.3.3.3.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id3.3.3.3.3.1.1.1">∗</span></sup> Work done during an internship at Microsoft.</span></span></span>, Alon Benhaim<sup class="ltx_sup" id="id5.5.5.5.5.4">2</sup>, Barun Patra<sup class="ltx_sup" id="id5.5.5.5.5.5">2</sup></span></td>
</tr>
<tr class="ltx_tr" id="id8.8.8">
<td class="ltx_td ltx_align_center" id="id8.8.8.3"><span class="ltx_text ltx_font_bold" id="id8.8.8.3.3" style="font-size:120%;">Jay Pujara<sup class="ltx_sup" id="id8.8.8.3.3.1">1</sup>, Saksham Singhal<sup class="ltx_sup" id="id8.8.8.3.3.2">2</sup>, Xia Song<sup class="ltx_sup" id="id8.8.8.3.3.3">2</sup></span></td>
</tr>
<tr class="ltx_tr" id="id9.9.9">
<td class="ltx_td ltx_align_center" id="id9.9.9.1">
<sup class="ltx_sup" id="id9.9.9.1.1"><span class="ltx_text ltx_font_italic" id="id9.9.9.1.1.1">1</span></sup> Information Sciences Institute, University of Southern California</td>
</tr>
<tr class="ltx_tr" id="id10.10.10">
<td class="ltx_td ltx_align_center" id="id10.10.10.1">
<sup class="ltx_sup" id="id10.10.10.1.1"><span class="ltx_text ltx_font_italic" id="id10.10.10.1.1.1">2</span></sup> Microsoft</td>
</tr>
<tr class="ltx_tr" id="id10.10.11">
<td class="ltx_td ltx_align_center" id="id10.10.11.1">
<span class="ltx_text ltx_font_typewriter" id="id10.10.11.1.1">ahrabian@usc.edu</span>, <span class="ltx_text ltx_font_typewriter" id="id10.10.11.1.2">{alonbenhaim,barun.patra}@microsoft.com</span>
</td>
</tr>
<tr class="ltx_tr" id="id10.10.12">
<td class="ltx_td ltx_align_center" id="id10.10.12.1">
<span class="ltx_text ltx_font_typewriter" id="id10.10.12.1.1">jpujara@isi.edu</span>, <span class="ltx_text ltx_font_typewriter" id="id10.10.12.1.2">{saksham.singhal,xiaso}@microsoft.com</span>
</td>
</tr>
</table>
<p class="ltx_p ltx_align_center" id="id11.p1.2"><span class="ltx_text ltx_font_italic" id="id11.p1.2.1">Abstract content</span></p>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, large language models (LLMs) have become critical to many language-based technologies, such as conversational and search systems.
LLMs have shown state-of-the-art performance on sequence-to-sequence downstream tasks such as summarization and question-answering.
However, the performance of these models is bounded by the information that can fit in their context (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.F3" title="Figure 3 ‣ 4.1. Datasets &amp; Tasks ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">3</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.SS3" title="4.3. Evaluation Setup ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">4.3</span></a>).
Despite the efforts in the community <cite class="ltx_cite ltx_citemacro_cite">Choromanski et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib7" title="">2021</a>); Beltagy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib2" title="">2020</a>); Ivgi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib15" title="">2023</a>)</cite>, most of the common open-source models, e.g., MPT <cite class="ltx_cite ltx_citemacro_cite">Team (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib31" title="">2023</a>)</cite>, Falcon <cite class="ltx_cite ltx_citemacro_cite">Penedo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib22" title="">2023</a>)</cite>, and LLama-2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib32" title="">2023</a>)</cite>, have a context length of 4096 or less.
As such, efficiently overcoming this limitation would allow a broader and fairer adaptation of LLMs while increasing their performance across benchmarks.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="502" id="S1.F1.g1" src="extracted/5896239/assets/overview.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Overview of an example of the adapted decoder-only model where only a single layer (e.g., 16th) uses cross-attention.
Here, <math alttext="T_{i}" class="ltx_Math" display="inline" id="S1.F1.3.m1.1"><semantics id="S1.F1.3.m1.1b"><msub id="S1.F1.3.m1.1.1" xref="S1.F1.3.m1.1.1.cmml"><mi id="S1.F1.3.m1.1.1.2" xref="S1.F1.3.m1.1.1.2.cmml">T</mi><mi id="S1.F1.3.m1.1.1.3" xref="S1.F1.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><apply id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1"><csymbol cd="ambiguous" id="S1.F1.3.m1.1.1.1.cmml" xref="S1.F1.3.m1.1.1">subscript</csymbol><ci id="S1.F1.3.m1.1.1.2.cmml" xref="S1.F1.3.m1.1.1.2">𝑇</ci><ci id="S1.F1.3.m1.1.1.3.cmml" xref="S1.F1.3.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">T_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.3.m1.1e">italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="G_{i}" class="ltx_Math" display="inline" id="S1.F1.4.m2.1"><semantics id="S1.F1.4.m2.1b"><msub id="S1.F1.4.m2.1.1" xref="S1.F1.4.m2.1.1.cmml"><mi id="S1.F1.4.m2.1.1.2" xref="S1.F1.4.m2.1.1.2.cmml">G</mi><mi id="S1.F1.4.m2.1.1.3" xref="S1.F1.4.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><apply id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.4.m2.1.1.1.cmml" xref="S1.F1.4.m2.1.1">subscript</csymbol><ci id="S1.F1.4.m2.1.1.2.cmml" xref="S1.F1.4.m2.1.1.2">𝐺</ci><ci id="S1.F1.4.m2.1.1.3.cmml" xref="S1.F1.4.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">G_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.4.m2.1e">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represent the original context and generated tokens.
The cyan section encapsulates the first pass to create the indices, while the pink section illustrates the second pass to generate sequences.
Note that the first chunk appears in both the input and the index by design, keeping the input the same in all variations of our experiments.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In general, most of the existing methods for extending the contextual information in LLMs focus on one of the following approaches:
1) extending positional embeddings through extrapolation or interpolation <cite class="ltx_cite ltx_citemacro_cite">Press et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib25" title="">2022</a>); Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib29" title="">2023</a>)</cite>, 2) introducing recurrence in the transformer <cite class="ltx_cite ltx_citemacro_cite">Hutchins et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib14" title="">2022</a>); Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib34" title="">2019</a>)</cite>, 3) introducing sparsity in the attention mechanism <cite class="ltx_cite ltx_citemacro_cite">Beltagy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib2" title="">2020</a>); Zaheer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib35" title="">2020</a>)</cite>, and 4) augmenting the transformer with a vector-retrieval module <cite class="ltx_cite ltx_citemacro_cite">Rubin and Berant (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib26" title="">2023</a>)</cite>.
One popular approach that has gained much traction recently for extending contextual information to unlimited inputs (theoretically) is Unlimiformer <cite class="ltx_cite ltx_citemacro_cite">Bertsch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib3" title="">2023</a>)</cite>.
Unlimiformer is a vector-retrieval augmentation method that offloads the cross-attention computations to a kNN index and can wrap <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">any existing encoder-decoder model</span>.
However, its incompatibility with decoder-only models is a significant shortcoming.</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Contributions:</h4>
<div class="ltx_para ltx_noindent" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">In this work, we present a set of modifications to overcome this limitation of the Unlimiformer architecture, adapting it to the decoder-only models (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">1</span></a>).
These modifications consist of 1) modifying the cross-attention formulation to include information fusion, 2) updating the index creation procedure, 3) addressing the index staleness problem, and 4) adapting the chunk encoding procedure to causal attention (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S3" title="3. Methodology ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">3</span></a>).
Moreover, we introduce a new evaluation setting and present our experiments on four long-document datasets across two tasks: summarization and free-form Q&amp;A.
Our experiments show that our modifications improve summarization datasets, performing on par with a model with 2x context length.
We also discuss the limitations and future directions for free-form Q&amp;A and instruction-tuned models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Works</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Many prior works, such as Linformer <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib33" title="">2020</a>)</cite> and Reformer <cite class="ltx_cite ltx_citemacro_cite">Kitaev et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib16" title="">2020</a>)</cite>, have been focused on creating more efficient transformers.
<cite class="ltx_cite ltx_citemacro_citet">Tay et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib30" title="">2022</a>)</cite> present a comprehensive study on these models.
Moreover, there have been efforts to accelerate dense attention calculations by crafting IO-aware CUDA kernels <cite class="ltx_cite ltx_citemacro_cite">Dao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib9" title="">2022</a>); Dao (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib8" title="">2023</a>)</cite>.
Recently, <cite class="ltx_cite ltx_citemacro_citet">Rubin and Berant (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib26" title="">2023</a>)</cite> have introduced another retrieval-augmented attention model for decoder-only transformers; however, unlike Unlimiformer, this model does not work in zero-shot settings.
Finally, there have been attempts to break away from attention-based models entirely through linear RNNs <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib23" title="">2023</a>)</cite> or convolutions <cite class="ltx_cite ltx_citemacro_cite">Poli et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib24" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Methodology</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In summary, Unlimiformer consists of three main steps:
1) split the input into overlapping digestible chunks,
2) encode each chunk and store the hidden states of the middle-half tokens in a kNN index,
and 3) approximate the dense attention in the decoder using a subset of hidden states retrieved from the index.
In this section, we discuss our changes to make Unlimiformer compatible with decoder-only models<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Concurrently, <cite class="ltx_cite ltx_citemacro_citet">Bertsch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib3" title="">2023</a>)</cite> have released an implementation for decoder-only models. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#A1" title="Appendix A Implementation Comparison ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">A</span></a> details the differences in our adaptation.</span></span></span>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.1.   Cross-Attention</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.4">Formally, in Unlimiformer, the dot-product part of the cross-attention mechanism in decoder layers is approximated as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="QK^{T}\approx(h_{d}W_{q}W_{k}^{T})h_{e}^{T}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">Q</mi><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">⁢</mo><msup id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">K</mi><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">T</mi></msup></mrow><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">≈</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml">h</mi><mi id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">d</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">W</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml">q</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.4.2.2" xref="S3.E1.m1.1.1.1.1.1.1.4.2.2.cmml">W</mi><mi id="S3.E1.m1.1.1.1.1.1.1.4.2.3" xref="S3.E1.m1.1.1.1.1.1.1.4.2.3.cmml">k</mi><mi id="S3.E1.m1.1.1.1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.1.1.1.4.3.cmml">T</mi></msubsup></mrow><mo id="S3.E1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">⁢</mo><msubsup id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.3.2.2.cmml">h</mi><mi id="S3.E1.m1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.3.2.3.cmml">e</mi><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">T</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><approx id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></approx><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><times id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝑄</ci><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3">superscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">𝐾</ci><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">𝑇</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"></times><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">ℎ</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3">𝑑</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">𝑊</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3">𝑞</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.4.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.4.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4.2.2">𝑊</ci><ci id="S3.E1.m1.1.1.1.1.1.1.4.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4.2.3">𝑘</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4.3">𝑇</ci></apply></apply><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.3.2.2">ℎ</ci><ci id="S3.E1.m1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.3.2.3">𝑒</ci></apply><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">QK^{T}\approx(h_{d}W_{q}W_{k}^{T})h_{e}^{T}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ≈ ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) italic_h start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.3">where <math alttext="h_{d}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ℎ</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">h_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> is the decoder’s hidden states, and <math alttext="h_{e}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ℎ</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">h_{e}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_h start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math> is the retrieved hidden states that maximize <math alttext="(h_{d}^{(-1)}W_{q}W_{k}^{T})h_{e}^{T}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.2"><semantics id="S3.SS1.p1.3.m3.2a"><mrow id="S3.SS1.p1.3.m3.2.2" xref="S3.SS1.p1.3.m3.2.2.cmml"><mrow id="S3.SS1.p1.3.m3.2.2.1.1" xref="S3.SS1.p1.3.m3.2.2.1.1.1.cmml"><mo id="S3.SS1.p1.3.m3.2.2.1.1.2" stretchy="false" xref="S3.SS1.p1.3.m3.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.3.m3.2.2.1.1.1" xref="S3.SS1.p1.3.m3.2.2.1.1.1.cmml"><msubsup id="S3.SS1.p1.3.m3.2.2.1.1.1.2" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.2.2.1.1.1.2.2.2" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2.2.2.cmml">h</mi><mi id="S3.SS1.p1.3.m3.2.2.1.1.1.2.2.3" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2.2.3.cmml">d</mi><mrow id="S3.SS1.p1.3.m3.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.3.m3.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.3.m3.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.3.m3.1.1.1.1.1a" xref="S3.SS1.p1.3.m3.1.1.1.1.1.cmml">−</mo><mn id="S3.SS1.p1.3.m3.1.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S3.SS1.p1.3.m3.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.SS1.p1.3.m3.2.2.1.1.1.1" xref="S3.SS1.p1.3.m3.2.2.1.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p1.3.m3.2.2.1.1.1.3" xref="S3.SS1.p1.3.m3.2.2.1.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.2.2.1.1.1.3.2" xref="S3.SS1.p1.3.m3.2.2.1.1.1.3.2.cmml">W</mi><mi id="S3.SS1.p1.3.m3.2.2.1.1.1.3.3" xref="S3.SS1.p1.3.m3.2.2.1.1.1.3.3.cmml">q</mi></msub><mo id="S3.SS1.p1.3.m3.2.2.1.1.1.1a" xref="S3.SS1.p1.3.m3.2.2.1.1.1.1.cmml">⁢</mo><msubsup id="S3.SS1.p1.3.m3.2.2.1.1.1.4" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4.cmml"><mi id="S3.SS1.p1.3.m3.2.2.1.1.1.4.2.2" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4.2.2.cmml">W</mi><mi id="S3.SS1.p1.3.m3.2.2.1.1.1.4.2.3" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4.2.3.cmml">k</mi><mi id="S3.SS1.p1.3.m3.2.2.1.1.1.4.3" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4.3.cmml">T</mi></msubsup></mrow><mo id="S3.SS1.p1.3.m3.2.2.1.1.3" stretchy="false" xref="S3.SS1.p1.3.m3.2.2.1.1.1.cmml">)</mo></mrow><mo id="S3.SS1.p1.3.m3.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.cmml">⁢</mo><msubsup id="S3.SS1.p1.3.m3.2.2.3" xref="S3.SS1.p1.3.m3.2.2.3.cmml"><mi id="S3.SS1.p1.3.m3.2.2.3.2.2" xref="S3.SS1.p1.3.m3.2.2.3.2.2.cmml">h</mi><mi id="S3.SS1.p1.3.m3.2.2.3.2.3" xref="S3.SS1.p1.3.m3.2.2.3.2.3.cmml">e</mi><mi id="S3.SS1.p1.3.m3.2.2.3.3" xref="S3.SS1.p1.3.m3.2.2.3.3.cmml">T</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.2b"><apply id="S3.SS1.p1.3.m3.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2"><times id="S3.SS1.p1.3.m3.2.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.2"></times><apply id="S3.SS1.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1"><times id="S3.SS1.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.1"></times><apply id="S3.SS1.p1.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.1.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2">superscript</csymbol><apply id="S3.SS1.p1.3.m3.2.2.1.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.1.1.1.2.2.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.1.1.1.2.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2.2.2">ℎ</ci><ci id="S3.SS1.p1.3.m3.2.2.1.1.1.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2.2.3">𝑑</ci></apply><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1"><minus id="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1"></minus><cn id="S3.SS1.p1.3.m3.1.1.1.1.1.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2">1</cn></apply></apply><apply id="S3.SS1.p1.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.1.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.1.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.3.2">𝑊</ci><ci id="S3.SS1.p1.3.m3.2.2.1.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.3.3">𝑞</ci></apply><apply id="S3.SS1.p1.3.m3.2.2.1.1.1.4.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.1.1.1.4.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4">superscript</csymbol><apply id="S3.SS1.p1.3.m3.2.2.1.1.1.4.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.1.1.1.4.2.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.1.1.1.4.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4.2.2">𝑊</ci><ci id="S3.SS1.p1.3.m3.2.2.1.1.1.4.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4.2.3">𝑘</ci></apply><ci id="S3.SS1.p1.3.m3.2.2.1.1.1.4.3.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.4.3">𝑇</ci></apply></apply><apply id="S3.SS1.p1.3.m3.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.3.1.cmml" xref="S3.SS1.p1.3.m3.2.2.3">superscript</csymbol><apply id="S3.SS1.p1.3.m3.2.2.3.2.cmml" xref="S3.SS1.p1.3.m3.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.3.2.1.cmml" xref="S3.SS1.p1.3.m3.2.2.3">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.3.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.3.2.2">ℎ</ci><ci id="S3.SS1.p1.3.m3.2.2.3.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.3.2.3">𝑒</ci></apply><ci id="S3.SS1.p1.3.m3.2.2.3.3.cmml" xref="S3.SS1.p1.3.m3.2.2.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.2c">(h_{d}^{(-1)}W_{q}W_{k}^{T})h_{e}^{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.2d">( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) italic_h start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.2">In decoder-only transformers, due to the absence of a natural encoding/decoder splitting layer, we can arbitrarily choose any layer to use cross-attention instead of self-attention.
This allows us to use various simple or complex patterns for the set of cross-attention layers, <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">caligraphic_L</annotation></semantics></math> (e.g., <math alttext="\mathcal{L}=\{16\}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.2" xref="S3.SS1.p2.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.2.m2.1.2.2" xref="S3.SS1.p2.2.m2.1.2.2.cmml">ℒ</mi><mo id="S3.SS1.p2.2.m2.1.2.1" xref="S3.SS1.p2.2.m2.1.2.1.cmml">=</mo><mrow id="S3.SS1.p2.2.m2.1.2.3.2" xref="S3.SS1.p2.2.m2.1.2.3.1.cmml"><mo id="S3.SS1.p2.2.m2.1.2.3.2.1" stretchy="false" xref="S3.SS1.p2.2.m2.1.2.3.1.cmml">{</mo><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">16</mn><mo id="S3.SS1.p2.2.m2.1.2.3.2.2" stretchy="false" xref="S3.SS1.p2.2.m2.1.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.2.cmml" xref="S3.SS1.p2.2.m2.1.2"><eq id="S3.SS1.p2.2.m2.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.2.1"></eq><ci id="S3.SS1.p2.2.m2.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2">ℒ</ci><set id="S3.SS1.p2.2.m2.1.2.3.1.cmml" xref="S3.SS1.p2.2.m2.1.2.3.2"><cn id="S3.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1">16</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathcal{L}=\{16\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">caligraphic_L = { 16 }</annotation></semantics></math> in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">1</span></a>).
Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#A2" title="Appendix B Hyperparameters ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">B</span></a> details how these patterns are tuned as hyperparameters of the model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.7">Let <math alttext="h_{\text{CA}}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">h</mi><mtext id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3a.cmml">CA</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">ℎ</ci><ci id="S3.SS1.p3.1.m1.1.1.3a.cmml" xref="S3.SS1.p3.1.m1.1.1.3"><mtext id="S3.SS1.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p3.1.m1.1.1.3">CA</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">h_{\text{CA}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_h start_POSTSUBSCRIPT CA end_POSTSUBSCRIPT</annotation></semantics></math> be the input to a cross-attention layer.
Based on <math alttext="h_{\text{CA}}^{(-1)}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><msubsup id="S3.SS1.p3.2.m2.1.2" xref="S3.SS1.p3.2.m2.1.2.cmml"><mi id="S3.SS1.p3.2.m2.1.2.2.2" xref="S3.SS1.p3.2.m2.1.2.2.2.cmml">h</mi><mtext id="S3.SS1.p3.2.m2.1.2.2.3" xref="S3.SS1.p3.2.m2.1.2.2.3a.cmml">CA</mtext><mrow id="S3.SS1.p3.2.m2.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.2.m2.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.2.m2.1.1.1.1.1a" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">−</mo><mn id="S3.SS1.p3.2.m2.1.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S3.SS1.p3.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.2.cmml" xref="S3.SS1.p3.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.2.1.cmml" xref="S3.SS1.p3.2.m2.1.2">superscript</csymbol><apply id="S3.SS1.p3.2.m2.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.2.2.1.cmml" xref="S3.SS1.p3.2.m2.1.2">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.2.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2.2">ℎ</ci><ci id="S3.SS1.p3.2.m2.1.2.2.3a.cmml" xref="S3.SS1.p3.2.m2.1.2.2.3"><mtext id="S3.SS1.p3.2.m2.1.2.2.3.cmml" mathsize="70%" xref="S3.SS1.p3.2.m2.1.2.2.3">CA</mtext></ci></apply><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1"><minus id="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1"></minus><cn id="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml" type="integer" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">h_{\text{CA}}^{(-1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_h start_POSTSUBSCRIPT CA end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>, we can retrieve the relevant vectors, <math alttext="h_{\text{kNN}}" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">h</mi><mtext id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3a.cmml">kNN</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">ℎ</ci><ci id="S3.SS1.p3.3.m3.1.1.3a.cmml" xref="S3.SS1.p3.3.m3.1.1.3"><mtext id="S3.SS1.p3.3.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p3.3.m3.1.1.3">kNN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">h_{\text{kNN}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">italic_h start_POSTSUBSCRIPT kNN end_POSTSUBSCRIPT</annotation></semantics></math>, from the index.
Similar to the memory transformer <cite class="ltx_cite ltx_citemacro_cite">Burtsev et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib6" title="">2020</a>)</cite>, by fusing <math alttext="h_{\text{kNN}}" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><msub id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">h</mi><mtext id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3a.cmml">kNN</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">ℎ</ci><ci id="S3.SS1.p3.4.m4.1.1.3a.cmml" xref="S3.SS1.p3.4.m4.1.1.3"><mtext id="S3.SS1.p3.4.m4.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p3.4.m4.1.1.3">kNN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">h_{\text{kNN}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">italic_h start_POSTSUBSCRIPT kNN end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="h_{\text{CA}}" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m5.1"><semantics id="S3.SS1.p3.5.m5.1a"><msub id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml">h</mi><mtext id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3a.cmml">CA</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2">ℎ</ci><ci id="S3.SS1.p3.5.m5.1.1.3a.cmml" xref="S3.SS1.p3.5.m5.1.1.3"><mtext id="S3.SS1.p3.5.m5.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p3.5.m5.1.1.3">CA</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">h_{\text{CA}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.m5.1d">italic_h start_POSTSUBSCRIPT CA end_POSTSUBSCRIPT</annotation></semantics></math> we form the new query matrix <math alttext="h_{q}" class="ltx_Math" display="inline" id="S3.SS1.p3.6.m6.1"><semantics id="S3.SS1.p3.6.m6.1a"><msub id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><mi id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml">h</mi><mi id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2">ℎ</ci><ci id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">h_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.6.m6.1d">italic_h start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math>, and the new key-value matrix, <math alttext="h_{kv}" class="ltx_Math" display="inline" id="S3.SS1.p3.7.m7.1"><semantics id="S3.SS1.p3.7.m7.1a"><msub id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml"><mi id="S3.SS1.p3.7.m7.1.1.2" xref="S3.SS1.p3.7.m7.1.1.2.cmml">h</mi><mrow id="S3.SS1.p3.7.m7.1.1.3" xref="S3.SS1.p3.7.m7.1.1.3.cmml"><mi id="S3.SS1.p3.7.m7.1.1.3.2" xref="S3.SS1.p3.7.m7.1.1.3.2.cmml">k</mi><mo id="S3.SS1.p3.7.m7.1.1.3.1" xref="S3.SS1.p3.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.7.m7.1.1.3.3" xref="S3.SS1.p3.7.m7.1.1.3.3.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.1b"><apply id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.7.m7.1.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p3.7.m7.1.1.2.cmml" xref="S3.SS1.p3.7.m7.1.1.2">ℎ</ci><apply id="S3.SS1.p3.7.m7.1.1.3.cmml" xref="S3.SS1.p3.7.m7.1.1.3"><times id="S3.SS1.p3.7.m7.1.1.3.1.cmml" xref="S3.SS1.p3.7.m7.1.1.3.1"></times><ci id="S3.SS1.p3.7.m7.1.1.3.2.cmml" xref="S3.SS1.p3.7.m7.1.1.3.2">𝑘</ci><ci id="S3.SS1.p3.7.m7.1.1.3.3.cmml" xref="S3.SS1.p3.7.m7.1.1.3.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.1c">h_{kv}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.7.m7.1d">italic_h start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT</annotation></semantics></math> as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.EGx1">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle h_{q}" class="ltx_Math" display="inline" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><msub id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">h</mi><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">ℎ</ci><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle h_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_h start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=[h_{\text{kNN}}[\alpha_{q}:];h_{\text{\text{CA}}}[\beta_{q}:]]" class="ltx_math_unparsed" display="inline" id="S3.E2.m2.1"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1b"><mo id="S3.E2.m2.1.1">=</mo><mrow id="S3.E2.m2.1.2"><mo id="S3.E2.m2.1.2.1" stretchy="false">[</mo><msub id="S3.E2.m2.1.2.2"><mi id="S3.E2.m2.1.2.2.2">h</mi><mtext id="S3.E2.m2.1.2.2.3">kNN</mtext></msub><mrow id="S3.E2.m2.1.2.3"><mo id="S3.E2.m2.1.2.3.1" stretchy="false">[</mo><msub id="S3.E2.m2.1.2.3.2"><mi id="S3.E2.m2.1.2.3.2.2">α</mi><mi id="S3.E2.m2.1.2.3.2.3">q</mi></msub><mo id="S3.E2.m2.1.2.3.3" lspace="0.278em" rspace="0em">:</mo><mo id="S3.E2.m2.1.2.3.4" stretchy="false">]</mo></mrow><mo id="S3.E2.m2.1.2.4">;</mo><msub id="S3.E2.m2.1.2.5"><mi id="S3.E2.m2.1.2.5.2">h</mi><mtext id="S3.E2.m2.1.2.5.3">CA</mtext></msub><mrow id="S3.E2.m2.1.2.6"><mo id="S3.E2.m2.1.2.6.1" stretchy="false">[</mo><msub id="S3.E2.m2.1.2.6.2"><mi id="S3.E2.m2.1.2.6.2.2">β</mi><mi id="S3.E2.m2.1.2.6.2.3">q</mi></msub><mo id="S3.E2.m2.1.2.6.3" lspace="0.278em" rspace="0em">:</mo><mo id="S3.E2.m2.1.2.6.4" stretchy="false">]</mo></mrow><mo id="S3.E2.m2.1.2.7" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=[h_{\text{kNN}}[\alpha_{q}:];h_{\text{\text{CA}}}[\beta_{q}:]]</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m2.1d">= [ italic_h start_POSTSUBSCRIPT kNN end_POSTSUBSCRIPT [ italic_α start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT : ] ; italic_h start_POSTSUBSCRIPT CA end_POSTSUBSCRIPT [ italic_β start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT : ] ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle h_{kv}" class="ltx_Math" display="inline" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><msub id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">h</mi><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">k</mi><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">ℎ</ci><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><times id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></times><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝑘</ci><ci id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle h_{kv}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_h start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=[h_{\text{kNN}}[\alpha_{kv}:];h_{\text{\text{CA}}}[\beta_{kv}:]]" class="ltx_math_unparsed" display="inline" id="S3.E3.m2.1"><semantics id="S3.E3.m2.1a"><mrow id="S3.E3.m2.1b"><mo id="S3.E3.m2.1.1">=</mo><mrow id="S3.E3.m2.1.2"><mo id="S3.E3.m2.1.2.1" stretchy="false">[</mo><msub id="S3.E3.m2.1.2.2"><mi id="S3.E3.m2.1.2.2.2">h</mi><mtext id="S3.E3.m2.1.2.2.3">kNN</mtext></msub><mrow id="S3.E3.m2.1.2.3"><mo id="S3.E3.m2.1.2.3.1" stretchy="false">[</mo><msub id="S3.E3.m2.1.2.3.2"><mi id="S3.E3.m2.1.2.3.2.2">α</mi><mrow id="S3.E3.m2.1.2.3.2.3"><mi id="S3.E3.m2.1.2.3.2.3.2">k</mi><mo id="S3.E3.m2.1.2.3.2.3.1">⁢</mo><mi id="S3.E3.m2.1.2.3.2.3.3">v</mi></mrow></msub><mo id="S3.E3.m2.1.2.3.3" lspace="0.278em" rspace="0em">:</mo><mo id="S3.E3.m2.1.2.3.4" stretchy="false">]</mo></mrow><mo id="S3.E3.m2.1.2.4">;</mo><msub id="S3.E3.m2.1.2.5"><mi id="S3.E3.m2.1.2.5.2">h</mi><mtext id="S3.E3.m2.1.2.5.3">CA</mtext></msub><mrow id="S3.E3.m2.1.2.6"><mo id="S3.E3.m2.1.2.6.1" stretchy="false">[</mo><msub id="S3.E3.m2.1.2.6.2"><mi id="S3.E3.m2.1.2.6.2.2">β</mi><mrow id="S3.E3.m2.1.2.6.2.3"><mi id="S3.E3.m2.1.2.6.2.3.2">k</mi><mo id="S3.E3.m2.1.2.6.2.3.1">⁢</mo><mi id="S3.E3.m2.1.2.6.2.3.3">v</mi></mrow></msub><mo id="S3.E3.m2.1.2.6.3" lspace="0.278em" rspace="0em">:</mo><mo id="S3.E3.m2.1.2.6.4" stretchy="false">]</mo></mrow><mo id="S3.E3.m2.1.2.7" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E3.m2.1c">\displaystyle=[h_{\text{kNN}}[\alpha_{kv}:];h_{\text{\text{CA}}}[\beta_{kv}:]]</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m2.1d">= [ italic_h start_POSTSUBSCRIPT kNN end_POSTSUBSCRIPT [ italic_α start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT : ] ; italic_h start_POSTSUBSCRIPT CA end_POSTSUBSCRIPT [ italic_β start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT : ] ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p3.13">where <math alttext="\alpha_{q}" class="ltx_Math" display="inline" id="S3.SS1.p3.8.m1.1"><semantics id="S3.SS1.p3.8.m1.1a"><msub id="S3.SS1.p3.8.m1.1.1" xref="S3.SS1.p3.8.m1.1.1.cmml"><mi id="S3.SS1.p3.8.m1.1.1.2" xref="S3.SS1.p3.8.m1.1.1.2.cmml">α</mi><mi id="S3.SS1.p3.8.m1.1.1.3" xref="S3.SS1.p3.8.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.m1.1b"><apply id="S3.SS1.p3.8.m1.1.1.cmml" xref="S3.SS1.p3.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.8.m1.1.1.1.cmml" xref="S3.SS1.p3.8.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.8.m1.1.1.2.cmml" xref="S3.SS1.p3.8.m1.1.1.2">𝛼</ci><ci id="S3.SS1.p3.8.m1.1.1.3.cmml" xref="S3.SS1.p3.8.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m1.1c">\alpha_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.8.m1.1d">italic_α start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\beta_{q}" class="ltx_Math" display="inline" id="S3.SS1.p3.9.m2.1"><semantics id="S3.SS1.p3.9.m2.1a"><msub id="S3.SS1.p3.9.m2.1.1" xref="S3.SS1.p3.9.m2.1.1.cmml"><mi id="S3.SS1.p3.9.m2.1.1.2" xref="S3.SS1.p3.9.m2.1.1.2.cmml">β</mi><mi id="S3.SS1.p3.9.m2.1.1.3" xref="S3.SS1.p3.9.m2.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.9.m2.1b"><apply id="S3.SS1.p3.9.m2.1.1.cmml" xref="S3.SS1.p3.9.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.9.m2.1.1.1.cmml" xref="S3.SS1.p3.9.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.9.m2.1.1.2.cmml" xref="S3.SS1.p3.9.m2.1.1.2">𝛽</ci><ci id="S3.SS1.p3.9.m2.1.1.3.cmml" xref="S3.SS1.p3.9.m2.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.9.m2.1c">\beta_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.9.m2.1d">italic_β start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> (<math alttext="\alpha_{kv}" class="ltx_Math" display="inline" id="S3.SS1.p3.10.m3.1"><semantics id="S3.SS1.p3.10.m3.1a"><msub id="S3.SS1.p3.10.m3.1.1" xref="S3.SS1.p3.10.m3.1.1.cmml"><mi id="S3.SS1.p3.10.m3.1.1.2" xref="S3.SS1.p3.10.m3.1.1.2.cmml">α</mi><mrow id="S3.SS1.p3.10.m3.1.1.3" xref="S3.SS1.p3.10.m3.1.1.3.cmml"><mi id="S3.SS1.p3.10.m3.1.1.3.2" xref="S3.SS1.p3.10.m3.1.1.3.2.cmml">k</mi><mo id="S3.SS1.p3.10.m3.1.1.3.1" xref="S3.SS1.p3.10.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.10.m3.1.1.3.3" xref="S3.SS1.p3.10.m3.1.1.3.3.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.10.m3.1b"><apply id="S3.SS1.p3.10.m3.1.1.cmml" xref="S3.SS1.p3.10.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.10.m3.1.1.1.cmml" xref="S3.SS1.p3.10.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.10.m3.1.1.2.cmml" xref="S3.SS1.p3.10.m3.1.1.2">𝛼</ci><apply id="S3.SS1.p3.10.m3.1.1.3.cmml" xref="S3.SS1.p3.10.m3.1.1.3"><times id="S3.SS1.p3.10.m3.1.1.3.1.cmml" xref="S3.SS1.p3.10.m3.1.1.3.1"></times><ci id="S3.SS1.p3.10.m3.1.1.3.2.cmml" xref="S3.SS1.p3.10.m3.1.1.3.2">𝑘</ci><ci id="S3.SS1.p3.10.m3.1.1.3.3.cmml" xref="S3.SS1.p3.10.m3.1.1.3.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.10.m3.1c">\alpha_{kv}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.10.m3.1d">italic_α start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\beta_{kb}" class="ltx_Math" display="inline" id="S3.SS1.p3.11.m4.1"><semantics id="S3.SS1.p3.11.m4.1a"><msub id="S3.SS1.p3.11.m4.1.1" xref="S3.SS1.p3.11.m4.1.1.cmml"><mi id="S3.SS1.p3.11.m4.1.1.2" xref="S3.SS1.p3.11.m4.1.1.2.cmml">β</mi><mrow id="S3.SS1.p3.11.m4.1.1.3" xref="S3.SS1.p3.11.m4.1.1.3.cmml"><mi id="S3.SS1.p3.11.m4.1.1.3.2" xref="S3.SS1.p3.11.m4.1.1.3.2.cmml">k</mi><mo id="S3.SS1.p3.11.m4.1.1.3.1" xref="S3.SS1.p3.11.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.11.m4.1.1.3.3" xref="S3.SS1.p3.11.m4.1.1.3.3.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.11.m4.1b"><apply id="S3.SS1.p3.11.m4.1.1.cmml" xref="S3.SS1.p3.11.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.11.m4.1.1.1.cmml" xref="S3.SS1.p3.11.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.11.m4.1.1.2.cmml" xref="S3.SS1.p3.11.m4.1.1.2">𝛽</ci><apply id="S3.SS1.p3.11.m4.1.1.3.cmml" xref="S3.SS1.p3.11.m4.1.1.3"><times id="S3.SS1.p3.11.m4.1.1.3.1.cmml" xref="S3.SS1.p3.11.m4.1.1.3.1"></times><ci id="S3.SS1.p3.11.m4.1.1.3.2.cmml" xref="S3.SS1.p3.11.m4.1.1.3.2">𝑘</ci><ci id="S3.SS1.p3.11.m4.1.1.3.3.cmml" xref="S3.SS1.p3.11.m4.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.11.m4.1c">\beta_{kb}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.11.m4.1d">italic_β start_POSTSUBSCRIPT italic_k italic_b end_POSTSUBSCRIPT</annotation></semantics></math>) control the retention sizes of the retrieved and input vectors in the query (key-value) matrix.
This fusion scheme gives us more flexibility on what information is processed in the attention mechanism.
Given <math alttext="h_{q}" class="ltx_Math" display="inline" id="S3.SS1.p3.12.m5.1"><semantics id="S3.SS1.p3.12.m5.1a"><msub id="S3.SS1.p3.12.m5.1.1" xref="S3.SS1.p3.12.m5.1.1.cmml"><mi id="S3.SS1.p3.12.m5.1.1.2" xref="S3.SS1.p3.12.m5.1.1.2.cmml">h</mi><mi id="S3.SS1.p3.12.m5.1.1.3" xref="S3.SS1.p3.12.m5.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.12.m5.1b"><apply id="S3.SS1.p3.12.m5.1.1.cmml" xref="S3.SS1.p3.12.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.12.m5.1.1.1.cmml" xref="S3.SS1.p3.12.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.12.m5.1.1.2.cmml" xref="S3.SS1.p3.12.m5.1.1.2">ℎ</ci><ci id="S3.SS1.p3.12.m5.1.1.3.cmml" xref="S3.SS1.p3.12.m5.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.12.m5.1c">h_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.12.m5.1d">italic_h start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="h_{kv}" class="ltx_Math" display="inline" id="S3.SS1.p3.13.m6.1"><semantics id="S3.SS1.p3.13.m6.1a"><msub id="S3.SS1.p3.13.m6.1.1" xref="S3.SS1.p3.13.m6.1.1.cmml"><mi id="S3.SS1.p3.13.m6.1.1.2" xref="S3.SS1.p3.13.m6.1.1.2.cmml">h</mi><mrow id="S3.SS1.p3.13.m6.1.1.3" xref="S3.SS1.p3.13.m6.1.1.3.cmml"><mi id="S3.SS1.p3.13.m6.1.1.3.2" xref="S3.SS1.p3.13.m6.1.1.3.2.cmml">k</mi><mo id="S3.SS1.p3.13.m6.1.1.3.1" xref="S3.SS1.p3.13.m6.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.13.m6.1.1.3.3" xref="S3.SS1.p3.13.m6.1.1.3.3.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.13.m6.1b"><apply id="S3.SS1.p3.13.m6.1.1.cmml" xref="S3.SS1.p3.13.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.13.m6.1.1.1.cmml" xref="S3.SS1.p3.13.m6.1.1">subscript</csymbol><ci id="S3.SS1.p3.13.m6.1.1.2.cmml" xref="S3.SS1.p3.13.m6.1.1.2">ℎ</ci><apply id="S3.SS1.p3.13.m6.1.1.3.cmml" xref="S3.SS1.p3.13.m6.1.1.3"><times id="S3.SS1.p3.13.m6.1.1.3.1.cmml" xref="S3.SS1.p3.13.m6.1.1.3.1"></times><ci id="S3.SS1.p3.13.m6.1.1.3.2.cmml" xref="S3.SS1.p3.13.m6.1.1.3.2">𝑘</ci><ci id="S3.SS1.p3.13.m6.1.1.3.3.cmml" xref="S3.SS1.p3.13.m6.1.1.3.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.13.m6.1c">h_{kv}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.13.m6.1d">italic_h start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT</annotation></semantics></math>, we can approximate the dot-product part of the cross-attention as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="QK^{T}\approx(h_{q}W_{q})(h_{kv}W_{k})^{T}\ ." class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.4.cmml"><mi id="S3.E4.m1.1.1.1.1.4.2" xref="S3.E4.m1.1.1.1.1.4.2.cmml">Q</mi><mo id="S3.E4.m1.1.1.1.1.4.1" xref="S3.E4.m1.1.1.1.1.4.1.cmml">⁢</mo><msup id="S3.E4.m1.1.1.1.1.4.3" xref="S3.E4.m1.1.1.1.1.4.3.cmml"><mi id="S3.E4.m1.1.1.1.1.4.3.2" xref="S3.E4.m1.1.1.1.1.4.3.2.cmml">K</mi><mi id="S3.E4.m1.1.1.1.1.4.3.3" xref="S3.E4.m1.1.1.1.1.4.3.3.cmml">T</mi></msup></mrow><mo id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml">≈</mo><mrow id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.cmml">h</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.3.cmml">q</mi></msub><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml">W</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml">q</mi></msub></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E4.m1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.2.3.cmml">⁢</mo><msup id="S3.E4.m1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.2.2.cmml"><mrow id="S3.E4.m1.1.1.1.1.2.2.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.2.2.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.2.2.1.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.2.cmml">h</mi><mrow id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.2.cmml">k</mi><mo id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.3.cmml">v</mi></mrow></msub><mo id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.cmml">⁢</mo><msub id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.2.cmml">W</mi><mi id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.3.cmml">k</mi></msub></mrow><mo id="S3.E4.m1.1.1.1.1.2.2.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.cmml">)</mo></mrow><mi id="S3.E4.m1.1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.1.2.2.3.cmml">T</mi></msup></mrow></mrow><mo id="S3.E4.m1.1.1.1.2" lspace="0em" xref="S3.E4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><approx id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"></approx><apply id="S3.E4.m1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.4"><times id="S3.E4.m1.1.1.1.1.4.1.cmml" xref="S3.E4.m1.1.1.1.1.4.1"></times><ci id="S3.E4.m1.1.1.1.1.4.2.cmml" xref="S3.E4.m1.1.1.1.1.4.2">𝑄</ci><apply id="S3.E4.m1.1.1.1.1.4.3.cmml" xref="S3.E4.m1.1.1.1.1.4.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.4.3.1.cmml" xref="S3.E4.m1.1.1.1.1.4.3">superscript</csymbol><ci id="S3.E4.m1.1.1.1.1.4.3.2.cmml" xref="S3.E4.m1.1.1.1.1.4.3.2">𝐾</ci><ci id="S3.E4.m1.1.1.1.1.4.3.3.cmml" xref="S3.E4.m1.1.1.1.1.4.3.3">𝑇</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"><times id="S3.E4.m1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.3"></times><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.2">ℎ</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.3">𝑞</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2">𝑊</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.3">𝑞</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2">superscript</csymbol><apply id="S3.E4.m1.1.1.1.1.2.2.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1"><times id="S3.E4.m1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.1"></times><apply id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.2">ℎ</ci><apply id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3"><times id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.1"></times><ci id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.2">𝑘</ci><ci id="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.2.3.3">𝑣</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.2">𝑊</ci><ci id="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1.1.1.3.3">𝑘</ci></apply></apply><ci id="S3.E4.m1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">QK^{T}\approx(h_{q}W_{q})(h_{kv}W_{k})^{T}\ .</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ≈ ( italic_h start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) ( italic_h start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p3.15">Similar to the original approach, <math alttext="W_{q}" class="ltx_Math" display="inline" id="S3.SS1.p3.14.m1.1"><semantics id="S3.SS1.p3.14.m1.1a"><msub id="S3.SS1.p3.14.m1.1.1" xref="S3.SS1.p3.14.m1.1.1.cmml"><mi id="S3.SS1.p3.14.m1.1.1.2" xref="S3.SS1.p3.14.m1.1.1.2.cmml">W</mi><mi id="S3.SS1.p3.14.m1.1.1.3" xref="S3.SS1.p3.14.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.14.m1.1b"><apply id="S3.SS1.p3.14.m1.1.1.cmml" xref="S3.SS1.p3.14.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.14.m1.1.1.1.cmml" xref="S3.SS1.p3.14.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.14.m1.1.1.2.cmml" xref="S3.SS1.p3.14.m1.1.1.2">𝑊</ci><ci id="S3.SS1.p3.14.m1.1.1.3.cmml" xref="S3.SS1.p3.14.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.14.m1.1c">W_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.14.m1.1d">italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="W_{k}" class="ltx_Math" display="inline" id="S3.SS1.p3.15.m2.1"><semantics id="S3.SS1.p3.15.m2.1a"><msub id="S3.SS1.p3.15.m2.1.1" xref="S3.SS1.p3.15.m2.1.1.cmml"><mi id="S3.SS1.p3.15.m2.1.1.2" xref="S3.SS1.p3.15.m2.1.1.2.cmml">W</mi><mi id="S3.SS1.p3.15.m2.1.1.3" xref="S3.SS1.p3.15.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.15.m2.1b"><apply id="S3.SS1.p3.15.m2.1.1.cmml" xref="S3.SS1.p3.15.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.15.m2.1.1.1.cmml" xref="S3.SS1.p3.15.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.15.m2.1.1.2.cmml" xref="S3.SS1.p3.15.m2.1.1.2">𝑊</ci><ci id="S3.SS1.p3.15.m2.1.1.3.cmml" xref="S3.SS1.p3.15.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.15.m2.1c">W_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.15.m2.1d">italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> are head-specific but use the same index.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.2.   kNN Indices</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">Without separate encoder/decoder modules, operating with only one index for the whole model is impossible.
This is because each layer in a decoder-only model attends to the output of its previous layer, in contrast to the decoder layers in encoder-decoder models that attend to the encoder’s output.
Consequently, if we arbitrarily use a specific layer’s output for building our index, we create a distributional mismatch between the expected input and actual inputs of future layers.
Hence, we must create a separate index for each cross-attention layer to overcome this issue.
However, this approach requires much more memory and computation cost to build the indices.
For example, in our models, it could add up to <math alttext="|\mathcal{L}|" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.2.2" xref="S3.SS2.p1.1.m1.1.2.1.cmml"><mo id="S3.SS2.p1.1.m1.1.2.2.1" stretchy="false" xref="S3.SS2.p1.1.m1.1.2.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">ℒ</mi><mo id="S3.SS2.p1.1.m1.1.2.2.2" stretchy="false" xref="S3.SS2.p1.1.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2.2"><abs id="S3.SS2.p1.1.m1.1.2.1.1.cmml" xref="S3.SS2.p1.1.m1.1.2.2.1"></abs><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ℒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">|\mathcal{L}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">| caligraphic_L |</annotation></semantics></math> times the cost of having a single index.
Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S8" title="8. Limitations ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">8</span></a> discusses the potential ways of mitigating this issue.
In our experiments, we only tune our models to use at most three cross-attention layers, i.e., <math alttext="|\mathcal{L}|=3" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.2" xref="S3.SS2.p1.2.m2.1.2.cmml"><mrow id="S3.SS2.p1.2.m2.1.2.2.2" xref="S3.SS2.p1.2.m2.1.2.2.1.cmml"><mo id="S3.SS2.p1.2.m2.1.2.2.2.1" stretchy="false" xref="S3.SS2.p1.2.m2.1.2.2.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">ℒ</mi><mo id="S3.SS2.p1.2.m2.1.2.2.2.2" stretchy="false" xref="S3.SS2.p1.2.m2.1.2.2.1.1.cmml">|</mo></mrow><mo id="S3.SS2.p1.2.m2.1.2.1" xref="S3.SS2.p1.2.m2.1.2.1.cmml">=</mo><mn id="S3.SS2.p1.2.m2.1.2.3" xref="S3.SS2.p1.2.m2.1.2.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.2.cmml" xref="S3.SS2.p1.2.m2.1.2"><eq id="S3.SS2.p1.2.m2.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.2.1"></eq><apply id="S3.SS2.p1.2.m2.1.2.2.1.cmml" xref="S3.SS2.p1.2.m2.1.2.2.2"><abs id="S3.SS2.p1.2.m2.1.2.2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.2.2.2.1"></abs><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ℒ</ci></apply><cn id="S3.SS2.p1.2.m2.1.2.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.2.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">|\mathcal{L}|=3</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">| caligraphic_L | = 3</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.3.   Index Staleness</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.4">One of the potential issues of a static index is staleness.
Specifically, since Unlimiformer approximates dense attention, without updating the index, we would lose the information from the newly generated tokens, which might lead to incoherent outputs.
For example, assume we have a model with a context length of <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_N</annotation></semantics></math>.
In this scenario, at generation step <math alttext="N+2" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">N</mi><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">+</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><plus id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></plus><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝑁</ci><cn id="S3.SS3.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS3.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">N+2</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_N + 2</annotation></semantics></math>, the input to the model would be the last <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">italic_N</annotation></semantics></math> generated tokens, effectively discarding the first generated token as it is also absent from the index.
To fix this problem, at each generation step and each layer, we add <math alttext="h_{\text{CA}}^{(-1)}" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><msubsup id="S3.SS3.p1.4.m4.1.2" xref="S3.SS3.p1.4.m4.1.2.cmml"><mi id="S3.SS3.p1.4.m4.1.2.2.2" xref="S3.SS3.p1.4.m4.1.2.2.2.cmml">h</mi><mtext id="S3.SS3.p1.4.m4.1.2.2.3" xref="S3.SS3.p1.4.m4.1.2.2.3a.cmml">CA</mtext><mrow id="S3.SS3.p1.4.m4.1.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS3.p1.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS3.p1.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p1.4.m4.1.1.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS3.p1.4.m4.1.1.1.1.1a" xref="S3.SS3.p1.4.m4.1.1.1.1.1.cmml">−</mo><mn id="S3.SS3.p1.4.m4.1.1.1.1.1.2" xref="S3.SS3.p1.4.m4.1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S3.SS3.p1.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS3.p1.4.m4.1.1.1.1.1.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.2.cmml" xref="S3.SS3.p1.4.m4.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.2.1.cmml" xref="S3.SS3.p1.4.m4.1.2">superscript</csymbol><apply id="S3.SS3.p1.4.m4.1.2.2.cmml" xref="S3.SS3.p1.4.m4.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.2.2.1.cmml" xref="S3.SS3.p1.4.m4.1.2">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.2.2.2.cmml" xref="S3.SS3.p1.4.m4.1.2.2.2">ℎ</ci><ci id="S3.SS3.p1.4.m4.1.2.2.3a.cmml" xref="S3.SS3.p1.4.m4.1.2.2.3"><mtext id="S3.SS3.p1.4.m4.1.2.2.3.cmml" mathsize="70%" xref="S3.SS3.p1.4.m4.1.2.2.3">CA</mtext></ci></apply><apply id="S3.SS3.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1"><minus id="S3.SS3.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1"></minus><cn id="S3.SS3.p1.4.m4.1.1.1.1.1.2.cmml" type="integer" xref="S3.SS3.p1.4.m4.1.1.1.1.1.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">h_{\text{CA}}^{(-1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_h start_POSTSUBSCRIPT CA end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math> to its respective index.
This ensures the addition of the most recent token and keeps the indices from going stale.
Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S6.SS1" title="6.1. Index Staleness ‣ 6. Ablations ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">6.1</span></a> presents an ablation study that showcases the effectiveness of this simple change.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.4.   Chunks Encoding</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In contrast to encoder-decoder models, decoder-only transformers use causal (unidirectional) attention.
This difference means that a token has seen enough contextual information if a certain number of tokens are behind it.
As a result, instead of only storing the hidden states of the middle half tokens, we can keep all the non-overlapping ones.
This will allow us to be slightly more efficient when processing long documents.
Note that only the first instance of overlapping tokens is added to the index, as illustrated by orange tokens in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Experimental Setup</h2>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="509" id="S4.F2.g1" src="extracted/5896239/assets/sample_prompt.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A sample free-form Q&amp;A prompt. The article section consists of the truncated version of the full article that fits in the context. The question section is always fully included.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1" style="width:433.6pt;height:109pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(52.7pt,-13.2pt) scale(1.32105917281208,1.32105917281208) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Metrics</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">#Samples</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">Avg #Tokens</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.1">GovReport (GAO)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.2.2.1"><span class="ltx_text" id="S4.T1.1.1.2.2.1.1"></span> <span class="ltx_text" id="S4.T1.1.1.2.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.2.2.1.2.1">
<span class="ltx_tr" id="S4.T1.1.1.2.2.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.2.2.1.2.1.1.1">ROUGE-{1,2,L},</span></span>
<span class="ltx_tr" id="S4.T1.1.1.2.2.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.1.1.2.2.1.2.1.2.1">METEOR</span></span>
</span></span> <span class="ltx_text" id="S4.T1.1.1.2.2.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.3">611</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.4">11395</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.1">BookSum</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.2">46</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.3">164695</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.1">NarrativeQA</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.1.4.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.4.2.1">Token F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.3">10557</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4">76433</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.5.1">Qasper</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.5.2">1372</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.5.3">4836</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Statistics of the datasets. The average number of tokens is obtained using the GPT-4 tokenizer: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/openai/tiktoken" title="">https://github.com/openai/tiktoken</a>.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">4.1.   Datasets &amp; Tasks</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">For our experiments, we use datasets from the two tasks of summarization and free-form Q&amp;A, chosen due to the existence of long-document benchmarks <cite class="ltx_cite ltx_citemacro_cite">Shaham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib27" title="">2023</a>)</cite>.
Specifically, for summarization, we use GovReport (GAO) <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib13" title="">2021</a>)</cite> and BookSum <cite class="ltx_cite ltx_citemacro_cite">Kryscinski et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib18" title="">2022</a>)</cite>, and for free-form Q&amp;A, we use NarrativeQA <cite class="ltx_cite ltx_citemacro_cite">Kočiský et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib17" title="">2018</a>)</cite> and Qasper <cite class="ltx_cite ltx_citemacro_cite">Dasigi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib10" title="">2021</a>)</cite>.
Moreover, we tune the hyperparameters on the validation sets and report the results on the test sets.
All the experiments are carried out in a zero-shot setting.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.T1" title="Table 1 ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">1</span></a> presents the statistics of these datasets.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="354" id="S4.F3.g1" src="extracted/5896239/assets/dataset_experiments.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Performance comparison of GPT-Summ on GovReport (GAO) and BookSum, and GPT-Inst on NarrativeQA and Qasper at varying context lengths.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">4.2.   Models</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We used two distinct base models to evaluate our modifications: 1) <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">GPT-Summ:</span> a finetuned summarization model and 2) <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.2">GPT-Inst:</span> an instruction-tuned model.
To better understand the impact of our approach compared to dense attention, in contrast to prior works, we pre-train both variants of the models with a sequence length of 8192 using the same architecture as the GPT-3 6.7B model <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib5" title="">2020b</a>)</cite>, with the addition of RoPE embeddings <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib28" title="">2021</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="128" id="S4.F4.g1" src="extracted/5896239/assets/govreport.png" width="628"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>METEOR and ROUGE-{1,2,L} achieved by GPT-Summ on the GovReport (GAO) dataset.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="128" id="S4.F5.g1" src="extracted/5896239/assets/booksum.png" width="628"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>METEOR and ROUGE-{1,2,L} achieved by GPT-Summ on the BookSum dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">4.3.   Evaluation Setup</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.5">The original Unlimiformer paper presents two main experimental comparisons: 1) <math alttext="\text{BART}_{\text{base}}" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><msub id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mtext id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2a.cmml">BART</mtext><mtext id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2a.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><mtext id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">BART</mtext></ci><ci id="S4.SS3.p1.1.m1.1.1.3a.cmml" xref="S4.SS3.p1.1.m1.1.1.3"><mtext id="S4.SS3.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p1.1.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\text{BART}_{\text{base}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">BART start_POSTSUBSCRIPT base end_POSTSUBSCRIPT</annotation></semantics></math> vs. <math alttext="\text{BART}_{\text{base}}+\text{Unlimiformer}" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><msub id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml"><mtext id="S4.SS3.p1.2.m2.1.1.2.2" xref="S4.SS3.p1.2.m2.1.1.2.2a.cmml">BART</mtext><mtext id="S4.SS3.p1.2.m2.1.1.2.3" xref="S4.SS3.p1.2.m2.1.1.2.3a.cmml">base</mtext></msub><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">+</mo><mtext id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3a.cmml">Unlimiformer</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><plus id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></plus><apply id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.2.1.cmml" xref="S4.SS3.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.2a.cmml" xref="S4.SS3.p1.2.m2.1.1.2.2"><mtext id="S4.SS3.p1.2.m2.1.1.2.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2.2">BART</mtext></ci><ci id="S4.SS3.p1.2.m2.1.1.2.3a.cmml" xref="S4.SS3.p1.2.m2.1.1.2.3"><mtext id="S4.SS3.p1.2.m2.1.1.2.3.cmml" mathsize="70%" xref="S4.SS3.p1.2.m2.1.1.2.3">base</mtext></ci></apply><ci id="S4.SS3.p1.2.m2.1.1.3a.cmml" xref="S4.SS3.p1.2.m2.1.1.3"><mtext id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">Unlimiformer</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">\text{BART}_{\text{base}}+\text{Unlimiformer}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">BART start_POSTSUBSCRIPT base end_POSTSUBSCRIPT + Unlimiformer</annotation></semantics></math> and 2) <math alttext="\text{BART}_{\text{base}}+\text{Unlimiformer}" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.1"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><msub id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml"><mtext id="S4.SS3.p1.3.m3.1.1.2.2" xref="S4.SS3.p1.3.m3.1.1.2.2a.cmml">BART</mtext><mtext id="S4.SS3.p1.3.m3.1.1.2.3" xref="S4.SS3.p1.3.m3.1.1.2.3a.cmml">base</mtext></msub><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">+</mo><mtext id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3a.cmml">Unlimiformer</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><plus id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1"></plus><apply id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.1.1.2.1.cmml" xref="S4.SS3.p1.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.3.m3.1.1.2.2a.cmml" xref="S4.SS3.p1.3.m3.1.1.2.2"><mtext id="S4.SS3.p1.3.m3.1.1.2.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2.2">BART</mtext></ci><ci id="S4.SS3.p1.3.m3.1.1.2.3a.cmml" xref="S4.SS3.p1.3.m3.1.1.2.3"><mtext id="S4.SS3.p1.3.m3.1.1.2.3.cmml" mathsize="70%" xref="S4.SS3.p1.3.m3.1.1.2.3">base</mtext></ci></apply><ci id="S4.SS3.p1.3.m3.1.1.3a.cmml" xref="S4.SS3.p1.3.m3.1.1.3"><mtext id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3">Unlimiformer</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">\text{BART}_{\text{base}}+\text{Unlimiformer}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.1d">BART start_POSTSUBSCRIPT base end_POSTSUBSCRIPT + Unlimiformer</annotation></semantics></math> vs. SLED <cite class="ltx_cite ltx_citemacro_cite">Ivgi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib15" title="">2023</a>)</cite> and Longformer <cite class="ltx_cite ltx_citemacro_cite">Beltagy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib2" title="">2020</a>)</cite>.
These comparisons showcase the effectiveness of the proposed model; however, one missing crucial evaluation setup is the comparison to the same base model with longer context lengths, e.g., <math alttext="\text{GPT-Summ}[2048]" class="ltx_Math" display="inline" id="S4.SS3.p1.4.m4.1"><semantics id="S4.SS3.p1.4.m4.1a"><mrow id="S4.SS3.p1.4.m4.1.2" xref="S4.SS3.p1.4.m4.1.2.cmml"><mtext id="S4.SS3.p1.4.m4.1.2.2" xref="S4.SS3.p1.4.m4.1.2.2a.cmml">GPT-Summ</mtext><mo id="S4.SS3.p1.4.m4.1.2.1" xref="S4.SS3.p1.4.m4.1.2.1.cmml">⁢</mo><mrow id="S4.SS3.p1.4.m4.1.2.3.2" xref="S4.SS3.p1.4.m4.1.2.3.1.cmml"><mo id="S4.SS3.p1.4.m4.1.2.3.2.1" stretchy="false" xref="S4.SS3.p1.4.m4.1.2.3.1.1.cmml">[</mo><mn id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">2048</mn><mo id="S4.SS3.p1.4.m4.1.2.3.2.2" stretchy="false" xref="S4.SS3.p1.4.m4.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.2.cmml" xref="S4.SS3.p1.4.m4.1.2"><times id="S4.SS3.p1.4.m4.1.2.1.cmml" xref="S4.SS3.p1.4.m4.1.2.1"></times><ci id="S4.SS3.p1.4.m4.1.2.2a.cmml" xref="S4.SS3.p1.4.m4.1.2.2"><mtext id="S4.SS3.p1.4.m4.1.2.2.cmml" xref="S4.SS3.p1.4.m4.1.2.2">GPT-Summ</mtext></ci><apply id="S4.SS3.p1.4.m4.1.2.3.1.cmml" xref="S4.SS3.p1.4.m4.1.2.3.2"><csymbol cd="latexml" id="S4.SS3.p1.4.m4.1.2.3.1.1.cmml" xref="S4.SS3.p1.4.m4.1.2.3.2.1">delimited-[]</csymbol><cn id="S4.SS3.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS3.p1.4.m4.1.1">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">\text{GPT-Summ}[2048]</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.4.m4.1d">GPT-Summ [ 2048 ]</annotation></semantics></math> vs. <math alttext="\text{GPT-Summ}[1024]+\text{Unlimiformer}" class="ltx_Math" display="inline" id="S4.SS3.p1.5.m5.1"><semantics id="S4.SS3.p1.5.m5.1a"><mrow id="S4.SS3.p1.5.m5.1.2" xref="S4.SS3.p1.5.m5.1.2.cmml"><mrow id="S4.SS3.p1.5.m5.1.2.2" xref="S4.SS3.p1.5.m5.1.2.2.cmml"><mtext id="S4.SS3.p1.5.m5.1.2.2.2" xref="S4.SS3.p1.5.m5.1.2.2.2a.cmml">GPT-Summ</mtext><mo id="S4.SS3.p1.5.m5.1.2.2.1" xref="S4.SS3.p1.5.m5.1.2.2.1.cmml">⁢</mo><mrow id="S4.SS3.p1.5.m5.1.2.2.3.2" xref="S4.SS3.p1.5.m5.1.2.2.3.1.cmml"><mo id="S4.SS3.p1.5.m5.1.2.2.3.2.1" stretchy="false" xref="S4.SS3.p1.5.m5.1.2.2.3.1.1.cmml">[</mo><mn id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml">1024</mn><mo id="S4.SS3.p1.5.m5.1.2.2.3.2.2" stretchy="false" xref="S4.SS3.p1.5.m5.1.2.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="S4.SS3.p1.5.m5.1.2.1" xref="S4.SS3.p1.5.m5.1.2.1.cmml">+</mo><mtext id="S4.SS3.p1.5.m5.1.2.3" xref="S4.SS3.p1.5.m5.1.2.3a.cmml">Unlimiformer</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.2.cmml" xref="S4.SS3.p1.5.m5.1.2"><plus id="S4.SS3.p1.5.m5.1.2.1.cmml" xref="S4.SS3.p1.5.m5.1.2.1"></plus><apply id="S4.SS3.p1.5.m5.1.2.2.cmml" xref="S4.SS3.p1.5.m5.1.2.2"><times id="S4.SS3.p1.5.m5.1.2.2.1.cmml" xref="S4.SS3.p1.5.m5.1.2.2.1"></times><ci id="S4.SS3.p1.5.m5.1.2.2.2a.cmml" xref="S4.SS3.p1.5.m5.1.2.2.2"><mtext id="S4.SS3.p1.5.m5.1.2.2.2.cmml" xref="S4.SS3.p1.5.m5.1.2.2.2">GPT-Summ</mtext></ci><apply id="S4.SS3.p1.5.m5.1.2.2.3.1.cmml" xref="S4.SS3.p1.5.m5.1.2.2.3.2"><csymbol cd="latexml" id="S4.SS3.p1.5.m5.1.2.2.3.1.1.cmml" xref="S4.SS3.p1.5.m5.1.2.2.3.2.1">delimited-[]</csymbol><cn id="S4.SS3.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS3.p1.5.m5.1.1">1024</cn></apply></apply><ci id="S4.SS3.p1.5.m5.1.2.3a.cmml" xref="S4.SS3.p1.5.m5.1.2.3"><mtext id="S4.SS3.p1.5.m5.1.2.3.cmml" xref="S4.SS3.p1.5.m5.1.2.3">Unlimiformer</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">\text{GPT-Summ}[1024]+\text{Unlimiformer}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.5.m5.1d">GPT-Summ [ 1024 ] + Unlimiformer</annotation></semantics></math>.
Such a setup provides more insight into how efficiently Unlimiformer uses the extra information provided through the kNN index.
In this work, we focus on this setting by restricting the context length of the base model to 1024, 2048, 4096, and 8192 tokens and then comparing them to variations equipped with Unlimiformer.
To ensure that our models and datasets showcase meaningful differences across context lengths in such a setup, we ran experiments on the base models using the validation sets, estimating the performance changes as contextual information increased.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.F3" title="Figure 3 ‣ 4.1. Datasets &amp; Tasks ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the results of our experiments.
Moreover, Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S6.SS2" title="6.2. Contextual Information ‣ 6. Ablations ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">6.2</span></a> presents a case study to ensure the performance disparity in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.F3" title="Figure 3 ‣ 4.1. Datasets &amp; Tasks ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">3</span></a> is an artifact of the datasets, not a deficiency in the models.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">4.4.   Prompt Structure</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">For the free-form Q&amp;A datasets and the instruction-finetuned model, we opted for a simple three-part (Article, Question, and Answer) template.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.F2" title="Figure 2 ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates an example of the prompt structure.
We also investigated ZeroScrolls’s prompt template <cite class="ltx_cite ltx_citemacro_cite">Shaham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib27" title="">2023</a>)</cite>, but since we did not notice any significant difference in the performance, we continued with the more straightforward template.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="246" id="S4.F6.g1" src="extracted/5896239/assets/nqa_qasper.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Token F1 achieved by GPT-Inst on NarrativeQA (left) and Qasper (right) datasets.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">4.5.   Metrics</h3>
<div class="ltx_para ltx_noindent" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">For summarization, we report ROUGE-{1,2,L} <cite class="ltx_cite ltx_citemacro_cite">Lin (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib20" title="">2004</a>)</cite> and METEOR <cite class="ltx_cite ltx_citemacro_cite">Banerjee and Lavie (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib1" title="">2005</a>)</cite> as 1) they are standard metrics and 2) have shown good correlation to expert annotations <cite class="ltx_cite ltx_citemacro_cite">Fabbri et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib12" title="">2021</a>)</cite>.
Moreover, in our early experiments, we investigated BERTScore <cite class="ltx_cite ltx_citemacro_cite">Zhang* et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib36" title="">2020</a>)</cite>; however, similar to <cite class="ltx_cite ltx_citemacro_citet">Bertsch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#bib.bib3" title="">2023</a>)</cite>’s findings, we observed a lack of distinguishing power among the lengthy summaries, even when other metrics and manual inspection showed improvements.
For free-form Q&amp;A, we used the standard token F1 metric, similar to the ZeroScrolls benchmark.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Results</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">5.1.   Summarization</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.F4" title="Figure 4 ‣ 4.2. Models ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.F5" title="Figure 5 ‣ 4.2. Models ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">5</span></a> present our experimental results using the GPT-Summ model on the GovReport (GAO) and BookSum datasets, respectively.
As evident from these results, adding our modifications improves the model’s performance to a 2x level (e.g., <math alttext="\text{GPT-Summ}[1024]+\text{Unlimiformer}\approx\text{GPT-Summ}[2048]" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.2"><semantics id="S5.SS1.p1.1.m1.2a"><mrow id="S5.SS1.p1.1.m1.2.3" xref="S5.SS1.p1.1.m1.2.3.cmml"><mrow id="S5.SS1.p1.1.m1.2.3.2" xref="S5.SS1.p1.1.m1.2.3.2.cmml"><mrow id="S5.SS1.p1.1.m1.2.3.2.2" xref="S5.SS1.p1.1.m1.2.3.2.2.cmml"><mtext id="S5.SS1.p1.1.m1.2.3.2.2.2" xref="S5.SS1.p1.1.m1.2.3.2.2.2a.cmml">GPT-Summ</mtext><mo id="S5.SS1.p1.1.m1.2.3.2.2.1" xref="S5.SS1.p1.1.m1.2.3.2.2.1.cmml">⁢</mo><mrow id="S5.SS1.p1.1.m1.2.3.2.2.3.2" xref="S5.SS1.p1.1.m1.2.3.2.2.3.1.cmml"><mo id="S5.SS1.p1.1.m1.2.3.2.2.3.2.1" stretchy="false" xref="S5.SS1.p1.1.m1.2.3.2.2.3.1.1.cmml">[</mo><mn id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">1024</mn><mo id="S5.SS1.p1.1.m1.2.3.2.2.3.2.2" stretchy="false" xref="S5.SS1.p1.1.m1.2.3.2.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="S5.SS1.p1.1.m1.2.3.2.1" xref="S5.SS1.p1.1.m1.2.3.2.1.cmml">+</mo><mtext id="S5.SS1.p1.1.m1.2.3.2.3" xref="S5.SS1.p1.1.m1.2.3.2.3a.cmml">Unlimiformer</mtext></mrow><mo id="S5.SS1.p1.1.m1.2.3.1" xref="S5.SS1.p1.1.m1.2.3.1.cmml">≈</mo><mrow id="S5.SS1.p1.1.m1.2.3.3" xref="S5.SS1.p1.1.m1.2.3.3.cmml"><mtext id="S5.SS1.p1.1.m1.2.3.3.2" xref="S5.SS1.p1.1.m1.2.3.3.2a.cmml">GPT-Summ</mtext><mo id="S5.SS1.p1.1.m1.2.3.3.1" xref="S5.SS1.p1.1.m1.2.3.3.1.cmml">⁢</mo><mrow id="S5.SS1.p1.1.m1.2.3.3.3.2" xref="S5.SS1.p1.1.m1.2.3.3.3.1.cmml"><mo id="S5.SS1.p1.1.m1.2.3.3.3.2.1" stretchy="false" xref="S5.SS1.p1.1.m1.2.3.3.3.1.1.cmml">[</mo><mn id="S5.SS1.p1.1.m1.2.2" xref="S5.SS1.p1.1.m1.2.2.cmml">2048</mn><mo id="S5.SS1.p1.1.m1.2.3.3.3.2.2" stretchy="false" xref="S5.SS1.p1.1.m1.2.3.3.3.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.2b"><apply id="S5.SS1.p1.1.m1.2.3.cmml" xref="S5.SS1.p1.1.m1.2.3"><approx id="S5.SS1.p1.1.m1.2.3.1.cmml" xref="S5.SS1.p1.1.m1.2.3.1"></approx><apply id="S5.SS1.p1.1.m1.2.3.2.cmml" xref="S5.SS1.p1.1.m1.2.3.2"><plus id="S5.SS1.p1.1.m1.2.3.2.1.cmml" xref="S5.SS1.p1.1.m1.2.3.2.1"></plus><apply id="S5.SS1.p1.1.m1.2.3.2.2.cmml" xref="S5.SS1.p1.1.m1.2.3.2.2"><times id="S5.SS1.p1.1.m1.2.3.2.2.1.cmml" xref="S5.SS1.p1.1.m1.2.3.2.2.1"></times><ci id="S5.SS1.p1.1.m1.2.3.2.2.2a.cmml" xref="S5.SS1.p1.1.m1.2.3.2.2.2"><mtext id="S5.SS1.p1.1.m1.2.3.2.2.2.cmml" xref="S5.SS1.p1.1.m1.2.3.2.2.2">GPT-Summ</mtext></ci><apply id="S5.SS1.p1.1.m1.2.3.2.2.3.1.cmml" xref="S5.SS1.p1.1.m1.2.3.2.2.3.2"><csymbol cd="latexml" id="S5.SS1.p1.1.m1.2.3.2.2.3.1.1.cmml" xref="S5.SS1.p1.1.m1.2.3.2.2.3.2.1">delimited-[]</csymbol><cn id="S5.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1">1024</cn></apply></apply><ci id="S5.SS1.p1.1.m1.2.3.2.3a.cmml" xref="S5.SS1.p1.1.m1.2.3.2.3"><mtext id="S5.SS1.p1.1.m1.2.3.2.3.cmml" xref="S5.SS1.p1.1.m1.2.3.2.3">Unlimiformer</mtext></ci></apply><apply id="S5.SS1.p1.1.m1.2.3.3.cmml" xref="S5.SS1.p1.1.m1.2.3.3"><times id="S5.SS1.p1.1.m1.2.3.3.1.cmml" xref="S5.SS1.p1.1.m1.2.3.3.1"></times><ci id="S5.SS1.p1.1.m1.2.3.3.2a.cmml" xref="S5.SS1.p1.1.m1.2.3.3.2"><mtext id="S5.SS1.p1.1.m1.2.3.3.2.cmml" xref="S5.SS1.p1.1.m1.2.3.3.2">GPT-Summ</mtext></ci><apply id="S5.SS1.p1.1.m1.2.3.3.3.1.cmml" xref="S5.SS1.p1.1.m1.2.3.3.3.2"><csymbol cd="latexml" id="S5.SS1.p1.1.m1.2.3.3.3.1.1.cmml" xref="S5.SS1.p1.1.m1.2.3.3.3.2.1">delimited-[]</csymbol><cn id="S5.SS1.p1.1.m1.2.2.cmml" type="integer" xref="S5.SS1.p1.1.m1.2.2">2048</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.2c">\text{GPT-Summ}[1024]+\text{Unlimiformer}\approx\text{GPT-Summ}[2048]</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.2d">GPT-Summ [ 1024 ] + Unlimiformer ≈ GPT-Summ [ 2048 ]</annotation></semantics></math>) on the GovReport (GAO) dataset.
Similarly, we observe significant improvements in the BookSum dataset.
These results showcase the effectiveness of our proposed modifications.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">5.2.   Free-Form Q&amp;A</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.F6" title="Figure 6 ‣ 4.4. Prompt Structure ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates our experimental results using the GPT-Inst model on the NarrativeQA and Qasper datasets.
Although we can observe some improvements in both Qasper and (mostly) NarrativeQA, they are less significant than the summarization datasets’ results.
Given the prompt-based approach used for the free-from Q&amp;A task (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.F2" title="Figure 2 ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">2</span></a>), the insignificant improvements could be an artifact of the instruction-tuned model being too biased toward the input, making it insensitive to the added information in the cross-attention layers.
These results present exciting opportunities to investigate such models in future studies.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Ablations</h2>
<figure class="ltx_table" id="S6.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T2.1">
<tr class="ltx_tr" id="S6.T2.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1" style="font-size:90%;">Variation</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.2.1" style="font-size:90%;">Rouge-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.3.1" style="font-size:90%;">Rouge-2</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.4.1" style="font-size:90%;">Rouge-L</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.2.1"><span class="ltx_text" id="S6.T2.1.2.1.1" style="font-size:90%;">w/o</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.2.2"><span class="ltx_text" id="S6.T2.1.2.2.1" style="font-size:90%;">0.4247</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.2.3"><span class="ltx_text" id="S6.T2.1.2.3.1" style="font-size:90%;">0.1734</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.2.4"><span class="ltx_text" id="S6.T2.1.2.4.1" style="font-size:90%;">0.2512</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.3.1"><span class="ltx_text" id="S6.T2.1.3.1.1" style="font-size:90%;">w/</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.3.2"><span class="ltx_text ltx_font_bold" id="S6.T2.1.3.2.1" style="font-size:90%;">0.4263</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.3.3"><span class="ltx_text ltx_font_bold" id="S6.T2.1.3.3.1" style="font-size:90%;">0.1744</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.3.4"><span class="ltx_text ltx_font_bold" id="S6.T2.1.3.4.1" style="font-size:90%;">0.2523</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Effect of adding newly generated tokens to the index on the performance of the model.</figcaption>
</figure>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.1" style="width:433.6pt;height:98pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(23.4pt,-5.3pt) scale(1.12110742423998,1.12110742423998) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.1.1">
<tr class="ltx_tr" id="S6.T3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.1">Chunk</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.1.1.1.2">
<span class="ltx_text" id="S6.T3.1.1.1.2.1"></span><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.2.2"> <span class="ltx_text" id="S6.T3.1.1.1.2.2.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.1.1.1.2.2.1.1">
<span class="ltx_tr" id="S6.T3.1.1.1.2.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.1.1.1.2.2.1.1.1.1">Min Evidence</span></span>
<span class="ltx_tr" id="S6.T3.1.1.1.2.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.1.1.1.2.2.1.1.2.1">¿ 1024 (%)</span></span>
</span></span><span class="ltx_text" id="S6.T3.1.1.1.2.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.1.1.1.3">
<span class="ltx_text" id="S6.T3.1.1.1.3.1"></span><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.3.2"> <span class="ltx_text" id="S6.T3.1.1.1.3.2.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.1.1.1.3.2.1.1">
<span class="ltx_tr" id="S6.T3.1.1.1.3.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.1.1.1.3.2.1.1.1.1">Min Evidence</span></span>
<span class="ltx_tr" id="S6.T3.1.1.1.3.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.1.1.1.3.2.1.1.2.1">¿ 2048 (%)</span></span>
</span></span><span class="ltx_text" id="S6.T3.1.1.1.3.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.1.1.1.4">
<span class="ltx_text" id="S6.T3.1.1.1.4.1"></span><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.4.2"> <span class="ltx_text" id="S6.T3.1.1.1.4.2.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.1.1.1.4.2.1.1">
<span class="ltx_tr" id="S6.T3.1.1.1.4.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.1.1.1.4.2.1.1.1.1">Min Evidence</span></span>
<span class="ltx_tr" id="S6.T3.1.1.1.4.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.1.1.1.4.2.1.1.2.1">¿ 4096 (%)</span></span>
</span></span><span class="ltx_text" id="S6.T3.1.1.1.4.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.1.1.1.5">
<span class="ltx_text" id="S6.T3.1.1.1.5.1"></span><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.5.2"> <span class="ltx_text" id="S6.T3.1.1.1.5.2.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.1.1.1.5.2.1.1">
<span class="ltx_tr" id="S6.T3.1.1.1.5.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.1.1.1.5.2.1.1.1.1">Min Evidence</span></span>
<span class="ltx_tr" id="S6.T3.1.1.1.5.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.1.1.1.5.2.1.1.2.1">¿ 8192 (%)</span></span>
</span></span><span class="ltx_text" id="S6.T3.1.1.1.5.2.2"></span></span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.1">Train</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.2">66.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.3">47.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.4">19.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.5">1.42</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.3">
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.1">Valid</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.2">67.77</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.3">47.03</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.4">15.30</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.5">0.87</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.4.1">Test</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.4.2">68.22</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.4.3">46.02</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.4.4">13.71</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.4.5">0.75</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Percentage of answers with minimum evidence position outside a given context length in Qasper.</figcaption>
</figure>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">6.1.   Index Staleness</h3>
<div class="ltx_para ltx_noindent" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">To study the effect of index staleness, we experiment with an internal summarization dataset consisting of samples of up to 7k tokens using a model with a context length of 2048 and a generation limit of 700 tokens.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S6.T2" title="Table 2 ‣ 6. Ablations ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">2</span></a> presents the result of our experiments.
Although the generation length is still way under the 2048 limit, we can see a slight positive improvement in the performance, showcasing the usefulness of this simple addition with almost no cost.
Moreover, we believe the performance boost will increase as the generation length increases.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">6.2.   Contextual Information</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In the Qasper dataset, we have access to a set of evidence for each answer.
Hence, we can calculate the percentage of answers that all of their evidence falls out of the range of a specific context length.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S6.T3" title="Table 3 ‣ 6. Ablations ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">3</span></a> presents these numbers for different context lengths.
These numbers are consistent with the improvements in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S4.F3" title="Figure 3 ‣ 4.1. Datasets &amp; Tasks ‣ 4. Experimental Setup ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">3</span></a>, which showcase the validity of our models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This work presented a set of changes to adapt the Unlimiformer architecture to decoder-only models.
We evaluated these changes with a new set of experiments, showcasing their effectiveness, especially on summarization datasets.
We also identified a failure case that warrants further investigations in future studies.
We hope this work paves the way for the broader use of this architecture.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.   Limitations</h2>
<section class="ltx_paragraph" id="S8.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Query Bias</h4>
<div class="ltx_para ltx_noindent" id="S8.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S8.SS0.SSS0.Px1.p1.2">Since both Unlimiformer and our approach use a specific query vector to retrieve hidden states from the index, the retrieval process becomes biased on the query vector.
In Unlimiformer, this vector is <math alttext="h_{d}^{(-1)}" class="ltx_Math" display="inline" id="S8.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S8.SS0.SSS0.Px1.p1.1.m1.1a"><msubsup id="S8.SS0.SSS0.Px1.p1.1.m1.1.2" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.2.cmml"><mi id="S8.SS0.SSS0.Px1.p1.1.m1.1.2.2.2" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.2.2.2.cmml">h</mi><mi id="S8.SS0.SSS0.Px1.p1.1.m1.1.2.2.3" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.2.2.3.cmml">d</mi><mrow id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml"><mo id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml"><mo id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1a" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">−</mo><mn id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.2" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S8.SS0.SSS0.Px1.p1.1.m1.1.2.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S8.SS0.SSS0.Px1.p1.1.m1.1.2.1.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.2">superscript</csymbol><apply id="S8.SS0.SSS0.Px1.p1.1.m1.1.2.2.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S8.SS0.SSS0.Px1.p1.1.m1.1.2.2.1.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.2">subscript</csymbol><ci id="S8.SS0.SSS0.Px1.p1.1.m1.1.2.2.2.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.2.2.2">ℎ</ci><ci id="S8.SS0.SSS0.Px1.p1.1.m1.1.2.2.3.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.2.2.3">𝑑</ci></apply><apply id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1"><minus id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1"></minus><cn id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.2.cmml" type="integer" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px1.p1.1.m1.1c">h_{d}^{(-1)}</annotation><annotation encoding="application/x-llamapun" id="S8.SS0.SSS0.Px1.p1.1.m1.1d">italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>, which is calculated by attending to the generated tokens.
In our approach, this vector is <math alttext="h_{\text{CA}}^{(-1)}" class="ltx_Math" display="inline" id="S8.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S8.SS0.SSS0.Px1.p1.2.m2.1a"><msubsup id="S8.SS0.SSS0.Px1.p1.2.m2.1.2" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.2.cmml"><mi id="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.2" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.2.cmml">h</mi><mtext id="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.3" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.3a.cmml">CA</mtext><mrow id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml"><mo id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.2" stretchy="false" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml"><mo id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1a" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml">−</mo><mn id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.3" stretchy="false" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S8.SS0.SSS0.Px1.p1.2.m2.1.2.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.2"><csymbol cd="ambiguous" id="S8.SS0.SSS0.Px1.p1.2.m2.1.2.1.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.2">superscript</csymbol><apply id="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.2"><csymbol cd="ambiguous" id="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.1.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.2">subscript</csymbol><ci id="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.2.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.2">ℎ</ci><ci id="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.3a.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.3"><mtext id="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.3.cmml" mathsize="70%" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.2.2.3">CA</mtext></ci></apply><apply id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1"><minus id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1"></minus><cn id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2.cmml" type="integer" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px1.p1.2.m2.1c">h_{\text{CA}}^{(-1)}</annotation><annotation encoding="application/x-llamapun" id="S8.SS0.SSS0.Px1.p1.2.m2.1d">italic_h start_POSTSUBSCRIPT CA end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>, which is calculated by attending to the whole input, i.e., original context + generated tokens.
This dependence on the original context potentially reduces the expected performance gains when external indices are used.
Moreover, it could partially explain the lack of significant improvements in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S5.SS2" title="5.2. Free-Form Q&amp;A ‣ 5. Results ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S8.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Latency</h4>
<div class="ltx_para ltx_noindent" id="S8.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S8.SS0.SSS0.Px2.p1.1">The main setback of having many indices is the increase in latency.
To alleviate this problem, we could 1) use approximate indices and/or 2) use indices that operate on GPU to remove the CPU-GPU transfer overhead.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">9.   Ethical Considerations</h2>
<div class="ltx_para ltx_noindent" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">This paper does not have any ethical considerations</p>
</div>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">10.   Bibliographical References</h2>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography"></h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W05-0909" title="">METEOR: An automatic
metric for MT evaluation with improved correlation with human judgments</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or Summarization</em>, pages
65–72, Ann Arbor, Michigan. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et al. (2020)</span>
<span class="ltx_bibblock">
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2004.05150</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bertsch et al. (2023)</span>
<span class="ltx_bibblock">
Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R Gormley. 2023.

</span>
<span class="ltx_bibblock">Unlimiformer: Long-range transformers with unlimited length input.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2305.01625</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020a)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Advances in Neural Information Processing Systems</em>,
volume 33, pages 1877–1901. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020b)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al. 2020b.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in neural information processing systems</em>,
33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burtsev et al. (2020)</span>
<span class="ltx_bibblock">
Mikhail S Burtsev, Yuri Kuratov, Anton Peganov, and Grigory V Sapunov. 2020.

</span>
<span class="ltx_bibblock">Memory transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2006.11527</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choromanski et al. (2021)</span>
<span class="ltx_bibblock">
Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz
Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian
Weller. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Ua6zuk0WRH" title="">Rethinking
attention with performers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao (2023)</span>
<span class="ltx_bibblock">
Tri Dao. 2023.

</span>
<span class="ltx_bibblock">Flashattention-2: Faster attention with better parallelism and work
partitioning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2307.08691</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et al. (2022)</span>
<span class="ltx_bibblock">
Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=H4DqfPSibmx" title="">Flashattention:
Fast and memory-efficient exact attention with IO-awareness</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dasigi et al. (2021)</span>
<span class="ltx_bibblock">
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt
Gardner. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.365" title="">A dataset of
information-seeking questions and answers anchored in research papers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 4599–4610, Online. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1423" title="">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri et al. (2021)</span>
<span class="ltx_bibblock">
Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong,
Richard Socher, and Dragomir Radev. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00373" title="">SummEval:
Re-evaluating Summarization Evaluation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Transactions of the Association for Computational Linguistics</em>,
9:391–409.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2021)</span>
<span class="ltx_bibblock">
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.112" title="">Efficient
attentions for long document summarization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 1419–1436, Online. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutchins et al. (2022)</span>
<span class="ltx_bibblock">
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur.
2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=uloenYmLCAo" title="">Block-recurrent
transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivgi et al. (2023)</span>
<span class="ltx_bibblock">
Maor Ivgi, Uri Shaham, and Jonathan Berant. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00547" title="">Efficient long-text
understanding with short-text models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Transactions of the Association for Computational Linguistics</em>,
11:284–299.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kitaev et al. (2020)</span>
<span class="ltx_bibblock">
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=rkgNKkHtvB" title="">Reformer: The
efficient transformer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kočiský et al. (2018)</span>
<span class="ltx_bibblock">
Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00023" title="">The NarrativeQA
reading comprehension challenge</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Transactions of the Association for Computational Linguistics</em>,
6:317–328.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kryscinski et al. (2022)</span>
<span class="ltx_bibblock">
Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and
Dragomir Radev. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-emnlp.488" title="">BOOKSUM: A collection of datasets for long-form narrative summarization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2022</em>, pages 6536–6558, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.703" title="">BART:
Denoising sequence-to-sequence pre-training for natural language generation,
translation, and comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 7871–7880, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W04-1013" title="">ROUGE: A package for
automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Text Summarization Branches Out</em>, pages 74–81, Barcelona,
Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=SyxS0T4tvS" title="">Ro{bert}a:
A robustly optimized {bert} pretraining approach</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et al. (2023)</span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. 2023.

</span>
<span class="ltx_bibblock">The refinedweb dataset for falcon llm: outperforming curated corpora
with web data, and web data only.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2306.01116</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023)</span>
<span class="ltx_bibblock">
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi
Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. 2023.

</span>
<span class="ltx_bibblock">Rwkv: Reinventing rnns for the transformer era.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2305.13048</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poli et al. (2023)</span>
<span class="ltx_bibblock">
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen
Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. 2023.

</span>
<span class="ltx_bibblock">Hyena hierarchy: Towards larger convolutional language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2302.10866</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2022)</span>
<span class="ltx_bibblock">
Ofir Press, Noah Smith, and Mike Lewis. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=R8sQPpGCv0" title="">Train short, test
long: Attention with linear biases enables input length extrapolation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubin and Berant (2023)</span>
<span class="ltx_bibblock">
Ohad Rubin and Jonathan Berant. 2023.

</span>
<span class="ltx_bibblock">Long-range language modeling with self-retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2306.13421</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaham et al. (2023)</span>
<span class="ltx_bibblock">
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023.

</span>
<span class="ltx_bibblock">Zeroscrolls: A zero-shot benchmark for long text understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2305.14196</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2104.09864" title="">Roformer: Enhanced
transformer with rotary position embedding</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,
Vishrav Chaudhary, Xia Song, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.816" title="">A
length-extrapolatable transformer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14590–14604,
Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al. (2022)</span>
<span class="ltx_bibblock">
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3530811" title="">Efficient transformers: A
survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">ACM Comput. Surv.</em>, 55(6).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023)</span>
<span class="ltx_bibblock">
MosaicML NLP Team. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="www.mosaicml.com/blog/mpt-7b" title="">Introducing mpt-7b: A new
standard for open-source, commercially usable llms</a>.

</span>
<span class="ltx_bibblock">Accessed: 2023-10-05.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et al. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020.

</span>
<span class="ltx_bibblock">Linformer: Self-attention with linear complexity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2006.04768</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,
and Quoc V Le. 2019.

</span>
<span class="ltx_bibblock">Xlnet: Generalized autoregressive pretraining for language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Advances in neural information processing systems</em>, 32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaheer et al. (2020)</span>
<span class="ltx_bibblock">
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
et al. 2020.

</span>
<span class="ltx_bibblock">Big bird: Transformers for longer sequences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in neural information processing systems</em>,
33:17283–17297.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang* et al. (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav
Artzi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=SkeHuCVFDr" title="">Bertscore:
Evaluating text generation with bert</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">International Conference on Learning Representations</em>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation Comparison</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Although similar in using separate indices for each layer, we present additional modifications to the original methodology<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/abertsch72/unlimiformer" title="">https://github.com/abertsch72/unlimiformer</a></span></span></span>.
Specifically, 1) we introduce an update procedure for indices to avoid staleness, 2) we use a slightly more efficient chunk encoding approach, and 3) we introduce information fusion into the architecture and reformulate the attention calculations to be more comprehensive (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01637v1#S3.SS1" title="3.1. Cross-Attention ‣ 3. Methodology ‣ On The Adaptation of Unlimiformer for Decoder-Only Transformers"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
Moreover, to the best of our knowledge, no evaluation of their proposed methodology has been presented for decoder-only models, a shortcoming that our work aims to address using a new evaluation setting and new datasets.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Hyperparameters</h2>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Cross-Attention Layers (<math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.1.m1.1"><semantics id="A2.SS0.SSS0.Px1.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="A2.SS0.SSS0.Px1.1.m1.1.1" xref="A2.SS0.SSS0.Px1.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.1.m1.1c"><ci id="A2.SS0.SSS0.Px1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.1.m1.1d">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.1.m1.1e">caligraphic_L</annotation></semantics></math>)</h4>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.6">To find the best <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="A2.SS0.SSS0.Px1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="A2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.1.m1.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p1.1.m1.1d">caligraphic_L</annotation></semantics></math>, first, we find the highest-performing single-layer pattern.
Then, we expand that pattern by adding one more cross-attention layer before or after that layer, with varying distances, until no performance improvement can be seen over the validation set.
We continue this process up to three layers.
For example, if <math alttext="\mathcal{L}=\{16\}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="A2.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="A2.SS0.SSS0.Px1.p1.2.m2.1.2" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS0.SSS0.Px1.p1.2.m2.1.2.2" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.2.2.cmml">ℒ</mi><mo id="A2.SS0.SSS0.Px1.p1.2.m2.1.2.1" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.2.1.cmml">=</mo><mrow id="A2.SS0.SSS0.Px1.p1.2.m2.1.2.3.2" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.2.3.1.cmml"><mo id="A2.SS0.SSS0.Px1.p1.2.m2.1.2.3.2.1" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.2.3.1.cmml">{</mo><mn id="A2.SS0.SSS0.Px1.p1.2.m2.1.1" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">16</mn><mo id="A2.SS0.SSS0.Px1.p1.2.m2.1.2.3.2.2" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="A2.SS0.SSS0.Px1.p1.2.m2.1.2.cmml" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.2"><eq id="A2.SS0.SSS0.Px1.p1.2.m2.1.2.1.cmml" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.2.1"></eq><ci id="A2.SS0.SSS0.Px1.p1.2.m2.1.2.2.cmml" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.2.2">ℒ</ci><set id="A2.SS0.SSS0.Px1.p1.2.m2.1.2.3.1.cmml" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.2.3.2"><cn id="A2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1">16</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.2.m2.1c">\mathcal{L}=\{16\}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p1.2.m2.1d">caligraphic_L = { 16 }</annotation></semantics></math> is the highest-performing single-layer pattern, at the second step, we will try <math alttext="\mathcal{L}\in\{\{16,17\},\{15,16\},\{16,18\},\ldots\}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.3.m3.10"><semantics id="A2.SS0.SSS0.Px1.p1.3.m3.10a"><mrow id="A2.SS0.SSS0.Px1.p1.3.m3.10.10" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.5" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.5.cmml">ℒ</mi><mo id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.4" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.4.cmml">∈</mo><mrow id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.4.cmml"><mo id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.4" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.4.cmml">{</mo><mrow id="A2.SS0.SSS0.Px1.p1.3.m3.8.8.1.1.1.2" xref="A2.SS0.SSS0.Px1.p1.3.m3.8.8.1.1.1.1.cmml"><mo id="A2.SS0.SSS0.Px1.p1.3.m3.8.8.1.1.1.2.1" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.3.m3.8.8.1.1.1.1.cmml">{</mo><mn id="A2.SS0.SSS0.Px1.p1.3.m3.1.1" xref="A2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">16</mn><mo id="A2.SS0.SSS0.Px1.p1.3.m3.8.8.1.1.1.2.2" xref="A2.SS0.SSS0.Px1.p1.3.m3.8.8.1.1.1.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px1.p1.3.m3.2.2" xref="A2.SS0.SSS0.Px1.p1.3.m3.2.2.cmml">17</mn><mo id="A2.SS0.SSS0.Px1.p1.3.m3.8.8.1.1.1.2.3" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.3.m3.8.8.1.1.1.1.cmml">}</mo></mrow><mo id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.5" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.4.cmml">,</mo><mrow id="A2.SS0.SSS0.Px1.p1.3.m3.9.9.2.2.2.2" xref="A2.SS0.SSS0.Px1.p1.3.m3.9.9.2.2.2.1.cmml"><mo id="A2.SS0.SSS0.Px1.p1.3.m3.9.9.2.2.2.2.1" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.3.m3.9.9.2.2.2.1.cmml">{</mo><mn id="A2.SS0.SSS0.Px1.p1.3.m3.3.3" xref="A2.SS0.SSS0.Px1.p1.3.m3.3.3.cmml">15</mn><mo id="A2.SS0.SSS0.Px1.p1.3.m3.9.9.2.2.2.2.2" xref="A2.SS0.SSS0.Px1.p1.3.m3.9.9.2.2.2.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px1.p1.3.m3.4.4" xref="A2.SS0.SSS0.Px1.p1.3.m3.4.4.cmml">16</mn><mo id="A2.SS0.SSS0.Px1.p1.3.m3.9.9.2.2.2.2.3" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.3.m3.9.9.2.2.2.1.cmml">}</mo></mrow><mo id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.6" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.4.cmml">,</mo><mrow id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.3.2" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.3.1.cmml"><mo id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.3.2.1" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.3.1.cmml">{</mo><mn id="A2.SS0.SSS0.Px1.p1.3.m3.5.5" xref="A2.SS0.SSS0.Px1.p1.3.m3.5.5.cmml">16</mn><mo id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.3.2.2" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.3.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px1.p1.3.m3.6.6" xref="A2.SS0.SSS0.Px1.p1.3.m3.6.6.cmml">18</mn><mo id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.3.2.3" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.3.1.cmml">}</mo></mrow><mo id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.7" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.4.cmml">,</mo><mi id="A2.SS0.SSS0.Px1.p1.3.m3.7.7" mathvariant="normal" xref="A2.SS0.SSS0.Px1.p1.3.m3.7.7.cmml">…</mi><mo id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.8" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.3.m3.10b"><apply id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.cmml" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10"><in id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.4.cmml" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.4"></in><ci id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.5.cmml" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.5">ℒ</ci><set id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.4.cmml" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3"><set id="A2.SS0.SSS0.Px1.p1.3.m3.8.8.1.1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p1.3.m3.8.8.1.1.1.2"><cn id="A2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.3.m3.1.1">16</cn><cn id="A2.SS0.SSS0.Px1.p1.3.m3.2.2.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.3.m3.2.2">17</cn></set><set id="A2.SS0.SSS0.Px1.p1.3.m3.9.9.2.2.2.1.cmml" xref="A2.SS0.SSS0.Px1.p1.3.m3.9.9.2.2.2.2"><cn id="A2.SS0.SSS0.Px1.p1.3.m3.3.3.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.3.m3.3.3">15</cn><cn id="A2.SS0.SSS0.Px1.p1.3.m3.4.4.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.3.m3.4.4">16</cn></set><set id="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.3.1.cmml" xref="A2.SS0.SSS0.Px1.p1.3.m3.10.10.3.3.3.2"><cn id="A2.SS0.SSS0.Px1.p1.3.m3.5.5.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.3.m3.5.5">16</cn><cn id="A2.SS0.SSS0.Px1.p1.3.m3.6.6.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.3.m3.6.6">18</cn></set><ci id="A2.SS0.SSS0.Px1.p1.3.m3.7.7.cmml" xref="A2.SS0.SSS0.Px1.p1.3.m3.7.7">…</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.3.m3.10c">\mathcal{L}\in\{\{16,17\},\{15,16\},\{16,18\},\ldots\}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p1.3.m3.10d">caligraphic_L ∈ { { 16 , 17 } , { 15 , 16 } , { 16 , 18 } , … }</annotation></semantics></math>, and then at the third step, assuming <math alttext="\mathcal{L}=\{16,18\}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.4.m4.2"><semantics id="A2.SS0.SSS0.Px1.p1.4.m4.2a"><mrow id="A2.SS0.SSS0.Px1.p1.4.m4.2.3" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS0.SSS0.Px1.p1.4.m4.2.3.2" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3.2.cmml">ℒ</mi><mo id="A2.SS0.SSS0.Px1.p1.4.m4.2.3.1" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3.1.cmml">=</mo><mrow id="A2.SS0.SSS0.Px1.p1.4.m4.2.3.3.2" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3.3.1.cmml"><mo id="A2.SS0.SSS0.Px1.p1.4.m4.2.3.3.2.1" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">{</mo><mn id="A2.SS0.SSS0.Px1.p1.4.m4.1.1" xref="A2.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">16</mn><mo id="A2.SS0.SSS0.Px1.p1.4.m4.2.3.3.2.2" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px1.p1.4.m4.2.2" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.2.cmml">18</mn><mo id="A2.SS0.SSS0.Px1.p1.4.m4.2.3.3.2.3" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.4.m4.2b"><apply id="A2.SS0.SSS0.Px1.p1.4.m4.2.3.cmml" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3"><eq id="A2.SS0.SSS0.Px1.p1.4.m4.2.3.1.cmml" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3.1"></eq><ci id="A2.SS0.SSS0.Px1.p1.4.m4.2.3.2.cmml" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3.2">ℒ</ci><set id="A2.SS0.SSS0.Px1.p1.4.m4.2.3.3.1.cmml" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.3.3.2"><cn id="A2.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.4.m4.1.1">16</cn><cn id="A2.SS0.SSS0.Px1.p1.4.m4.2.2.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.4.m4.2.2">18</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.4.m4.2c">\mathcal{L}=\{16,18\}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p1.4.m4.2d">caligraphic_L = { 16 , 18 }</annotation></semantics></math> was the best-performing pattern, we will try <math alttext="\mathcal{L}\in\{\{14,16,18\},\{16,18,20\},\ldots\}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.5.m5.9"><semantics id="A2.SS0.SSS0.Px1.p1.5.m5.9a"><mrow id="A2.SS0.SSS0.Px1.p1.5.m5.9.9" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.4" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.4.cmml">ℒ</mi><mo id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.3" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.3.cmml">∈</mo><mrow id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.3.cmml"><mo id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.3" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.3.cmml">{</mo><mrow id="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.2" xref="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.1.cmml"><mo id="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.2.1" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.1.cmml">{</mo><mn id="A2.SS0.SSS0.Px1.p1.5.m5.1.1" xref="A2.SS0.SSS0.Px1.p1.5.m5.1.1.cmml">14</mn><mo id="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.2.2" xref="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px1.p1.5.m5.2.2" xref="A2.SS0.SSS0.Px1.p1.5.m5.2.2.cmml">16</mn><mo id="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.2.3" xref="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px1.p1.5.m5.3.3" xref="A2.SS0.SSS0.Px1.p1.5.m5.3.3.cmml">18</mn><mo id="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.2.4" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.1.cmml">}</mo></mrow><mo id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.4" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.3.cmml">,</mo><mrow id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.2" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.1.cmml"><mo id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.2.1" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.1.cmml">{</mo><mn id="A2.SS0.SSS0.Px1.p1.5.m5.4.4" xref="A2.SS0.SSS0.Px1.p1.5.m5.4.4.cmml">16</mn><mo id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.2.2" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px1.p1.5.m5.5.5" xref="A2.SS0.SSS0.Px1.p1.5.m5.5.5.cmml">18</mn><mo id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.2.3" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px1.p1.5.m5.6.6" xref="A2.SS0.SSS0.Px1.p1.5.m5.6.6.cmml">20</mn><mo id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.2.4" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.1.cmml">}</mo></mrow><mo id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.5" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.3.cmml">,</mo><mi id="A2.SS0.SSS0.Px1.p1.5.m5.7.7" mathvariant="normal" xref="A2.SS0.SSS0.Px1.p1.5.m5.7.7.cmml">…</mi><mo id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.6" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.5.m5.9b"><apply id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.cmml" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9"><in id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.3.cmml" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.3"></in><ci id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.4.cmml" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.4">ℒ</ci><set id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.3.cmml" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2"><set id="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p1.5.m5.8.8.1.1.1.2"><cn id="A2.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.5.m5.1.1">14</cn><cn id="A2.SS0.SSS0.Px1.p1.5.m5.2.2.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.5.m5.2.2">16</cn><cn id="A2.SS0.SSS0.Px1.p1.5.m5.3.3.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.5.m5.3.3">18</cn></set><set id="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.1.cmml" xref="A2.SS0.SSS0.Px1.p1.5.m5.9.9.2.2.2.2"><cn id="A2.SS0.SSS0.Px1.p1.5.m5.4.4.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.5.m5.4.4">16</cn><cn id="A2.SS0.SSS0.Px1.p1.5.m5.5.5.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.5.m5.5.5">18</cn><cn id="A2.SS0.SSS0.Px1.p1.5.m5.6.6.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.5.m5.6.6">20</cn></set><ci id="A2.SS0.SSS0.Px1.p1.5.m5.7.7.cmml" xref="A2.SS0.SSS0.Px1.p1.5.m5.7.7">…</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.5.m5.9c">\mathcal{L}\in\{\{14,16,18\},\{16,18,20\},\ldots\}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p1.5.m5.9d">caligraphic_L ∈ { { 14 , 16 , 18 } , { 16 , 18 , 20 } , … }</annotation></semantics></math>.
In most of our experiments, the best performance was achieved by <math alttext="\mathcal{L}=\{20,22,24\}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.6.m6.3"><semantics id="A2.SS0.SSS0.Px1.p1.6.m6.3a"><mrow id="A2.SS0.SSS0.Px1.p1.6.m6.3.4" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.2" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.2.cmml">ℒ</mi><mo id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.1" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.1.cmml">=</mo><mrow id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.2" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.1.cmml"><mo id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.2.1" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.1.cmml">{</mo><mn id="A2.SS0.SSS0.Px1.p1.6.m6.1.1" xref="A2.SS0.SSS0.Px1.p1.6.m6.1.1.cmml">20</mn><mo id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.2.2" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px1.p1.6.m6.2.2" xref="A2.SS0.SSS0.Px1.p1.6.m6.2.2.cmml">22</mn><mo id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.2.3" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px1.p1.6.m6.3.3" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.3.cmml">24</mn><mo id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.2.4" stretchy="false" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.6.m6.3b"><apply id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.cmml" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4"><eq id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.1.cmml" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.1"></eq><ci id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.2.cmml" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.2">ℒ</ci><set id="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.1.cmml" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.4.3.2"><cn id="A2.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.6.m6.1.1">20</cn><cn id="A2.SS0.SSS0.Px1.p1.6.m6.2.2.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.6.m6.2.2">22</cn><cn id="A2.SS0.SSS0.Px1.p1.6.m6.3.3.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.6.m6.3.3">24</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.6.m6.3c">\mathcal{L}=\{20,22,24\}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p1.6.m6.3d">caligraphic_L = { 20 , 22 , 24 }</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Retention Sizes (<math alttext="\alpha_{q},\beta_{q},\beta_{kv},\alpha_{kv}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.1.m1.4"><semantics id="A2.SS0.SSS0.Px2.1.m1.4b"><mrow id="A2.SS0.SSS0.Px2.1.m1.4.4.4" xref="A2.SS0.SSS0.Px2.1.m1.4.4.5.cmml"><msub id="A2.SS0.SSS0.Px2.1.m1.1.1.1.1" xref="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.cmml"><mi id="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.2" xref="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.2.cmml">α</mi><mi id="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.3" xref="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.3.cmml">q</mi></msub><mo id="A2.SS0.SSS0.Px2.1.m1.4.4.4.5" xref="A2.SS0.SSS0.Px2.1.m1.4.4.5.cmml">,</mo><msub id="A2.SS0.SSS0.Px2.1.m1.2.2.2.2" xref="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.cmml"><mi id="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.2" xref="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.2.cmml">β</mi><mi id="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.3" xref="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.3.cmml">q</mi></msub><mo id="A2.SS0.SSS0.Px2.1.m1.4.4.4.6" xref="A2.SS0.SSS0.Px2.1.m1.4.4.5.cmml">,</mo><msub id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.cmml"><mi id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.2" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.2.cmml">β</mi><mrow id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.cmml"><mi id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.2" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.2.cmml">k</mi><mo id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.1" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.3" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.3.cmml">v</mi></mrow></msub><mo id="A2.SS0.SSS0.Px2.1.m1.4.4.4.7" xref="A2.SS0.SSS0.Px2.1.m1.4.4.5.cmml">,</mo><msub id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.cmml"><mi id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.2" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.2.cmml">α</mi><mrow id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.cmml"><mi id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.2" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.2.cmml">k</mi><mo id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.1" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.3" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.3.cmml">v</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px2.1.m1.4c"><list id="A2.SS0.SSS0.Px2.1.m1.4.4.5.cmml" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4"><apply id="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.cmml" xref="A2.SS0.SSS0.Px2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.1.cmml" xref="A2.SS0.SSS0.Px2.1.m1.1.1.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.2.cmml" xref="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.2">𝛼</ci><ci id="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.3.cmml" xref="A2.SS0.SSS0.Px2.1.m1.1.1.1.1.3">𝑞</ci></apply><apply id="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.cmml" xref="A2.SS0.SSS0.Px2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.1.cmml" xref="A2.SS0.SSS0.Px2.1.m1.2.2.2.2">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.2.cmml" xref="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.2">𝛽</ci><ci id="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.3.cmml" xref="A2.SS0.SSS0.Px2.1.m1.2.2.2.2.3">𝑞</ci></apply><apply id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.cmml" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.1.cmml" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.2.cmml" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.2">𝛽</ci><apply id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.cmml" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3"><times id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.1.cmml" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.1"></times><ci id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.2.cmml" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.2">𝑘</ci><ci id="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.3.cmml" xref="A2.SS0.SSS0.Px2.1.m1.3.3.3.3.3.3">𝑣</ci></apply></apply><apply id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.cmml" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.1.cmml" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.2.cmml" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.2">𝛼</ci><apply id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.cmml" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3"><times id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.1.cmml" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.1"></times><ci id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.2.cmml" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.2">𝑘</ci><ci id="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.3.cmml" xref="A2.SS0.SSS0.Px2.1.m1.4.4.4.4.3.3">𝑣</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px2.1.m1.4d">\alpha_{q},\beta_{q},\beta_{kv},\alpha_{kv}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px2.1.m1.4e">italic_α start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_β start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_β start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT , italic_α start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT</annotation></semantics></math>)</h4>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.5">After tuning <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="A2.SS0.SSS0.Px2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A2.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px2.p1.1.m1.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px2.p1.1.m1.1d">caligraphic_L</annotation></semantics></math>, we tune these hyperparameters by first constraining them with <math alttext="\alpha_{q}+\beta_{q}=\beta_{kv}+\alpha_{kv}=S" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="A2.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="A2.SS0.SSS0.Px2.p1.2.m2.1.1" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mrow id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml"><msub id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.cmml"><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.2" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.2.cmml">α</mi><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.3" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.3.cmml">q</mi></msub><mo id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.1" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.1.cmml">+</mo><msub id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.cmml"><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.2" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.2.cmml">β</mi><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.3" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.3.cmml">q</mi></msub></mrow><mo id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">=</mo><mrow id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.cmml"><msub id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.cmml"><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.2" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.2.cmml">β</mi><mrow id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.cmml"><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.2" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.2.cmml">k</mi><mo id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.1" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.3" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.3.cmml">v</mi></mrow></msub><mo id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.1" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.1.cmml">+</mo><msub id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.cmml"><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.2" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.2.cmml">α</mi><mrow id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.cmml"><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.2" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.2.cmml">k</mi><mo id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.1" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.3" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.3.cmml">v</mi></mrow></msub></mrow><mo id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.5" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.5.cmml">=</mo><mi id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.6" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.6.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1"><and id="A2.SS0.SSS0.Px2.p1.2.m2.1.1a.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1"></and><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1b.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1"><eq id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.3"></eq><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2"><plus id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.1"></plus><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.1.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.2.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.2">𝛼</ci><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.3.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.3">𝑞</ci></apply><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.1.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.2.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.2">𝛽</ci><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.3.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.3">𝑞</ci></apply></apply><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4"><plus id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.1.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.1"></plus><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.1.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.2.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.2">𝛽</ci><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3"><times id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.1.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.1"></times><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.2.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.2">𝑘</ci><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.3.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.2.3.3">𝑣</ci></apply></apply><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.1.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.2.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.2">𝛼</ci><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3"><times id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.1.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.1"></times><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.2.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.2">𝑘</ci><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.3.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.3.3.3">𝑣</ci></apply></apply></apply></apply><apply id="A2.SS0.SSS0.Px2.p1.2.m2.1.1c.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1"><eq id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.5.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.5"></eq><share href="https://arxiv.org/html/2410.01637v1#A2.SS0.SSS0.Px2.p1.2.m2.1.1.4.cmml" id="A2.SS0.SSS0.Px2.p1.2.m2.1.1d.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1"></share><ci id="A2.SS0.SSS0.Px2.p1.2.m2.1.1.6.cmml" xref="A2.SS0.SSS0.Px2.p1.2.m2.1.1.6">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px2.p1.2.m2.1c">\alpha_{q}+\beta_{q}=\beta_{kv}+\alpha_{kv}=S</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px2.p1.2.m2.1d">italic_α start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT + italic_β start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = italic_β start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT + italic_α start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT = italic_S</annotation></semantics></math>, where <math alttext="S" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="A2.SS0.SSS0.Px2.p1.3.m3.1a"><mi id="A2.SS0.SSS0.Px2.p1.3.m3.1.1" xref="A2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px2.p1.3.m3.1b"><ci id="A2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="A2.SS0.SSS0.Px2.p1.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px2.p1.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px2.p1.3.m3.1d">italic_S</annotation></semantics></math> is the context length, and then doing a sweep on <math alttext="\alpha_{q},\alpha_{kv}\in\{0.05,0.1,0.15,\ldots,0.95,1\}\times S" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.4.m4.8"><semantics id="A2.SS0.SSS0.Px2.p1.4.m4.8a"><mrow id="A2.SS0.SSS0.Px2.p1.4.m4.8.8" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.cmml"><mrow id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.3.cmml"><msub id="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1" xref="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.cmml"><mi id="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.2" xref="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.2.cmml">α</mi><mi id="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.3" xref="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.3.cmml">q</mi></msub><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.3" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.3.cmml">,</mo><msub id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.cmml"><mi id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.2" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.2.cmml">α</mi><mrow id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.cmml"><mi id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.2" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.2.cmml">k</mi><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.1" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.3" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.3.cmml">v</mi></mrow></msub></mrow><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.3" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.3.cmml">∈</mo><mrow id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.cmml"><mrow id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.2" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.1.cmml"><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.2.1" stretchy="false" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.1.cmml">{</mo><mn id="A2.SS0.SSS0.Px2.p1.4.m4.1.1" xref="A2.SS0.SSS0.Px2.p1.4.m4.1.1.cmml">0.05</mn><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.2.2" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px2.p1.4.m4.2.2" xref="A2.SS0.SSS0.Px2.p1.4.m4.2.2.cmml">0.1</mn><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.2.3" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px2.p1.4.m4.3.3" xref="A2.SS0.SSS0.Px2.p1.4.m4.3.3.cmml">0.15</mn><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.2.4" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.1.cmml">,</mo><mi id="A2.SS0.SSS0.Px2.p1.4.m4.4.4" mathvariant="normal" xref="A2.SS0.SSS0.Px2.p1.4.m4.4.4.cmml">…</mi><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.2.5" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px2.p1.4.m4.5.5" xref="A2.SS0.SSS0.Px2.p1.4.m4.5.5.cmml">0.95</mn><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.2.6" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px2.p1.4.m4.6.6" xref="A2.SS0.SSS0.Px2.p1.4.m4.6.6.cmml">1</mn><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.2.7" rspace="0.055em" stretchy="false" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.1.cmml">}</mo></mrow><mo id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.1" rspace="0.222em" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.1.cmml">×</mo><mi id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.3" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.3.cmml">S</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px2.p1.4.m4.8b"><apply id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8"><in id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.3.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.3"></in><list id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.3.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2"><apply id="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.2.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.2">𝛼</ci><ci id="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.3.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.7.7.1.1.1.3">𝑞</ci></apply><apply id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.1.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.2.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.2">𝛼</ci><apply id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3"><times id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.1.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.1"></times><ci id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.2.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.2">𝑘</ci><ci id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.3.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.2.2.2.3.3">𝑣</ci></apply></apply></list><apply id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4"><times id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.1.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.1"></times><set id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.1.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.2.2"><cn id="A2.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" type="float" xref="A2.SS0.SSS0.Px2.p1.4.m4.1.1">0.05</cn><cn id="A2.SS0.SSS0.Px2.p1.4.m4.2.2.cmml" type="float" xref="A2.SS0.SSS0.Px2.p1.4.m4.2.2">0.1</cn><cn id="A2.SS0.SSS0.Px2.p1.4.m4.3.3.cmml" type="float" xref="A2.SS0.SSS0.Px2.p1.4.m4.3.3">0.15</cn><ci id="A2.SS0.SSS0.Px2.p1.4.m4.4.4.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.4.4">…</ci><cn id="A2.SS0.SSS0.Px2.p1.4.m4.5.5.cmml" type="float" xref="A2.SS0.SSS0.Px2.p1.4.m4.5.5">0.95</cn><cn id="A2.SS0.SSS0.Px2.p1.4.m4.6.6.cmml" type="integer" xref="A2.SS0.SSS0.Px2.p1.4.m4.6.6">1</cn></set><ci id="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.3.cmml" xref="A2.SS0.SSS0.Px2.p1.4.m4.8.8.4.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px2.p1.4.m4.8c">\alpha_{q},\alpha_{kv}\in\{0.05,0.1,0.15,\ldots,0.95,1\}\times S</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px2.p1.4.m4.8d">italic_α start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_α start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT ∈ { 0.05 , 0.1 , 0.15 , … , 0.95 , 1 } × italic_S</annotation></semantics></math>.
In most of our experiments, the best performance was achieved by <math alttext="\alpha_{q},\alpha_{kv}=0.4\times S" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.5.m5.2"><semantics id="A2.SS0.SSS0.Px2.p1.5.m5.2a"><mrow id="A2.SS0.SSS0.Px2.p1.5.m5.2.2" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.cmml"><mrow id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.3.cmml"><msub id="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1" xref="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.cmml"><mi id="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.2" xref="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.cmml">α</mi><mi id="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.3" xref="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.3.cmml">q</mi></msub><mo id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.3" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.3.cmml">,</mo><msub id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.cmml"><mi id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.2" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.cmml">α</mi><mrow id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.cmml"><mi id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.2" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.2.cmml">k</mi><mo id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.1" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.3" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.3.cmml">v</mi></mrow></msub></mrow><mo id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.3" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.3.cmml">=</mo><mrow id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.cmml"><mn id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.2" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.2.cmml">0.4</mn><mo id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.1" lspace="0.222em" rspace="0.222em" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.1.cmml">×</mo><mi id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.3" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.3.cmml">S</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px2.p1.5.m5.2b"><apply id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2"><eq id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.3.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.3"></eq><list id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.3.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2"><apply id="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.2">𝛼</ci><ci id="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.3.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.1.1.1.1.1.3">𝑞</ci></apply><apply id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.1.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.2">𝛼</ci><apply id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3"><times id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.1.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.1"></times><ci id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.2.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.2">𝑘</ci><ci id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.3.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.3">𝑣</ci></apply></apply></list><apply id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4"><times id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.1.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.1"></times><cn id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.2.cmml" type="float" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.2">0.4</cn><ci id="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.3.cmml" xref="A2.SS0.SSS0.Px2.p1.5.m5.2.2.4.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px2.p1.5.m5.2c">\alpha_{q},\alpha_{kv}=0.4\times S</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px2.p1.5.m5.2d">italic_α start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_α start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT = 0.4 × italic_S</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p2.1"></p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  2 14:58:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
