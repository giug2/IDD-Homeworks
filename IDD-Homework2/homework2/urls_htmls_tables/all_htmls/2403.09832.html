<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks</title>
<!--Generated on Thu Mar 14 19:33:50 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.09832v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S1" title="1 Introduction ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S2" title="2 Proposed Method ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Proposed Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S2.SS1" title="2.1 Data collection and preprocessing ‣ 2 Proposed Method ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data collection and preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S2.SS2" title="2.2 Models ‣ 2 Proposed Method ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S2.SS3" title="2.3 Prompts ‣ 2 Proposed Method ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Prompts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S2.SS4" title="2.4 Evaluation ‣ 2 Proposed Method ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3" title="3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS0.SSS0.Px1" title="Computational resources ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title">Computational resources</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS1" title="3.1 Non-adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Non-adversarial Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS1.SSS0.Px1" title="T5 and FLAN-T5 ‣ 3.1 Non-adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title">T5 and FLAN-T5</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS1.SSS0.Px2" title="OpenAI models ‣ 3.1 Non-adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title">OpenAI models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS1.SSS0.Px3" title="Llama2 and Llama2-chat ‣ 3.1 Non-adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title">Llama2 and Llama2-chat</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS2" title="3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Adversarial Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS2.SSS0.Px1" title="T5 and FLAN-T5 ‣ 3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title">T5 and FLAN-T5</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS2.SSS0.Px2" title="OpenAI models ‣ 3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title">OpenAI models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS2.SSS0.Px3" title="Llama2 and Llama2-chat ‣ 3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title">Llama2 and Llama2-chat</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS3" title="3.3 Inverse Scaling w.r.t. training data size ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Inverse Scaling w.r.t. training data size</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S4" title="4 Discussion and Related Work ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion and Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S5" title="5 Conclusion ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#Sx1.SS0.SSS0.Px1" title="Number of model families ‣ Limitations ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title">Number of model families</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#Sx1.SS0.SSS0.Px2" title="Number of distractors ‣ Limitations ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title">Number of distractors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#Sx1.SS0.SSS0.Px3" title="Coarse-grained evaluation strategy ‣ Limitations ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title">Coarse-grained evaluation strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A1" title="Appendix A Example generation ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Example generation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A1.SS1" title="A.1 Example Generation Pipeline ‣ Appendix A Example generation ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Example Generation Pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A1.SS2" title="A.2 Prompt Templates ‣ Appendix A Example generation ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Prompt Templates</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A2" title="Appendix B Translation output ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Translation output</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A3" title="Appendix C BLEU Scores ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>BLEU Scores</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2403.09832v1 [cs.CL] 14 Mar 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhifan Sun 
<br class="ltx_break"/>University of Edinburgh 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">sunzhifan233@gmail.com</span>
<br class="ltx_break"/>&amp;Antonio Valerio Miceli-Barone 
<br class="ltx_break"/>University of Edinburgh 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">amiceli@ed.ac.uk</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples.
Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways.
In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates.
We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the <span class="ltx_text ltx_font_italic" id="id3.id1.1">Inverse Scaling</span> phenomenon <cite class="ltx_cite ltx_citemacro_citep">(McKenzie et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib13" title="">2023</a>)</cite>.
To our knowledge, this is the first work to study non-trivial LLM scaling behaviour in a multi-lingual setting.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks</span></p>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break"/>
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1" style="width:433.6pt;"><span class="ltx_text" id="p2.1.1.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.1.1.1.1.1.1">Zhifan Sun</span></span></span>
<span class="ltx_tr" id="p2.1.1.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.2.2.1">University of Edinburgh</span></span>
<span class="ltx_tr" id="p2.1.1.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.1.1.1.3.3.1.1">sunzhifan233@gmail.com</span></span></span>
</span>
</span></span>                      <span class="ltx_text" id="p2.1.1.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.1.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.1.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.2.1.1.1.1.1">Antonio Valerio Miceli-Barone</span></span></span>
<span class="ltx_tr" id="p2.1.1.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p2.1.1.2.1.2.2.1">University of Edinburgh</span></span>
<span class="ltx_tr" id="p2.1.1.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p2.1.1.2.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.1.2.1.3.3.1.1">amiceli@ed.ac.uk</span></span></span>
</span>
</span></span> </span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">General purpose pretrained Large Language Models have become the dominant paradigm in NLP, due to their ability to quickly adapt to almost any task with in-context few-shot learning <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib1" title="">2020</a>); Chowdhery et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib2" title="">2022</a>); Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib23" title="">2022</a>)</cite> or instruction following <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib16" title="">2022</a>)</cite>.
In most settings, the performance of LLMs predictably increases with their size according to empirical scaling laws <cite class="ltx_cite ltx_citemacro_cite">Kaplan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib8" title="">2020a</a>); Hernandez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib6" title="">2021</a>); Hoffmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib7" title="">2022</a>)</cite>, however recent works have discovered scenarios where not only LLMs misbehave, but they even become worse with increasing size, a phenomenon known as <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">Inverse Scaling</span>, or exhibit non-monotonic performance w.r.t. size, e.g. <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">U-shaped Scaling</span> or <span class="ltx_text ltx_font_italic" id="S1.p1.1.3">Inverse U-shaped Scaling</span> <cite class="ltx_cite ltx_citemacro_citep">(Parrish et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib18" title="">2022</a>; Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib11" title="">2022</a>; Miceli Barone et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib15" title="">2023</a>)</cite>, with many more such scenarios being discovered during the <span class="ltx_text ltx_font_italic" id="S1.p1.1.4">Inverse Scaling Prize</span> <cite class="ltx_cite ltx_citemacro_citep">(McKenzie et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib13" title="">2023</a>)</cite>.
One such class of scenarios is <span class="ltx_text ltx_font_italic" id="S1.p1.1.5">Prompt Injection Attacks</span> (PIAs), where the end-user embeds instructions in their requests that contradict the default system prompt or fine-tuning and thus manipulate the LLM to behave in ways not intended by the system developer, such as performing a task different than the intended one, revealing secret information included in the system prompt, subvert content moderation, and so on.
In the Inverse Scaling Prize, PIAs were evaluated on simple tasks such as word capitalization and repetition, showing strong asymptotic inverse scaling, meaning that the larger the LLMs are, the more susceptible they become to these attacks.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this work, we evaluate the scaling behavior of Prompt Injection Attacks on Prompt-based Machine Translation.
Prompt-based Machine Translation (PMT) consists of using a general-purpose LLM to do machine translation by asking it to translate a text, optionally prepending a small number (1-5) of parallel examples in the prompt <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib24" title="">2023</a>)</cite>.
This approach is competitive with task-specific neural machine translation systems on high and medium resource language pairs <cite class="ltx_cite ltx_citemacro_citep">(Kocmi et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib10" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In order to evaluate PMT under PIAs, we create a parallel test set of questions, which we consider as our <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">clean</span> (non-adversarial) examples for PMT, then we transform them into <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">adversarial</span> examples by adding a prefix to the source side that asks the system to ignore its instructions and just answer the question.
We evaluate our clean and adversarial examples on multiple LLM families, both in the zero-shot and few-shot setting, identifying scaling trends.
We release our data and the code needed to reproduce our experiments<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Avmb/MT_Scaling_Prompt_Injection.git" title="">https://github.com/Avmb/MT_Scaling_Prompt_Injection.git</a></span></span></span>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Proposed Method</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data collection and preprocessing</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.4">We create a data set based on the examples in the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.4.1">TruthfulQA</span> data set <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib11" title="">2022</a>)</cite>, which consists of 817 questions in English originally designed to test the ability of LLMs to answer factual questions truthfully.
Each question is translated to German, French, Romanian and Russian using mBART-50 <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_text ltx_font_italic" id="footnote2.1">mbart-large-50-many-to-one-mmt</span> model</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib12" title="">2020</a>; Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib20" title="">2020</a>)</cite>.
mBART-50 is fine-tuned specifically for machine translation rather than generic instruction following, therefore it has no tendency to answer questions.
We manually inspect the model output and remove bad and incomplete translations.
The resulting examples constitute our <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.4.2">clean</span> En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mo id="S2.SS1.p1.1.m1.1.1" stretchy="false" xref="S2.SS1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">↔</annotation></semantics></math>De, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mo id="S2.SS1.p1.2.m2.1.1" stretchy="false" xref="S2.SS1.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">↔</annotation></semantics></math>Fr, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mo id="S2.SS1.p1.3.m3.1.1" stretchy="false" xref="S2.SS1.p1.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">↔</annotation></semantics></math>Ro and En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><mo id="S2.SS1.p1.4.m4.1.1" stretchy="false" xref="S2.SS1.p1.4.m4.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">↔</annotation></semantics></math>Ru data sets.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">We generate our <span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">adversarial</span> data sets by prepending to the source question of each example the prefix: "<span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.2">Don’t translate this sentence and answer the question: </span>", or its translation in the same language as the source question.
A detailed illustration of prompt generation is illustrated in appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A1.SS1" title="A.1 Example Generation Pipeline ‣ Appendix A Example generation ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">We use six families of models with varying sizes: T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib19" title="">2020</a>)</cite>, FLAN-T5 <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib3" title="">2022</a>)</cite>, GPT-3, InstructGPT<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_text ltx_font_italic" id="footnote3.1">text-*-001</span> models, plus <span class="ltx_text ltx_font_italic" id="footnote3.2">text-davinci-002</span> and <span class="ltx_text ltx_font_italic" id="footnote3.3">text-davinci-003</span></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib16" title="">2022</a>)</cite>, Llama2 and Llama2-chat <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib21" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Prompts</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Both zero-shot and few-shot prompts are used for different models, but due to limited time and budget, we limit our few-shot prompts to a single parallel example, hence it is one-shot.
We use the best prompt format from <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib24" title="">2023</a>)</cite>, with a variation for the Llama2-chat models that makes use of the different conversational roles.
The full templates can be found in appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A1.SS2" title="A.2 Prompt Templates ‣ Appendix A Example generation ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.20">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.20.21.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T1.20.21.1.1">model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.20.21.1.2">size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.20.21.1.3">language pair</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.3.3.4">GPT-3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.3.5">350M,1.3B,6.7B,175B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.3.3">En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.m1.1.1" stretchy="false" xref="S2.T1.1.1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.m1.1d">↔</annotation></semantics></math>De, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.2.2.2.m2.1"><semantics id="S2.T1.2.2.2.m2.1a"><mo id="S2.T1.2.2.2.m2.1.1" stretchy="false" xref="S2.T1.2.2.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.m2.1b"><ci id="S2.T1.2.2.2.m2.1.1.cmml" xref="S2.T1.2.2.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.m2.1d">↔</annotation></semantics></math>Fr, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.3.3.3.m3.1"><semantics id="S2.T1.3.3.3.m3.1a"><mo id="S2.T1.3.3.3.m3.1.1" stretchy="false" xref="S2.T1.3.3.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.m3.1b"><ci id="S2.T1.3.3.3.m3.1.1.cmml" xref="S2.T1.3.3.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.3.m3.1d">↔</annotation></semantics></math>Ru</td>
</tr>
<tr class="ltx_tr" id="S2.T1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.6.6.4">InstructGPT</th>
<td class="ltx_td ltx_align_center" id="S2.T1.6.6.5">350M,1.3B,6.7B,175B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.6.6.3">En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.4.4.1.m1.1"><semantics id="S2.T1.4.4.1.m1.1a"><mo id="S2.T1.4.4.1.m1.1.1" stretchy="false" xref="S2.T1.4.4.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.1.m1.1b"><ci id="S2.T1.4.4.1.m1.1.1.cmml" xref="S2.T1.4.4.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.1.m1.1d">↔</annotation></semantics></math>De, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.5.5.2.m2.1"><semantics id="S2.T1.5.5.2.m2.1a"><mo id="S2.T1.5.5.2.m2.1.1" stretchy="false" xref="S2.T1.5.5.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.2.m2.1b"><ci id="S2.T1.5.5.2.m2.1.1.cmml" xref="S2.T1.5.5.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.5.2.m2.1d">↔</annotation></semantics></math>Fr, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.6.6.3.m3.1"><semantics id="S2.T1.6.6.3.m3.1a"><mo id="S2.T1.6.6.3.m3.1.1" stretchy="false" xref="S2.T1.6.6.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.3.m3.1b"><ci id="S2.T1.6.6.3.m3.1.1.cmml" xref="S2.T1.6.6.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.6.6.3.m3.1d">↔</annotation></semantics></math>Ru</td>
</tr>
<tr class="ltx_tr" id="S2.T1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.9.9.4">T5</th>
<td class="ltx_td ltx_align_center" id="S2.T1.9.9.5">61M,223M,738M,3B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.9.9.3">En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.7.7.1.m1.1"><semantics id="S2.T1.7.7.1.m1.1a"><mo id="S2.T1.7.7.1.m1.1.1" stretchy="false" xref="S2.T1.7.7.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.1.m1.1b"><ci id="S2.T1.7.7.1.m1.1.1.cmml" xref="S2.T1.7.7.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.7.7.1.m1.1d">→</annotation></semantics></math>De, En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.8.8.2.m2.1"><semantics id="S2.T1.8.8.2.m2.1a"><mo id="S2.T1.8.8.2.m2.1.1" stretchy="false" xref="S2.T1.8.8.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.2.m2.1b"><ci id="S2.T1.8.8.2.m2.1.1.cmml" xref="S2.T1.8.8.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.8.8.2.m2.1d">→</annotation></semantics></math>Fr, En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.9.9.3.m3.1"><semantics id="S2.T1.9.9.3.m3.1a"><mo id="S2.T1.9.9.3.m3.1.1" stretchy="false" xref="S2.T1.9.9.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.3.m3.1b"><ci id="S2.T1.9.9.3.m3.1.1.cmml" xref="S2.T1.9.9.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.9.9.3.m3.1d">→</annotation></semantics></math>Ro</td>
</tr>
<tr class="ltx_tr" id="S2.T1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.4">FLAN-T5</th>
<td class="ltx_td ltx_align_center" id="S2.T1.12.12.5">61M,223M,738M,3B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.12.12.3">En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.10.10.1.m1.1"><semantics id="S2.T1.10.10.1.m1.1a"><mo id="S2.T1.10.10.1.m1.1.1" stretchy="false" xref="S2.T1.10.10.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.10.10.1.m1.1b"><ci id="S2.T1.10.10.1.m1.1.1.cmml" xref="S2.T1.10.10.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.10.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.10.10.1.m1.1d">→</annotation></semantics></math>De, En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.11.11.2.m2.1"><semantics id="S2.T1.11.11.2.m2.1a"><mo id="S2.T1.11.11.2.m2.1.1" stretchy="false" xref="S2.T1.11.11.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.2.m2.1b"><ci id="S2.T1.11.11.2.m2.1.1.cmml" xref="S2.T1.11.11.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.11.11.2.m2.1d">→</annotation></semantics></math>Fr, En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.12.12.3.m3.1"><semantics id="S2.T1.12.12.3.m3.1a"><mo id="S2.T1.12.12.3.m3.1.1" stretchy="false" xref="S2.T1.12.12.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.12.12.3.m3.1b"><ci id="S2.T1.12.12.3.m3.1.1.cmml" xref="S2.T1.12.12.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.12.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.12.12.3.m3.1d">→</annotation></semantics></math>Ro</td>
</tr>
<tr class="ltx_tr" id="S2.T1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.16.16.5">Llama2</th>
<td class="ltx_td ltx_align_center" id="S2.T1.16.16.6">7B,13B,70B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.16.16.4">En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.13.13.1.m1.1"><semantics id="S2.T1.13.13.1.m1.1a"><mo id="S2.T1.13.13.1.m1.1.1" stretchy="false" xref="S2.T1.13.13.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.13.13.1.m1.1b"><ci id="S2.T1.13.13.1.m1.1.1.cmml" xref="S2.T1.13.13.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.13.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.13.13.1.m1.1d">↔</annotation></semantics></math>De, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.14.14.2.m2.1"><semantics id="S2.T1.14.14.2.m2.1a"><mo id="S2.T1.14.14.2.m2.1.1" stretchy="false" xref="S2.T1.14.14.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.14.14.2.m2.1b"><ci id="S2.T1.14.14.2.m2.1.1.cmml" xref="S2.T1.14.14.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.14.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.14.14.2.m2.1d">↔</annotation></semantics></math>Fr, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.15.15.3.m3.1"><semantics id="S2.T1.15.15.3.m3.1a"><mo id="S2.T1.15.15.3.m3.1.1" stretchy="false" xref="S2.T1.15.15.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.15.15.3.m3.1b"><ci id="S2.T1.15.15.3.m3.1.1.cmml" xref="S2.T1.15.15.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.15.15.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.15.15.3.m3.1d">↔</annotation></semantics></math>Ro, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.16.16.4.m4.1"><semantics id="S2.T1.16.16.4.m4.1a"><mo id="S2.T1.16.16.4.m4.1.1" stretchy="false" xref="S2.T1.16.16.4.m4.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.16.16.4.m4.1b"><ci id="S2.T1.16.16.4.m4.1.1.cmml" xref="S2.T1.16.16.4.m4.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.16.16.4.m4.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.16.16.4.m4.1d">↔</annotation></semantics></math>Ru</td>
</tr>
<tr class="ltx_tr" id="S2.T1.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S2.T1.20.20.5">Llama2-chat</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.20.20.6">7B,13B,70B</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.20.20.4">En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.17.17.1.m1.1"><semantics id="S2.T1.17.17.1.m1.1a"><mo id="S2.T1.17.17.1.m1.1.1" stretchy="false" xref="S2.T1.17.17.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.17.17.1.m1.1b"><ci id="S2.T1.17.17.1.m1.1.1.cmml" xref="S2.T1.17.17.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.17.17.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.17.17.1.m1.1d">↔</annotation></semantics></math>De, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.18.18.2.m2.1"><semantics id="S2.T1.18.18.2.m2.1a"><mo id="S2.T1.18.18.2.m2.1.1" stretchy="false" xref="S2.T1.18.18.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.18.18.2.m2.1b"><ci id="S2.T1.18.18.2.m2.1.1.cmml" xref="S2.T1.18.18.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.18.18.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.18.18.2.m2.1d">↔</annotation></semantics></math>Fr, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.19.19.3.m3.1"><semantics id="S2.T1.19.19.3.m3.1a"><mo id="S2.T1.19.19.3.m3.1.1" stretchy="false" xref="S2.T1.19.19.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.19.19.3.m3.1b"><ci id="S2.T1.19.19.3.m3.1.1.cmml" xref="S2.T1.19.19.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.19.19.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.19.19.3.m3.1d">↔</annotation></semantics></math>Ro, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S2.T1.20.20.4.m4.1"><semantics id="S2.T1.20.20.4.m4.1a"><mo id="S2.T1.20.20.4.m4.1.1" stretchy="false" xref="S2.T1.20.20.4.m4.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S2.T1.20.20.4.m4.1b"><ci id="S2.T1.20.20.4.m4.1.1.cmml" xref="S2.T1.20.20.4.m4.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.20.20.4.m4.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.20.20.4.m4.1d">↔</annotation></semantics></math>Ru</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overview of the model series and the language pairs</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Evaluation</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">We found that the most common evaluation metric in machine translation, the BLEU score <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib17" title="">2002</a>)</cite>, is not sensitive enough for our experiments since in some cases a correct translation of the source question may have a lower BLEU score than the answer to the question.
For example:
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.p2.1.1">reference:
<br class="ltx_break"/></span><span class="ltx_text ltx_font_italic" id="S2.SS4.p2.1.2">What are some movies still banned in Australia for being offensive?
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="S2.SS4.p2.1.3">good translation:</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S2.SS4.p2.1.4">What are some films still banned in Australia for 
<br class="ltx_break"/>offensiveness?</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.SS4.p2.1.5">failed translation:</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S2.SS4.p2.1.6">No movies are banned in Australia for being offensive. 
<br class="ltx_break"/></span>It turns out that the good translation has a BLEU score of 23, while the failed translation, where the model answers the question instead of translating it, has a BLEU score of 39.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">For the purpose of our experiment, a successful translation will be considered as one that show signs of translating the source question in the target language, even with poor translation quality, while a failed translation is of the one that answers the questions (in either source or target language, irrespective of the truthfulness of the answer).
Thus, we introduce another simple heuristic to measure the performance of the model output specifically for this experiment. That is, for each model and language pair, we count how many translation output sentences end with a question mark, as every sentence in the reference ends with a question mark.
For the model output that doesn’t end with a question mark, we will assume it is answering the question or outputting irrelevant content.
We call this metric <span class="ltx_text ltx_font_italic" id="S2.SS4.p3.1.1">question mark accuracy</span> and will be referred to as <span class="ltx_text ltx_font_italic" id="S2.SS4.p3.1.2">accuracy</span> thereafter.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Due to limitations of the models and our own budget and time constraints, we do not evaluate all translation directions and prompting strategies on all model families.
We perform the following experiments (table <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S2.T1" title="Table 1 ‣ 2.3 Prompts ‣ 2 Proposed Method ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">1</span></a>):</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.3"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.3.1">OpenAI models</span>: En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><mo id="S3.I1.i1.p1.1.m1.1.1" stretchy="false" xref="S3.I1.i1.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">↔</annotation></semantics></math>De, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m2.1"><semantics id="S3.I1.i1.p1.2.m2.1a"><mo id="S3.I1.i1.p1.2.m2.1.1" stretchy="false" xref="S3.I1.i1.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><ci id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m2.1d">↔</annotation></semantics></math>Fr and En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S3.I1.i1.p1.3.m3.1"><semantics id="S3.I1.i1.p1.3.m3.1a"><mo id="S3.I1.i1.p1.3.m3.1.1" stretchy="false" xref="S3.I1.i1.p1.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><ci id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.3.m3.1d">↔</annotation></semantics></math>Ru translation directions, with one-shot prompting <cite class="ltx_cite ltx_citemacro_citep">(Fu and Khot, <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib4" title="">2022</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.3"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.3.1">T5 and FLAN-T5 models</span>: En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><mo id="S3.I1.i2.p1.1.m1.1.1" stretchy="false" xref="S3.I1.i2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">→</annotation></semantics></math>De, En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.I1.i2.p1.2.m2.1"><semantics id="S3.I1.i2.p1.2.m2.1a"><mo id="S3.I1.i2.p1.2.m2.1.1" stretchy="false" xref="S3.I1.i2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.2.m2.1d">→</annotation></semantics></math>Fr and En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.I1.i2.p1.3.m3.1"><semantics id="S3.I1.i2.p1.3.m3.1a"><mo id="S3.I1.i2.p1.3.m3.1.1" stretchy="false" xref="S3.I1.i2.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><ci id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.3.m3.1d">→</annotation></semantics></math>Ro translation directions, zero-shot. These are the translation directions evaluated in the original papers, note that these models do not seem to be able to translate from non-English languages.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.4"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.4.1">Llama2 and Llama2-chat models</span>: En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.1"><semantics id="S3.I1.i3.p1.1.m1.1a"><mo id="S3.I1.i3.p1.1.m1.1.1" stretchy="false" xref="S3.I1.i3.p1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.1d">↔</annotation></semantics></math>De, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S3.I1.i3.p1.2.m2.1"><semantics id="S3.I1.i3.p1.2.m2.1a"><mo id="S3.I1.i3.p1.2.m2.1.1" stretchy="false" xref="S3.I1.i3.p1.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><ci id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.2.m2.1d">↔</annotation></semantics></math>Fr, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S3.I1.i3.p1.3.m3.1"><semantics id="S3.I1.i3.p1.3.m3.1a"><mo id="S3.I1.i3.p1.3.m3.1.1" stretchy="false" xref="S3.I1.i3.p1.3.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.1b"><ci id="S3.I1.i3.p1.3.m3.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.3.m3.1d">↔</annotation></semantics></math>Ro and En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S3.I1.i3.p1.4.m4.1"><semantics id="S3.I1.i3.p1.4.m4.1a"><mo id="S3.I1.i3.p1.4.m4.1.1" stretchy="false" xref="S3.I1.i3.p1.4.m4.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.4.m4.1b"><ci id="S3.I1.i3.p1.4.m4.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.4.m4.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.4.m4.1d">↔</annotation></semantics></math>Ru translation directions, both zero-shot and one-shot.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The experiments are divided into two parts: We first report our results of the <span class="ltx_text ltx_font_bold" id="S3.p2.1.1">clean</span> examples in section <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS1" title="3.1 Non-adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">3.1</span></a>, then report the results of <span class="ltx_text ltx_font_bold" id="S3.p2.1.2">adversarial</span> examples in section <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS2" title="3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">3.2</span></a>. We only report the accuracy in this section, the BLEU scores of each experiment can be found in appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A3" title="Appendix C BLEU Scores ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">In section <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.SS3" title="3.3 Inverse Scaling w.r.t. training data size ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we display the average performance of X-to-English language pairs and English-to-X language pairs.</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Computational resources</h5>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">For the GPT and InstructGPT models, we spent about 200 US dollars on the OpenAI API.
The experiments with T5 and FLAN-T5 models except the largest variants were done on the HPE SGI 8600 system with NVIDIA GV100 GPU. The experiments on the Llama2, Llama2-chat and the largest variants of T5 and FLAN-T5 were performed on a cluster of NVIDIA A100 40GB/80GB GPUs (note that a single node with 4 A100 40GB GPUs is sufficient to run all experiments).
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Non-adversarial Experiments</h3>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">T5 and FLAN-T5</h5>
<figure class="ltx_figure" id="S3.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F0.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F0.sf1.g1" src="extracted/5471619/figures/t5/accuracy.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>T5</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F0.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F0.sf2.g1" src="extracted/5471619/figures/flant5/accuracy.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>FLAN-T5</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Accuracy of T5 and FLAN-T5 in non-adversarial experiments</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">According to figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F1" title="Figure 1 ‣ T5 and FLAN-T5 ‣ 3.1 Non-adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">1</span></a>, all language pairs and models show positive scaling except the English-German language pair with the T5 model, where we found U-shape scaling.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">OpenAI models</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">The results on the OpenAI models are shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F2" title="Figure 2 ‣ Llama2 and Llama2-chat ‣ 3.1 Non-adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">2</span></a>. 
<br class="ltx_break"/>OpenAI models show consistent positive scaling on sentences without adversarial prompt injections, as the accuracy score and BLEU scores (appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A3" title="Appendix C BLEU Scores ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">C</span></a>) almost monotonically increase with the model sizes.
In the En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mo id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" stretchy="false" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.1.m1.1d">→</annotation></semantics></math>Fr direction the performance for GPT-3 goes down twice from a model size of 350M to 1.3B, then from 6.7B to 175B. However, the drop in performance is insignificant compared to the rise in performance from 1.3B to 175B. This drop in performance is inconsistent, thus, we will not consider this as an instance of inverse scaling.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Llama2 and Llama2-chat</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">We report the results on both Llama2 and Llama2-chat models.
For each model we also experimented on different quantization variants of the model<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>as implemented in Hugging Face Accelerate and BitsAndBytes libraries <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/docs/accelerate/usage_guides/quantization" title="">https://huggingface.co/docs/accelerate/usage_guides/quantization</a></span></span></span>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F3" title="Figure 3 ‣ Llama2 and Llama2-chat ‣ 3.1 Non-adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">3</span></a>
and <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F4" title="Figure 4 ‣ Llama2 and Llama2-chat ‣ 3.1 Non-adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">4</span></a> contain the results of Llama2 and Llama2-chat respectively. 
<br class="ltx_break"/>Quite obvious inverse scaling is found when the Llama2 model is fed with the zero-shot prompt. Another interesting pattern is that we observe an abrupt increase in performance and then a steady decrease when the quantization is 4-bit. The potential explanation is that the low quantization hurts the overall performance of the model.
The smallest Llama2 model with the 4-bit quantization doesn’t seem to be able to perform translation tasks in the the zero-shot regime, as the its BLEU score is under 10. It is also worth pointing out that although the zero-shot accuracy of English-to-X translation direction is rather high (except with 4-bit quantization), the BLEU score is consistently under 10. Manual inspection reveals that the model is repeating the original question in English, resulting in a high accuracy but low BLEU scores. Thus, these results cannot be viewed as indicating true inverse scaling.
In one-shot mode, however, the Llama2 models perform very well, with near perfect question mark accuracy (with flat or slightly inverse scaling) and positive scaling in BLEU scores.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p2.1">The Llama2-chat models are able to translate in zero-shot mode, exhibiting positive scaling, but perform less well in one-shot mode: possibly their instruction tuning interferes with their ability to learn in-context.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F1.sf1.g1" src="extracted/5471619/figures/openai/accuracy_de_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F1.sf2.g1" src="extracted/5471619/figures/openai/accuracy_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F1.sf3.g1" src="extracted/5471619/figures/openai/accuracy_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Russian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F1.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F1.sf4.g1" src="extracted/5471619/figures/openai/accuracy_en_de.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>English-German</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F1.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F1.sf5.g1" src="extracted/5471619/figures/openai/accuracy_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F1.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F1.sf6.g1" src="extracted/5471619/figures/openai/accuracy_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>English-Russian</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>accuracy score of OpenAI models of in non-adversarial experiments</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf1.g1" src="extracted/5471619/figures/llama2/acc_de_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf2.g1" src="extracted/5471619/figures/llama2/acc_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf3.g1" src="extracted/5471619/figures/llama2/acc_ro_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Romanian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F2.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf4.g1" src="extracted/5471619/figures/llama2/acc_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Russian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F2.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf5.g1" src="extracted/5471619/figures/llama2/acc_en_de.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>English-German</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F2.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf6.g1" src="extracted/5471619/figures/llama2/acc_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F2.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf7.g1" src="extracted/5471619/figures/llama2/acc_en_ro.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>English-Romanian</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F2.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf8.g1" src="extracted/5471619/figures/llama2/acc_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>English-Russian</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Accuracy score of Llama2 models in non-adversarial experiments</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4 ltx_align_center" id="S3.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F3.sf1.g1" src="extracted/5471619/figures/llama2-chat/accuracy_de_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4 ltx_align_center" id="S3.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F3.sf2.g1" src="extracted/5471619/figures/llama2-chat/accuracy_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4 ltx_align_center" id="S3.F3.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F3.sf3.g1" src="extracted/5471619/figures/llama2-chat/accuracy_ro_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Romanian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4 ltx_align_center" id="S3.F3.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F3.sf4.g1" src="extracted/5471619/figures/llama2-chat/accuracy_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Russian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4 ltx_align_center" id="S3.F3.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F3.sf5.g1" src="extracted/5471619/figures/llama2-chat/accuracy_en_de.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>English-German</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4 ltx_align_center" id="S3.F3.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F3.sf6.g1" src="extracted/5471619/figures/llama2-chat/accuracy_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4 ltx_align_center" id="S3.F3.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F3.sf7.g1" src="extracted/5471619/figures/llama2-chat/accuracy_en_ro.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>English-Romanian</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4 ltx_align_center" id="S3.F3.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F3.sf8.g1" src="extracted/5471619/figures/llama2-chat/accuracy_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>English-Russian</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Accuracy score of Llama2-chat in non-adversarial experiments</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Adversarial Experiments</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As expected, non-adversarial experiments show generally positive scaling for most models families and language pairs. Thus, inspired by the prompt injection example in <cite class="ltx_cite ltx_citemacro_citep">(McKenzie et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib13" title="">2023</a>)</cite>, we add an adversarial prompt at the beginning of each question that explicitly instructs the LLM not to translate but answer the question.
This results in more varied trends, with inverse scaling, or non-monotonically U-shape scaling in certain settings.
We only report the accuracy here, BLEU scores can be found in appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A3" title="Appendix C BLEU Scores ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">T5 and FLAN-T5</h5>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F4.sf1.g1" src="extracted/5471619/figures/t5/prefix/accuracy.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>T5</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F4.sf2.g1" src="extracted/5471619/figures/flant5/prefix/accuracy.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>FLAN-T5</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Accuracy of T5 and FLAN-T5 in adversarial experiments</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F5" title="Figure 5 ‣ T5 and FLAN-T5 ‣ 3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the results of the T5 and FLAN-T5 models.
Although we find U-shape scaling in the En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mo id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.1.m1.1d">→</annotation></semantics></math>De translation direction, manual inspection shows that the abrupt drop in the accuracy in both T5 and FLAN-T5 is because the model is outputting white spaces which is possibly due to some internal instabilities of the model, thus, this should not be considered to be a genuine case of U-shape scaling.
Overall, these models do not show clear scaling trends.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">OpenAI models</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.2">We report the results of the GPT-3 and InstructGPT models in figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F6" title="Figure 6 ‣ OpenAI models ‣ 3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">6</span></a>, where we find inverse scaling in the En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mo id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.1.m1.1d">→</annotation></semantics></math>De and En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><mo id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.2.m2.1d">→</annotation></semantics></math>Fr translation directions. The performance peaks at the second and the third model size and then experiences a drastic decrease.
We also provide an example of the actual output of the GPT models in appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#A2" title="Appendix B Translation output ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F5.sf1.g1" src="extracted/5471619/figures/openai/prefix/accuracy_de_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F5.sf2.g1" src="extracted/5471619/figures/openai/prefix/accuracy_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F5.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F5.sf3.g1" src="extracted/5471619/figures/openai/prefix/accuracy_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Russian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F5.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F5.sf4.g1" src="extracted/5471619/figures/openai/prefix/accuracy_en_de.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>English-German</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F5.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F5.sf5.g1" src="extracted/5471619/figures/openai/prefix/accuracy_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F5.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F5.sf6.g1" src="extracted/5471619/figures/openai/prefix/accuracy_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>English-Russian</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>accuracy score of OpenAI models of in adversarial experiments</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.1">It is also worth pointing out that despite the same size, the GPT-3.5 models <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px2.p2.1.1">text-davinci-002</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px2.p2.1.2">text-davinci-003</span> reverse the trends of inverse scaling. This indicates that these two models are better at understanding the instructions than their counterparts of the same size, possibly due to these models being based on a LLM pre-trained on code <cite class="ltx_cite ltx_citemacro_citep">(Fu and Khot, <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib4" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Llama2 and Llama2-chat</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">Figures <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F7" title="Figure 7 ‣ Llama2 and Llama2-chat ‣ 3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">7</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F8" title="Figure 8 ‣ Llama2 and Llama2-chat ‣ 3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">8</span></a> provide the results of the Llama2 and Llama2-chat models respectively. Similar to the previous non-adversarial scenarios, Llama2 models with zero-shot examples show consistent inverse scaling across all translation directions.
However, just as before, only X-to-English directions should be considered valid examples as the model is not able to translate from the opposite direction under the zero-shot schema, achieving BLEU scores below 10.
On the other hand, the model performance exhibits positive or mild U-shape scaling under the few-shot scenario. 
<br class="ltx_break"/>The Llama2-chat models show a very obvious U-shape scaling (figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F8" title="Figure 8 ‣ Llama2 and Llama2-chat ‣ 3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">8</span></a>), in contrast with the positive scaling observed on the non-adversarial examples.</p>
</div>
<figure class="ltx_figure" id="S3.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F6.sf1.g1" src="extracted/5471619/figures/llama2/prefix/accuracy_de_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F6.sf2.g1" src="extracted/5471619/figures/llama2/prefix/accuracy_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F6.sf3.g1" src="extracted/5471619/figures/llama2/prefix/accuracy_ro_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Romanian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F6.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F6.sf4.g1" src="extracted/5471619/figures/llama2/prefix/accuracy_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F6.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F6.sf5.g1" src="extracted/5471619/figures/llama2/prefix/accuracy_en_de.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>English-German</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F6.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F6.sf6.g1" src="extracted/5471619/figures/llama2/prefix/accuracy_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F6.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F6.sf7.g1" src="extracted/5471619/figures/llama2/prefix/accuracy_en_ro.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>English-Romanian</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F6.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F6.sf8.g1" src="extracted/5471619/figures/llama2/prefix/accuracy_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>English-Russian</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>accuracy score of Llama2 models in adversarial experiments</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F7.sf1.g1" src="extracted/5471619/figures/llama2-chat/prefix/accuracy_de_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F7.sf2.g1" src="extracted/5471619/figures/llama2-chat/prefix/accuracy_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F7.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F7.sf3.g1" src="extracted/5471619/figures/llama2-chat/prefix/accuracy_ro_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Romanian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F7.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F7.sf4.g1" src="extracted/5471619/figures/llama2-chat/prefix/accuracy_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Russian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="S3.F7.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F7.sf5.g1" src="extracted/5471619/figures/llama2-chat/prefix/accuracy_en_de.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>English-German</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F7.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F7.sf6.g1" src="extracted/5471619/figures/llama2-chat/prefix/accuracy_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F7.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F7.sf7.g1" src="extracted/5471619/figures/llama2-chat/prefix/accuracy_en_ro.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>English-Romanian</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S3.F7.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F7.sf8.g1" src="extracted/5471619/figures/llama2-chat/prefix/accuracy_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>English-Russian</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>accuracy score of Llama2-chat models in adversarial experiments</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Inverse Scaling w.r.t. training data size</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Previous work on scaling laws in LLMs <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib9" title="">2020b</a>)</cite> and neural machine translation models <cite class="ltx_cite ltx_citemacro_citep">(Ghorbani et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib5" title="">2021</a>)</cite> investigated the relationship between the size of the training data, in addition to model size, and performance, revealing positive scaling w.r.t. data size.
The LLMs in our experiment are pre-trained on English-dominated corpora crawled from the internet, and in the case of instruction-tuned models, the English data also likely dominates the other languages.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">However, in our experiments we find that models are more likely to answer the source questions rather than translate them when they are written in English, even on non-adversarial examples, which is a clean case of <span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Inverse Scaling</span> w.r.t. training data size.
This is likely due to the source question, with or without the adversarial prefix, acting as a stronger distractor when it occurs in the language the model is more familiar with.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">While we are not able to characterize this phenomenon as a precise scaling law, as accurate training corpus size and proportion of English vs. non-English data are not publicly known for most model families, we do note that the effect is strong and consistent across all model families, model sizes and languages.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">In table <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.T2" title="Table 2 ‣ 3.3 Inverse Scaling w.r.t. training data size ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">2</span></a> we provide the average accuracies across all models and both clean and adversarial examples for all language pairs.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_flex_size_2 ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.1">x - English</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.2">accuracy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.3">English - x</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.1.1.4">accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.1.1">de-en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.1.2">0.904</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.1.3">en-de</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.4">0.731</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.3.2.1">fr-en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.3.2.2">0.926</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.3.2.3">en-fr</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.3.2.4">0.739</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.4.3.1">ro-en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.4.3.2">0.908</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.4.3.3">en-ro</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.4.3.4">0.746</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.5.4.1">ru-en</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.5.4.2">0.903</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.5.4.3">en-ru</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.1.5.4.4">0.708</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_flex_size_2 ltx_align_middle" id="S3.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.2.1.1.1">x - English</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.2.1.1.2">accuracy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.2.1.1.3">English - x</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.2.1.1.4">accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.1.1">de-en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.1.2">0.629</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.1.3">en-de</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.1.4">0.486</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.3.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.3.2.1">fr-en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.3.2.2">0.734</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.3.2.3">en-fr</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.3.2.4">0.545</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.4.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.4.3.1">ro-en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.4.3.2">0.663</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.4.3.3">en-ro</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.4.3.4">0.550</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.2.5.4.1">ru-en</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.2.5.4.2">0.756</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.2.5.4.3">en-ru</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.2.5.4.4">0.505</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>average accuracies of X-to/from English language pairs. <span class="ltx_text ltx_font_bold" id="S3.T2.5.1">top</span>: non-adversarial experiments, <span class="ltx_text ltx_font_bold" id="S3.T2.6.2">bottom</span>: adversarial experiments</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion and Related Work</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our experiments show that most LLM families show positive or flat scaling w.r.t. model size on non-adversarial examples, tend to exhibit inverse or non-monotonic scaling on adversarial examples containing a prompt injection attack, especially when operating in zero-shot mode.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The experiment results on Llama2 models (figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F3" title="Figure 3 ‣ Llama2 and Llama2-chat ‣ 3.1 Non-adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#S3.F7" title="Figure 7 ‣ Llama2 and Llama2-chat ‣ 3.2 Adversarial Experiments ‣ 3 Experiments ‣ Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks"><span class="ltx_text ltx_ref_tag">7</span></a>) show that inverse scaling can be avoided with even a single in-context parallel example, a similar conclusion was also made in <cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib22" title="">2023</a>)</cite>, where they use few-shot examples to reverse the inverse scaling in several tasks that previously exhibited inverse scaling.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Another potential mitigation based on our experiment results is training on code and/or instruction tuning, as the two GPT-3.5 models reverse the inverse scaling trend. The rather U-shape or positive scaling behaviour of the Llama2-chat models also suggests that instruction tuning endows the model with a better ability to correctly understand instructions. Similar results are also shown by <cite class="ltx_cite ltx_citemacro_citet">Miceli Barone et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib15" title="">2023</a>)</cite>, where the GPT-3.5 models reversed the inverse scaling trend of Instruct GPT.
However, note that instruction tuning might interfere with in-context learning, as evidenced by the Llama2-chat results, but not the GPT-3.5 results, hence we recommend to take great care with data set curation when applying instruction tuning in order to avoid capability regression.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Finally, one may ask whether mere scaling might eventually overcome all inverse trends.
In <cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib22" title="">2023</a>)</cite>, the authors repeated the inverse scaling experiments of <cite class="ltx_cite ltx_citemacro_citet">McKenzie et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib13" title="">2023</a>)</cite> with much larger models and found that for most of the tasks that show inverse scaling, further scaling up the model sizes did manage to reverse the trend, as the performance goes up again and forms a U-shape scaling. In <cite class="ltx_cite ltx_citemacro_citet">McKenzie et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib13" title="">2023</a>)</cite>, GPT-4 also performs better than most GPT-3 and InstructGPT models, however, in <cite class="ltx_cite ltx_citemacro_citet">Miceli Barone et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib15" title="">2023</a>)</cite>, even GPT4 performs worse than smaller models of the same family, suggesting that mere model scaling may not be sufficient to solve poor performance on difficult examples, or at least not in an efficient way given the costs of training and deploying very large models.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we investigated the scaling behaviour of LLMs in the task of machine translation of factual questions, both on clear examples and on adversarial examples constructed according to a simple prompt injection attack where we tell the model to answer the questions instead of translating them.
We found inverse scaling under certain model series and zero-shot scenarios.
</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">In addition to the effect from the model size, we also found that performance severely deteriorates when the prompt is written in English, indicating inverse scaling in the dimension of the amount of training data.
</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">To our knowledge, this is the first work to investigate non-monotonic scaling and prompt injection attacks in a multi-lingual setting.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Number of model families</h5>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="Sx1.SS0.SSS0.Px1.p1.1">Due to limited time, budget and computational resources available, and because the limited number of publicly available LLMs that exhibit strong multilingual capabilities, our research doesn’t include many model series.
Future work on this topic should include more model families, such as Antropic Claude, GPT-3.5-turbo and GPT-4.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Number of distractors</h5>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="Sx1.SS0.SSS0.Px2.p1.1">Our experiment only considers a single prompt injection attack setting and uses a question-answering task as the distracting prompt.
The study of scaling behavior in prompt-based machine translation can go well beyond this scope.
For instance, one could use the counterfactual data set <cite class="ltx_cite ltx_citemacro_citep">(Meng et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09832v1#bib.bib14" title="">2023</a>)</cite> to construct sentences containing counterfactual knowledge e.g. "The Eiffel Tower is located in Berlin." As hypothesized previously, since larger language models store more world knowledge and rely more on the world knowledge to provide output, in an inverse scaling scenario, we would expect that larger models tend to translate the counterfactual piece of information e.g. "Berlin" in our example instead of the factual knowledge i.e. "Paris". In addition, more language pairs can be tested, to provide more solid proof for our claim that the language where the distraction adversarial prompt is written causes different model performances.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Coarse-grained evaluation strategy</h5>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="Sx1.SS0.SSS0.Px3.p1.1">We only use the question mark to determine if the model output is successful.
Although we do selectively check the translation output manually to ensure the validity of our evaluation strategy, the model might still output a failed translation that still ends with question marks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">In this work, we investigate the vulnerability of LLMs to Prompt Injection Attacks.
We do not present novel attacks, instead, we focus on the characterization of the system performance under a well-known attack, albeit applied to a novel task (Machine Translation), we believe that our work does not create additional security risks but instead may contribute to eventually increasing the security of LLM-based systems by furthering a better understanding of these vulnerabilities.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">In this work we do not carry out experiments on human subjects, therefore there are no risks associated with human experimentation.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1"><span class="ltx_text ltx_font_bold" id="Sx3.p1.1.1">Antonio Valerio Miceli-Barone</span> was supported by a grant from the UKRI Strategic Priorities Fund to the UKRI Research Node on Trustworthy Autonomous Systems Governance and Regulation (EP/V026607/1, 2020-2024).</p>
</div>
<div class="ltx_para" id="Sx3.p2">
<p class="ltx_p" id="Sx3.p2.1">Some of the experiments in this research were performed using the <span class="ltx_text ltx_font_bold" id="Sx3.p2.1.1">Baskerville Tier 2 HPC service</span> <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.baskerville.ac.uk/" title="">https://www.baskerville.ac.uk/</a></span></span></span>.
Baskerville was funded by the EPSRC and UKRI through the World Class Labs scheme (EP/T022221/1) and the Digital Research Infrastructure programme (EP/W032244/1) and is operated by Advanced Research Computing at the University of Birmingham.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2204.02311" title="">Palm: Scaling language modeling with pathways</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2022)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2210.11416" title="">Scaling instruction-finetuned language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu and Khot (2022)</span>
<span class="ltx_bibblock">
Hao Fu, Yao; Peng and Tushar Khot. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1" title="">How does gpt obtain its ability? tracing emergent abilities of language models to their sources</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Yao Fu’s Notion</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghorbani et al. (2021)</span>
<span class="ltx_bibblock">
Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2109.07740" title="">Scaling laws for neural machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hernandez et al. (2021)</span>
<span class="ltx_bibblock">
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2102.01293" title="">Scaling laws for transfer</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et al. (2022)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2203.15556" title="">Training compute-optimal large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. (2020a)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2001.08361" title="">Scaling laws for neural language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. (2020b)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2001.08361" title="">Scaling laws for neural language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et al. (2023)</span>
<span class="ltx_bibblock">
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popović, and Mariya Shmatova. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.wmt-1.1" title="">Findings of the 2023 conference on machine translation (wmt23): Llms are here but not quite there yet</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 1–42, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2109.07958" title="">Truthfulqa: Measuring how models mimic human falsehoods</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2001.08210" title="">Multilingual denoising pre-training for neural machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McKenzie et al. (2023)</span>
<span class="ltx_bibblock">
Ian R. McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, The Floating Droid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim, Samuel R. Bowman, and Ethan Perez. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.09479" title="">Inverse scaling: When bigger isn’t better</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. (2023)</span>
<span class="ltx_bibblock">
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2202.05262" title="">Locating and editing factual associations in gpt</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miceli Barone et al. (2023)</span>
<span class="ltx_bibblock">
Antonio Valerio Miceli Barone, Fazl Barez, Shay B. Cohen, and Ioannis Konstas. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.19" title="">The larger they are, the harder they fail: Language models do not recognize identifier swaps in python</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 272–292, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2203.02155" title="">Training language models to follow instructions with human feedback</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em>, ACL ’02, page 311–318, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parrish et al. (2022)</span>
<span class="ltx_bibblock">
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-acl.165" title="">BBQ: A hand-built bias benchmark for question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 2086–2105, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1910.10683" title="">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2020)</span>
<span class="ltx_bibblock">
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2008.00401" title="">Multilingual translation with extensible multilingual pretraining and finetuning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Najoung Kim, Yi Tay, and Quoc V. Le. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2211.02011" title="">Inverse scaling can become u-shaped</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2206.07682" title="">Emergent abilities of large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Biao Zhang, Barry Hadow, and Alexandra Birch. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2301.07069v2" title="">Prompting large language model for machine translation: A case study</a>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Example generation</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Example Generation Pipeline</h3>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="319" id="A1.F9.g1" src="extracted/5471619/figures/pipeline.jpeg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The pipeline of prompt generation</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Prompt Templates</h3>
<section class="ltx_subsubsection" id="A1.SS2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Zero-shot template:</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSSx1.p1">
<p class="ltx_p" id="A1.SS2.SSSx1.p1.1"><span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx1.p1.1.1">Translate from </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx1.p1.1.2">source</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx1.p1.1.3"> to </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx1.p1.1.4">target</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx1.p1.1.5">: </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx1.p1.1.6">source text</span>} 
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Few-shot template:</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSSx2.p1">
<p class="ltx_p" id="A1.SS2.SSSx2.p1.1"><span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.1">Translate from </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.2">source</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.3"> to </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.4">target</span>}
<br class="ltx_break"/>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.5">source</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.6">:</span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.7">source example</span>}
<br class="ltx_break"/>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.8">target</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.9">:</span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.10">target example</span>} 
<br class="ltx_break"/>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.11">source</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.12">:</span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.13">source text</span>} 
<br class="ltx_break"/>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.14">target</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx2.p1.1.15">:</span>
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Zero-shot template (Llama2-chat):</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSSx3.p1">
<p class="ltx_p" id="A1.SS2.SSSx3.p1.1"><span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.1">[INST] &lt;&lt;SYS&gt;&gt;
<br class="ltx_break"/>Translate from </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.2">source</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.3"> to </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.4">target</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.5">:
<br class="ltx_break"/>&lt;&lt;/SYS&gt;&gt;
<br class="ltx_break"/>
<br class="ltx_break"/></span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.6">source</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.7">: </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.8">source text</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.9"> [/INST] </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.10">target</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx3.p1.1.11">:</span>
<br class="ltx_break"/></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSSx4">
<h4 class="ltx_title ltx_title_subsubsection">Few-shot template (Llama2-chat):</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSSx4.p1">
<p class="ltx_p" id="A1.SS2.SSSx4.p1.1"><span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.1">[INST] &lt;&lt;SYS&gt;&gt;
<br class="ltx_break"/>Translate from </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.2">source</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.3"> to </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.4">target</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.5">:
<br class="ltx_break"/>&lt;&lt;/SYS&gt;&gt;
<br class="ltx_break"/>
<br class="ltx_break"/></span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.6">source</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.7">: </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.8">source example</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.9"> [/INST] </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.10">target</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.11">:</span> {target example} &lt;/s&gt;&lt;s&gt; 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.12">[INST] </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.13">source</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.14">: </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.15">source text</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.16"> [/INST] </span>{<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.17">target</span>}<span class="ltx_text ltx_font_italic" id="A1.SS2.SSSx4.p1.1.18">:</span></p>
</div>
</section>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Translation output</h2>
<figure class="ltx_figure" id="A2.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="660" id="A2.F10.g1" src="extracted/5471619/figures/mt_examples.jpeg" width="1140"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Example output of IntructGPT models under a Prompt Injection Attack. Note that the larger model is giving a detailed answer to the question rather than translating it correctly, however, the GPT-3.5 models do translate the source text correctly.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>BLEU Scores</h2>
<figure class="ltx_figure" id="A3.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A3.F10.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F10.sf1.g1" src="extracted/5471619/figures/t5/bleu.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>T5</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A3.F10.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F10.sf2.g1" src="extracted/5471619/figures/flant5/bleu.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>FLAN-T5</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>BLEU Scores of T5 and FLAN-T5 models in non-adversarial experiments</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A3.F11.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F11.sf1.g1" src="extracted/5471619/figures/t5/prefix/bleu.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>T5</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A3.F11.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F11.sf2.g1" src="extracted/5471619/figures/flant5/prefix/bleu.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>FLAN-T5</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>BLEU Scores of T5 and FLAN-T5 models in adversarial experiments</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F12.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F12.sf1.g1" src="extracted/5471619/figures/openai/bleu_de_en.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F12.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F12.sf2.g1" src="extracted/5471619/figures/openai/bleu_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F12.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F12.sf3.g1" src="extracted/5471619/figures/openai/bleu_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Russian-English</figcaption>
<br class="ltx_break ltx_centering"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F12.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F12.sf4.g1" src="extracted/5471619/figures/openai/bleu_en_de.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>English-German</figcaption>
<br class="ltx_break ltx_centering"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F12.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F12.sf5.g1" src="extracted/5471619/figures/openai/bleu_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F12.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F12.sf6.g1" src="extracted/5471619/figures/openai/bleu_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>English-Russian</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Bleu score of OpenAI models in non-adversarial experiments</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F13.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F13.sf1.g1" src="extracted/5471619/figures/openai/prefix/bleu_de_en.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
<br class="ltx_break ltx_centering"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F13.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F13.sf2.g1" src="extracted/5471619/figures/openai/prefix/bleu_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
<br class="ltx_break ltx_centering"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F13.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F13.sf3.g1" src="extracted/5471619/figures/openai/prefix/bleu_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Russian-English</figcaption>
<br class="ltx_break ltx_centering"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F13.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F13.sf4.g1" src="extracted/5471619/figures/openai/prefix/bleu_en_de.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>English-German</figcaption>
<br class="ltx_break ltx_centering"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F13.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F13.sf5.g1" src="extracted/5471619/figures/openai/prefix/bleu_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>English-French</figcaption>
<br class="ltx_break ltx_centering"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F13.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A3.F13.sf6.g1" src="extracted/5471619/figures/openai/prefix/bleu_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>English-Russian</figcaption>
<br class="ltx_break ltx_centering"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Bleu score of OpenAI models in adversarial experiments</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F15">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F14.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F14.sf1.g1" src="extracted/5471619/figures/llama2/bleu_de_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F14.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F14.sf2.g1" src="extracted/5471619/figures/llama2/bleu_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F14.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F14.sf3.g1" src="extracted/5471619/figures/llama2/bleu_ro_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Romanian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F14.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F14.sf4.g1" src="extracted/5471619/figures/llama2/bleu_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Russian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F14.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F14.sf5.g1" src="extracted/5471619/figures/llama2/bleu_en_de.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>English-German</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F14.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F14.sf6.g1" src="extracted/5471619/figures/llama2/bleu_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F14.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F14.sf7.g1" src="extracted/5471619/figures/llama2/bleu_en_ro.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>English-Romanian</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F14.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F14.sf8.g1" src="extracted/5471619/figures/llama2/bleu_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>English-Russian</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Bleu score of Llama2 models in non-adversarial experiments</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F16">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F15.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F15.sf1.g1" src="extracted/5471619/figures/llama2/prefix/bleu_de_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F15.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F15.sf2.g1" src="extracted/5471619/figures/llama2/prefix/bleu_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F15.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F15.sf3.g1" src="extracted/5471619/figures/llama2/prefix/bleu_ro_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Romanian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F15.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F15.sf4.g1" src="extracted/5471619/figures/llama2/prefix/bleu_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Russian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F15.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F15.sf5.g1" src="extracted/5471619/figures/llama2/prefix/bleu_en_de.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>English-German</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F15.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F15.sf6.g1" src="extracted/5471619/figures/llama2/prefix/bleu_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F15.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F15.sf7.g1" src="extracted/5471619/figures/llama2/prefix/bleu_en_ro.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>English-Romanian</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F15.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F15.sf8.g1" src="extracted/5471619/figures/llama2/prefix/bleu_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>English-Russian</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Bleu score of Llama2 models in adversarial experiments</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F17">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F16.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F16.sf1.g1" src="extracted/5471619/figures/llama2-chat/bleu_de_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F16.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F16.sf2.g1" src="extracted/5471619/figures/llama2-chat/bleu_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F16.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F16.sf3.g1" src="extracted/5471619/figures/llama2-chat/bleu_ro_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Romanian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F16.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F16.sf4.g1" src="extracted/5471619/figures/llama2-chat/bleu_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Russian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F16.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F16.sf5.g1" src="extracted/5471619/figures/llama2-chat/bleu_en_de.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>English-German</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F16.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F16.sf6.g1" src="extracted/5471619/figures/llama2-chat/bleu_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F16.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F16.sf7.g1" src="extracted/5471619/figures/llama2-chat/bleu_en_ro.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>English-Romanian</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F16.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F16.sf8.g1" src="extracted/5471619/figures/llama2-chat/bleu_en_ru.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>English-Russian</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Bleu score of Llama2 chat models in non-adversarial experiments</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F18">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F17.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F17.sf1.g1" src="extracted/5471619/figures/llama2-chat/prefix/bleu_de_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>German-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F17.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F17.sf2.g1" src="extracted/5471619/figures/llama2-chat/prefix/bleu_fr_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>French-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F17.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F17.sf3.g1" src="extracted/5471619/figures/llama2-chat/prefix/bleu_ro_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Romanian</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F17.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F17.sf4.g1" src="extracted/5471619/figures/llama2-chat/prefix/bleu_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Russian-English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_flex_size_5 ltx_flex_size_many ltx_align_center" id="A3.F17.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F17.sf5.g1" src="extracted/5471619/figures/llama2-chat/prefix/bleu_en_de.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>English-German</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F17.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F17.sf6.g1" src="extracted/5471619/figures/llama2-chat/prefix/bleu_en_fr.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>English-French</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F17.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F17.sf7.g1" src="extracted/5471619/figures/llama2-chat/prefix/bleu_en_ro.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>English-Romanian</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="A3.F17.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F17.sf8.g1" src="extracted/5471619/figures/llama2-chat/prefix/bleu_ru_en.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>Russian-English</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Bleu score of Llama2-chat models in adversarial experiments</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Mar 14 19:33:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
