<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization</title>
<!--Generated on Sat Aug 24 21:33:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="ColBERT,  BERT,  mask tokens,  term weighting,  query augmentation" lang="en" name="keywords"/>
<base href="/html/2408.13672v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S1" title="In ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S2" title="In ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S3" title="In ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S3.SS0.SSS0.Px1" title="In 3. Methodology ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_title">RQ1: Do [MASK] tokens primarily weight non-[MASK] tokens in a query when using ColBERTv2?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S3.SS0.SSS0.Px2" title="In 3. Methodology ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">RQ2: Does effectiveness increase with the number of </span><span class="ltx_text ltx_font_typewriter">[MASK]</span><span class="ltx_text ltx_font_bold">s, up to four times the number ColBERT has been trained with?</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S4" title="In ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S4.SS0.SSS0.Px1" title="In 4. Results ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_title">RQ1: Do [MASK] tokens primarily weight non-[MASK] tokens in a query when using ColBERTv2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S4.SS0.SSS0.Px2" title="In 4. Results ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">RQ2: Does effectiveness increase as the number of </span><span class="ltx_text ltx_font_typewriter">[MASK]</span><span class="ltx_text ltx_font_bold">s increases up to four times the number ColBERT has been trained with?</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S5" title="In ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">ColBERT <span class="ltx_text ltx_font_typewriter" id="id1.id1">[MASK]</span> Tokens Perform Term Weighting and Exhibit Cyclic Contextualization </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ben Giacalone
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:bsg8294@rit.edu">bsg8294@rit.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Rochester Institute of Technology</span><span class="ltx_text ltx_affiliation_streetaddress" id="id8.2.id2">1 Lomb Memorial Dr</span><span class="ltx_text ltx_affiliation_city" id="id9.3.id3">Rochester</span><span class="ltx_text ltx_affiliation_state" id="id10.4.id4">New York</span><span class="ltx_text ltx_affiliation_country" id="id11.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id12.6.id6">14623</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Richard Zanibbi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:rxzvcs@rit.edu">rxzvcs@rit.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Rochester Institute of Technology</span><span class="ltx_text ltx_affiliation_streetaddress" id="id14.2.id2">1 Lomb Memorial Dr</span><span class="ltx_text ltx_affiliation_city" id="id15.3.id3">Rochester</span><span class="ltx_text ltx_affiliation_state" id="id16.4.id4">New York</span><span class="ltx_text ltx_affiliation_country" id="id17.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id18.6.id6">14623</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<h1 class="ltx_title ltx_title_document">ColBERT <span class="ltx_text ltx_font_typewriter" id="id2.id1">[MASK]</span> Tokens Weight Non-<span class="ltx_text ltx_font_typewriter" id="id3.id2">[MASK]</span> Tokens and Have a Cyclic Contextualization Pattern Based on Position </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ben Giacalone
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:bsg8294@rit.edu">bsg8294@rit.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Rochester Institute of Technology</span><span class="ltx_text ltx_affiliation_streetaddress" id="id8.2.id2">1 Lomb Memorial Dr</span><span class="ltx_text ltx_affiliation_city" id="id9.3.id3">Rochester</span><span class="ltx_text ltx_affiliation_state" id="id10.4.id4">New York</span><span class="ltx_text ltx_affiliation_country" id="id11.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id12.6.id6">14623</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Richard Zanibbi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:rxzvcs@rit.edu">rxzvcs@rit.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Rochester Institute of Technology</span><span class="ltx_text ltx_affiliation_streetaddress" id="id14.2.id2">1 Lomb Memorial Dr</span><span class="ltx_text ltx_affiliation_city" id="id15.3.id3">Rochester</span><span class="ltx_text ltx_affiliation_state" id="id16.4.id4">New York</span><span class="ltx_text ltx_affiliation_country" id="id17.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id18.6.id6">14623</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<h1 class="ltx_title ltx_title_document">ColBERT <span class="ltx_text ltx_font_typewriter" id="id4.id1">[MASK]</span> Tokens Weight Non-<span class="ltx_text ltx_font_typewriter" id="id5.id2">[MASK]</span> Terms and Repeat Across Input Position Past the Input Length for Scoring</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ben Giacalone
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:bsg8294@rit.edu">bsg8294@rit.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Rochester Institute of Technology</span><span class="ltx_text ltx_affiliation_streetaddress" id="id8.2.id2">1 Lomb Memorial Dr</span><span class="ltx_text ltx_affiliation_city" id="id9.3.id3">Rochester</span><span class="ltx_text ltx_affiliation_state" id="id10.4.id4">New York</span><span class="ltx_text ltx_affiliation_country" id="id11.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id12.6.id6">14623</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Richard Zanibbi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:rxzvcs@rit.edu">rxzvcs@rit.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Rochester Institute of Technology</span><span class="ltx_text ltx_affiliation_streetaddress" id="id14.2.id2">1 Lomb Memorial Dr</span><span class="ltx_text ltx_affiliation_city" id="id15.3.id3">Rochester</span><span class="ltx_text ltx_affiliation_state" id="id16.4.id4">New York</span><span class="ltx_text ltx_affiliation_country" id="id17.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id18.6.id6">14623</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<h1 class="ltx_title ltx_title_document">ColBERT’s <span class="ltx_text ltx_font_typewriter" id="id6.id1">[MASK]</span>-based Query Augmentation:
<br class="ltx_break"/>Effects of Quadrupling the Query Input Length</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ben Giacalone
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:bsg8294@rit.edu">bsg8294@rit.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Rochester Institute of Technology</span><span class="ltx_text ltx_affiliation_streetaddress" id="id8.2.id2">1 Lomb Memorial Dr</span><span class="ltx_text ltx_affiliation_city" id="id9.3.id3">Rochester</span><span class="ltx_text ltx_affiliation_state" id="id10.4.id4">New York</span><span class="ltx_text ltx_affiliation_country" id="id11.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id12.6.id6">14623</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Richard Zanibbi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:rxzvcs@rit.edu">rxzvcs@rit.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Rochester Institute of Technology</span><span class="ltx_text ltx_affiliation_streetaddress" id="id14.2.id2">1 Lomb Memorial Dr</span><span class="ltx_text ltx_affiliation_city" id="id15.3.id3">Rochester</span><span class="ltx_text ltx_affiliation_state" id="id16.4.id4">New York</span><span class="ltx_text ltx_affiliation_country" id="id17.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id18.6.id6">14623</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id19.id1">A unique aspect of ColBERT is its use of <span class="ltx_text ltx_font_typewriter" id="id19.id1.1">[MASK]</span> tokens in queries to score documents (<em class="ltx_emph ltx_font_italic" id="id19.id1.2">query augmentation)</em>.
Prior work shows <span class="ltx_text ltx_font_typewriter" id="id19.id1.3">[MASK]</span> tokens weighting non-<span class="ltx_text ltx_font_typewriter" id="id19.id1.4">[MASK]</span> query terms, emphasizing certain tokens over others
, rather than introducing whole new terms as initially proposed.
We begin by demonstrating that a term weighting behavior
previously reported for <span class="ltx_text ltx_font_typewriter" id="id19.id1.5">[MASK]</span> tokens
in ColBERTv1
holds for ColBERTv2.
We then examine the effect of changing the number of <span class="ltx_text ltx_font_typewriter" id="id19.id1.6">[MASK]</span> tokens from zero to up to four times past the query input length used in training, both for first stage retrieval, and for scoring candidates,
observing an initial decrease in performance with few <span class="ltx_text ltx_font_typewriter" id="id19.id1.7">[MASK]</span>s, a large increase when enough <span class="ltx_text ltx_font_typewriter" id="id19.id1.8">[MASK]</span>s are added to pad queries to an average length of 32, then a plateau in performance afterwards.
Additionally, we compare baseline performance to performance when the query length is extended to 128 tokens, and find that differences are small (e.g., within 1% on various metrics) and generally statistically insignificant, indicating performance does not collapse if ColBERT is presented with more <span class="ltx_text ltx_font_typewriter" id="id19.id1.9">[MASK]</span> tokens than expected.</p>
</div>
<div class="ltx_keywords">ColBERT, BERT, mask tokens, term weighting, query augmentation
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; ; </span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="370" id="S1.F1.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>
Cosine similarity of embedded tokens to each non-<span class="ltx_text ltx_font_typewriter" id="S1.F1.2.1">[MASK]</span> token for positions 0 through 64.
A cyclical pattern attending to the most relevant terms in the query (e.g. “period”, “calculus”, “california”, “austria”) can be seen, both before and after 32 tokens (the length trained with).</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.5">ColBERT <cite class="ltx_cite ltx_citemacro_citep">(Khattab and Zaharia, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib6" title="">2020</a>)</cite>’s use of multiple token embedding vectors supports fine-grained matching between queries and documents. The model ranks documents by adding the maximum similarity of a document token embedding
to each query token embedding, as shown in Equation <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S1.E1" title="In 1. Introduction ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_tag">1</span></a>. This greedy alignment of query to document token embeddings has been dubbed <em class="ltx_emph ltx_font_italic" id="S1.p1.5.1">MaxSim</em>.</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="S_{d,q}:=\sum_{i\in[|E_{q}|]}\max_{j\in[|E_{d}|]}E_{q_{i}}\cdot E_{d_{j}}^{T}" class="ltx_Math" display="block" id="S1.E1.m1.4"><semantics id="S1.E1.m1.4a"><mrow id="S1.E1.m1.4.5" xref="S1.E1.m1.4.5.cmml"><msub id="S1.E1.m1.4.5.2" xref="S1.E1.m1.4.5.2.cmml"><mi id="S1.E1.m1.4.5.2.2" xref="S1.E1.m1.4.5.2.2.cmml">S</mi><mrow id="S1.E1.m1.2.2.2.4" xref="S1.E1.m1.2.2.2.3.cmml"><mi id="S1.E1.m1.1.1.1.1" xref="S1.E1.m1.1.1.1.1.cmml">d</mi><mo id="S1.E1.m1.2.2.2.4.1" xref="S1.E1.m1.2.2.2.3.cmml">,</mo><mi id="S1.E1.m1.2.2.2.2" xref="S1.E1.m1.2.2.2.2.cmml">q</mi></mrow></msub><mo id="S1.E1.m1.4.5.1" lspace="0.278em" rspace="0.111em" xref="S1.E1.m1.4.5.1.cmml">:=</mo><mrow id="S1.E1.m1.4.5.3" xref="S1.E1.m1.4.5.3.cmml"><munder id="S1.E1.m1.4.5.3.1" xref="S1.E1.m1.4.5.3.1.cmml"><mo id="S1.E1.m1.4.5.3.1.2" movablelimits="false" xref="S1.E1.m1.4.5.3.1.2.cmml">∑</mo><mrow id="S1.E1.m1.3.3.1" xref="S1.E1.m1.3.3.1.cmml"><mi id="S1.E1.m1.3.3.1.3" xref="S1.E1.m1.3.3.1.3.cmml">i</mi><mo id="S1.E1.m1.3.3.1.2" xref="S1.E1.m1.3.3.1.2.cmml">∈</mo><mrow id="S1.E1.m1.3.3.1.1.1" xref="S1.E1.m1.3.3.1.1.2.cmml"><mo id="S1.E1.m1.3.3.1.1.1.2" stretchy="false" xref="S1.E1.m1.3.3.1.1.2.1.cmml">[</mo><mrow id="S1.E1.m1.3.3.1.1.1.1.1" xref="S1.E1.m1.3.3.1.1.1.1.2.cmml"><mo id="S1.E1.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S1.E1.m1.3.3.1.1.1.1.2.1.cmml">|</mo><msub id="S1.E1.m1.3.3.1.1.1.1.1.1" xref="S1.E1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S1.E1.m1.3.3.1.1.1.1.1.1.2" xref="S1.E1.m1.3.3.1.1.1.1.1.1.2.cmml">E</mi><mi id="S1.E1.m1.3.3.1.1.1.1.1.1.3" xref="S1.E1.m1.3.3.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S1.E1.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="S1.E1.m1.3.3.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S1.E1.m1.3.3.1.1.1.3" stretchy="false" xref="S1.E1.m1.3.3.1.1.2.1.cmml">]</mo></mrow></mrow></munder><mrow id="S1.E1.m1.4.5.3.2" xref="S1.E1.m1.4.5.3.2.cmml"><munder id="S1.E1.m1.4.5.3.2.1" xref="S1.E1.m1.4.5.3.2.1.cmml"><mi id="S1.E1.m1.4.5.3.2.1.2" xref="S1.E1.m1.4.5.3.2.1.2.cmml">max</mi><mrow id="S1.E1.m1.4.4.1" xref="S1.E1.m1.4.4.1.cmml"><mi id="S1.E1.m1.4.4.1.3" xref="S1.E1.m1.4.4.1.3.cmml">j</mi><mo id="S1.E1.m1.4.4.1.2" xref="S1.E1.m1.4.4.1.2.cmml">∈</mo><mrow id="S1.E1.m1.4.4.1.1.1" xref="S1.E1.m1.4.4.1.1.2.cmml"><mo id="S1.E1.m1.4.4.1.1.1.2" stretchy="false" xref="S1.E1.m1.4.4.1.1.2.1.cmml">[</mo><mrow id="S1.E1.m1.4.4.1.1.1.1.1" xref="S1.E1.m1.4.4.1.1.1.1.2.cmml"><mo id="S1.E1.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S1.E1.m1.4.4.1.1.1.1.2.1.cmml">|</mo><msub id="S1.E1.m1.4.4.1.1.1.1.1.1" xref="S1.E1.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S1.E1.m1.4.4.1.1.1.1.1.1.2" xref="S1.E1.m1.4.4.1.1.1.1.1.1.2.cmml">E</mi><mi id="S1.E1.m1.4.4.1.1.1.1.1.1.3" xref="S1.E1.m1.4.4.1.1.1.1.1.1.3.cmml">d</mi></msub><mo id="S1.E1.m1.4.4.1.1.1.1.1.3" stretchy="false" xref="S1.E1.m1.4.4.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S1.E1.m1.4.4.1.1.1.3" stretchy="false" xref="S1.E1.m1.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></munder><mo id="S1.E1.m1.4.5.3.2a" lspace="0.167em" xref="S1.E1.m1.4.5.3.2.cmml">⁡</mo><mrow id="S1.E1.m1.4.5.3.2.2" xref="S1.E1.m1.4.5.3.2.2.cmml"><msub id="S1.E1.m1.4.5.3.2.2.2" xref="S1.E1.m1.4.5.3.2.2.2.cmml"><mi id="S1.E1.m1.4.5.3.2.2.2.2" xref="S1.E1.m1.4.5.3.2.2.2.2.cmml">E</mi><msub id="S1.E1.m1.4.5.3.2.2.2.3" xref="S1.E1.m1.4.5.3.2.2.2.3.cmml"><mi id="S1.E1.m1.4.5.3.2.2.2.3.2" xref="S1.E1.m1.4.5.3.2.2.2.3.2.cmml">q</mi><mi id="S1.E1.m1.4.5.3.2.2.2.3.3" xref="S1.E1.m1.4.5.3.2.2.2.3.3.cmml">i</mi></msub></msub><mo id="S1.E1.m1.4.5.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="S1.E1.m1.4.5.3.2.2.1.cmml">⋅</mo><msubsup id="S1.E1.m1.4.5.3.2.2.3" xref="S1.E1.m1.4.5.3.2.2.3.cmml"><mi id="S1.E1.m1.4.5.3.2.2.3.2.2" xref="S1.E1.m1.4.5.3.2.2.3.2.2.cmml">E</mi><msub id="S1.E1.m1.4.5.3.2.2.3.2.3" xref="S1.E1.m1.4.5.3.2.2.3.2.3.cmml"><mi id="S1.E1.m1.4.5.3.2.2.3.2.3.2" xref="S1.E1.m1.4.5.3.2.2.3.2.3.2.cmml">d</mi><mi id="S1.E1.m1.4.5.3.2.2.3.2.3.3" xref="S1.E1.m1.4.5.3.2.2.3.2.3.3.cmml">j</mi></msub><mi id="S1.E1.m1.4.5.3.2.2.3.3" xref="S1.E1.m1.4.5.3.2.2.3.3.cmml">T</mi></msubsup></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.E1.m1.4b"><apply id="S1.E1.m1.4.5.cmml" xref="S1.E1.m1.4.5"><csymbol cd="latexml" id="S1.E1.m1.4.5.1.cmml" xref="S1.E1.m1.4.5.1">assign</csymbol><apply id="S1.E1.m1.4.5.2.cmml" xref="S1.E1.m1.4.5.2"><csymbol cd="ambiguous" id="S1.E1.m1.4.5.2.1.cmml" xref="S1.E1.m1.4.5.2">subscript</csymbol><ci id="S1.E1.m1.4.5.2.2.cmml" xref="S1.E1.m1.4.5.2.2">𝑆</ci><list id="S1.E1.m1.2.2.2.3.cmml" xref="S1.E1.m1.2.2.2.4"><ci id="S1.E1.m1.1.1.1.1.cmml" xref="S1.E1.m1.1.1.1.1">𝑑</ci><ci id="S1.E1.m1.2.2.2.2.cmml" xref="S1.E1.m1.2.2.2.2">𝑞</ci></list></apply><apply id="S1.E1.m1.4.5.3.cmml" xref="S1.E1.m1.4.5.3"><apply id="S1.E1.m1.4.5.3.1.cmml" xref="S1.E1.m1.4.5.3.1"><csymbol cd="ambiguous" id="S1.E1.m1.4.5.3.1.1.cmml" xref="S1.E1.m1.4.5.3.1">subscript</csymbol><sum id="S1.E1.m1.4.5.3.1.2.cmml" xref="S1.E1.m1.4.5.3.1.2"></sum><apply id="S1.E1.m1.3.3.1.cmml" xref="S1.E1.m1.3.3.1"><in id="S1.E1.m1.3.3.1.2.cmml" xref="S1.E1.m1.3.3.1.2"></in><ci id="S1.E1.m1.3.3.1.3.cmml" xref="S1.E1.m1.3.3.1.3">𝑖</ci><apply id="S1.E1.m1.3.3.1.1.2.cmml" xref="S1.E1.m1.3.3.1.1.1"><csymbol cd="latexml" id="S1.E1.m1.3.3.1.1.2.1.cmml" xref="S1.E1.m1.3.3.1.1.1.2">delimited-[]</csymbol><apply id="S1.E1.m1.3.3.1.1.1.1.2.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1"><abs id="S1.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1.2"></abs><apply id="S1.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S1.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1.1.2">𝐸</ci><ci id="S1.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S1.E1.m1.3.3.1.1.1.1.1.1.3">𝑞</ci></apply></apply></apply></apply></apply><apply id="S1.E1.m1.4.5.3.2.cmml" xref="S1.E1.m1.4.5.3.2"><apply id="S1.E1.m1.4.5.3.2.1.cmml" xref="S1.E1.m1.4.5.3.2.1"><csymbol cd="ambiguous" id="S1.E1.m1.4.5.3.2.1.1.cmml" xref="S1.E1.m1.4.5.3.2.1">subscript</csymbol><max id="S1.E1.m1.4.5.3.2.1.2.cmml" xref="S1.E1.m1.4.5.3.2.1.2"></max><apply id="S1.E1.m1.4.4.1.cmml" xref="S1.E1.m1.4.4.1"><in id="S1.E1.m1.4.4.1.2.cmml" xref="S1.E1.m1.4.4.1.2"></in><ci id="S1.E1.m1.4.4.1.3.cmml" xref="S1.E1.m1.4.4.1.3">𝑗</ci><apply id="S1.E1.m1.4.4.1.1.2.cmml" xref="S1.E1.m1.4.4.1.1.1"><csymbol cd="latexml" id="S1.E1.m1.4.4.1.1.2.1.cmml" xref="S1.E1.m1.4.4.1.1.1.2">delimited-[]</csymbol><apply id="S1.E1.m1.4.4.1.1.1.1.2.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1"><abs id="S1.E1.m1.4.4.1.1.1.1.2.1.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.2"></abs><apply id="S1.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.E1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S1.E1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.1.2">𝐸</ci><ci id="S1.E1.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.1.3">𝑑</ci></apply></apply></apply></apply></apply><apply id="S1.E1.m1.4.5.3.2.2.cmml" xref="S1.E1.m1.4.5.3.2.2"><ci id="S1.E1.m1.4.5.3.2.2.1.cmml" xref="S1.E1.m1.4.5.3.2.2.1">⋅</ci><apply id="S1.E1.m1.4.5.3.2.2.2.cmml" xref="S1.E1.m1.4.5.3.2.2.2"><csymbol cd="ambiguous" id="S1.E1.m1.4.5.3.2.2.2.1.cmml" xref="S1.E1.m1.4.5.3.2.2.2">subscript</csymbol><ci id="S1.E1.m1.4.5.3.2.2.2.2.cmml" xref="S1.E1.m1.4.5.3.2.2.2.2">𝐸</ci><apply id="S1.E1.m1.4.5.3.2.2.2.3.cmml" xref="S1.E1.m1.4.5.3.2.2.2.3"><csymbol cd="ambiguous" id="S1.E1.m1.4.5.3.2.2.2.3.1.cmml" xref="S1.E1.m1.4.5.3.2.2.2.3">subscript</csymbol><ci id="S1.E1.m1.4.5.3.2.2.2.3.2.cmml" xref="S1.E1.m1.4.5.3.2.2.2.3.2">𝑞</ci><ci id="S1.E1.m1.4.5.3.2.2.2.3.3.cmml" xref="S1.E1.m1.4.5.3.2.2.2.3.3">𝑖</ci></apply></apply><apply id="S1.E1.m1.4.5.3.2.2.3.cmml" xref="S1.E1.m1.4.5.3.2.2.3"><csymbol cd="ambiguous" id="S1.E1.m1.4.5.3.2.2.3.1.cmml" xref="S1.E1.m1.4.5.3.2.2.3">superscript</csymbol><apply id="S1.E1.m1.4.5.3.2.2.3.2.cmml" xref="S1.E1.m1.4.5.3.2.2.3"><csymbol cd="ambiguous" id="S1.E1.m1.4.5.3.2.2.3.2.1.cmml" xref="S1.E1.m1.4.5.3.2.2.3">subscript</csymbol><ci id="S1.E1.m1.4.5.3.2.2.3.2.2.cmml" xref="S1.E1.m1.4.5.3.2.2.3.2.2">𝐸</ci><apply id="S1.E1.m1.4.5.3.2.2.3.2.3.cmml" xref="S1.E1.m1.4.5.3.2.2.3.2.3"><csymbol cd="ambiguous" id="S1.E1.m1.4.5.3.2.2.3.2.3.1.cmml" xref="S1.E1.m1.4.5.3.2.2.3.2.3">subscript</csymbol><ci id="S1.E1.m1.4.5.3.2.2.3.2.3.2.cmml" xref="S1.E1.m1.4.5.3.2.2.3.2.3.2">𝑑</ci><ci id="S1.E1.m1.4.5.3.2.2.3.2.3.3.cmml" xref="S1.E1.m1.4.5.3.2.2.3.2.3.3">𝑗</ci></apply></apply><ci id="S1.E1.m1.4.5.3.2.2.3.3.cmml" xref="S1.E1.m1.4.5.3.2.2.3.3">𝑇</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.E1.m1.4c">S_{d,q}:=\sum_{i\in[|E_{q}|]}\max_{j\in[|E_{d}|]}E_{q_{i}}\cdot E_{d_{j}}^{T}</annotation><annotation encoding="application/x-llamapun" id="S1.E1.m1.4d">italic_S start_POSTSUBSCRIPT italic_d , italic_q end_POSTSUBSCRIPT := ∑ start_POSTSUBSCRIPT italic_i ∈ [ | italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT | ] end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_j ∈ [ | italic_E start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | ] end_POSTSUBSCRIPT italic_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ⋅ italic_E start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S1.p1.4">Here the score for document <math alttext="d" class="ltx_Math" display="inline" id="S1.p1.1.m1.1"><semantics id="S1.p1.1.m1.1a"><mi id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><ci id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S1.p1.1.m1.1d">italic_d</annotation></semantics></math> given query <math alttext="q" class="ltx_Math" display="inline" id="S1.p1.2.m2.1"><semantics id="S1.p1.2.m2.1a"><mi id="S1.p1.2.m2.1.1" xref="S1.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S1.p1.2.m2.1b"><ci id="S1.p1.2.m2.1.1.cmml" xref="S1.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S1.p1.2.m2.1d">italic_q</annotation></semantics></math>, is computed from the set of query and document token embeddings (<math alttext="E_{q}" class="ltx_Math" display="inline" id="S1.p1.3.m3.1"><semantics id="S1.p1.3.m3.1a"><msub id="S1.p1.3.m3.1.1" xref="S1.p1.3.m3.1.1.cmml"><mi id="S1.p1.3.m3.1.1.2" xref="S1.p1.3.m3.1.1.2.cmml">E</mi><mi id="S1.p1.3.m3.1.1.3" xref="S1.p1.3.m3.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p1.3.m3.1b"><apply id="S1.p1.3.m3.1.1.cmml" xref="S1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S1.p1.3.m3.1.1.1.cmml" xref="S1.p1.3.m3.1.1">subscript</csymbol><ci id="S1.p1.3.m3.1.1.2.cmml" xref="S1.p1.3.m3.1.1.2">𝐸</ci><ci id="S1.p1.3.m3.1.1.3.cmml" xref="S1.p1.3.m3.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.3.m3.1c">E_{q}</annotation><annotation encoding="application/x-llamapun" id="S1.p1.3.m3.1d">italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="E_{d}" class="ltx_Math" display="inline" id="S1.p1.4.m4.1"><semantics id="S1.p1.4.m4.1a"><msub id="S1.p1.4.m4.1.1" xref="S1.p1.4.m4.1.1.cmml"><mi id="S1.p1.4.m4.1.1.2" xref="S1.p1.4.m4.1.1.2.cmml">E</mi><mi id="S1.p1.4.m4.1.1.3" xref="S1.p1.4.m4.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p1.4.m4.1b"><apply id="S1.p1.4.m4.1.1.cmml" xref="S1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S1.p1.4.m4.1.1.1.cmml" xref="S1.p1.4.m4.1.1">subscript</csymbol><ci id="S1.p1.4.m4.1.1.2.cmml" xref="S1.p1.4.m4.1.1.2">𝐸</ci><ci id="S1.p1.4.m4.1.1.3.cmml" xref="S1.p1.4.m4.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.4.m4.1c">E_{d}</annotation><annotation encoding="application/x-llamapun" id="S1.p1.4.m4.1d">italic_E start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math>, respectively). Embeddings are produced by a BERT-based model <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib3" title="">2019</a>)</cite> finetuned with ColBERT’s training objective. For queries, ColBERT prepends a <span class="ltx_text ltx_font_typewriter" id="S1.p1.4.1">[Q]</span> token to indicate a query is being contextualized, and surrounds the tokens with <span class="ltx_text ltx_font_typewriter" id="S1.p1.4.2">[CLS]</span> and <span class="ltx_text ltx_font_typewriter" id="S1.p1.4.3">[SEP]</span> tokens
to indicate the beginning and ending of a passage. Finally, the query is padded with <span class="ltx_text ltx_font_typewriter" id="S1.p1.4.4">[MASK]</span> tokens up to a maximum length of 32 tokens.
Augmenting the query with
<span class="ltx_text ltx_font_typewriter" id="S1.p1.4.5">[MASK]</span> rather than standard <span class="ltx_text ltx_font_typewriter" id="S1.p1.4.6">[PAD]</span> tokens is key to ColBERT’s effectiveness.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In Khattab and Zaharia’s original ColBERT paper <cite class="ltx_cite ltx_citemacro_citep">(Khattab and Zaharia, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib6" title="">2020</a>)</cite>, they show using augmentation with <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.1">[MASK]</span> tokens increases
MRR@10 on MS MARCO <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib8" title="">2016</a>)</cite>.
Their rationale is that <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.2">[MASK]</span> tokens help
introduce new terms to the query, and reweight other query terms.
However, later work suggests that <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.3">[MASK]</span> tokens primarily weight other tokens in the query, as summarized in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S2" title="2. Related Work ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_tag">2</span></a>. In this paper we present new experiments to obtain additional insight into how query augmentation maps <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.4">[MASK]</span>s into the contextualized token embedding space.
We consider two main research questions:</p>
</div>
<div class="ltx_para" id="S1.p3">
<dl class="ltx_description" id="S1.I1">
<dt class="ltx_item" id="S1.I1.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.ix1.1.1.1" style="font-size:90%;">RQ1.: </span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1">Do <span class="ltx_text ltx_font_typewriter" id="S1.I1.ix1.p1.1.1">[MASK]</span> tokens primarily weight non-<span class="ltx_text ltx_font_typewriter" id="S1.I1.ix1.p1.1.2">[MASK]</span> tokens in a query when using ColBERTv2?</p>
</div>
</dd>
<dt class="ltx_item" id="S1.I1.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.ix2.1.1.1" style="font-size:90%;">RQ2.: </span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1">Does effectiveness increase with the number of <span class="ltx_text ltx_font_typewriter" id="S1.I1.ix2.p1.1.1">[MASK]</span>s, up to four times the number ColBERT has been trained with?</p>
</div>
</dd>
</dl>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Prior work has analyzed how ColBERT contextualizes tokens.
<cite class="ltx_cite ltx_citemacro_citet">Formal et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib4" title="">2021</a>)</cite> focused their analysis on query text tokens, using both a model trained with <span class="ltx_text ltx_font_typewriter" id="S2.p1.1.1">[MASK]</span>s and a model finetuned without <span class="ltx_text ltx_font_typewriter" id="S2.p1.1.2">[MASK]</span>s during ranking. They found that query text tokens implicitly capture term importance, because terms with higher IDF tend to produce more exact matches, and change their embedded representation less. When using a model that was finetuned to not use <span class="ltx_text ltx_font_typewriter" id="S2.p1.1.3">[MASK]</span>s, this effect was even more apparent.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib16" title="">2023</a>)</cite> considered whether <span class="ltx_text ltx_font_typewriter" id="S2.p2.1.1">[MASK]</span>s in ColBERT actually add new terms to the query, as <cite class="ltx_cite ltx_citemacro_citet">Khattab and Zaharia (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib6" title="">2020</a>)</cite> proposed in their original paper. They found that it did not, and presented an IDF-based approach for adding new terms to the query.
In the same paper, the authors show that <span class="ltx_text ltx_font_typewriter" id="S2.p2.1.2">[MASK]</span> tokens tend to cluster around items already present the query, rather than produce novel query terms, necessitating an approach such as pseudorelevance feedback to add additional query terms.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">More recently, <cite class="ltx_cite ltx_citemacro_citet">Giacalone et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib5" title="">2024</a>)</cite> remapped contextualized <span class="ltx_text ltx_font_typewriter" id="S2.p3.1.1">[MASK]</span> embeddings to their nearest non-<span class="ltx_text ltx_font_typewriter" id="S2.p3.1.2">[MASK]</span> embedding (i.e. <span class="ltx_text ltx_font_typewriter" id="S2.p3.1.3">[CLS]</span>, <span class="ltx_text ltx_font_typewriter" id="S2.p3.1.4">[SEP]</span>, <span class="ltx_text ltx_font_typewriter" id="S2.p3.1.5">[Q]</span>, and the query text tokens), and found no significant difference in MRR@10, nDCG@10/@1000. However, a significant <em class="ltx_emph ltx_font_italic" id="S2.p3.1.6">increase</em> in MAP was observed both when remapping <span class="ltx_text ltx_font_typewriter" id="S2.p3.1.7">[MASK]</span> vectors to their nearest query text token vector, and when remapping <span class="ltx_text ltx_font_typewriter" id="S2.p3.1.8">[MASK]</span> vectors to their nearest non-<span class="ltx_text ltx_font_typewriter" id="S2.p3.1.9">[MASK]</span> token vector.
While interesting, a shortcoming is that their expeirments consider only ColBERTv1, instead of the more effective ColBERTv2 <cite class="ltx_cite ltx_citemacro_citep">(Santhanam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib10" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">ColBERTv2 uses a more powerful cross-encoding ranker to generate positives and negatives to train with, while ColBERTv1 uses labelled positives and random negatives. This results in an almost 4% gain in MRR@10 on the MS MARCO dev set, allowing it to compete with newer dense retriever models that take advantage of distillation (e.g. PAIR <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib9" title="">2021</a>)</cite>).
This may change the behavior of how <span class="ltx_text ltx_font_typewriter" id="S2.p4.1.1">[MASK]</span>s interact with non-<span class="ltx_text ltx_font_typewriter" id="S2.p4.1.2">[MASK]</span> tokens. In our first experiment, we attempt to replicate <cite class="ltx_cite ltx_citemacro_citet">Giacalone et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib5" title="">2024</a>)</cite>’s results using ColBERTv2.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><cite class="ltx_cite ltx_citemacro_citet">Tonellotto and Macdonald (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib12" title="">2021</a>)</cite> demonstrated that the number of query token embeddings required for <em class="ltx_emph ltx_font_italic" id="S2.p5.1.1">initial retrieval</em> in ColBERT can be reduced to as little as 3 by pruning terms frequently present in the collection. They found that <span class="ltx_text ltx_font_typewriter" id="S2.p5.1.2">[MASK]</span>s tend to add less documents to the initial set of documents retrieved, since <span class="ltx_text ltx_font_typewriter" id="S2.p5.1.3">[MASK]</span>s tend to be very similar to existing terms in the documents.
Similar to this paper, in our second set of experiments we perturb the model by modifying the number of <span class="ltx_text ltx_font_typewriter" id="S2.p5.1.4">[MASK]</span> tokens available.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We run our experiments using PyTerrier <cite class="ltx_cite ltx_citemacro_citep">(Macdonald et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib7" title="">2021</a>)</cite>, which contains advanced bindings for ColBERT. Into this framework we load the ColBERT v2 <cite class="ltx_cite ltx_citemacro_citep">(Santhanam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib10" title="">2022</a>)</cite> checkpoint provided by the ColBERT team.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz" title="">https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz</a></span></span></span> We confirmed that this checkpoint was trained using the default query length of 32, and that <span class="ltx_text ltx_font_typewriter" id="S3.p1.1.1">[MASK]</span>s had their attention scores zeroed out during training (i.e. no token can attend to a <span class="ltx_text ltx_font_typewriter" id="S3.p1.1.2">[MASK]</span> token during self attention).
PyTerrier officially supports only ColBERTv1, but we have verified that the keys PyTrrier expects are also present in the our v2 checkpoint.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We do not use v2’s index compression, but we believe this is acceptable, since this is not a core feature of the retrieval model. Using the uncompressed index does
slightly change performance on MS MARCO from the official metrics. On the MS MARCO dev set, we obtained an MRR@10 of 39.8, Recall@50 of 86.0, and Recall@1000 of 96.2, compared to the official reported metrics of MRR@10 of 39.7, R@50 of 86.8, and R@1000 of 98.4. We suspect this increase in Recall is due to some terms becoming more similar when index compression is applied.
</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We run our experiments on a server with 4 Intel Xeon E5-2667v4 CPUs, 4 NVIDIA RTX2080-Ti GPUs, and 512 GB RAM.
We use two datasets from <cite class="ltx_cite ltx_citemacro_citet">Giacalone et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib5" title="">2024</a>)</cite>:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">MS MARCO <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib8" title="">2016</a>)</cite>’s passage retrieval dev set (8.8 million documents, 1 million queries, binary relevance judgements). Each query has at most 1 matching document.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">A dataset combining queries from the TREC 2019 <cite class="ltx_cite ltx_citemacro_citep">(Voorhees and Ellis, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib14" title="">2019</a>)</cite> and 2020 <cite class="ltx_cite ltx_citemacro_citep">(Craswell et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib2" title="">2020</a>)</cite> deep passage retrieval task (99 queries, graded relevance judgements). Collection is the same as MS MARCO.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.p3.2">As in <cite class="ltx_cite ltx_citemacro_citep">(Giacalone et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib5" title="">2024</a>)</cite> we use MS MARCO when relevance grades are unimportant important, and use the latter when it is, and consider different relevance levels during evaluation.
Additionally, for RQ2, we also use the TREC COVID dataset <cite class="ltx_cite ltx_citemacro_citep">(Voorhees et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib13" title="">2021</a>)</cite> in addition to the TREC 2019-2020 dataset. This dataset contains 50 queries with graded relevance judgements from 0 to 3. Note that we use the CORD-19 variant <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib15" title="">2020</a>)</cite> instead of the BEIR variant <cite class="ltx_cite ltx_citemacro_citep">(Thakur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib11" title="">2021</a>)</cite> used in the ColBERTv2 paper; thus our baseline measurement differs from the officially reported figure.</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_font_bold ltx_title_paragraph">RQ1: Do [MASK] tokens primarily weight non-[MASK] tokens in a query when using ColBERTv2?</h3>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">We reproduce the experiments from <cite class="ltx_cite ltx_citemacro_citet">Giacalone et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib5" title="">2024</a>)</cite> on ColBERT v2, using the TREC 2019-2020 collection.
In the first experiment, we compare a baseline of the standard retrieval pipeline against three conditions where certain token embeddings are replaced with others:
1. We remap <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.1.1">all</em> structural token embeddings (i.e. <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.2">[CLS]</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.3">[SEP]</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.4">[Q]</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.5">[MASK]</span>) to their nearest query text token embedding.
2. We remap <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.6">[MASK]</span> tokens to their nearest non-MASK token (i.e. <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.7">[CLS]</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.8">[SEP]</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.9">[Q]</span>, query text tokens).
3. We remap <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.10">[MASK]</span> tokens to their nearest query text embedding, but leave other structural token embeddings (i.e. <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.11">[CLS]</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.12">[SEP]</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p1.1.13">[Q]</span>) alone.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p2.1">In the second experiment, we modify all queries in the TREC 2019-2020 collection with a length of 3-8 tokens that start with “what is” by moving these two tokens to the end of the query and swapping their positions (e.g. “<em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p2.1.1">what is</em> love” becomes “love <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p2.1.2">is what</em>”).
As indicated in the original paper, this
avoids changing query semantics, while shifting the position of every query token.
We check the change in cosine distance for <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p2.1.3">[CLS]</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p2.1.4">[SEP]</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p2.1.5">[Q]</span>, the first and third query text token, and the 13th and 32nd token in the query, which are guaranteed to be <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px1.p2.1.6">[MASK]</span> tokens.
As a baseline, we repeat the same experiment without requiring queries to start with ”what is”,
possibly generating nonsense (e.g. “<em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p2.1.7">cost of</em> swim spa” becomes “swim spa <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p2.1.8">of cost</em>”).</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px2.1.1">RQ2: Does effectiveness increase with the number of </span><span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.2.2">[MASK]</span><span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px2.3.3">s, up to four times the number ColBERT has been trained with?</span>
</h3>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_tag">1</span></a>, when extending the maximum length of a query past the 32 token window it was trained with, we see a repeating pattern of cosine similarities between <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p1.1.1">[MASK]</span> and non-<span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p1.1.2">[MASK]</span> tokens.
It appears that BERT keeps outputting the same weighting pattern for longer query lengths.
A natural question then, is how ColBERT fares when the maximum query length is increased, and <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p1.1.3">[MASK]</span>-based term weighting dominates document scoring.
One may be wary of the unintentional effects of changing <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p1.1.4">[MASK]</span> counts this way. For instance, could adding an extra <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p1.1.5">[MASK]</span> to the end of a query cause the previous <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p1.1.6">[MASK]</span>s, or even the query text tokens, to change their representations in response?</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p2.1">An easily missed detail about ColBERT is that it treats <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.1">[MASK]</span> and non-<span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.2">[MASK]</span> tokens differently during the contextualization process —
<span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.3">[MASK]</span> tokens cannot be attended to during self attention<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>To our knowledge, this has not been reported in the ColBERT papers.</span></span></span>.
This has two interesting consequences.
One, adding or subtracting <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.4">[MASK]</span>s cannot affect how non-<span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.5">[MASK]</span> tokens are contextualized.
Non-<span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.6">[MASK]</span> tokens cannot attend to <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.7">[MASK]</span> tokens, thus removing <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.8">[MASK]</span>s from the query entirely will not change any of the non-<span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.9">[MASK]</span> representations.
Two, each <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.10">[MASK]</span> token’s computed representation cannot be affected by the existence of <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p2.1.11">other</em> <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.12">[MASK]</span> tokens.
Each <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.13">[MASK]</span> token can only look at the query and itself,
thus, the only change to scoring when adding or removing a <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.14">[MASK]</span> token is the existence of the token’s score. In other words, other tokens cannot change their representations in response to to different numbers of <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p2.1.15">[MASK]</span> tokens.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p3.1">In our second experiment,
we vary the maximum length of the query from 0 to 96 in steps of two, and measure the resulting performance on TREC 2019-2020. Since we start from a length of 0, we hypothesize that performance will initially increase greatly with each additional <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p3.1.1">[MASK]</span>, reflecting the importance of query augmentation.
Performance will then plateau, even as more <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p3.1.2">[MASK]</span>s are added than seen during training, as the <span class="ltx_text ltx_font_typewriter" id="S3.SS0.SSS0.Px2.p3.1.3">[MASK]</span>s repeatedly perform a similar term weighting.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p4">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p4.1">Separately, we report nDCG@10 and nDCG@1000 when the maximum query length is set to 32 to 128, to identify the effect of increasing the total number of tokens seen for each query. In addition to the TREC 2019-2020 dataset, we also use the TREC COVID dataset for this experiment.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p5">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p5.1">ColBERT performs ranking in two phases: an initial set retrieval phase, where documents with at least one embedding very similar to a query embedding are fetched, and a subsequent reranking phase, where documents are reranked by MaxSim. In all experiments, we report metrics for (1) only initial set retrieval is modified, (2) only reranking is modified, and (3) both phases are modified.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Results</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Replacing structural token embeddings by other query token embeddings (TREC 2019-2020, RQ1).
Maximum values are in bold; significant differences from “None” are shown with a dagger (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S4.T1.3.m1.1"><semantics id="S4.T1.3.m1.1b"><mrow id="S4.T1.3.m1.1.1" xref="S4.T1.3.m1.1.1.cmml"><mi id="S4.T1.3.m1.1.1.2" xref="S4.T1.3.m1.1.1.2.cmml">p</mi><mo id="S4.T1.3.m1.1.1.1" xref="S4.T1.3.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T1.3.m1.1.1.3" xref="S4.T1.3.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.m1.1c"><apply id="S4.T1.3.m1.1.1.cmml" xref="S4.T1.3.m1.1.1"><lt id="S4.T1.3.m1.1.1.1.cmml" xref="S4.T1.3.m1.1.1.1"></lt><ci id="S4.T1.3.m1.1.1.2.cmml" xref="S4.T1.3.m1.1.1.2">𝑝</ci><cn id="S4.T1.3.m1.1.1.3.cmml" type="float" xref="S4.T1.3.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.m1.1d">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.m1.1e">italic_p &lt; 0.05</annotation></semantics></math>, Bonferroni-corrected <math alttext="t" class="ltx_Math" display="inline" id="S4.T1.4.m2.1"><semantics id="S4.T1.4.m2.1b"><mi id="S4.T1.4.m2.1.1" xref="S4.T1.4.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.T1.4.m2.1c"><ci id="S4.T1.4.m2.1.1.cmml" xref="S4.T1.4.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.m2.1d">t</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.m2.1e">italic_t</annotation></semantics></math>-tests).</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.20" style="width:368.6pt;height:303pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(42.0pt,-34.5pt) scale(1.29522561586561,1.29522561586561) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.20.16">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.20.16.17.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.20.16.17.1.1"></th>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="4" id="S4.T1.20.16.17.1.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.20.16.17.1.2.1">*ColBERTv2:  Structural Token Remapping</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.7.3.3.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.7.3.3.4.1">Metric</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.7.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.7.3.3.5.1">
<span class="ltx_p" id="S4.T1.7.3.3.5.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.7.3.3.5.1.1.1">None</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.5.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.5.1.1.1.1">
<span class="ltx_p" id="S4.T1.5.1.1.1.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.1.1.1.1.2">All </span><span class="ltx_text ltx_font_typewriter" id="S4.T1.5.1.1.1.1.1.3">[X]</span><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.1.1.1.1.1"> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T1.5.1.1.1.1.1.1.m1.1"><semantics id="S4.T1.5.1.1.1.1.1.1.m1.1a"><mo id="S4.T1.5.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.5.1.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.1.1.1.1.1.1.m1.1b"><ci id="S4.T1.5.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.5.1.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.1.1.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.1.1.1.1.1.1.m1.1d">→</annotation></semantics></math> Text</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.6.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.6.2.2.2.1">
<span class="ltx_p" id="S4.T1.6.2.2.2.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_typewriter" id="S4.T1.6.2.2.2.1.1.2">[MASK]</span><span class="ltx_text ltx_font_bold" id="S4.T1.6.2.2.2.1.1.1"> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T1.6.2.2.2.1.1.1.m1.1"><semantics id="S4.T1.6.2.2.2.1.1.1.m1.1a"><mo id="S4.T1.6.2.2.2.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.6.2.2.2.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.2.2.2.1.1.1.m1.1b"><ci id="S4.T1.6.2.2.2.1.1.1.m1.1.1.cmml" xref="S4.T1.6.2.2.2.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.2.2.2.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.2.2.2.1.1.1.m1.1d">→</annotation></semantics></math> Text</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.7.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.7.3.3.3.1">
<span class="ltx_p" id="S4.T1.7.3.3.3.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_typewriter" id="S4.T1.7.3.3.3.1.1.2">[MASK]</span><span class="ltx_text ltx_font_bold" id="S4.T1.7.3.3.3.1.1.1"> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T1.7.3.3.3.1.1.1.m1.1"><semantics id="S4.T1.7.3.3.3.1.1.1.m1.1a"><mo id="S4.T1.7.3.3.3.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.7.3.3.3.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.3.3.3.1.1.1.m1.1b"><ci id="S4.T1.7.3.3.3.1.1.1.m1.1.1.cmml" xref="S4.T1.7.3.3.3.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.3.3.3.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.3.3.3.1.1.1.m1.1d">→</annotation></semantics></math> Str. &amp; Text</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.20.16.18.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.20.16.18.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.20.16.18.2.1.1">Binary Rel.</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.20.16.18.2.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.20.16.18.2.3"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.20.16.18.2.4"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.20.16.18.2.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.8.4.4.1">MAP(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.T1.8.4.4.1.m1.1"><semantics id="S4.T1.8.4.4.1.m1.1a"><mo id="S4.T1.8.4.4.1.m1.1.1" xref="S4.T1.8.4.4.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.4.4.1.m1.1b"><geq id="S4.T1.8.4.4.1.m1.1.1.cmml" xref="S4.T1.8.4.4.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.4.4.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.4.4.1.m1.1d">≥</annotation></semantics></math>1)</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.10.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.10.6.6.4.1">
<span class="ltx_p" id="S4.T1.10.6.6.4.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.10.6.6.4.1.1.1">0.514</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.9.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.9.5.5.2.1">
<span class="ltx_p" id="S4.T1.9.5.5.2.1.1" style="width:36.1pt;"><math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T1.9.5.5.2.1.1.m1.1"><semantics id="S4.T1.9.5.5.2.1.1.m1.1a"><mo id="S4.T1.9.5.5.2.1.1.m1.1.1" xref="S4.T1.9.5.5.2.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.5.5.2.1.1.m1.1b"><ci id="S4.T1.9.5.5.2.1.1.m1.1.1.cmml" xref="S4.T1.9.5.5.2.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.5.5.2.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.5.5.2.1.1.m1.1d">†</annotation></semantics></math>0.496</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.10.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.10.6.6.3.1">
<span class="ltx_p" id="S4.T1.10.6.6.3.1.1" style="width:36.1pt;"><math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T1.10.6.6.3.1.1.m1.1"><semantics id="S4.T1.10.6.6.3.1.1.m1.1a"><mo id="S4.T1.10.6.6.3.1.1.m1.1.1" xref="S4.T1.10.6.6.3.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.6.6.3.1.1.m1.1b"><ci id="S4.T1.10.6.6.3.1.1.m1.1.1.cmml" xref="S4.T1.10.6.6.3.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.6.6.3.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.6.6.3.1.1.m1.1d">†</annotation></semantics></math>0.508</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.10.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.10.6.6.5.1">
<span class="ltx_p" id="S4.T1.10.6.6.5.1.1" style="width:36.1pt;">0.510</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.11.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.11.7.7.1">MRR(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.T1.11.7.7.1.m1.1"><semantics id="S4.T1.11.7.7.1.m1.1a"><mo id="S4.T1.11.7.7.1.m1.1.1" xref="S4.T1.11.7.7.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.7.7.1.m1.1b"><geq id="S4.T1.11.7.7.1.m1.1.1.cmml" xref="S4.T1.11.7.7.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.7.7.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.7.7.1.m1.1d">≥</annotation></semantics></math>1)@10</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.11.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.11.7.7.2.1">
<span class="ltx_p" id="S4.T1.11.7.7.2.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.11.7.7.2.1.1.1">0.964</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.11.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.11.7.7.3.1">
<span class="ltx_p" id="S4.T1.11.7.7.3.1.1" style="width:36.1pt;">0.958</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.11.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.11.7.7.4.1">
<span class="ltx_p" id="S4.T1.11.7.7.4.1.1" style="width:36.1pt;">0.959</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.11.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.11.7.7.5.1">
<span class="ltx_p" id="S4.T1.11.7.7.5.1.1" style="width:36.1pt;">0.960</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.13.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.12.8.8.1">MAP(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.T1.12.8.8.1.m1.1"><semantics id="S4.T1.12.8.8.1.m1.1a"><mo id="S4.T1.12.8.8.1.m1.1.1" xref="S4.T1.12.8.8.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.8.8.1.m1.1b"><geq id="S4.T1.12.8.8.1.m1.1.1.cmml" xref="S4.T1.12.8.8.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.8.8.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.8.8.1.m1.1d">≥</annotation></semantics></math>2)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.13.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.13.9.9.3.1">
<span class="ltx_p" id="S4.T1.13.9.9.3.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.13.9.9.3.1.1.1">0.502</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.13.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.13.9.9.2.1">
<span class="ltx_p" id="S4.T1.13.9.9.2.1.1" style="width:36.1pt;"><math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T1.13.9.9.2.1.1.m1.1"><semantics id="S4.T1.13.9.9.2.1.1.m1.1a"><mo id="S4.T1.13.9.9.2.1.1.m1.1.1" xref="S4.T1.13.9.9.2.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.9.9.2.1.1.m1.1b"><ci id="S4.T1.13.9.9.2.1.1.m1.1.1.cmml" xref="S4.T1.13.9.9.2.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.9.9.2.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T1.13.9.9.2.1.1.m1.1d">†</annotation></semantics></math>0.489</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.13.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.13.9.9.4.1">
<span class="ltx_p" id="S4.T1.13.9.9.4.1.1" style="width:36.1pt;">0.496</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.13.9.9.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.13.9.9.5.1">
<span class="ltx_p" id="S4.T1.13.9.9.5.1.1" style="width:36.1pt;">0.498</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.14.10.10.1">MRR(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.T1.14.10.10.1.m1.1"><semantics id="S4.T1.14.10.10.1.m1.1a"><mo id="S4.T1.14.10.10.1.m1.1.1" xref="S4.T1.14.10.10.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.10.10.1.m1.1b"><geq id="S4.T1.14.10.10.1.m1.1.1.cmml" xref="S4.T1.14.10.10.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.10.10.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.T1.14.10.10.1.m1.1d">≥</annotation></semantics></math>2)@10</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.14.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.14.10.10.2.1">
<span class="ltx_p" id="S4.T1.14.10.10.2.1.1" style="width:36.1pt;">0.870</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.14.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.14.10.10.3.1">
<span class="ltx_p" id="S4.T1.14.10.10.3.1.1" style="width:36.1pt;">0.871</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.14.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.14.10.10.4.1">
<span class="ltx_p" id="S4.T1.14.10.10.4.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.14.10.10.4.1.1.1">0.888</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.14.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.14.10.10.5.1">
<span class="ltx_p" id="S4.T1.14.10.10.5.1.1" style="width:36.1pt;">0.874</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.15.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.15.11.11.1">MAP(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.T1.15.11.11.1.m1.1"><semantics id="S4.T1.15.11.11.1.m1.1a"><mo id="S4.T1.15.11.11.1.m1.1.1" xref="S4.T1.15.11.11.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.11.11.1.m1.1b"><geq id="S4.T1.15.11.11.1.m1.1.1.cmml" xref="S4.T1.15.11.11.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.11.11.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.T1.15.11.11.1.m1.1d">≥</annotation></semantics></math>3)</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.15.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.15.11.11.2.1">
<span class="ltx_p" id="S4.T1.15.11.11.2.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.15.11.11.2.1.1.1">0.395</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.15.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.15.11.11.3.1">
<span class="ltx_p" id="S4.T1.15.11.11.3.1.1" style="width:36.1pt;">0.388</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.15.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.15.11.11.4.1">
<span class="ltx_p" id="S4.T1.15.11.11.4.1.1" style="width:36.1pt;">0.387</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.15.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.15.11.11.5.1">
<span class="ltx_p" id="S4.T1.15.11.11.5.1.1" style="width:36.1pt;">0.391</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.16.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.16.12.12.1">MRR(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.T1.16.12.12.1.m1.1"><semantics id="S4.T1.16.12.12.1.m1.1a"><mo id="S4.T1.16.12.12.1.m1.1.1" xref="S4.T1.16.12.12.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.12.12.1.m1.1b"><geq id="S4.T1.16.12.12.1.m1.1.1.cmml" xref="S4.T1.16.12.12.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.12.12.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.T1.16.12.12.1.m1.1d">≥</annotation></semantics></math>3)@10</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.16.12.12.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.16.12.12.2.1">
<span class="ltx_p" id="S4.T1.16.12.12.2.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.16.12.12.2.1.1.1">0.616</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.16.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.16.12.12.3.1">
<span class="ltx_p" id="S4.T1.16.12.12.3.1.1" style="width:36.1pt;">0.593</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.16.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.16.12.12.4.1">
<span class="ltx_p" id="S4.T1.16.12.12.4.1.1" style="width:36.1pt;">0.598</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.16.12.12.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.16.12.12.5.1">
<span class="ltx_p" id="S4.T1.16.12.12.5.1.1" style="width:36.1pt;">0.605</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.20.16.19.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.20.16.19.3.1"><span class="ltx_text ltx_font_bold" id="S4.T1.20.16.19.3.1.1">Graded Rel.</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.20.16.19.3.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.20.16.19.3.3"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.20.16.19.3.4"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.20.16.19.3.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.17.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.17.13.13.2">nDCG@10</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.17.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.17.13.13.3.1">
<span class="ltx_p" id="S4.T1.17.13.13.3.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.17.13.13.3.1.1.1">0.749</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.17.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.17.13.13.1.1">
<span class="ltx_p" id="S4.T1.17.13.13.1.1.1" style="width:36.1pt;"><math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T1.17.13.13.1.1.1.m1.1"><semantics id="S4.T1.17.13.13.1.1.1.m1.1a"><mo id="S4.T1.17.13.13.1.1.1.m1.1.1" xref="S4.T1.17.13.13.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.17.13.13.1.1.1.m1.1b"><ci id="S4.T1.17.13.13.1.1.1.m1.1.1.cmml" xref="S4.T1.17.13.13.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.13.13.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T1.17.13.13.1.1.1.m1.1d">†</annotation></semantics></math>0.733</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.17.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.17.13.13.4.1">
<span class="ltx_p" id="S4.T1.17.13.13.4.1.1" style="width:36.1pt;">0.741</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.17.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.17.13.13.5.1">
<span class="ltx_p" id="S4.T1.17.13.13.5.1.1" style="width:36.1pt;">0.745</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.20.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.20.16.16.4">nDCG@1000</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T1.20.16.16.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.20.16.16.5.1">
<span class="ltx_p" id="S4.T1.20.16.16.5.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.20.16.16.5.1.1.1">0.712</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T1.18.14.14.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.18.14.14.1.1">
<span class="ltx_p" id="S4.T1.18.14.14.1.1.1" style="width:36.1pt;"><math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T1.18.14.14.1.1.1.m1.1"><semantics id="S4.T1.18.14.14.1.1.1.m1.1a"><mo id="S4.T1.18.14.14.1.1.1.m1.1.1" xref="S4.T1.18.14.14.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.18.14.14.1.1.1.m1.1b"><ci id="S4.T1.18.14.14.1.1.1.m1.1.1.cmml" xref="S4.T1.18.14.14.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.14.14.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T1.18.14.14.1.1.1.m1.1d">†</annotation></semantics></math>0.691</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T1.19.15.15.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.19.15.15.2.1">
<span class="ltx_p" id="S4.T1.19.15.15.2.1.1" style="width:36.1pt;"><math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T1.19.15.15.2.1.1.m1.1"><semantics id="S4.T1.19.15.15.2.1.1.m1.1a"><mo id="S4.T1.19.15.15.2.1.1.m1.1.1" xref="S4.T1.19.15.15.2.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.19.15.15.2.1.1.m1.1b"><ci id="S4.T1.19.15.15.2.1.1.m1.1.1.cmml" xref="S4.T1.19.15.15.2.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.15.15.2.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T1.19.15.15.2.1.1.m1.1d">†</annotation></semantics></math> 0.702</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T1.20.16.16.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.20.16.16.3.1">
<span class="ltx_p" id="S4.T1.20.16.16.3.1.1" style="width:36.1pt;"><math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T1.20.16.16.3.1.1.m1.1"><semantics id="S4.T1.20.16.16.3.1.1.m1.1a"><mo id="S4.T1.20.16.16.3.1.1.m1.1.1" xref="S4.T1.20.16.16.3.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.20.16.16.3.1.1.m1.1b"><ci id="S4.T1.20.16.16.3.1.1.m1.1.1.cmml" xref="S4.T1.20.16.16.3.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.20.16.16.3.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T1.20.16.16.3.1.1.m1.1d">†</annotation></semantics></math>0.703</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_font_bold ltx_title_paragraph">RQ1: Do [MASK] tokens primarily weight non-[MASK] tokens in a query when using ColBERTv2</h3>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.2">For the <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.2.1">[MASK]</span> remapping experiment, we see that on ColBERTv2, remapping <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.2.2">[MASK]</span>s causes a consistent decrease in performance (see Table <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S4.T1" title="Table 1 ‣ 4. Results ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_tag">1</span></a>). For nDCG@1000, all conditions are significantly worse than the baseline. The “All [X] <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px1.p1.1.m1.1a"><mo id="S4.SS0.SSS0.Px1.p1.1.m1.1.1" stretchy="false" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.1.m1.1d">→</annotation></semantics></math> Text” condition performs worse than any other condition, many times being significantly worse than the baseline. The “<span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.2.3">[MASK]</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px1.p1.2.m2.1a"><mo id="S4.SS0.SSS0.Px1.p1.2.m2.1.1" stretchy="false" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.2.m2.1d">→</annotation></semantics></math> Str. &amp; Text” condition performs best of the three conditions. This is both consistent with the ColBERTv1 results from <cite class="ltx_cite ltx_citemacro_citet">Giacalone et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib5" title="">2024</a>)</cite>, and provides more evidence for that <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.2.4">[MASK]</span> embeddings simply select all non-<span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.2.5">[MASK]</span>s as candidates for term weighting.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F2.1"><span class="ltx_text ltx_font_smallcaps" id="S4.F2.1.1">ColBERTv1</span>
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="296" id="S4.F2.1.g1" src="x2.png" width="830"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F2.2"><span class="ltx_text ltx_font_smallcaps" id="S4.F2.2.1">ColBERTv2</span>
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="300" id="S4.F2.2.g1" src="x3.png" width="830"/></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Left column: Cosine distance after tokens are switched from ”what is” to ”is what” in ColBERTv1 vs. ColBERTv2. We see the same trend of <span class="ltx_text ltx_font_typewriter" id="S4.F2.7.1">[Q]</span> and <span class="ltx_text ltx_font_typewriter" id="S4.F2.8.2">[MASK]</span> tokens having the most shifting, and an overall increase in shifting when “what is” is not a requirement (right column). The contrast between <span class="ltx_text ltx_font_typewriter" id="S4.F2.9.3">[Q]</span> and <span class="ltx_text ltx_font_typewriter" id="S4.F2.10.4">[MASK]</span> versus other tokens is more apparent in ColBERTv2 than ColBERTv1.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p2.1">For the query shift experiment shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S4.F2" title="Figure 2 ‣ RQ1: Do [MASK] tokens primarily weight non-[MASK] tokens in a query when using ColBERTv2 ‣ 4. Results ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_tag">2</span></a>, we see the same pattern reported in <cite class="ltx_cite ltx_citemacro_citet">Giacalone et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib5" title="">2024</a>)</cite>: <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p2.1.1">[Q]</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p2.1.2">[MASK]</span> tokens vary greatly after “what is” is swapped and moved, while <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p2.1.3">[CLS]</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p2.1.4">[SEP]</span>, and query text tokens do not change nearly as much. In fact, with ColBERTv2, this difference is even starker. Given that this is a pattern that has now manifested itself across two separately trained checkpoints, with two different training objectives, we suspect that the <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p2.1.5">[Q]</span> token performs a similar function to <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p2.1.6">[MASK]</span> tokens – adding weight to certain tokens to influence scoring.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p3.1">This would also explain the pattern demonstrated by the <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p3.1.1">[Q]</span> token in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_tag">1</span></a>, where <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p3.1.2">[MASK]</span>s that are very similar to the <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p3.1.3">[Q]</span> token are always also very similar to some other token. When we visualized several different queries using the same visualization shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_tag">1</span></a>, we saw that <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p3.1.4">[Q]</span> was the only non-<span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p3.1.5">[MASK]</span> structural token consistently very similar to query text tokens.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Changing the maximum length of queries from 32 to 128 with <span class="ltx_text ltx_font_typewriter" id="S4.T2.7.1">[MASK]</span> padding.
Maximum values are in bold; significant differences from “32” are shown with a dagger (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S4.T2.3.m1.1"><semantics id="S4.T2.3.m1.1b"><mrow id="S4.T2.3.m1.1.1" xref="S4.T2.3.m1.1.1.cmml"><mi id="S4.T2.3.m1.1.1.2" xref="S4.T2.3.m1.1.1.2.cmml">p</mi><mo id="S4.T2.3.m1.1.1.1" xref="S4.T2.3.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T2.3.m1.1.1.3" xref="S4.T2.3.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.m1.1c"><apply id="S4.T2.3.m1.1.1.cmml" xref="S4.T2.3.m1.1.1"><lt id="S4.T2.3.m1.1.1.1.cmml" xref="S4.T2.3.m1.1.1.1"></lt><ci id="S4.T2.3.m1.1.1.2.cmml" xref="S4.T2.3.m1.1.1.2">𝑝</ci><cn id="S4.T2.3.m1.1.1.3.cmml" type="float" xref="S4.T2.3.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.m1.1d">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.m1.1e">italic_p &lt; 0.05</annotation></semantics></math>, Bonferroni-corrected <math alttext="t" class="ltx_Math" display="inline" id="S4.T2.4.m2.1"><semantics id="S4.T2.4.m2.1b"><mi id="S4.T2.4.m2.1.1" xref="S4.T2.4.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.T2.4.m2.1c"><ci id="S4.T2.4.m2.1.1.cmml" xref="S4.T2.4.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.m2.1d">t</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.m2.1e">italic_t</annotation></semantics></math>-tests).</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.5" style="width:433.6pt;height:256pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(49.1pt,-29.0pt) scale(1.29273221951408,1.29273221951408) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.5.1.2.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.5.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.5.1.2.1.2"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.5.1.2.1.2.1">TREC 2019-2020</span></th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.5.1.2.1.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.5.1.2.1.3.1">TREC COVID</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T2.5.1.3.2.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.5.1.3.2.1.1">Metric</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S4.T2.5.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.3.2.2.1">
<span class="ltx_p" id="S4.T2.5.1.3.2.2.1.1" style="width:36.1pt;">32</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r" id="S4.T2.5.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.3.2.3.1">
<span class="ltx_p" id="S4.T2.5.1.3.2.3.1.1" style="width:36.1pt;">128</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S4.T2.5.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.3.2.4.1">
<span class="ltx_p" id="S4.T2.5.1.3.2.4.1.1" style="width:36.1pt;">32</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S4.T2.5.1.3.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.3.2.5.1">
<span class="ltx_p" id="S4.T2.5.1.3.2.5.1.1" style="width:36.1pt;">128</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.5.1.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.5.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.4.1.1.1">Only Set Retrieval</span></th>
<td class="ltx_td ltx_align_top ltx_border_t" id="S4.T2.5.1.4.1.2"></td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.5.1.4.1.3"></td>
<td class="ltx_td ltx_align_top ltx_border_t" id="S4.T2.5.1.4.1.4"></td>
<td class="ltx_td ltx_align_top ltx_border_t" id="S4.T2.5.1.4.1.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.1.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.5.1.5.2.1">nDCG@10</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.5.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.5.2.2.1">
<span class="ltx_p" id="S4.T2.5.1.5.2.2.1.1" style="width:36.1pt;">0.749</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.5.1.5.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.5.2.3.1">
<span class="ltx_p" id="S4.T2.5.1.5.2.3.1.1" style="width:36.1pt;">0.749</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.5.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.5.2.4.1">
<span class="ltx_p" id="S4.T2.5.1.5.2.4.1.1" style="width:36.1pt;">0.612</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.5.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.5.2.5.1">
<span class="ltx_p" id="S4.T2.5.1.5.2.5.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.5.2.5.1.1.1">0.616</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.5.1.1.2">nDCG@1000</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.1.3.1">
<span class="ltx_p" id="S4.T2.5.1.1.3.1.1" style="width:36.1pt;">0.712</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.5.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.1.4.1">
<span class="ltx_p" id="S4.T2.5.1.1.4.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.1.4.1.1.1">0.717</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.1.5.1">
<span class="ltx_p" id="S4.T2.5.1.1.5.1.1" style="width:36.1pt;">0.343</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.1.1.1">
<span class="ltx_p" id="S4.T2.5.1.1.1.1.1" style="width:36.1pt;"><math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T2.5.1.1.1.1.1.m1.1"><semantics id="S4.T2.5.1.1.1.1.1.m1.1a"><mo id="S4.T2.5.1.1.1.1.1.m1.1.1" xref="S4.T2.5.1.1.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.1.1.1.1.1.m1.1b"><ci id="S4.T2.5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.5.1.1.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.1.1.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.1.1.1.1.1.m1.1d">†</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.1.1.1.1.1">0.350</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.1.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.5.1.6.3.1"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.6.3.1.1">Only Reranking</span></th>
<td class="ltx_td ltx_align_top ltx_border_t" id="S4.T2.5.1.6.3.2"></td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.5.1.6.3.3"></td>
<td class="ltx_td ltx_align_top ltx_border_t" id="S4.T2.5.1.6.3.4"></td>
<td class="ltx_td ltx_align_top ltx_border_t" id="S4.T2.5.1.6.3.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.1.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.5.1.7.4.1">nDCG@10</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.7.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.7.4.2.1">
<span class="ltx_p" id="S4.T2.5.1.7.4.2.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.7.4.2.1.1.1">0.749</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.5.1.7.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.7.4.3.1">
<span class="ltx_p" id="S4.T2.5.1.7.4.3.1.1" style="width:36.1pt;">0.739</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.7.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.7.4.4.1">
<span class="ltx_p" id="S4.T2.5.1.7.4.4.1.1" style="width:36.1pt;">0.612</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.7.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.7.4.5.1">
<span class="ltx_p" id="S4.T2.5.1.7.4.5.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.7.4.5.1.1.1">0.640</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.1.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.5.1.8.5.1">nDCG@1000</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.8.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.8.5.2.1">
<span class="ltx_p" id="S4.T2.5.1.8.5.2.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.8.5.2.1.1.1">0.712</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.5.1.8.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.8.5.3.1">
<span class="ltx_p" id="S4.T2.5.1.8.5.3.1.1" style="width:36.1pt;">0.707</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.8.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.8.5.4.1">
<span class="ltx_p" id="S4.T2.5.1.8.5.4.1.1" style="width:36.1pt;">0.343</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.8.5.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.8.5.5.1">
<span class="ltx_p" id="S4.T2.5.1.8.5.5.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.8.5.5.1.1.1">0.349</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.1.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.5.1.9.6.1"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.9.6.1.1">Set Retrieval and Reranking</span></th>
<td class="ltx_td ltx_align_top ltx_border_t" id="S4.T2.5.1.9.6.2"></td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.5.1.9.6.3"></td>
<td class="ltx_td ltx_align_top ltx_border_t" id="S4.T2.5.1.9.6.4"></td>
<td class="ltx_td ltx_align_top ltx_border_t" id="S4.T2.5.1.9.6.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.1.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.5.1.10.7.1">nDCG@10</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.10.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.10.7.2.1">
<span class="ltx_p" id="S4.T2.5.1.10.7.2.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.10.7.2.1.1.1">0.749</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T2.5.1.10.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.10.7.3.1">
<span class="ltx_p" id="S4.T2.5.1.10.7.3.1.1" style="width:36.1pt;">0.743</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.10.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.10.7.4.1">
<span class="ltx_p" id="S4.T2.5.1.10.7.4.1.1" style="width:36.1pt;">0.612</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.5.1.10.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.10.7.5.1">
<span class="ltx_p" id="S4.T2.5.1.10.7.5.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.10.7.5.1.1.1">0.643</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.1.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.5.1.11.8.1">nDCG@1000</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T2.5.1.11.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.11.8.2.1">
<span class="ltx_p" id="S4.T2.5.1.11.8.2.1.1" style="width:36.1pt;">0.712</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" id="S4.T2.5.1.11.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.11.8.3.1">
<span class="ltx_p" id="S4.T2.5.1.11.8.3.1.1" style="width:36.1pt;">0.712</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T2.5.1.11.8.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.11.8.4.1">
<span class="ltx_p" id="S4.T2.5.1.11.8.4.1.1" style="width:36.1pt;">0.343</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T2.5.1.11.8.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.5.1.11.8.5.1">
<span class="ltx_p" id="S4.T2.5.1.11.8.5.1.1" style="width:36.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.11.8.5.1.1.1">0.355</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S4.SS0.SSS0.Px2.1.1">RQ2: Does effectiveness increase as the number of </span><span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.2.2">[MASK]</span><span class="ltx_text ltx_font_bold" id="S4.SS0.SSS0.Px2.3.3">s increases up to four times the number ColBERT has been trained with?</span>
</h3>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.2">In Table <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S4.T2" title="Table 2 ‣ RQ1: Do [MASK] tokens primarily weight non-[MASK] tokens in a query when using ColBERTv2 ‣ 4. Results ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_tag">2</span></a>, we see nDCG@/@1000 on both TREC 2019-2020 and TREC COVID as we vary the maximum query length.
We first focus on the results from the TREC 2019-2020 dataset. Modifying only set retrieval causes a minor increase in nDCG@1000, but appears to have no effect on nDCG@10, likely due to baseline set retrieval already retrieving most relevant documents.
Modifying only reranking on TREC 2019-2020 causes both nDCG@10/@1000 to decrease. When modifying both phases, nDCG@10 very slightly increases, but nDCG@1000 does not change, likely due to the increase from set retrieval and the decrease from reranking negating each other.
Ultimately, all changes observed on TREC 2019-2020 are small, and we never saw an increase or decrease greater than 1%, nor did we observe any statistically significant <math alttext="p" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.1.m1.1d">italic_p</annotation></semantics></math>-values when performing Bonferroni-corrected <math alttext="t" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px2.p1.2.m2.1a"><mi id="S4.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.2.m2.1b"><ci id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.2.m2.1d">italic_t</annotation></semantics></math>-tests.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p2.1">On the TREC COVID dataset, we see an increase in nDCG@/@1000 as we increase the length of the query to 128 tokens, for both reranking and set retrieval. These changes are still very small, in the range of 1-3%. The increase in nDCG@1000, however, is statistically significant.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p3.1">A possible reason for this difference in behavior between TREC 2019-2020 and COVID is that the former dataset has less tokens per query on average compared to the latter (9.68 versus 13.92 tokens), potentially causing certain queries to be incompletely weighted when using only 32 tokens.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="780" id="S4.F3.g1" src="x4.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>nDCG@10, MRR(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.F3.4.m1.1"><semantics id="S4.F3.4.m1.1b"><mo id="S4.F3.4.m1.1.1" xref="S4.F3.4.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.F3.4.m1.1c"><geq id="S4.F3.4.m1.1.1.cmml" xref="S4.F3.4.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.4.m1.1d">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.F3.4.m1.1e">≥</annotation></semantics></math>2)@10, nDCG@1000, and MAP(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.F3.5.m2.1"><semantics id="S4.F3.5.m2.1b"><mo id="S4.F3.5.m2.1.1" xref="S4.F3.5.m2.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.F3.5.m2.1c"><geq id="S4.F3.5.m2.1.1.cmml" xref="S4.F3.5.m2.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.5.m2.1d">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.F3.5.m2.1e">≥</annotation></semantics></math>2) increasing number of <span class="ltx_text ltx_font_typewriter" id="S4.F3.8.1">[MASK]</span> tokens from 0 to 96 on TREC 2019-2020. The red line shows a standard length of 32 total tokens. Significant differences from the baseline indicated with a start (Bonferroni correction, <math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S4.F3.6.m3.1"><semantics id="S4.F3.6.m3.1b"><mrow id="S4.F3.6.m3.1.1" xref="S4.F3.6.m3.1.1.cmml"><mi id="S4.F3.6.m3.1.1.2" xref="S4.F3.6.m3.1.1.2.cmml">p</mi><mo id="S4.F3.6.m3.1.1.1" xref="S4.F3.6.m3.1.1.1.cmml">&lt;</mo><mn id="S4.F3.6.m3.1.1.3" xref="S4.F3.6.m3.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.6.m3.1c"><apply id="S4.F3.6.m3.1.1.cmml" xref="S4.F3.6.m3.1.1"><lt id="S4.F3.6.m3.1.1.1.cmml" xref="S4.F3.6.m3.1.1.1"></lt><ci id="S4.F3.6.m3.1.1.2.cmml" xref="S4.F3.6.m3.1.1.2">𝑝</ci><cn id="S4.F3.6.m3.1.1.3.cmml" type="float" xref="S4.F3.6.m3.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m3.1d">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.F3.6.m3.1e">italic_p &lt; 0.05</annotation></semantics></math>).</figcaption>
</figure>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p4">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p4.4">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S4.F3" title="Figure 3 ‣ RQ2: Does effectiveness increase as the number of [MASK]s increases up to four times the number ColBERT has been trained with? ‣ 4. Results ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_tag">3</span></a>, we see nDCG@10/@1000, MRR(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p4.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p4.1.m1.1a"><mo id="S4.SS0.SSS0.Px2.p4.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p4.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p4.1.m1.1b"><geq id="S4.SS0.SSS0.Px2.p4.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p4.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p4.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p4.1.m1.1d">≥</annotation></semantics></math>2)@10, and MAP(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p4.2.m2.1"><semantics id="S4.SS0.SSS0.Px2.p4.2.m2.1a"><mo id="S4.SS0.SSS0.Px2.p4.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p4.2.m2.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p4.2.m2.1b"><geq id="S4.SS0.SSS0.Px2.p4.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p4.2.m2.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p4.2.m2.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p4.2.m2.1d">≥</annotation></semantics></math>2) as we vary the number of <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p4.4.1">[MASK]</span> tokens each query has.
For most of the metrics, moving from 0 to 4 <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p4.4.2">[MASK]</span>s appears to actually have a detrimental affect, indicating only using a couple <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p4.4.3">[MASK]</span> tokens is worse than none at all.
From 4 to <math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p4.3.m3.1"><semantics id="S4.SS0.SSS0.Px2.p4.3.m3.1a"><mo id="S4.SS0.SSS0.Px2.p4.3.m3.1.1" xref="S4.SS0.SSS0.Px2.p4.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p4.3.m3.1b"><csymbol cd="latexml" id="S4.SS0.SSS0.Px2.p4.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px2.p4.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p4.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p4.3.m3.1d">∼</annotation></semantics></math>24 <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p4.4.4">[MASK]</span>s, however, we see a sharp increase in nDCG@10/@1000 and MAP(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p4.4.m4.1"><semantics id="S4.SS0.SSS0.Px2.p4.4.m4.1a"><mo id="S4.SS0.SSS0.Px2.p4.4.m4.1.1" xref="S4.SS0.SSS0.Px2.p4.4.m4.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p4.4.m4.1b"><geq id="S4.SS0.SSS0.Px2.p4.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px2.p4.4.m4.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p4.4.m4.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p4.4.m4.1d">≥</annotation></semantics></math>2).
This peak coincides with the point where on average, queries have an overall length of 32 (i.e., the input size used for training).
From there on, there is a slight decrease across all metrics, which we expect from the results of the previous experiment.
However,
we also
can see that despite this slight reduction in performance, it is still far better than not having any <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p4.4.5">[MASK]</span>s at all.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p5">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p5.2">It appears that as more <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p5.2.1">[MASK]</span>s are used on this collection, performance tends to converge to slightly below the baseline. As seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#S4.F3" title="Figure 3 ‣ RQ2: Does effectiveness increase as the number of [MASK]s increases up to four times the number ColBERT has been trained with? ‣ 4. Results ‣ ColBERT [MASK] Tokens Perform Term Weighting and Exhibit Cyclic Contextualization"><span class="ltx_text ltx_ref_tag">3</span></a>, using 8 <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p5.2.2">[MASK]</span>s or less causes a statistically significant reduction in performance, while using more than that results in performance that is not significantly different from the baseline.
Also,
while increasing the number of <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p5.2.3">[MASK]</span> tokens from 0 to 96, RR(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p5.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p5.1.m1.1a"><mo id="S4.SS0.SSS0.Px2.p5.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p5.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p5.1.m1.1b"><geq id="S4.SS0.SSS0.Px2.p5.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p5.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p5.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p5.1.m1.1d">≥</annotation></semantics></math>2)@10 does not change in a statistically significant way. For the TREC 2019-2020 dataset, query augmentation does not significantly impact RR(rel<math alttext="\geq" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p5.2.m2.1"><semantics id="S4.SS0.SSS0.Px2.p5.2.m2.1a"><mo id="S4.SS0.SSS0.Px2.p5.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p5.2.m2.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p5.2.m2.1b"><geq id="S4.SS0.SSS0.Px2.p5.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p5.2.m2.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p5.2.m2.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p5.2.m2.1d">≥</annotation></semantics></math>2)@10.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The unconventional decision to have ColBERT integrate the padding token used for queries (<span class="ltx_text ltx_font_typewriter" id="S5.p1.1.1">[MASK]</span>) directly into its scoring mechanism has resulted in state of the art performance.
Padding with <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.2">[MASK]</span> tokens has been demonstrated to act analogous to term weighting, making it more important for documents to match against some terms than others.
An interesting aspect of <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.3">[MASK]</span> representations is that they form a repeating pattern, even when expanding the query past the maximum query length trained with.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We were able to confirm the findings of <cite class="ltx_cite ltx_citemacro_citet">Giacalone et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.13672v1#bib.bib5" title="">2024</a>)</cite> on ColBERTv1, showing that even with ColBERTv2, remapping <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.1">[MASK]</span>s to their nearest non-<span class="ltx_text ltx_font_typewriter" id="S5.p2.1.2">[MASK]</span> generally produces non-significant differences in effectiveness metrics, and that <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.3">[MASK]</span>s are much more sensitive to token order than <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.4">[CLS]</span>, <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.5">[SEP]</span>, and even query text tokens.
We also found that a partial term weighting using fewer <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.6">[MASK]</span> tokens than used in trained causes effectiveness to decrease, i.e. using no <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.7">[MASK]</span>s performs better than using a small number of <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.8">[MASK]</span>s.
Increasing the number of <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.9">[MASK]</span>s from this low point to the amount trained with causes performance to shoot up.
Afterwards, performance slightly reduces as <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.10">[MASK]</span>s are added across most metrics, but still performs much better than not using <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.11">[MASK]</span>s at all.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Overall, though there is a slight drop in performance, ColBERT’s <span class="ltx_text ltx_font_typewriter" id="S5.p3.1.1">[MASK]</span>-based term weighting strategy performs well past the maximum query length it was trained with, converging to near baseline levels as the size of the query input increases.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Craswell et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Nick Craswell, Bhaskar
Mitra, Emine Yilmaz, and Daniel
Campos. 2020.

</span>
<span class="ltx_bibblock">Overview of the TREC 2020 Deep Learning Track.
In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proc. Text REtrieval Conference (TREC)</em>
<em class="ltx_emph ltx_font_italic" id="bib.bib2.4.2">(NIST Special Publication,
Vol. 1266)</em>, Ellen M.
Voorhees and Angela Ellis (Eds.).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei
Chang, Kenton Lee, and Kristina
Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. In
<em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers)</em>.
Association for Computational Linguistics,
Minneapolis, Minnesota, 4171–4186.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/N19-1423" title="">https://doi.org/10.18653/v1/N19-1423</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Formal et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Thibault Formal, Benjamin
Piwowarski, and Stéphane Clinchant.
2021.

</span>
<span class="ltx_bibblock">A White Box Analysis of ColBERT. In
<em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Advances in Information Retrieval</em>,
Djoerd Hiemstra,
Marie-Francine Moens, Josiane Mothe,
Raffaele Perego, Martin Potthast, and
Fabrizio Sebastiani (Eds.). Springer
International Publishing, Cham,
257–263.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Giacalone et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ben Giacalone, Greg
Paiement, Quinn Tucker, and Richard
Zanibbi. 2024.

</span>
<span class="ltx_bibblock">Beneath the [MASK]: An Analysis of Structural
Tokens in ColBERT. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Lecture Notes in Computer
Science</em>. Springer, to appear.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab and Zaharia (2020)</span>
<span class="ltx_bibblock">
Omar Khattab and Matei
Zaharia. 2020.

</span>
<span class="ltx_bibblock">ColBERT: Efficient and Effective Passage Search
via Contextualized Late Interaction over BERT. In
<em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proc. SIGIR</em>,
Jimmy X. Huang,
Yi Chang, Xueqi Cheng,
Jaap Kamps, Vanessa Murdock,
Ji-Rong Wen, and Yiqun Liu (Eds.).
39–48.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3397271.3401075" title="">https://doi.org/10.1145/3397271.3401075</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Macdonald et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Craig Macdonald, Nicola
Tonellotto, Sean MacAvaney, and Iadh
Ounis. 2021.

</span>
<span class="ltx_bibblock">PyTerrier: Declarative Experimentation in Python
from BM25 to Dense Retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of
the 30th ACM International Conference on Information &amp; Knowledge
Management</em> (Virtual Event, Queensland, Australia)
<em class="ltx_emph ltx_font_italic" id="bib.bib7.4.2">(CIKM ’21)</em>. Association for
Computing Machinery, New York, NY, USA,
4526–4533.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3459637.3482013" title="">https://doi.org/10.1145/3459637.3482013</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Tri Nguyen, Mir
Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and
Li Deng. 2016.

</span>
<span class="ltx_bibblock">MS MARCO: A Human Generated MAchine Reading
COmprehension Dataset. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the
Workshop on Cognitive Computation: Integrating neural and symbolic approaches
2016 co-located with the 30th Annual Conference on Neural Information
Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016</em>
<em class="ltx_emph ltx_font_italic" id="bib.bib8.4.2">(CEUR Workshop Proceedings,
Vol. 1773)</em>,
Tarek Richard Besold,
Antoine Bordes, Artur S. d’Avila Garcez,
and Greg Wayne (Eds.). CEUR-WS.org.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf" title="">https://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ruiyang Ren, Shangwen Lv,
Yingqi Qu, Jing Liu,
Wayne Xin Zhao, Qiaoqiao She,
Hua Wu, Haifeng Wang, and
Ji-Rong Wen. 2021.

</span>
<span class="ltx_bibblock">PAIR: Leveraging Passage-Centric Similarity
Relation for Improving Dense Passage Retrieval. In
<em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Findings of the Association for Computational
Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em>
<em class="ltx_emph ltx_font_italic" id="bib.bib9.4.2">(Findings of ACL,
Vol. ACL/IJCNLP 2021)</em>,
Chengqing Zong, Fei
Xia, Wenjie Li, and Roberto Navigli
(Eds.). Association for Computational Linguistics,
2173–2183.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/V1/2021.FINDINGS-ACL.191" title="">https://doi.org/10.18653/V1/2021.FINDINGS-ACL.191</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santhanam et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Keshav Santhanam, Omar
Khattab, Jon Saad-Falcon, Christopher
Potts, and Matei Zaharia.
2022.

</span>
<span class="ltx_bibblock">ColBERTv2: Effective and Efficient Retrieval
via Lightweight Late Interaction. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proc. North
American Chapter of the Association for Computational Linguistics (NAACL)</em>,
Marine Carpuat,
Marie-Catherine de Marneffe, and
Ivan Vladimir Meza Ruiz (Eds.).
3715–3734.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2022.naacl-main.272" title="">https://doi.org/10.18653/v1/2022.naacl-main.272</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nandan Thakur, Nils
Reimers, Andreas Rücklé,
Abhishek Srivastava, and Iryna
Gurevych. 2021.

</span>
<span class="ltx_bibblock">BEIR: A Heterogeneous Benchmark for Zero-shot
Evaluation of Information Retrieval Models. In
<em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the Neural Information Processing
Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks
2021, December 2021, virtual</em>, Joaquin
Vanschoren and Sai-Kit Yeung (Eds.).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html" title="">https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tonellotto and Macdonald (2021)</span>
<span class="ltx_bibblock">
Nicola Tonellotto and
Craig Macdonald. 2021.

</span>
<span class="ltx_bibblock">Query Embedding Pruning for Dense Retrieval. In
<em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 30th ACM International
Conference on Information &amp; Knowledge Management</em> (Virtual Event,
Queensland, Australia) <em class="ltx_emph ltx_font_italic" id="bib.bib12.2.2">(CIKM ’21)</em>.
Association for Computing Machinery,
New York, NY, USA, 3453–3457.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3459637.3482162" title="">https://doi.org/10.1145/3459637.3482162</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voorhees et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ellen Voorhees, Tasmeer
Alam, Steven Bedrick, Dina
Demner-Fushman, William R. Hersh, Kyle
Lo, Kirk Roberts, Ian Soboroff, and
Lucy Lu Wang. 2021.

</span>
<span class="ltx_bibblock">TREC-COVID: constructing a pandemic information
retrieval test collection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">SIGIR Forum</em> 54,
1, Article 1 (feb
2021), 12 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3451964.3451965" title="">https://doi.org/10.1145/3451964.3451965</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voorhees and Ellis (2019)</span>
<span class="ltx_bibblock">
Ellen M. Voorhees and
Angela Ellis (Eds.). 2019.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the Twenty-Eighth Text
REtrieval Conference, TREC 2019, Gaithersburg, Maryland, USA, November
13-15, 2019</em>. NIST Special Publication,
Vol. 1250. National Institute of
Standards and Technology (NIST).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trec.nist.gov/pubs/trec28/trec2019.html" title="">https://trec.nist.gov/pubs/trec28/trec2019.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Lucy Lu Wang, Kyle Lo,
Yoganand Chandrasekhar, Russell Reas,
Jiangjiang Yang, Doug Burdick,
Darrin Eide, Kathryn Funk,
Yannis Katsis, Rodney Michael Kinney,
Yunyao Li, Ziyang Liu,
William Merrill, Paul Mooney,
Dewey A. Murdick, Devvret Rishi,
Jerry Sheehan, Zhihong Shen,
Brandon Stilson, Alex D. Wade,
Kuansan Wang, Nancy Xin Ru Wang,
Christopher Wilhelm, Boya Xie,
Douglas M. Raymond, Daniel S. Weld,
Oren Etzioni, and Sebastian Kohlmeier.
2020.

</span>
<span class="ltx_bibblock">CORD-19: The COVID-19 Open Research Dataset.
In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the 1st Workshop on NLP for
COVID-19 at ACL 2020</em>, Karin
Verspoor, Kevin Bretonnel Cohen, Mark
Dredze, Emilio Ferrara, Jonathan May,
Robert Munro, Cecile Paris, and
Byron Wallace (Eds.). Association for
Computational Linguistics, Online.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.nlpcovid19-acl.1" title="">https://aclanthology.org/2020.nlpcovid19-acl.1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiao Wang, Craig
Macdonald, Nicola Tonellotto, and Iadh
Ounis. 2023.

</span>
<span class="ltx_bibblock">ColBERT-PRF: Semantic pseudo-relevance feedback for
dense passage and document retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">ACM Transactions on the Web</em>
17, 1 (2023),
1–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Aug 24 21:33:54 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
