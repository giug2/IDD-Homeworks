<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.06561] Performance Improvement of Federated Learning Server using Smart NIC</title><meta property="og:description" content="Federated learning is a distributed machine learning approach where local weight parameters trained by clients locally are aggregated as global parameters by a server.
The global parameters can be trained without uploa…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Performance Improvement of Federated Learning Server using Smart NIC">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Performance Improvement of Federated Learning Server using Smart NIC">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.06561">

<!--Generated on Wed Feb 28 17:55:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Performance Improvement of Federated Learning Server using Smart NIC</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Naoki Shibahara
<br class="ltx_break">Keio University
<br class="ltx_break">3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">shibahara@arc.ics.keio.ac.jp
<br class="ltx_break"></span>&amp;Michihiro Koibuchi
<br class="ltx_break">National Institute of Informatics
<br class="ltx_break">2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">koibuchi@nii.ac.jp</span> 
<br class="ltx_break">&amp;Hiroki Matsutani
<br class="ltx_break">Keio University
<br class="ltx_break">3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">matutani@arc.ics.keio.ac.jp</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Federated learning is a distributed machine learning approach where local weight parameters trained by clients locally are aggregated as global parameters by a server.
The global parameters can be trained without uploading privacy-sensitive raw data owned by clients to the server.
The aggregation on the server is simply done by averaging the local weight parameters, so it is an I/O intensive task where a network processing accounts for a large portion compared to the computation.
The network processing workload further increases as the number of clients increases.
To mitigate the network processing workload, in this paper, the federated learning server is offloaded to NVIDIA BlueField-2 DPU which is a smart NIC (Network Interface Card) that has eight processing cores.
Dedicated processing cores are assigned by DPDK (Data Plane Development Kit) for receiving the local weight parameters and sending the global parameters.
The aggregation task is parallelized by exploiting multiple cores available on the DPU.
To further improve the performance, an approximated design that eliminates an exclusive access control between the computation threads is also implemented.
Evaluation results show that the proposed DPDK-based federated learning server on the DPU with the approximation accelerates the execution time by 1.39 times with a negligible accuracy loss compared with a baseline server on the host CPU.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.4" class="ltx_p"><em id="p1.4.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.4.2" class="ltx_text ltx_font_bold">eywords</span> Federated Learning  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Smart NIC  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
DPU  <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
DPDK  <math id="p1.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation></semantics></math>
25GbE</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Due to the proliferation of smartphones and Internet-of-Things (IoT)
devices, the volume of data generated in our life is continuously
increasing, and Artificial Intelligence (AI) technologies to exploit
such data are rapidly evolving.
At the same time, uploading personal data to servers increases the
concern about data privacy.
To address this issue, federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is a
promising distributed machine learning approach that does not upload
privacy-sensitive raw data to servers.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">In the federated learning, clients download a model from a server and
train it locally.
Then the trained weight parameters are sent to the server.
The server aggregates the local parameters and sends back the
aggregated global parameters to the clients.
The weight parameters are exchanged between the server and clients
multiple times during the federated learning.
The communication workload on the server increases as the number of
clients increases or the size of the model becomes larger.
Nevertheless, the aggregation process is not computationally heavy
since it is simply averaging the local parameters received from
clients.
Thus, it is an I/O intensive task with a high network processing workload
compared with the computation.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">To mitigate the network processing workload, in this paper, the federated
learning server is offloaded to NVIDIA BlueField-2 DPU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
which is a smart NIC (Network Interface Card) that has eight
processing cores.
Dedicated processing cores are assigned for receiving the local weight
parameters and sending the global parameters.
The server is implemented as user-space application with DPDK (Data Plane
Development Kit) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> so that a network protocol stack of Linux
kernel is bypassed, resulting in a lower processing latency and
higher network throughput.
The aggregation task is parallelized by exploiting multiple cores
available on the DPU.
To further improve the performance, an approximated design that
eliminates an exclusive access control between the computation threads
is implemented.
The baseline server and the approximated server are evaluated in terms
of the execution time and learning convergence to show the performance
and accuracy tradeoffs.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">The rest of this paper is organized as follows.
Section <a href="#S2" title="2 Background and Related Work ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> introduces background knowledge about the
federated learning, DPDK, and smart NIC.
Sections <a href="#S3" title="3 Federated Learning Server on DPU ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S4" title="4 Implementation Details ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describe the
design and implementation of the proposed federated learning server on
the smart NIC, respectively.
Section <a href="#S5" title="5 Evaluations ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> evaluates it in terms of the execution time and
learning convergence.
Section <a href="#S6" title="6 Summary ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes this paper and mentions our future
work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Modern mobile devices such as smartphones are major sources of
valuable data that can enhance user experiences while such personal
data are privacy-sensitive.
To obtain a global model trained from such privacy-sensitive data
owned by clients without uploading them to the server, the federated
learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> has been extensively studied.
Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Federated Learning ‣ 2 Background and Related Work ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates a basic federated learning system with a
single server and <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">N</annotation></semantics></math> clients.
Each client trains its local model using its own data and sends the
trained local parameters to the server.
The server aggregates the local parameters to produce global
parameters, which are then sent back to the clients.
Thus, the clients can share their trained results by incorporating
the global parameters in their local parameters.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2307.06561/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="293" height="166" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Basic federated learning system</figcaption>
</figure>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Federated Averaging</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p1.13" class="ltx_p">FedAvg (Federated Averaging) is a typical federated learning
algorithm, and it is shown in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 2.1.1 Federated Averaging ‣ 2.1 Federated Learning ‣ 2 Background and Related Work ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
First, weight parameters of the target model are initialized.
For each round, <math id="S2.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S2.SS1.SSS1.p1.1.m1.1a"><mi id="S2.SS1.SSS1.p1.1.m1.1.1" xref="S2.SS1.SSS1.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.1.m1.1b"><ci id="S2.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.1.m1.1c">m</annotation></semantics></math> clients are randomly selected, where <math id="S2.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS1.SSS1.p1.2.m2.1a"><mi id="S2.SS1.SSS1.p1.2.m2.1.1" xref="S2.SS1.SSS1.p1.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.2.m2.1b"><ci id="S2.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.2.m2.1c">C</annotation></semantics></math> is a
probability that a client is selected.
The selected clients join the aggregation, while the others keep
their local weight parameters.
In round <math id="S2.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.SSS1.p1.3.m3.1a"><mi id="S2.SS1.SSS1.p1.3.m3.1.1" xref="S2.SS1.SSS1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.3.m3.1b"><ci id="S2.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.3.m3.1c">t</annotation></semantics></math>, a selected client <math id="S2.SS1.SSS1.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.SSS1.p1.4.m4.1a"><mi id="S2.SS1.SSS1.p1.4.m4.1.1" xref="S2.SS1.SSS1.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.4.m4.1b"><ci id="S2.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.4.m4.1c">k</annotation></semantics></math> trains its local parameters
<math id="S2.SS1.SSS1.p1.5.m5.1" class="ltx_Math" alttext="w^{k}_{t}" display="inline"><semantics id="S2.SS1.SSS1.p1.5.m5.1a"><msubsup id="S2.SS1.SSS1.p1.5.m5.1.1" xref="S2.SS1.SSS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.SSS1.p1.5.m5.1.1.2.2" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.2.cmml">w</mi><mi id="S2.SS1.SSS1.p1.5.m5.1.1.3" xref="S2.SS1.SSS1.p1.5.m5.1.1.3.cmml">t</mi><mi id="S2.SS1.SSS1.p1.5.m5.1.1.2.3" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.5.m5.1b"><apply id="S2.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1">subscript</csymbol><apply id="S2.SS1.SSS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.5.m5.1.1.2.1.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1">superscript</csymbol><ci id="S2.SS1.SSS1.p1.5.m5.1.1.2.2.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.2">𝑤</ci><ci id="S2.SS1.SSS1.p1.5.m5.1.1.2.3.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.3">𝑘</ci></apply><ci id="S2.SS1.SSS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.5.m5.1c">w^{k}_{t}</annotation></semantics></math> using its own local data, based on the formula in line
<a href="#alg1.l13" title="In Algorithm 1 ‣ 2.1.1 Federated Averaging ‣ 2.1 Federated Learning ‣ 2 Background and Related Work ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, to produce <math id="S2.SS1.SSS1.p1.6.m6.1" class="ltx_Math" alttext="w^{k}_{t+1}" display="inline"><semantics id="S2.SS1.SSS1.p1.6.m6.1a"><msubsup id="S2.SS1.SSS1.p1.6.m6.1.1" xref="S2.SS1.SSS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.SSS1.p1.6.m6.1.1.2.2" xref="S2.SS1.SSS1.p1.6.m6.1.1.2.2.cmml">w</mi><mrow id="S2.SS1.SSS1.p1.6.m6.1.1.3" xref="S2.SS1.SSS1.p1.6.m6.1.1.3.cmml"><mi id="S2.SS1.SSS1.p1.6.m6.1.1.3.2" xref="S2.SS1.SSS1.p1.6.m6.1.1.3.2.cmml">t</mi><mo id="S2.SS1.SSS1.p1.6.m6.1.1.3.1" xref="S2.SS1.SSS1.p1.6.m6.1.1.3.1.cmml">+</mo><mn id="S2.SS1.SSS1.p1.6.m6.1.1.3.3" xref="S2.SS1.SSS1.p1.6.m6.1.1.3.3.cmml">1</mn></mrow><mi id="S2.SS1.SSS1.p1.6.m6.1.1.2.3" xref="S2.SS1.SSS1.p1.6.m6.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.6.m6.1b"><apply id="S2.SS1.SSS1.p1.6.m6.1.1.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1">subscript</csymbol><apply id="S2.SS1.SSS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.6.m6.1.1.2.1.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1">superscript</csymbol><ci id="S2.SS1.SSS1.p1.6.m6.1.1.2.2.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1.2.2">𝑤</ci><ci id="S2.SS1.SSS1.p1.6.m6.1.1.2.3.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1.2.3">𝑘</ci></apply><apply id="S2.SS1.SSS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1.3"><plus id="S2.SS1.SSS1.p1.6.m6.1.1.3.1.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1.3.1"></plus><ci id="S2.SS1.SSS1.p1.6.m6.1.1.3.2.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1.3.2">𝑡</ci><cn type="integer" id="S2.SS1.SSS1.p1.6.m6.1.1.3.3.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.6.m6.1c">w^{k}_{t+1}</annotation></semantics></math>.
The trained local parameters <math id="S2.SS1.SSS1.p1.7.m7.1" class="ltx_Math" alttext="w^{k}_{t+1}" display="inline"><semantics id="S2.SS1.SSS1.p1.7.m7.1a"><msubsup id="S2.SS1.SSS1.p1.7.m7.1.1" xref="S2.SS1.SSS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.SSS1.p1.7.m7.1.1.2.2" xref="S2.SS1.SSS1.p1.7.m7.1.1.2.2.cmml">w</mi><mrow id="S2.SS1.SSS1.p1.7.m7.1.1.3" xref="S2.SS1.SSS1.p1.7.m7.1.1.3.cmml"><mi id="S2.SS1.SSS1.p1.7.m7.1.1.3.2" xref="S2.SS1.SSS1.p1.7.m7.1.1.3.2.cmml">t</mi><mo id="S2.SS1.SSS1.p1.7.m7.1.1.3.1" xref="S2.SS1.SSS1.p1.7.m7.1.1.3.1.cmml">+</mo><mn id="S2.SS1.SSS1.p1.7.m7.1.1.3.3" xref="S2.SS1.SSS1.p1.7.m7.1.1.3.3.cmml">1</mn></mrow><mi id="S2.SS1.SSS1.p1.7.m7.1.1.2.3" xref="S2.SS1.SSS1.p1.7.m7.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.7.m7.1b"><apply id="S2.SS1.SSS1.p1.7.m7.1.1.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1">subscript</csymbol><apply id="S2.SS1.SSS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.7.m7.1.1.2.1.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1">superscript</csymbol><ci id="S2.SS1.SSS1.p1.7.m7.1.1.2.2.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1.2.2">𝑤</ci><ci id="S2.SS1.SSS1.p1.7.m7.1.1.2.3.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1.2.3">𝑘</ci></apply><apply id="S2.SS1.SSS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1.3"><plus id="S2.SS1.SSS1.p1.7.m7.1.1.3.1.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1.3.1"></plus><ci id="S2.SS1.SSS1.p1.7.m7.1.1.3.2.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1.3.2">𝑡</ci><cn type="integer" id="S2.SS1.SSS1.p1.7.m7.1.1.3.3.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.7.m7.1c">w^{k}_{t+1}</annotation></semantics></math> are sent to the server.
The server averages the received local parameters, based on the
formula in line <a href="#alg1.l8" title="In Algorithm 1 ‣ 2.1.1 Federated Averaging ‣ 2.1 Federated Learning ‣ 2 Background and Related Work ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, to produce the aggregated global
parameters <math id="S2.SS1.SSS1.p1.8.m8.1" class="ltx_Math" alttext="w_{t+1}" display="inline"><semantics id="S2.SS1.SSS1.p1.8.m8.1a"><msub id="S2.SS1.SSS1.p1.8.m8.1.1" xref="S2.SS1.SSS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.SSS1.p1.8.m8.1.1.2" xref="S2.SS1.SSS1.p1.8.m8.1.1.2.cmml">w</mi><mrow id="S2.SS1.SSS1.p1.8.m8.1.1.3" xref="S2.SS1.SSS1.p1.8.m8.1.1.3.cmml"><mi id="S2.SS1.SSS1.p1.8.m8.1.1.3.2" xref="S2.SS1.SSS1.p1.8.m8.1.1.3.2.cmml">t</mi><mo id="S2.SS1.SSS1.p1.8.m8.1.1.3.1" xref="S2.SS1.SSS1.p1.8.m8.1.1.3.1.cmml">+</mo><mn id="S2.SS1.SSS1.p1.8.m8.1.1.3.3" xref="S2.SS1.SSS1.p1.8.m8.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.8.m8.1b"><apply id="S2.SS1.SSS1.p1.8.m8.1.1.cmml" xref="S2.SS1.SSS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.SSS1.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.SSS1.p1.8.m8.1.1.2">𝑤</ci><apply id="S2.SS1.SSS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.SSS1.p1.8.m8.1.1.3"><plus id="S2.SS1.SSS1.p1.8.m8.1.1.3.1.cmml" xref="S2.SS1.SSS1.p1.8.m8.1.1.3.1"></plus><ci id="S2.SS1.SSS1.p1.8.m8.1.1.3.2.cmml" xref="S2.SS1.SSS1.p1.8.m8.1.1.3.2">𝑡</ci><cn type="integer" id="S2.SS1.SSS1.p1.8.m8.1.1.3.3.cmml" xref="S2.SS1.SSS1.p1.8.m8.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.8.m8.1c">w_{t+1}</annotation></semantics></math>.
In the algorithm, <math id="S2.SS1.SSS1.p1.9.m9.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.SSS1.p1.9.m9.1a"><mi id="S2.SS1.SSS1.p1.9.m9.1.1" xref="S2.SS1.SSS1.p1.9.m9.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.9.m9.1b"><ci id="S2.SS1.SSS1.p1.9.m9.1.1.cmml" xref="S2.SS1.SSS1.p1.9.m9.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.9.m9.1c">n</annotation></semantics></math> is the total number of data samples, and <math id="S2.SS1.SSS1.p1.10.m10.1" class="ltx_Math" alttext="n_{k}" display="inline"><semantics id="S2.SS1.SSS1.p1.10.m10.1a"><msub id="S2.SS1.SSS1.p1.10.m10.1.1" xref="S2.SS1.SSS1.p1.10.m10.1.1.cmml"><mi id="S2.SS1.SSS1.p1.10.m10.1.1.2" xref="S2.SS1.SSS1.p1.10.m10.1.1.2.cmml">n</mi><mi id="S2.SS1.SSS1.p1.10.m10.1.1.3" xref="S2.SS1.SSS1.p1.10.m10.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.10.m10.1b"><apply id="S2.SS1.SSS1.p1.10.m10.1.1.cmml" xref="S2.SS1.SSS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.10.m10.1.1.1.cmml" xref="S2.SS1.SSS1.p1.10.m10.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p1.10.m10.1.1.2.cmml" xref="S2.SS1.SSS1.p1.10.m10.1.1.2">𝑛</ci><ci id="S2.SS1.SSS1.p1.10.m10.1.1.3.cmml" xref="S2.SS1.SSS1.p1.10.m10.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.10.m10.1c">n_{k}</annotation></semantics></math>
is the number of data samples owned by client <math id="S2.SS1.SSS1.p1.11.m11.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.SSS1.p1.11.m11.1a"><mi id="S2.SS1.SSS1.p1.11.m11.1.1" xref="S2.SS1.SSS1.p1.11.m11.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.11.m11.1b"><ci id="S2.SS1.SSS1.p1.11.m11.1.1.cmml" xref="S2.SS1.SSS1.p1.11.m11.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.11.m11.1c">k</annotation></semantics></math>.
The global parameters <math id="S2.SS1.SSS1.p1.12.m12.1" class="ltx_Math" alttext="w_{t+1}" display="inline"><semantics id="S2.SS1.SSS1.p1.12.m12.1a"><msub id="S2.SS1.SSS1.p1.12.m12.1.1" xref="S2.SS1.SSS1.p1.12.m12.1.1.cmml"><mi id="S2.SS1.SSS1.p1.12.m12.1.1.2" xref="S2.SS1.SSS1.p1.12.m12.1.1.2.cmml">w</mi><mrow id="S2.SS1.SSS1.p1.12.m12.1.1.3" xref="S2.SS1.SSS1.p1.12.m12.1.1.3.cmml"><mi id="S2.SS1.SSS1.p1.12.m12.1.1.3.2" xref="S2.SS1.SSS1.p1.12.m12.1.1.3.2.cmml">t</mi><mo id="S2.SS1.SSS1.p1.12.m12.1.1.3.1" xref="S2.SS1.SSS1.p1.12.m12.1.1.3.1.cmml">+</mo><mn id="S2.SS1.SSS1.p1.12.m12.1.1.3.3" xref="S2.SS1.SSS1.p1.12.m12.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.12.m12.1b"><apply id="S2.SS1.SSS1.p1.12.m12.1.1.cmml" xref="S2.SS1.SSS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.12.m12.1.1.1.cmml" xref="S2.SS1.SSS1.p1.12.m12.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p1.12.m12.1.1.2.cmml" xref="S2.SS1.SSS1.p1.12.m12.1.1.2">𝑤</ci><apply id="S2.SS1.SSS1.p1.12.m12.1.1.3.cmml" xref="S2.SS1.SSS1.p1.12.m12.1.1.3"><plus id="S2.SS1.SSS1.p1.12.m12.1.1.3.1.cmml" xref="S2.SS1.SSS1.p1.12.m12.1.1.3.1"></plus><ci id="S2.SS1.SSS1.p1.12.m12.1.1.3.2.cmml" xref="S2.SS1.SSS1.p1.12.m12.1.1.3.2">𝑡</ci><cn type="integer" id="S2.SS1.SSS1.p1.12.m12.1.1.3.3.cmml" xref="S2.SS1.SSS1.p1.12.m12.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.12.m12.1c">w_{t+1}</annotation></semantics></math> are sent back to the clients, and each
client updates its local parameters using the received global
parameters.
These steps are repeated for <math id="S2.SS1.SSS1.p1.13.m13.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS1.SSS1.p1.13.m13.1a"><mi id="S2.SS1.SSS1.p1.13.m13.1.1" xref="S2.SS1.SSS1.p1.13.m13.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.13.m13.1b"><ci id="S2.SS1.SSS1.p1.13.m13.1.1.cmml" xref="S2.SS1.SSS1.p1.13.m13.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.13.m13.1c">T</annotation></semantics></math> rounds to obtain the final global and
local parameters.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.12.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Federated Averaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
<math id="alg1.6.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="alg1.6.m1.1b"><mi id="alg1.6.m1.1.1" xref="alg1.6.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="alg1.6.m1.1c"><ci id="alg1.6.m1.1.1.cmml" xref="alg1.6.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.6.m1.1d">K</annotation></semantics></math> clients are indexed by <math id="alg1.7.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg1.7.m2.1b"><mi id="alg1.7.m2.1.1" xref="alg1.7.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.7.m2.1c"><ci id="alg1.7.m2.1.1.cmml" xref="alg1.7.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.7.m2.1d">k</annotation></semantics></math>.
<math id="alg1.8.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="alg1.8.m3.1b"><mi id="alg1.8.m3.1.1" xref="alg1.8.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="alg1.8.m3.1c"><ci id="alg1.8.m3.1.1.cmml" xref="alg1.8.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.8.m3.1d">B</annotation></semantics></math> is local minibatch size, <math id="alg1.9.m4.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg1.9.m4.1b"><mi id="alg1.9.m4.1.1" xref="alg1.9.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.9.m4.1c"><ci id="alg1.9.m4.1.1.cmml" xref="alg1.9.m4.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.9.m4.1d">E</annotation></semantics></math> is number of local epochs, and
<math id="alg1.10.m5.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="alg1.10.m5.1b"><mi id="alg1.10.m5.1.1" xref="alg1.10.m5.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="alg1.10.m5.1c"><ci id="alg1.10.m5.1.1.cmml" xref="alg1.10.m5.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.10.m5.1d">\eta</annotation></semantics></math> is learning rate.</figcaption>
<div id="alg1.13" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1:</span><span id="alg1.l1.1" class="ltx_text ltx_font_bold">function</span> ExecuteServer(<span id="alg1.l1.2" class="ltx_text"></span>)

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2:</span>   Initialize <math id="alg1.l2.m1.1" class="ltx_Math" alttext="w_{0}" display="inline"><semantics id="alg1.l2.m1.1a"><msub id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mi id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml">w</mi><mn id="alg1.l2.m1.1.1.3" xref="alg1.l2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><csymbol cd="ambiguous" id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1">subscript</csymbol><ci id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2">𝑤</ci><cn type="integer" id="alg1.l2.m1.1.1.3.cmml" xref="alg1.l2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">w_{0}</annotation></semantics></math>

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3:</span>   <span id="alg1.l3.1" class="ltx_text ltx_font_bold">for each</span> round <math id="alg1.l3.m1.3" class="ltx_Math" alttext="t=1,2,\ldots" display="inline"><semantics id="alg1.l3.m1.3a"><mrow id="alg1.l3.m1.3.4" xref="alg1.l3.m1.3.4.cmml"><mi id="alg1.l3.m1.3.4.2" xref="alg1.l3.m1.3.4.2.cmml">t</mi><mo id="alg1.l3.m1.3.4.1" xref="alg1.l3.m1.3.4.1.cmml">=</mo><mrow id="alg1.l3.m1.3.4.3.2" xref="alg1.l3.m1.3.4.3.1.cmml"><mn id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">1</mn><mo id="alg1.l3.m1.3.4.3.2.1" xref="alg1.l3.m1.3.4.3.1.cmml">,</mo><mn id="alg1.l3.m1.2.2" xref="alg1.l3.m1.2.2.cmml">2</mn><mo id="alg1.l3.m1.3.4.3.2.2" xref="alg1.l3.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="alg1.l3.m1.3.3" xref="alg1.l3.m1.3.3.cmml">…</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.3b"><apply id="alg1.l3.m1.3.4.cmml" xref="alg1.l3.m1.3.4"><eq id="alg1.l3.m1.3.4.1.cmml" xref="alg1.l3.m1.3.4.1"></eq><ci id="alg1.l3.m1.3.4.2.cmml" xref="alg1.l3.m1.3.4.2">𝑡</ci><list id="alg1.l3.m1.3.4.3.1.cmml" xref="alg1.l3.m1.3.4.3.2"><cn type="integer" id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">1</cn><cn type="integer" id="alg1.l3.m1.2.2.cmml" xref="alg1.l3.m1.2.2">2</cn><ci id="alg1.l3.m1.3.3.cmml" xref="alg1.l3.m1.3.3">…</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.3c">t=1,2,\ldots</annotation></semantics></math> <span id="alg1.l3.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4:</span>      <math id="alg1.l4.m1.3" class="ltx_Math" alttext="m\leftarrow\max(C\cdot K,1)" display="inline"><semantics id="alg1.l4.m1.3a"><mrow id="alg1.l4.m1.3.3" xref="alg1.l4.m1.3.3.cmml"><mi id="alg1.l4.m1.3.3.3" xref="alg1.l4.m1.3.3.3.cmml">m</mi><mo stretchy="false" id="alg1.l4.m1.3.3.2" xref="alg1.l4.m1.3.3.2.cmml">←</mo><mrow id="alg1.l4.m1.3.3.1.1" xref="alg1.l4.m1.3.3.1.2.cmml"><mi id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">max</mi><mo id="alg1.l4.m1.3.3.1.1a" xref="alg1.l4.m1.3.3.1.2.cmml">⁡</mo><mrow id="alg1.l4.m1.3.3.1.1.1" xref="alg1.l4.m1.3.3.1.2.cmml"><mo stretchy="false" id="alg1.l4.m1.3.3.1.1.1.2" xref="alg1.l4.m1.3.3.1.2.cmml">(</mo><mrow id="alg1.l4.m1.3.3.1.1.1.1" xref="alg1.l4.m1.3.3.1.1.1.1.cmml"><mi id="alg1.l4.m1.3.3.1.1.1.1.2" xref="alg1.l4.m1.3.3.1.1.1.1.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="alg1.l4.m1.3.3.1.1.1.1.1" xref="alg1.l4.m1.3.3.1.1.1.1.1.cmml">⋅</mo><mi id="alg1.l4.m1.3.3.1.1.1.1.3" xref="alg1.l4.m1.3.3.1.1.1.1.3.cmml">K</mi></mrow><mo id="alg1.l4.m1.3.3.1.1.1.3" xref="alg1.l4.m1.3.3.1.2.cmml">,</mo><mn id="alg1.l4.m1.2.2" xref="alg1.l4.m1.2.2.cmml">1</mn><mo stretchy="false" id="alg1.l4.m1.3.3.1.1.1.4" xref="alg1.l4.m1.3.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.3b"><apply id="alg1.l4.m1.3.3.cmml" xref="alg1.l4.m1.3.3"><ci id="alg1.l4.m1.3.3.2.cmml" xref="alg1.l4.m1.3.3.2">←</ci><ci id="alg1.l4.m1.3.3.3.cmml" xref="alg1.l4.m1.3.3.3">𝑚</ci><apply id="alg1.l4.m1.3.3.1.2.cmml" xref="alg1.l4.m1.3.3.1.1"><max id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"></max><apply id="alg1.l4.m1.3.3.1.1.1.1.cmml" xref="alg1.l4.m1.3.3.1.1.1.1"><ci id="alg1.l4.m1.3.3.1.1.1.1.1.cmml" xref="alg1.l4.m1.3.3.1.1.1.1.1">⋅</ci><ci id="alg1.l4.m1.3.3.1.1.1.1.2.cmml" xref="alg1.l4.m1.3.3.1.1.1.1.2">𝐶</ci><ci id="alg1.l4.m1.3.3.1.1.1.1.3.cmml" xref="alg1.l4.m1.3.3.1.1.1.1.3">𝐾</ci></apply><cn type="integer" id="alg1.l4.m1.2.2.cmml" xref="alg1.l4.m1.2.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.3c">m\leftarrow\max(C\cdot K,1)</annotation></semantics></math>

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5:</span>      <math id="alg1.l5.m1.1" class="ltx_Math" alttext="S_{t}\leftarrow\text{(random set of $m$ clients)}" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.2" xref="alg1.l5.m1.1.2.cmml"><msub id="alg1.l5.m1.1.2.2" xref="alg1.l5.m1.1.2.2.cmml"><mi id="alg1.l5.m1.1.2.2.2" xref="alg1.l5.m1.1.2.2.2.cmml">S</mi><mi id="alg1.l5.m1.1.2.2.3" xref="alg1.l5.m1.1.2.2.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.l5.m1.1.2.1" xref="alg1.l5.m1.1.2.1.cmml">←</mo><mrow id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1c.cmml"><mtext id="alg1.l5.m1.1.1.1a" xref="alg1.l5.m1.1.1.1c.cmml">(random set of </mtext><mi id="alg1.l5.m1.1.1.1.m1.1.1" xref="alg1.l5.m1.1.1.1.m1.1.1.cmml">m</mi><mtext id="alg1.l5.m1.1.1.1b" xref="alg1.l5.m1.1.1.1c.cmml"> clients)</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.2.cmml" xref="alg1.l5.m1.1.2"><ci id="alg1.l5.m1.1.2.1.cmml" xref="alg1.l5.m1.1.2.1">←</ci><apply id="alg1.l5.m1.1.2.2.cmml" xref="alg1.l5.m1.1.2.2"><csymbol cd="ambiguous" id="alg1.l5.m1.1.2.2.1.cmml" xref="alg1.l5.m1.1.2.2">subscript</csymbol><ci id="alg1.l5.m1.1.2.2.2.cmml" xref="alg1.l5.m1.1.2.2.2">𝑆</ci><ci id="alg1.l5.m1.1.2.2.3.cmml" xref="alg1.l5.m1.1.2.2.3">𝑡</ci></apply><ci id="alg1.l5.m1.1.1.1c.cmml" xref="alg1.l5.m1.1.1.1"><mrow id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1"><mtext id="alg1.l5.m1.1.1.1a.cmml" xref="alg1.l5.m1.1.1.1">(random set of </mtext><mi id="alg1.l5.m1.1.1.1.m1.1.1.cmml" xref="alg1.l5.m1.1.1.1.m1.1.1">m</mi><mtext id="alg1.l5.m1.1.1.1b.cmml" xref="alg1.l5.m1.1.1.1"> clients)</mtext></mrow></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">S_{t}\leftarrow\text{(random set of $m$ clients)}</annotation></semantics></math>

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6:</span>      <span id="alg1.l6.1" class="ltx_text ltx_font_bold">for each</span> client <math id="alg1.l6.m1.1" class="ltx_Math" alttext="k\in S_{t}" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">k</mi><mo id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">∈</mo><msub id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml"><mi id="alg1.l6.m1.1.1.3.2" xref="alg1.l6.m1.1.1.3.2.cmml">S</mi><mi id="alg1.l6.m1.1.1.3.3" xref="alg1.l6.m1.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><in id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"></in><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">𝑘</ci><apply id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.3.1.cmml" xref="alg1.l6.m1.1.1.3">subscript</csymbol><ci id="alg1.l6.m1.1.1.3.2.cmml" xref="alg1.l6.m1.1.1.3.2">𝑆</ci><ci id="alg1.l6.m1.1.1.3.3.cmml" xref="alg1.l6.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">k\in S_{t}</annotation></semantics></math> <span id="alg1.l6.2" class="ltx_text ltx_font_bold">in parallel do</span>

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7:</span>         <math id="alg1.l7.m1.1" class="ltx_Math" alttext="w_{t+1}^{k}\leftarrow" display="inline"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><msubsup id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml"><mi id="alg1.l7.m1.1.1.2.2.2" xref="alg1.l7.m1.1.1.2.2.2.cmml">w</mi><mrow id="alg1.l7.m1.1.1.2.2.3" xref="alg1.l7.m1.1.1.2.2.3.cmml"><mi id="alg1.l7.m1.1.1.2.2.3.2" xref="alg1.l7.m1.1.1.2.2.3.2.cmml">t</mi><mo id="alg1.l7.m1.1.1.2.2.3.1" xref="alg1.l7.m1.1.1.2.2.3.1.cmml">+</mo><mn id="alg1.l7.m1.1.1.2.2.3.3" xref="alg1.l7.m1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="alg1.l7.m1.1.1.2.3" xref="alg1.l7.m1.1.1.2.3.cmml">k</mi></msubsup><mo stretchy="false" id="alg1.l7.m1.1.1.1" xref="alg1.l7.m1.1.1.1.cmml">←</mo><mi id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><ci id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1">←</ci><apply id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.2.1.cmml" xref="alg1.l7.m1.1.1.2">superscript</csymbol><apply id="alg1.l7.m1.1.1.2.2.cmml" xref="alg1.l7.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.2.2.1.cmml" xref="alg1.l7.m1.1.1.2">subscript</csymbol><ci id="alg1.l7.m1.1.1.2.2.2.cmml" xref="alg1.l7.m1.1.1.2.2.2">𝑤</ci><apply id="alg1.l7.m1.1.1.2.2.3.cmml" xref="alg1.l7.m1.1.1.2.2.3"><plus id="alg1.l7.m1.1.1.2.2.3.1.cmml" xref="alg1.l7.m1.1.1.2.2.3.1"></plus><ci id="alg1.l7.m1.1.1.2.2.3.2.cmml" xref="alg1.l7.m1.1.1.2.2.3.2">𝑡</ci><cn type="integer" id="alg1.l7.m1.1.1.2.2.3.3.cmml" xref="alg1.l7.m1.1.1.2.2.3.3">1</cn></apply></apply><ci id="alg1.l7.m1.1.1.2.3.cmml" xref="alg1.l7.m1.1.1.2.3">𝑘</ci></apply><csymbol cd="latexml" id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">w_{t+1}^{k}\leftarrow</annotation></semantics></math> ClientUpdate(<math id="alg1.l7.m2.2" class="ltx_Math" alttext="k,w_{t}" display="inline"><semantics id="alg1.l7.m2.2a"><mrow id="alg1.l7.m2.2.2.1" xref="alg1.l7.m2.2.2.2.cmml"><mi id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml">k</mi><mo id="alg1.l7.m2.2.2.1.2" xref="alg1.l7.m2.2.2.2.cmml">,</mo><msub id="alg1.l7.m2.2.2.1.1" xref="alg1.l7.m2.2.2.1.1.cmml"><mi id="alg1.l7.m2.2.2.1.1.2" xref="alg1.l7.m2.2.2.1.1.2.cmml">w</mi><mi id="alg1.l7.m2.2.2.1.1.3" xref="alg1.l7.m2.2.2.1.1.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.2b"><list id="alg1.l7.m2.2.2.2.cmml" xref="alg1.l7.m2.2.2.1"><ci id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1">𝑘</ci><apply id="alg1.l7.m2.2.2.1.1.cmml" xref="alg1.l7.m2.2.2.1.1"><csymbol cd="ambiguous" id="alg1.l7.m2.2.2.1.1.1.cmml" xref="alg1.l7.m2.2.2.1.1">subscript</csymbol><ci id="alg1.l7.m2.2.2.1.1.2.cmml" xref="alg1.l7.m2.2.2.1.1.2">𝑤</ci><ci id="alg1.l7.m2.2.2.1.1.3.cmml" xref="alg1.l7.m2.2.2.1.1.3">𝑡</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.2c">k,w_{t}</annotation></semantics></math>)
      
</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8:</span>      <math id="alg1.l8.m1.1" class="ltx_Math" alttext="w_{t+1}\leftarrow\displaystyle\sum_{k=1}^{K}\ \frac{n_{k}}{n}w_{t+1}^{k}" display="inline"><semantics id="alg1.l8.m1.1a"><mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><msub id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2.cmml"><mi id="alg1.l8.m1.1.1.2.2" xref="alg1.l8.m1.1.1.2.2.cmml">w</mi><mrow id="alg1.l8.m1.1.1.2.3" xref="alg1.l8.m1.1.1.2.3.cmml"><mi id="alg1.l8.m1.1.1.2.3.2" xref="alg1.l8.m1.1.1.2.3.2.cmml">t</mi><mo id="alg1.l8.m1.1.1.2.3.1" xref="alg1.l8.m1.1.1.2.3.1.cmml">+</mo><mn id="alg1.l8.m1.1.1.2.3.3" xref="alg1.l8.m1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="alg1.l8.m1.1.1.1" xref="alg1.l8.m1.1.1.1.cmml">←</mo><mrow id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3.cmml"><mstyle displaystyle="true" id="alg1.l8.m1.1.1.3.1" xref="alg1.l8.m1.1.1.3.1.cmml"><munderover id="alg1.l8.m1.1.1.3.1a" xref="alg1.l8.m1.1.1.3.1.cmml"><mo movablelimits="false" id="alg1.l8.m1.1.1.3.1.2.2" xref="alg1.l8.m1.1.1.3.1.2.2.cmml">∑</mo><mrow id="alg1.l8.m1.1.1.3.1.2.3" xref="alg1.l8.m1.1.1.3.1.2.3.cmml"><mi id="alg1.l8.m1.1.1.3.1.2.3.2" xref="alg1.l8.m1.1.1.3.1.2.3.2.cmml">k</mi><mo id="alg1.l8.m1.1.1.3.1.2.3.1" xref="alg1.l8.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="alg1.l8.m1.1.1.3.1.2.3.3" xref="alg1.l8.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="alg1.l8.m1.1.1.3.1.3" xref="alg1.l8.m1.1.1.3.1.3.cmml">K</mi></munderover></mstyle><mrow id="alg1.l8.m1.1.1.3.2" xref="alg1.l8.m1.1.1.3.2.cmml"><mstyle displaystyle="true" id="alg1.l8.m1.1.1.3.2.2" xref="alg1.l8.m1.1.1.3.2.2.cmml"><mfrac id="alg1.l8.m1.1.1.3.2.2a" xref="alg1.l8.m1.1.1.3.2.2.cmml"><msub id="alg1.l8.m1.1.1.3.2.2.2" xref="alg1.l8.m1.1.1.3.2.2.2.cmml"><mi id="alg1.l8.m1.1.1.3.2.2.2.2" xref="alg1.l8.m1.1.1.3.2.2.2.2.cmml">n</mi><mi id="alg1.l8.m1.1.1.3.2.2.2.3" xref="alg1.l8.m1.1.1.3.2.2.2.3.cmml">k</mi></msub><mi id="alg1.l8.m1.1.1.3.2.2.3" xref="alg1.l8.m1.1.1.3.2.2.3.cmml">n</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.3.2.1" xref="alg1.l8.m1.1.1.3.2.1.cmml">​</mo><msubsup id="alg1.l8.m1.1.1.3.2.3" xref="alg1.l8.m1.1.1.3.2.3.cmml"><mi id="alg1.l8.m1.1.1.3.2.3.2.2" xref="alg1.l8.m1.1.1.3.2.3.2.2.cmml">w</mi><mrow id="alg1.l8.m1.1.1.3.2.3.2.3" xref="alg1.l8.m1.1.1.3.2.3.2.3.cmml"><mi id="alg1.l8.m1.1.1.3.2.3.2.3.2" xref="alg1.l8.m1.1.1.3.2.3.2.3.2.cmml">t</mi><mo id="alg1.l8.m1.1.1.3.2.3.2.3.1" xref="alg1.l8.m1.1.1.3.2.3.2.3.1.cmml">+</mo><mn id="alg1.l8.m1.1.1.3.2.3.2.3.3" xref="alg1.l8.m1.1.1.3.2.3.2.3.3.cmml">1</mn></mrow><mi id="alg1.l8.m1.1.1.3.2.3.3" xref="alg1.l8.m1.1.1.3.2.3.3.cmml">k</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><ci id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1">←</ci><apply id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.2.1.cmml" xref="alg1.l8.m1.1.1.2">subscript</csymbol><ci id="alg1.l8.m1.1.1.2.2.cmml" xref="alg1.l8.m1.1.1.2.2">𝑤</ci><apply id="alg1.l8.m1.1.1.2.3.cmml" xref="alg1.l8.m1.1.1.2.3"><plus id="alg1.l8.m1.1.1.2.3.1.cmml" xref="alg1.l8.m1.1.1.2.3.1"></plus><ci id="alg1.l8.m1.1.1.2.3.2.cmml" xref="alg1.l8.m1.1.1.2.3.2">𝑡</ci><cn type="integer" id="alg1.l8.m1.1.1.2.3.3.cmml" xref="alg1.l8.m1.1.1.2.3.3">1</cn></apply></apply><apply id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3"><apply id="alg1.l8.m1.1.1.3.1.cmml" xref="alg1.l8.m1.1.1.3.1"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.1.1.cmml" xref="alg1.l8.m1.1.1.3.1">superscript</csymbol><apply id="alg1.l8.m1.1.1.3.1.2.cmml" xref="alg1.l8.m1.1.1.3.1"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.1.2.1.cmml" xref="alg1.l8.m1.1.1.3.1">subscript</csymbol><sum id="alg1.l8.m1.1.1.3.1.2.2.cmml" xref="alg1.l8.m1.1.1.3.1.2.2"></sum><apply id="alg1.l8.m1.1.1.3.1.2.3.cmml" xref="alg1.l8.m1.1.1.3.1.2.3"><eq id="alg1.l8.m1.1.1.3.1.2.3.1.cmml" xref="alg1.l8.m1.1.1.3.1.2.3.1"></eq><ci id="alg1.l8.m1.1.1.3.1.2.3.2.cmml" xref="alg1.l8.m1.1.1.3.1.2.3.2">𝑘</ci><cn type="integer" id="alg1.l8.m1.1.1.3.1.2.3.3.cmml" xref="alg1.l8.m1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="alg1.l8.m1.1.1.3.1.3.cmml" xref="alg1.l8.m1.1.1.3.1.3">𝐾</ci></apply><apply id="alg1.l8.m1.1.1.3.2.cmml" xref="alg1.l8.m1.1.1.3.2"><times id="alg1.l8.m1.1.1.3.2.1.cmml" xref="alg1.l8.m1.1.1.3.2.1"></times><apply id="alg1.l8.m1.1.1.3.2.2.cmml" xref="alg1.l8.m1.1.1.3.2.2"><divide id="alg1.l8.m1.1.1.3.2.2.1.cmml" xref="alg1.l8.m1.1.1.3.2.2"></divide><apply id="alg1.l8.m1.1.1.3.2.2.2.cmml" xref="alg1.l8.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.2.2.2.1.cmml" xref="alg1.l8.m1.1.1.3.2.2.2">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.2.2.2.2.cmml" xref="alg1.l8.m1.1.1.3.2.2.2.2">𝑛</ci><ci id="alg1.l8.m1.1.1.3.2.2.2.3.cmml" xref="alg1.l8.m1.1.1.3.2.2.2.3">𝑘</ci></apply><ci id="alg1.l8.m1.1.1.3.2.2.3.cmml" xref="alg1.l8.m1.1.1.3.2.2.3">𝑛</ci></apply><apply id="alg1.l8.m1.1.1.3.2.3.cmml" xref="alg1.l8.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.2.3.1.cmml" xref="alg1.l8.m1.1.1.3.2.3">superscript</csymbol><apply id="alg1.l8.m1.1.1.3.2.3.2.cmml" xref="alg1.l8.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.2.3.2.1.cmml" xref="alg1.l8.m1.1.1.3.2.3">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.2.3.2.2.cmml" xref="alg1.l8.m1.1.1.3.2.3.2.2">𝑤</ci><apply id="alg1.l8.m1.1.1.3.2.3.2.3.cmml" xref="alg1.l8.m1.1.1.3.2.3.2.3"><plus id="alg1.l8.m1.1.1.3.2.3.2.3.1.cmml" xref="alg1.l8.m1.1.1.3.2.3.2.3.1"></plus><ci id="alg1.l8.m1.1.1.3.2.3.2.3.2.cmml" xref="alg1.l8.m1.1.1.3.2.3.2.3.2">𝑡</ci><cn type="integer" id="alg1.l8.m1.1.1.3.2.3.2.3.3.cmml" xref="alg1.l8.m1.1.1.3.2.3.2.3.3">1</cn></apply></apply><ci id="alg1.l8.m1.1.1.3.2.3.3.cmml" xref="alg1.l8.m1.1.1.3.2.3.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">w_{t+1}\leftarrow\displaystyle\sum_{k=1}^{K}\ \frac{n_{k}}{n}w_{t+1}^{k}</annotation></semantics></math> 
   
</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9:</span><span id="alg1.l9.3" class="ltx_text ltx_font_bold">function</span> ClientUpdate(<math id="alg1.l9.m1.2" class="ltx_Math" alttext="k,w" display="inline"><semantics id="alg1.l9.m1.2a"><mrow id="alg1.l9.m1.2.3.2" xref="alg1.l9.m1.2.3.1.cmml"><mi id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml">k</mi><mo id="alg1.l9.m1.2.3.2.1" xref="alg1.l9.m1.2.3.1.cmml">,</mo><mi id="alg1.l9.m1.2.2" xref="alg1.l9.m1.2.2.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.2b"><list id="alg1.l9.m1.2.3.1.cmml" xref="alg1.l9.m1.2.3.2"><ci id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1">𝑘</ci><ci id="alg1.l9.m1.2.2.cmml" xref="alg1.l9.m1.2.2">𝑤</ci></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.2c">k,w</annotation></semantics></math>) <span id="alg1.l9.2" class="ltx_text" style="float:right;"><math id="alg1.l9.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l9.1.m1.1a"><mo id="alg1.l9.1.m1.1.1" xref="alg1.l9.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l9.1.m1.1b"><ci id="alg1.l9.1.m1.1.1.cmml" xref="alg1.l9.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.1.m1.1c">\triangleright</annotation></semantics></math> <span id="alg1.l9.2.1" class="ltx_text ltx_font_italic">Run on client <math id="alg1.l9.2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg1.l9.2.1.m1.1a"><mi id="alg1.l9.2.1.m1.1.1" xref="alg1.l9.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.2.1.m1.1b"><ci id="alg1.l9.2.1.m1.1.1.cmml" xref="alg1.l9.2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.2.1.m1.1c">k</annotation></semantics></math></span>
</span>
</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10:</span>   <math id="alg1.l10.m1.2" class="ltx_Math" alttext="\mathcal{B}\leftarrow\text{(split $\mathcal{P}_{k}$ into batches of size $B$)}" display="inline"><semantics id="alg1.l10.m1.2a"><mrow id="alg1.l10.m1.2.3" xref="alg1.l10.m1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l10.m1.2.3.2" xref="alg1.l10.m1.2.3.2.cmml">ℬ</mi><mo stretchy="false" id="alg1.l10.m1.2.3.1" xref="alg1.l10.m1.2.3.1.cmml">←</mo><mrow id="alg1.l10.m1.2.2.2" xref="alg1.l10.m1.2.2.2d.cmml"><mtext id="alg1.l10.m1.2.2.2a" xref="alg1.l10.m1.2.2.2d.cmml">(split </mtext><msub id="alg1.l10.m1.1.1.1.m1.1.1" xref="alg1.l10.m1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l10.m1.1.1.1.m1.1.1.2" xref="alg1.l10.m1.1.1.1.m1.1.1.2.cmml">𝒫</mi><mi id="alg1.l10.m1.1.1.1.m1.1.1.3" xref="alg1.l10.m1.1.1.1.m1.1.1.3.cmml">k</mi></msub><mtext id="alg1.l10.m1.2.2.2b" xref="alg1.l10.m1.2.2.2d.cmml"> into batches of size </mtext><mi id="alg1.l10.m1.2.2.2.m2.1.1" xref="alg1.l10.m1.2.2.2.m2.1.1.cmml">B</mi><mtext id="alg1.l10.m1.2.2.2c" xref="alg1.l10.m1.2.2.2d.cmml">)</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.2b"><apply id="alg1.l10.m1.2.3.cmml" xref="alg1.l10.m1.2.3"><ci id="alg1.l10.m1.2.3.1.cmml" xref="alg1.l10.m1.2.3.1">←</ci><ci id="alg1.l10.m1.2.3.2.cmml" xref="alg1.l10.m1.2.3.2">ℬ</ci><ci id="alg1.l10.m1.2.2.2d.cmml" xref="alg1.l10.m1.2.2.2"><mrow id="alg1.l10.m1.2.2.2.cmml" xref="alg1.l10.m1.2.2.2"><mtext id="alg1.l10.m1.2.2.2a.cmml" xref="alg1.l10.m1.2.2.2">(split </mtext><msub id="alg1.l10.m1.1.1.1.m1.1.1.cmml" xref="alg1.l10.m1.1.1.1.m1.1.1"><mi class="ltx_font_mathcaligraphic" id="alg1.l10.m1.1.1.1.m1.1.1.2.cmml" xref="alg1.l10.m1.1.1.1.m1.1.1.2">𝒫</mi><mi id="alg1.l10.m1.1.1.1.m1.1.1.3.cmml" xref="alg1.l10.m1.1.1.1.m1.1.1.3">k</mi></msub><mtext id="alg1.l10.m1.2.2.2b.cmml" xref="alg1.l10.m1.2.2.2"> into batches of size </mtext><mi id="alg1.l10.m1.2.2.2.m2.1.1.cmml" xref="alg1.l10.m1.2.2.2.m2.1.1">B</mi><mtext id="alg1.l10.m1.2.2.2c.cmml" xref="alg1.l10.m1.2.2.2">)</mtext></mrow></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.2c">\mathcal{B}\leftarrow\text{(split $\mathcal{P}_{k}$ into batches of size $B$)}</annotation></semantics></math>

</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11:</span>   <span id="alg1.l11.1" class="ltx_text ltx_font_bold">for each</span> local epoch <math id="alg1.l11.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="alg1.l11.m1.1a"><mi id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><ci id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">i</annotation></semantics></math> from <math id="alg1.l11.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="alg1.l11.m2.1a"><mn id="alg1.l11.m2.1.1" xref="alg1.l11.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="alg1.l11.m2.1b"><cn type="integer" id="alg1.l11.m2.1.1.cmml" xref="alg1.l11.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m2.1c">1</annotation></semantics></math> to <math id="alg1.l11.m3.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg1.l11.m3.1a"><mi id="alg1.l11.m3.1.1" xref="alg1.l11.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.l11.m3.1b"><ci id="alg1.l11.m3.1.1.cmml" xref="alg1.l11.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m3.1c">E</annotation></semantics></math> <span id="alg1.l11.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12:</span>      <span id="alg1.l12.1" class="ltx_text ltx_font_bold">for each</span> batch <math id="alg1.l12.m1.1" class="ltx_Math" alttext="b\in\mathcal{B}" display="inline"><semantics id="alg1.l12.m1.1a"><mrow id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml"><mi id="alg1.l12.m1.1.1.2" xref="alg1.l12.m1.1.1.2.cmml">b</mi><mo id="alg1.l12.m1.1.1.1" xref="alg1.l12.m1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="alg1.l12.m1.1.1.3" xref="alg1.l12.m1.1.1.3.cmml">ℬ</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.1b"><apply id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1"><in id="alg1.l12.m1.1.1.1.cmml" xref="alg1.l12.m1.1.1.1"></in><ci id="alg1.l12.m1.1.1.2.cmml" xref="alg1.l12.m1.1.1.2">𝑏</ci><ci id="alg1.l12.m1.1.1.3.cmml" xref="alg1.l12.m1.1.1.3">ℬ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.1c">b\in\mathcal{B}</annotation></semantics></math> <span id="alg1.l12.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">13:</span>         <math id="alg1.l13.m1.2" class="ltx_Math" alttext="w\leftarrow w-\eta\nabla\ell(w;b)" display="inline"><semantics id="alg1.l13.m1.2a"><mrow id="alg1.l13.m1.2.3" xref="alg1.l13.m1.2.3.cmml"><mi id="alg1.l13.m1.2.3.2" xref="alg1.l13.m1.2.3.2.cmml">w</mi><mo stretchy="false" id="alg1.l13.m1.2.3.1" xref="alg1.l13.m1.2.3.1.cmml">←</mo><mrow id="alg1.l13.m1.2.3.3" xref="alg1.l13.m1.2.3.3.cmml"><mi id="alg1.l13.m1.2.3.3.2" xref="alg1.l13.m1.2.3.3.2.cmml">w</mi><mo id="alg1.l13.m1.2.3.3.1" xref="alg1.l13.m1.2.3.3.1.cmml">−</mo><mrow id="alg1.l13.m1.2.3.3.3" xref="alg1.l13.m1.2.3.3.3.cmml"><mi id="alg1.l13.m1.2.3.3.3.2" xref="alg1.l13.m1.2.3.3.3.2.cmml">η</mi><mo lspace="0.167em" rspace="0em" id="alg1.l13.m1.2.3.3.3.1" xref="alg1.l13.m1.2.3.3.3.1.cmml">​</mo><mrow id="alg1.l13.m1.2.3.3.3.3" xref="alg1.l13.m1.2.3.3.3.3.cmml"><mo rspace="0.167em" id="alg1.l13.m1.2.3.3.3.3.1" xref="alg1.l13.m1.2.3.3.3.3.1.cmml">∇</mo><mi mathvariant="normal" id="alg1.l13.m1.2.3.3.3.3.2" xref="alg1.l13.m1.2.3.3.3.3.2.cmml">ℓ</mi></mrow><mo lspace="0em" rspace="0em" id="alg1.l13.m1.2.3.3.3.1a" xref="alg1.l13.m1.2.3.3.3.1.cmml">​</mo><mrow id="alg1.l13.m1.2.3.3.3.4.2" xref="alg1.l13.m1.2.3.3.3.4.1.cmml"><mo stretchy="false" id="alg1.l13.m1.2.3.3.3.4.2.1" xref="alg1.l13.m1.2.3.3.3.4.1.cmml">(</mo><mi id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml">w</mi><mo id="alg1.l13.m1.2.3.3.3.4.2.2" xref="alg1.l13.m1.2.3.3.3.4.1.cmml">;</mo><mi id="alg1.l13.m1.2.2" xref="alg1.l13.m1.2.2.cmml">b</mi><mo stretchy="false" id="alg1.l13.m1.2.3.3.3.4.2.3" xref="alg1.l13.m1.2.3.3.3.4.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.2b"><apply id="alg1.l13.m1.2.3.cmml" xref="alg1.l13.m1.2.3"><ci id="alg1.l13.m1.2.3.1.cmml" xref="alg1.l13.m1.2.3.1">←</ci><ci id="alg1.l13.m1.2.3.2.cmml" xref="alg1.l13.m1.2.3.2">𝑤</ci><apply id="alg1.l13.m1.2.3.3.cmml" xref="alg1.l13.m1.2.3.3"><minus id="alg1.l13.m1.2.3.3.1.cmml" xref="alg1.l13.m1.2.3.3.1"></minus><ci id="alg1.l13.m1.2.3.3.2.cmml" xref="alg1.l13.m1.2.3.3.2">𝑤</ci><apply id="alg1.l13.m1.2.3.3.3.cmml" xref="alg1.l13.m1.2.3.3.3"><times id="alg1.l13.m1.2.3.3.3.1.cmml" xref="alg1.l13.m1.2.3.3.3.1"></times><ci id="alg1.l13.m1.2.3.3.3.2.cmml" xref="alg1.l13.m1.2.3.3.3.2">𝜂</ci><apply id="alg1.l13.m1.2.3.3.3.3.cmml" xref="alg1.l13.m1.2.3.3.3.3"><ci id="alg1.l13.m1.2.3.3.3.3.1.cmml" xref="alg1.l13.m1.2.3.3.3.3.1">∇</ci><ci id="alg1.l13.m1.2.3.3.3.3.2.cmml" xref="alg1.l13.m1.2.3.3.3.3.2">ℓ</ci></apply><list id="alg1.l13.m1.2.3.3.3.4.1.cmml" xref="alg1.l13.m1.2.3.3.3.4.2"><ci id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1">𝑤</ci><ci id="alg1.l13.m1.2.2.cmml" xref="alg1.l13.m1.2.2">𝑏</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.2c">w\leftarrow w-\eta\nabla\ell(w;b)</annotation></semantics></math> 
         
</div>
</div>
</figure>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Advanced Federated Learning Algorithms</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">In FedAvg algorithm, clients replace their local parameters with
the global parameters entirely every round.
It is effective when the goal is to improve the global model accuracy
for an entire data distribution that all clients will encounter in
future.
In reality, however, data distribution may differ depending on client
environments.
For such non-i.i.d. (independently and identically distributed) data,
Per-FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and APFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> are representative
algorithms to improve the local model accuracy for the local data
distribution of each client.
Although these algorithms have a similarity to FedAvg algorithm at the
point that the global parameters are obtained by averaging local
parameters, clients update their local parameters through a
weighted average of both the local and global parameters.
Please note that the server-side averaging process of FedAvg can also
be used in these advanced algorithms.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>DPDK</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">DPDK provides libraries and network drivers to accelerate packet
processing.
More specifically, network processing is executed as a user-space
application that bypasses the network protocol stack of OS kernel.
Dedicated CPU cores are assigned for receiving packets, on which the
user-space applications are polling the NIC to receive packets.
This can reduce the overheads for context switching and data copying
compared with a network protocol stack of OS kernel triggered by
interrupts, resulting in a lower processing latency and higher network
throughput.
Please note that utilization of CPU cores that are polling the NIC is
always 100 percent.
In addition, the memory access can be accelerated by utilizing
hugepages supported by Linux.
Their sizes are larger than standard <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="4$\mathrm{kB}$" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.2" xref="S2.SS2.p1.1.m1.1.2.cmml"><mn id="S2.SS2.p1.1.m1.1.2.2" xref="S2.SS2.p1.1.m1.1.2.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p1.1.m1.1.2.1" xref="S2.SS2.p1.1.m1.1.2.1.cmml">​</mo><mi class="ltx_unit" id="S2.SS2.p1.1.m1.1.2.3" xref="S2.SS2.p1.1.m1.1.2.3.cmml">kB</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.2"><times id="S2.SS2.p1.1.m1.1.2.1.cmml" xref="S2.SS2.p1.1.m1.1.2.1"></times><cn type="integer" id="S2.SS2.p1.1.m1.1.2.2.cmml" xref="S2.SS2.p1.1.m1.1.2.2">4</cn><csymbol cd="latexml" id="S2.SS2.p1.1.m1.1.2.3.cmml" xref="S2.SS2.p1.1.m1.1.2.3">kilobyte</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">4$\mathrm{kB}$</annotation></semantics></math> pages so that
TLB (Translation Lookaside Buffer) misses can be reduced.
Since they are statically allocated in a physical memory, page-in and
page-out overheads can also be eliminated.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">There are some high-performance network processing frameworks that
support TCP/IP on top of DPDK.
F-Stack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, DPDK-ANS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and mTCP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> are
open-source network processing frameworks based on TCP/IP stack of
FreeBSD on top of DPDK.
These frameworks assume the shared nothing architecture, in which data
are not shared between multiple processing cores.
In this case, incoming packets are distributed to each processing core
by Receive Side Scaling (RSS) of NICs and then the packets are
processed within the assigned core.
ZygOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and Shenango <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> support a shared
memory so that multiple processing cores can share data.
A task scheduler that can distribute the workload to multiple cores
to balance their workload is also implemented.
In this paper, the federated learning server workload is predictable
since the number of clients and the model size are determined
beforehand.
We can thus balance their workload by assigning the same number of
clients for each processing core.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Smart NIC</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">Smart NIC is a kind of NICs that have processing cores to perform
custom packet processing and routing control functions.
These tasks are typically executed by the CPU of the host machine.
By offloading them to the smart NIC, the CPU workload of the host machine
is reduced, so that the host CPU can concentrate on the other user
applications.
For example, VPNs (Virtual Private Networks) employ packet
encapsulation and encryption to ensure a secure data
communication.
In this case, adding encapsulation headers increases the packet sizes,
and the encryption and decryption increases the computation
overheads.
In addition to the standard packet processing and routing functions,
intrusion detection from external networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>,
data encryption/decryption, and data compression/decompression can
be offloaded to the smart NICs.
As the networking technology continues to evolve, it becomes
increasingly complex.
Smart NICs have a good potential to offload such network processing to
the NIC and reduce the CPU workload of the host machine.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, various smart NIC products are evaluated in
detail.
Generally, processing cores implemented on the smart NICs are slower
than those of host CPUs.
Also, their L2/L3 caches and DRAMs are not abundant.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, application characteristics that can be
efficiently offloaded to smart NICs are analyzed, and a task scheduling
that considers this insight is proposed.
In this paper, we focus on the aggregation process of the federated
learning server, in which a shared memory that can store only a single
set of global parameters is used and its memory access pattern is
straightforward.
This suggests that using smart NICs is a promising solution to offload
the federated learning server.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p">In this paper, we use NVIDIA BlueField-2 DPU MBF2H332A-AENOT as a
target smart NIC.
It is comprised of an SoC (System-on-Chip) that includes an 8-core ARM
processor running at 2.5GHz, 16GB DRAM, two 25Gbit Ethernet (GbE)
interfaces, and PCIe Gen4 interface.
It is connected to the network via the 25GbE interfaces and connected
to the host machine via the PCIe Gen4 interface.
Operating systems such as Linux is running on the DPU, and DPDK
applications can be executed on it.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, a data augmentation task is offloaded to the DPU in
order to accelerate deep learning tasks by overlapping the data
augmentation performed on the DPU and other training steps performed
on the host CPU.
Since the data augmentation does not require a network processing and
the DPU is used as an additional computation resource, benefits of the
25GbE interfaces of the DPU are not fully exploited.
In this paper, on the other hand, the federated learning server is
offloaded to the DPU as an I/O intensive task, so it can fully utilize
the benefits of smart NICs.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS3.p5.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, a host CPU workload is
offloaded to the DPU by exploiting the RDMA (Remote Direct Memory Access)
functionalities.
Specifically, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, communication primitives that support
non-blocking point-to-point communication and collective communication
are implemented on the DPU.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, communication performance between DPU and
host CPU via PCIe and that of RDMA are studied.
As a case study, KVS (Key-Value Store) is implemented on DPU.
Specifically, a part of keys in the KVS is cached in local DRAM of the
DPU to accelerate the KVS application.
Please note that the federated learning server running on the DPU in
this paper is quite simple.
It processes incoming packets directly and returns the aggregated
results to clients without communicating with the host CPU via PCIe.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para ltx_noindent">
<p id="S2.SS3.p6.1" class="ltx_p">In this paper, we offload the aggregation process of federated
learning onto the DPU.
It is effective to reduce the CPU workload of the host machine.
Other than DPU, using FPGA (Field Programmable Gate Array) based
programmable NICs is another solution to offload the CPU workload.
However, since Linux and development tools/libraries are available on
the DPU, the portability of software programs is high, which is
attractive especially for machine learning tasks.
</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Federated Learning Server on DPU</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2307.06561/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="268" height="136" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Federated learning system where federated learning server
is running on DPU</figcaption>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Federated Learning Server on DPU ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates a federated learning system
consisting of a single server and clients.
In this paper, the server is running on the DPU.
They are connected by a copper cable.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Client Process</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">Each client has its local model and trains it with its own local data.
The local training is repeated several times, and then the client
sends the trained weight parameters to the server via the 25GbE
network.
After the client receives the aggregated global parameters from the
server, it updates its local parameters by substituting them with the
received global parameters.
These steps are referred to as a “round” in federated learning.
By repeating these rounds, we can lower the training error for the
local data and share the local training results to the other
clients.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">To exchange the weight parameters between the server and clients, we
use UDP as a lightweight transport layer protocol as will be described
more specifically in Section <a href="#S3.SS2.SSS3" title="3.2.3 Lightweight Network Protocol ‣ 3.2 Server Process ‣ 3 Federated Learning Server on DPU ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>.
It is compared with a baseline implementation that uses TCP.
In both the cases, the client processes are implemented in Python.
They use socket APIs for the communication with TCP or UDP via a
TCP/IP protocol suite of the OS kernel.
In the case of packet loss, the missing global parameters are
complemented with the local parameters.
In other words, the missing part is left as the local parameters.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Server Process</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The federated learning server receives local parameters from
clients and aggregates them based on Algorithm
<a href="#alg1" title="Algorithm 1 ‣ 2.1.1 Federated Averaging ‣ 2.1 Federated Learning ‣ 2 Background and Related Work ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
That is, the local parameters are averaged to produce global
parameters.
The global parameters are then sent back to the clients.
The federated learning server is implemented in C++.
The baseline server uses socket APIs for the TCP communication via
a TCP/IP protocol stack of the OS kernel.
The proposed server running on the DPU uses UDP communication, and it
is implemented with DPDK.
It directly accesses the NIC and handles the UDP communication without
using the protocol stack of the OS kernel.
It thus parses the Ethernet, IP, and UDP headers of incoming packets
and generates outgoing packets in the DPU.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>DPDK Model</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">DPDK applications can be modeled as “Run to Completion” model or
“Pipeline” model.
In the Run to Completion model, packet reception, packet processing,
and packet transmission steps are executed by a single logical CPU
core.
On the other hand, they are partitioned and executed by multiple
logical cores in the Pipeline model.
In the Run to Completion model, the packet processing step becomes a
bottleneck if the processing is complicated and time-consuming,
resulting in a lower overall throughput.
In this paper, we employ the Pipeline model that distributes the
processing steps to multiple processing cores in order to eliminate
the performance bottleneck.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Packet Processing</h4>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2307.06561/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="284" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Multiple threads of server process on DPU</figcaption>
</figure>
<div id="S3.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.2 Packet Processing ‣ 3.2 Server Process ‣ 3 Federated Learning Server on DPU ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the server process based on the
Pipeline model.
An RX thread, <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mi id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">K</annotation></semantics></math> Worker threads, and a TX thread are connected via
ring queues.
Each thread is executed on a dedicated processing core.
Red arrows indicate dequeueing of packets, while blue ones are
enqueueing of packets.
These ring queues employ the Ring library provided by DPDK for
communication between the threads.
They enqueue and dequeue pointers of mbuf objects, which represent
packets, to exchange packets between these threads.
The RX thread polls the packet receiving queue of the NIC.
When a packet is received, it verifies the Ethernet and IP headers of
the packet to confirm that it is the packet coming from a federated
learning client.
It also checks the source port number in the UDP header and puts the
packet into the RX ring of the corresponding client.
Additionally, the RX thread sends acknowledgment packets to clients as
will be described in Section <a href="#S3.SS2.SSS3" title="3.2.3 Lightweight Network Protocol ‣ 3.2 Server Process ‣ 3 Federated Learning Server on DPU ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p2.2" class="ltx_p">Worker thread <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mi id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><ci id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">i</annotation></semantics></math> sequentially polls RX ring queues <math id="S3.SS2.SSS2.p2.2.m2.4" class="ltx_Math" alttext="i,i+K,i+2K,..." display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.4a"><mrow id="S3.SS2.SSS2.p2.2.m2.4.4.2" xref="S3.SS2.SSS2.p2.2.m2.4.4.3.cmml"><mi id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml">i</mi><mo id="S3.SS2.SSS2.p2.2.m2.4.4.2.3" xref="S3.SS2.SSS2.p2.2.m2.4.4.3.cmml">,</mo><mrow id="S3.SS2.SSS2.p2.2.m2.3.3.1.1" xref="S3.SS2.SSS2.p2.2.m2.3.3.1.1.cmml"><mi id="S3.SS2.SSS2.p2.2.m2.3.3.1.1.2" xref="S3.SS2.SSS2.p2.2.m2.3.3.1.1.2.cmml">i</mi><mo id="S3.SS2.SSS2.p2.2.m2.3.3.1.1.1" xref="S3.SS2.SSS2.p2.2.m2.3.3.1.1.1.cmml">+</mo><mi id="S3.SS2.SSS2.p2.2.m2.3.3.1.1.3" xref="S3.SS2.SSS2.p2.2.m2.3.3.1.1.3.cmml">K</mi></mrow><mo id="S3.SS2.SSS2.p2.2.m2.4.4.2.4" xref="S3.SS2.SSS2.p2.2.m2.4.4.3.cmml">,</mo><mrow id="S3.SS2.SSS2.p2.2.m2.4.4.2.2" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.cmml"><mi id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.2" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.2.cmml">i</mi><mo id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.1" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.1.cmml">+</mo><mrow id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.cmml"><mn id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.2" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.1" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.3" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.3.cmml">K</mi></mrow></mrow><mo id="S3.SS2.SSS2.p2.2.m2.4.4.2.5" xref="S3.SS2.SSS2.p2.2.m2.4.4.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS2.p2.2.m2.2.2" xref="S3.SS2.SSS2.p2.2.m2.2.2.cmml">…</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.4b"><list id="S3.SS2.SSS2.p2.2.m2.4.4.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.4.4.2"><ci id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">𝑖</ci><apply id="S3.SS2.SSS2.p2.2.m2.3.3.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.3.3.1.1"><plus id="S3.SS2.SSS2.p2.2.m2.3.3.1.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.3.3.1.1.1"></plus><ci id="S3.SS2.SSS2.p2.2.m2.3.3.1.1.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.3.3.1.1.2">𝑖</ci><ci id="S3.SS2.SSS2.p2.2.m2.3.3.1.1.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.3.3.1.1.3">𝐾</ci></apply><apply id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2"><plus id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.1"></plus><ci id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.2">𝑖</ci><apply id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3"><times id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.1"></times><cn type="integer" id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.2">2</cn><ci id="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.4.4.2.2.3.3">𝐾</ci></apply></apply><ci id="S3.SS2.SSS2.p2.2.m2.2.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.2.2">…</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.4c">i,i+K,i+2K,...</annotation></semantics></math>.
Once a packet is retrieved from the RX ring queue, local parameters
are extracted from its payload and added to a float array which has
been initialized with 0.
After local parameters of all the clients have been added to the float
array, a single Worker thread divides each element of the float array by the
number of clients in order to calculate element-wise averages of the
local parameters (i.e., new global parameters).
Local parameters that are missing due to packet loss are not included
in the divisor.
In other words, the number of clients participating in the aggregation
may differ depending on the element of the global parameters.
The other Worker threads wait until the element-wise averages are
calculated using a spinlock mechanism.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">After the averages are calculated, each Worker thread copies the
global parameters to payload of packets for clients, fills out
Ethernet, IP, and UDP headers of the packets, and puts them into the
TX ring.
The TX thread polls the TX ring.
Once a packet is retrieved from the TX ring, it is enqueued to the
transmission port of the NIC and sent to the client.
The TX thread then releases the mbuf objects, in which the packet was
stored.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Lightweight Network Protocol</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">In DPDK, since a network protocol stack of OS kernel is bypassed, a
flow control functionality has to be provided by the application
layer.
A client process has two communication states: sending local
parameters and receiving global parameters.
Similarly, a server process has three states: receiving local
parameters, computation, and sending global parameters.
To guarantee the correct state transitions, a simple yet reliable
network protocol that uses acknowledgement packets is implemented in
the application layer.
For example, to detect the completion of local parameter reception, a
server may be able to count the total number of packets received.
However, if a packet is lost, the server may still be in the state of
receiving local parameters while the client has already finished
sending local parameters and has transitioned to the state of
receiving global parameters.
In this case, the federated learning may stop.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">To avoid such a situation, the clients send an acknowledgement packet
denoted as “END” to indicate the end of transmission after sending
the local parameters.
The server then responds to the clients with a response packet denoted
as “END ACK”.
The client keeps sending END packets until an END ACK packet is
received, so that it can ensure that the server has finished receiving
the local parameters.
The client transitions to the state of receiving global parameters
after the reception of END ACK.
In our implementation, two types of control packets, namely START and
END, and their corresponding response packets, namely START ACK and
END ACK are used.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2307.06561/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="296" height="227" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Lightweight protocol for UDP communication</figcaption>
</figure>
<div id="S3.SS2.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p3.1" class="ltx_p">Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2.3 Lightweight Network Protocol ‣ 3.2 Server Process ‣ 3 Federated Learning Server on DPU ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the communication protocol
between a client and a server.
When the client completes its local training, it sends a START packet
to the server.
Upon receiving a START ACK packet from the server, the client sends
local parameters and then an END packet.
When an END ACK packet is received from the server, the client
terminates the state of sending local parameters and transitions to
the state of receiving global parameters.
After receiving the global parameters, when an END packet is received
from the server, the client responds with an END ACK packet.
The client then replaces the local parameters with the received global
parameters, and completes a single round of federated learning.
Since there is a possibility that the END ACK packet is lost,
retransmitted END packets can be handled for one second after the first END packet is received.
TCP also has a similar waiting period when terminating a connection.
RFC 793 defines this period as twice the MSL (Maximum Segment
Lifetime), which is commonly two minutes.
In our communication protocol, the waiting period is set to this minimum
value, though this did not affect the evaluation results in
this paper.
</p>
</div>
<div id="S3.SS2.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p4.1" class="ltx_p">When the server receives a START packet, it responds with a START ACK
packet and then receives local parameters until an END packet is
received from the client.
As described in Section <a href="#S3.SS2.SSS2" title="3.2.2 Packet Processing ‣ 3.2 Server Process ‣ 3 Federated Learning Server on DPU ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>, the RX thread enqueues
incoming packets to the RX rings immediately after receiving them.
Worker threads poll the RX rings to retrieve the packets as soon
as they arrive, and then the element-wise addition of the received
local parameters is executed.
That is, the reception and addition of local parameters are performed
in parallel until an END packet is received.
When an END packet is received, the server waits until the Worker
threads process all the packets stored in the RX rings and then
terminates the state of receiving local parameters.
The server then transitions to the state of computation by responding
with an END ACK packet, so that it performs element-wise division to
the float array to obtain the global parameters.
When the RX thread receives an END packet, it puts an END ACK packet
directly into the TX ring without passing the packet to the Worker
thread.
If the END ACK packet is lost at this point, the RX thread can receive
another END packet retransmitted by the client while the server has
transitioned to the state of computation.
This implementation can reduce the number of context switches between
these threads.
After the element-wise division is completed, the server sends the
global parameters to each client.
The global parameters are sent in the same way as the local
parameters.
A single round is then completed once an END ACK packet is received
from the clients.</p>
</div>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Elimination of Exclusive Access Control</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS4.p1.1" class="ltx_p">In this server design, multiple Worker threads running on multiple processing
cores execute the element-wise addition on the same float array stored
in the shared memory of the DPU.
There is a possibility of write-write conflicts (e.g., lost update)
between multiple Worker threads.
To avoid the conflicts, an exclusive access control mechanism is
typically required for the threads to ensure precise computation
results.
To further improve the performance, in this paper an approximated
federated learning server that eliminates this exclusive access
control is also implemented.
In Section <a href="#S5" title="5 Evaluations ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the baseline server and the approximated
server are evaluated in terms of the execution time and federated
learning convergence to show the performance and accuracy tradeoffs
between them.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Implementation Details</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Client Process</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.6" class="ltx_p">Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1 Client Process ‣ 4 Implementation Details ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the packet format for the UDP
communication.
Each client is identified by source port number of the UDP header.
In the client process, the local parameters are extracted from the
trained local model and converted to numpy.float32.
Then, they are serialized and transmitted to the server.
To detect packet loss and guarantee the ordered data transfer, a
<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="4$\mathrm{B}$" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.2" xref="S4.SS1.p1.1.m1.1.2.cmml"><mn id="S4.SS1.p1.1.m1.1.2.2" xref="S4.SS1.p1.1.m1.1.2.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.2.1" xref="S4.SS1.p1.1.m1.1.2.1.cmml">​</mo><mi class="ltx_unit" mathvariant="normal" id="S4.SS1.p1.1.m1.1.2.3" xref="S4.SS1.p1.1.m1.1.2.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.2"><times id="S4.SS1.p1.1.m1.1.2.1.cmml" xref="S4.SS1.p1.1.m1.1.2.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.2.2.cmml" xref="S4.SS1.p1.1.m1.1.2.2">4</cn><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.2.3.cmml" xref="S4.SS1.p1.1.m1.1.2.3">byte</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">4$\mathrm{B}$</annotation></semantics></math> index number of packets is added to the beginning of
each payload.
The payload size without the index number is <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="1468$\mathrm{B}$" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.2" xref="S4.SS1.p1.2.m2.1.2.cmml"><mn id="S4.SS1.p1.2.m2.1.2.2" xref="S4.SS1.p1.2.m2.1.2.2.cmml">1468</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.1.2.1" xref="S4.SS1.p1.2.m2.1.2.1.cmml">​</mo><mi class="ltx_unit" mathvariant="normal" id="S4.SS1.p1.2.m2.1.2.3" xref="S4.SS1.p1.2.m2.1.2.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.2.cmml" xref="S4.SS1.p1.2.m2.1.2"><times id="S4.SS1.p1.2.m2.1.2.1.cmml" xref="S4.SS1.p1.2.m2.1.2.1"></times><cn type="integer" id="S4.SS1.p1.2.m2.1.2.2.cmml" xref="S4.SS1.p1.2.m2.1.2.2">1468</cn><csymbol cd="latexml" id="S4.SS1.p1.2.m2.1.2.3.cmml" xref="S4.SS1.p1.2.m2.1.2.3">byte</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">1468$\mathrm{B}$</annotation></semantics></math> if we
assume MTU (Maximum Transmission Unit) is <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="1500$\mathrm{B}$" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.2" xref="S4.SS1.p1.3.m3.1.2.cmml"><mn id="S4.SS1.p1.3.m3.1.2.2" xref="S4.SS1.p1.3.m3.1.2.2.cmml">1500</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.2.1" xref="S4.SS1.p1.3.m3.1.2.1.cmml">​</mo><mi class="ltx_unit" mathvariant="normal" id="S4.SS1.p1.3.m3.1.2.3" xref="S4.SS1.p1.3.m3.1.2.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.2.cmml" xref="S4.SS1.p1.3.m3.1.2"><times id="S4.SS1.p1.3.m3.1.2.1.cmml" xref="S4.SS1.p1.3.m3.1.2.1"></times><cn type="integer" id="S4.SS1.p1.3.m3.1.2.2.cmml" xref="S4.SS1.p1.3.m3.1.2.2">1500</cn><csymbol cd="latexml" id="S4.SS1.p1.3.m3.1.2.3.cmml" xref="S4.SS1.p1.3.m3.1.2.3">byte</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">1500$\mathrm{B}$</annotation></semantics></math> and IP and
UDP headers are <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="20$\mathrm{B}$" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.2" xref="S4.SS1.p1.4.m4.1.2.cmml"><mn id="S4.SS1.p1.4.m4.1.2.2" xref="S4.SS1.p1.4.m4.1.2.2.cmml">20</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.2.1" xref="S4.SS1.p1.4.m4.1.2.1.cmml">​</mo><mi class="ltx_unit" mathvariant="normal" id="S4.SS1.p1.4.m4.1.2.3" xref="S4.SS1.p1.4.m4.1.2.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.2.cmml" xref="S4.SS1.p1.4.m4.1.2"><times id="S4.SS1.p1.4.m4.1.2.1.cmml" xref="S4.SS1.p1.4.m4.1.2.1"></times><cn type="integer" id="S4.SS1.p1.4.m4.1.2.2.cmml" xref="S4.SS1.p1.4.m4.1.2.2">20</cn><csymbol cd="latexml" id="S4.SS1.p1.4.m4.1.2.3.cmml" xref="S4.SS1.p1.4.m4.1.2.3">byte</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">20$\mathrm{B}$</annotation></semantics></math> and <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="8$\mathrm{B}$" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mrow id="S4.SS1.p1.5.m5.1.2" xref="S4.SS1.p1.5.m5.1.2.cmml"><mn id="S4.SS1.p1.5.m5.1.2.2" xref="S4.SS1.p1.5.m5.1.2.2.cmml">8</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.5.m5.1.2.1" xref="S4.SS1.p1.5.m5.1.2.1.cmml">​</mo><mi class="ltx_unit" mathvariant="normal" id="S4.SS1.p1.5.m5.1.2.3" xref="S4.SS1.p1.5.m5.1.2.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><apply id="S4.SS1.p1.5.m5.1.2.cmml" xref="S4.SS1.p1.5.m5.1.2"><times id="S4.SS1.p1.5.m5.1.2.1.cmml" xref="S4.SS1.p1.5.m5.1.2.1"></times><cn type="integer" id="S4.SS1.p1.5.m5.1.2.2.cmml" xref="S4.SS1.p1.5.m5.1.2.2">8</cn><csymbol cd="latexml" id="S4.SS1.p1.5.m5.1.2.3.cmml" xref="S4.SS1.p1.5.m5.1.2.3">byte</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">8$\mathrm{B}$</annotation></semantics></math>, respectively.
In this case, each UDP packet can convey <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="367" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mn id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">367</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><cn type="integer" id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">367</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">367</annotation></semantics></math> weight parameters.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2307.06561/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="306" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Packet format for UDP communication</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Server Process</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">As mentioned in Section <a href="#S2.SS3" title="2.3 Smart NIC ‣ 2 Background and Related Work ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>, we use NVIDIA BlueField-2
DPU MBF2H332A-AENOT as a platform of the proposed DPDK-based federated
learning server.
In this section, the baseline and proposed servers are described
below.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Baseline TCP Server</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p1.3" class="ltx_p">Here, we describe a baseline implementation of federated learning
server using TCP.
It uses socket APIs provided by OS kernel for communication.
It is compared with the proposed DPDK server using UDP in Section
<a href="#S5" title="5 Evaluations ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
In the TCP communication, MSS (Maximum Segment Size) is set to
<math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="1460$\mathrm{B}$" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.p1.1.m1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.2.cmml"><mn id="S4.SS2.SSS1.p1.1.m1.1.2.2" xref="S4.SS2.SSS1.p1.1.m1.1.2.2.cmml">1460</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS1.p1.1.m1.1.2.1" xref="S4.SS2.SSS1.p1.1.m1.1.2.1.cmml">​</mo><mi class="ltx_unit" mathvariant="normal" id="S4.SS2.SSS1.p1.1.m1.1.2.3" xref="S4.SS2.SSS1.p1.1.m1.1.2.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.2"><times id="S4.SS2.SSS1.p1.1.m1.1.2.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.2.1"></times><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.2.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.2.2">1460</cn><csymbol cd="latexml" id="S4.SS2.SSS1.p1.1.m1.1.2.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.2.3">byte</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">1460$\mathrm{B}$</annotation></semantics></math> to comply with MTU of the UDP communication, which is
<math id="S4.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="1500$\mathrm{B}$" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mrow id="S4.SS2.SSS1.p1.2.m2.1.2" xref="S4.SS2.SSS1.p1.2.m2.1.2.cmml"><mn id="S4.SS2.SSS1.p1.2.m2.1.2.2" xref="S4.SS2.SSS1.p1.2.m2.1.2.2.cmml">1500</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS1.p1.2.m2.1.2.1" xref="S4.SS2.SSS1.p1.2.m2.1.2.1.cmml">​</mo><mi class="ltx_unit" mathvariant="normal" id="S4.SS2.SSS1.p1.2.m2.1.2.3" xref="S4.SS2.SSS1.p1.2.m2.1.2.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><apply id="S4.SS2.SSS1.p1.2.m2.1.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.2"><times id="S4.SS2.SSS1.p1.2.m2.1.2.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.2.1"></times><cn type="integer" id="S4.SS2.SSS1.p1.2.m2.1.2.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.2.2">1500</cn><csymbol cd="latexml" id="S4.SS2.SSS1.p1.2.m2.1.2.3.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.2.3">byte</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">1500$\mathrm{B}$</annotation></semantics></math>.
The overall packet length including the Ethernet header is
<math id="S4.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="1514$\mathrm{B}$" display="inline"><semantics id="S4.SS2.SSS1.p1.3.m3.1a"><mrow id="S4.SS2.SSS1.p1.3.m3.1.2" xref="S4.SS2.SSS1.p1.3.m3.1.2.cmml"><mn id="S4.SS2.SSS1.p1.3.m3.1.2.2" xref="S4.SS2.SSS1.p1.3.m3.1.2.2.cmml">1514</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS1.p1.3.m3.1.2.1" xref="S4.SS2.SSS1.p1.3.m3.1.2.1.cmml">​</mo><mi class="ltx_unit" mathvariant="normal" id="S4.SS2.SSS1.p1.3.m3.1.2.3" xref="S4.SS2.SSS1.p1.3.m3.1.2.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m3.1b"><apply id="S4.SS2.SSS1.p1.3.m3.1.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.2"><times id="S4.SS2.SSS1.p1.3.m3.1.2.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.2.1"></times><cn type="integer" id="S4.SS2.SSS1.p1.3.m3.1.2.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.2.2">1514</cn><csymbol cd="latexml" id="S4.SS2.SSS1.p1.3.m3.1.2.3.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.2.3">byte</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.1c">1514$\mathrm{B}$</annotation></semantics></math> in both cases.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">The baseline TCP implementation runs either on the host CPU or the
<math id="S4.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.SS2.SSS1.p2.1.m1.1a"><mn id="S4.SS2.SSS1.p2.1.m1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.1.m1.1b"><cn type="integer" id="S4.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.1.m1.1c">8</annotation></semantics></math>-core processor of DPU.
When a TCP connection request is received from a client, a new thread
is created through the use of the standard library std::thread.
The assignment of threads to cores is done by the OS kernel.
Each thread receives local parameters from the client and subsequently
performs the element-wise addition of the local parameters to the
float array.
Only a single thread executes the element-wise division to produce
global parameters, while the other threads wait for the division by
using std::mutex and std::condition_variable, which are provided by
the standard library.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>DPDK Server</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p1.6" class="ltx_p">In the proposed DPDK server implementation, the global parameters are
declared as a conventional float array.
By employing the operator+= of std::atomic_ref<math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mo id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><lt id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">&lt;</annotation></semantics></math>float<math id="S4.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mo id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><gt id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">&gt;</annotation></semantics></math>, which is
an atomic reference to a float variable implemented in C++20, an
exclusive access control is enforced only during the execution of the
element-wise addition.
On the other hand, since the division operation is carried out by a
single representative Worker thread, it is executed without using
std::atomic_ref<math id="S4.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS2.SSS2.p1.3.m3.1a"><mo id="S4.SS2.SSS2.p1.3.m3.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.1b"><lt id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.1c">&lt;</annotation></semantics></math>float<math id="S4.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS2.SSS2.p1.4.m4.1a"><mo id="S4.SS2.SSS2.p1.4.m4.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.4.m4.1b"><gt id="S4.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.4.m4.1c">&gt;</annotation></semantics></math>.
As mentioned in Section <a href="#S3" title="3 Federated Learning Server on DPU ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, if the exclusive access
control between these threads is eliminated, the precise averages
cannot be guaranteed.
We implement such an approximated server without using
std::atomic_ref<math id="S4.SS2.SSS2.p1.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS2.SSS2.p1.5.m5.1a"><mo id="S4.SS2.SSS2.p1.5.m5.1.1" xref="S4.SS2.SSS2.p1.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.5.m5.1b"><lt id="S4.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.5.m5.1c">&lt;</annotation></semantics></math>float<math id="S4.SS2.SSS2.p1.6.m6.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS2.SSS2.p1.6.m6.1a"><mo id="S4.SS2.SSS2.p1.6.m6.1.1" xref="S4.SS2.SSS2.p1.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.6.m6.1b"><gt id="S4.SS2.SSS2.p1.6.m6.1.1.cmml" xref="S4.SS2.SSS2.p1.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.6.m6.1c">&gt;</annotation></semantics></math>.
It is compared with the baseline DPDK server that uses the exclusive
access control in terms of the speed and learning convergence.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluations</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation Environment</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Evaluation Environment ‣ 5 Evaluations ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the evaluation environment of server and
client machines.
The DPU is attached to the server machine via PCIe Gen4 interface as a
NIC.
The DPU and client machine are connected by a 2m 25GbE direct attach
copper cable.
The server process is executed either on the DPU or the server
machine.
When the server process is executed on the DPU, packets coming from
the client machine to the DPU’s physical interface are forwarded to
the ARM processor of the DPU.
In this case, federated learning packets are not forwarded to the
server machine (i.e., host CPU) since the aggregation process is
entirely offloaded onto the DPU.
On the other hand, when the server process is executed on the host
CPU, the packets are forwarded from the DPU’s physical interface to
the host machine’s interface.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">In this experiment, the dataset is CIFAR-10, which consists of 50,000
training samples and 10,000 test samples.
The number of client processes is ten.
The training samples are partitioned to the ten client processes
equally, so each client process has 5,000 i.i.d. training samples.
The global model is tested with the 10,000 test samples.
The model architecture is CNN consisting of four convolutional layers
and two fully connected layers
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Conv(32, 3) <math id="footnote1.m1.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m1.1b"><mo stretchy="false" id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><ci id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">\to</annotation></semantics></math> Relu <math id="footnote1.m2.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m2.1b"><mo stretchy="false" id="footnote1.m2.1.1" xref="footnote1.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m2.1c"><ci id="footnote1.m2.1.1.cmml" xref="footnote1.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m2.1d">\to</annotation></semantics></math> Conv(64, 3) <math id="footnote1.m3.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m3.1b"><mo stretchy="false" id="footnote1.m3.1.1" xref="footnote1.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m3.1c"><ci id="footnote1.m3.1.1.cmml" xref="footnote1.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m3.1d">\to</annotation></semantics></math> Relu <math id="footnote1.m4.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m4.1b"><mo stretchy="false" id="footnote1.m4.1.1" xref="footnote1.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m4.1c"><ci id="footnote1.m4.1.1.cmml" xref="footnote1.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m4.1d">\to</annotation></semantics></math>
Maxpool(2) <math id="footnote1.m5.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m5.1b"><mo stretchy="false" id="footnote1.m5.1.1" xref="footnote1.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m5.1c"><ci id="footnote1.m5.1.1.cmml" xref="footnote1.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m5.1d">\to</annotation></semantics></math> Conv(128, 3) <math id="footnote1.m6.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m6.1b"><mo stretchy="false" id="footnote1.m6.1.1" xref="footnote1.m6.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m6.1c"><ci id="footnote1.m6.1.1.cmml" xref="footnote1.m6.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m6.1d">\to</annotation></semantics></math> Relu <math id="footnote1.m7.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m7.1b"><mo stretchy="false" id="footnote1.m7.1.1" xref="footnote1.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m7.1c"><ci id="footnote1.m7.1.1.cmml" xref="footnote1.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m7.1d">\to</annotation></semantics></math> Conv(256, 3) <math id="footnote1.m8.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m8.1b"><mo stretchy="false" id="footnote1.m8.1.1" xref="footnote1.m8.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m8.1c"><ci id="footnote1.m8.1.1.cmml" xref="footnote1.m8.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m8.1d">\to</annotation></semantics></math>
Relu <math id="footnote1.m9.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m9.1b"><mo stretchy="false" id="footnote1.m9.1.1" xref="footnote1.m9.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m9.1c"><ci id="footnote1.m9.1.1.cmml" xref="footnote1.m9.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m9.1d">\to</annotation></semantics></math> Maxpool(2) <math id="footnote1.m10.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m10.1b"><mo stretchy="false" id="footnote1.m10.1.1" xref="footnote1.m10.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m10.1c"><ci id="footnote1.m10.1.1.cmml" xref="footnote1.m10.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m10.1d">\to</annotation></semantics></math> FC(256) <math id="footnote1.m11.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m11.1b"><mo stretchy="false" id="footnote1.m11.1.1" xref="footnote1.m11.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m11.1c"><ci id="footnote1.m11.1.1.cmml" xref="footnote1.m11.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m11.1d">\to</annotation></semantics></math> Dropout(0.5) <math id="footnote1.m12.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m12.1b"><mo stretchy="false" id="footnote1.m12.1.1" xref="footnote1.m12.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m12.1c"><ci id="footnote1.m12.1.1.cmml" xref="footnote1.m12.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m12.1d">\to</annotation></semantics></math> FC(10)
<math id="footnote1.m13.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="footnote1.m13.1b"><mo stretchy="false" id="footnote1.m13.1.1" xref="footnote1.m13.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote1.m13.1c"><ci id="footnote1.m13.1.1.cmml" xref="footnote1.m13.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m13.1d">\to</annotation></semantics></math> Softmax(10)</span></span></span>.
The number of parameters is about two million, and each parameter is
represented as a 32-bit float.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">The server process is executed on the 8-core ARM processor of the
DPU, in which two cores are dedicated to the RX and TX threads,
respectively.
Worker threads are executed on five cores.
Since there are ten clients, each thread handles two clients.
One core is left for other tasks including the OS task scheduling.
The client processes are implemented with Python 3.11.4, Pytorch
2.0.1, and torchvision 0.15.2.
The server process is implemented with C++ and DPDK 20.11.3, and
compiled with -O3 optimization level.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Specification of server machine, DPU, and client machine</figcaption>
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.2.1.1" class="ltx_td ltx_border_t"></td>
<th id="S5.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Server Machine</th>
<th id="S5.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">DPU</th>
<th id="S5.T1.1.2.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Client Machine</th>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t">OS</td>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">Ubuntu 20.04</td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t">Ubuntu 20.04</td>
<td id="S5.T1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_t">Ubuntu 20.04</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.4.3.1" class="ltx_td ltx_align_left">CPU</td>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_left">Intel Core i7-11700</td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_left">ARM Cortex-A72</td>
<td id="S5.T1.1.4.3.4" class="ltx_td ltx_align_left">Intel Core i7-10700</td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<td id="S5.T1.1.5.4.1" class="ltx_td ltx_align_left">RAM</td>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_left">16 GB</td>
<td id="S5.T1.1.5.4.3" class="ltx_td ltx_align_left">16 GB</td>
<td id="S5.T1.1.5.4.4" class="ltx_td ltx_align_left">32 GB</td>
</tr>
<tr id="S5.T1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.2" class="ltx_td ltx_align_left ltx_border_b">NIC</td>
<td id="S5.T1.1.1.3" class="ltx_td ltx_align_left ltx_border_b">NVIDIA BF-2 DPU</td>
<td id="S5.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_b"><math id="S5.T1.1.1.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S5.T1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><minus id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">-</annotation></semantics></math></td>
<td id="S5.T1.1.1.4" class="ltx_td ltx_align_left ltx_border_b">Intel XXV710-DA2</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2307.06561/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="345" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Server response time measured in client</figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2307.06561/assets/x7.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="345" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Server execution time measured in server</figcaption>
</figure>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2307.06561/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Training convergence (rounds vs. test loss)</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Results</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2.3 Lightweight Network Protocol ‣ 3.2 Server Process ‣ 3 Federated Learning Server on DPU ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, a client sends a START packet to
a server, and then it receives an END packet from the server.
The proposed federated learning server is evaluated in terms of the
latency to receive the END packet after the START packet is sent.
This is the server’s response time for the aggregation which is
observed by the client.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">Figure <a href="#S5.F6" title="Figure 6 ‣ 5.1 Evaluation Environment ‣ 5 Evaluations ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the evaluation results of the
following six server implementations.</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Server running on host CPU using TCP/IP protocol stack with
exclusive access control</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Server running on host CPU using TCP/IP protocol stack without
exclusive access control</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Server running on DPU using TCP/IP protocol stack with exclusive
access control</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">Server running on DPU using TCP/IP protocol stack without
exclusive access control</p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p id="S5.I1.i5.p1.1" class="ltx_p">Server running on DPU using DPDK with exclusive access control</p>
</div>
</li>
<li id="S5.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S5.I1.i6.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i6.p1.1" class="ltx_p">Server running on DPU using DPDK without exclusive access control</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">In addition, these servers are evaluated in terms of the latency to
complete the parameter aggregation after a START packet is received.
Figure <a href="#S5.F7" title="Figure 7 ‣ 5.1 Evaluation Environment ‣ 5 Evaluations ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the evaluation results.
The blue bar shows the receiving time of local parameters, which means
the latency to receive the END packet after the START packet is
received.
Then, the element-wise addition of received local parameters and the
element-wise division of accumulated local parameters are executed.
The red bar shows the computation time for the addition and division.
Please note that Figure <a href="#S5.F7" title="Figure 7 ‣ 5.1 Evaluation Environment ‣ 5 Evaluations ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the latencies
measured at the server side; so the transmission time of the global
parameters is not included.
The complete latencies including the transmission time are shown in
Figure <a href="#S5.F6" title="Figure 6 ‣ 5.1 Evaluation Environment ‣ 5 Evaluations ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p">Regarding (1) and (3), although they are the same program, the program
execution on the DPU is much slower than that on the CPU.
Especially, the computation time (red part) is increased in the DPU
since processor performance of DPU is lower than that of the host CPU.
Regarding (3) and (4), their difference is the exclusive access
control.
Eliminating the exclusive access control speeds up the computation
time of global parameters by 6.66 times.
The comparison between (1) and (2) also shows a similar tendency while
the speedup is smaller than that on the DPU.
Regarding (3) and (5), their difference is the implementation of the
communication; (3) uses a standard TCP/IP stack while (5) uses the
proposed DPDK-based optimized communication.
Using the DPDK-based optimized communication, the receiving time of
local parameters at server is improved by 1.65 times, and the
aggregation response time is improved by 1.25 times from the client’s
view.
The server computation time for global parameters is also slightly
improved (i.e., 1.09 times speed up).
This is because a part of computation (red part) is overlapped with
the parameter reception (blue part) as mentioned in Section
<a href="#S3.SS2.SSS3" title="3.2.3 Lightweight Network Protocol ‣ 3.2 Server Process ‣ 3 Federated Learning Server on DPU ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a> and thus reduced.
Utilizing hugepages may also contribute the performance improvement.
The proposed approach (6) combines the elimination of exclusive access
control and use of the DPDK-based optimized communication implemented
on the DPU.
The proposed approach improves the aggregation response time by 3.93
times compared with (3).
It also improves the response time by 1.39 times compared with (1)
which is executed on the host CPU.
</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Training Convergence</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">Figure <a href="#S5.F8" title="Figure 8 ‣ 5.1 Evaluation Environment ‣ 5 Evaluations ‣ Performance Improvement of Federated Learning Server using Smart NIC" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows training curves of the six
approaches at a client.
We conducted the same experiments five times.
The X-axis represents the rounds, while the Y-axis represents the
average and standard deviation of the test loss.
Although the approximated computation on the baseline CPU
implementation introduces fluctuations in the training curve as shown
in (2), the negative impact of the approximation is small in (4) and
negligible in (6).
Since the performance and parallelism of the host CPU are higher than
those of DPU, it is expected that (2) introduces more
write-write conflicts and fluctuations.
Although the DPDK-based UDP communication introduces packet loss
especially in the global parameter transfer from the server to clients
(e.g., 4.68% in (6)), since our lightweight protocol can handle the
packet loss, the accuracy loss is limited.
As a result, the training curve of the proposed approach (6) is very
close to the CPU baseline (1).</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Summary</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">Although the federated learning algorithm is an emerging research
topic and continuously becoming sophisticated, the major computation
task at the server side is typically averaging the received local
parameters.
In this I/O intensive task, the network processing accounts for a
large portion compared to the computation.
In this paper, we implemented the aggregation process of the federated
learning server on NVIDIA BlueField-2 DPU as a smart NIC.
Although offloading the federated learning server on the DPU can
mitigate the host CPU workload, a simple offloading increases the
execution time compared with that on the host CPU due to its lower
processor performance.
Our approach thus combines the elimination of exclusive access control
and use of the DPDK-based lightweight communication implemented on the
DPU.
The experiment results showed that the proposed approach significantly
improves the aggregation response time compared with the DPU baseline
and it is even higher than the host CPU baseline by 1.39 times.
Training curve of the proposed approach using CIFAR-10 dataset on the
DPU showed a similar learning convergence to the CPU baseline.
Further investigations on more practical environments using non
i.i.d. datasets are our future work.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgements</span>
This paper is based on results obtained from “Research and Development Project of the Enhanced Infrastructures for Post 5G Information and Communication Systems” JPNP20017)), commissioned by the New Energy and Industrial
Technology Development Organization (NEDO).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y
Arcas.

</span>
<span class="ltx_bibblock">Communication-Efficient Learning of Deep Networks from Decentralized
Data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the International Conference on Artificial
Intelligence and Statistics (AISTATS)</span>, pages 1273–1282, April 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
NVIDIA BlueField-2 DPU.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nvidia.com/content/dam/en-zz/ja/Solutions/Data-Center/documents/bluefield-2-dpu-datasheet-jp.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nvidia.com/content/dam/en-zz/ja/Solutions/Data-Center/documents/bluefield-2-dpu-datasheet-jp.pdf</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Data Plane Development Kit.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.dpdk.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.dpdk.org</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.

</span>
<span class="ltx_bibblock">Personalized Federated Learning with Theoretical Guarantees: A
Model-Agnostic Meta-Learning Approach.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the International Conference on Neural
Information Processing Systems (NeurIPS)</span>, pages 3557–3568, December 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi.

</span>
<span class="ltx_bibblock">Adaptive Personalized Federated Learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv:2003.13461</span>, November 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
F-Stack.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.f-stack.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.f-stack.org</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
DPDK-ANS.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/ansyun/dpdk-ans" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ansyun/dpdk-ans</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
EunYoung Jeong, Shinae Wood, Muhammad Jamshed, Haewon Jeong, Sunghwan Ihm,
Dongsu Han, and KyoungSoo Park.

</span>
<span class="ltx_bibblock">mTCP: a Highly Scalable User-level TCP Stack for Multicore
Systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the USENIX Symposium on Networked Systems
Design and Implementation (NSDI)</span>, pages 489–502, April 2014.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
George Prekas, Marios Kogias, and Edouard Bugnion.

</span>
<span class="ltx_bibblock">ZygOS: Achieving Low Tail Latency for Microsecond-Scale Networked
Tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM Symposium on Operating Systems
Principles</span>, pages 325–341, October 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Amy Ousterhout, Joshua Fried, Jonathan Behrens, Adam Belay, and Hari
Balakrishnan.

</span>
<span class="ltx_bibblock">Shenango: Achieving High CPU Efficiency for Latency-sensitive
Datacenter Workloads.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the USENIX Symposium on Networked Systems
Design and Implementation (NSDI)</span>, pages 361–378, February 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Man Wu, Hiroki Matsutani, and Masaaki Kondo.

</span>
<span class="ltx_bibblock">ONLAD-IDS: ONLAD-Based Intrusion Detection System Using SmartNIC.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the International Conference on High
Performance Computing and Communications (HPCC)</span>, pages 546–553, December
2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Ming Liu, Tianyi Cui, Henry Schuh, Arvind Krishnamurthy, Simon Peter, and Karan
Gupta.

</span>
<span class="ltx_bibblock">Offloading Distributed Applications onto SmartNICs Using IPipe.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM Special Interest Group on Data
Communication (SIGCOMM)</span>, pages 318–333, August 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Arpan Jain, Nawras Alnaasan, Aamir Shafi, Hari Subramoni, and Dhabaleswar K
Panda.

</span>
<span class="ltx_bibblock">Accelerating CPU-based Distributed DNN Training on Modern HPC
Clusters using BlueField-2 DPUs.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Symposium on High-Performance
Interconnects (HOTI)</span>, pages 17–24, August 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Kaushik Kandadi Suresh, Benjamin T. Michalowicz, Bharath Ramesh, Nick Contini,
Jinghan Yao, Shulei Xu, Aamir Shafi, Hari Subramoni, and Dhabaleswar K.
Panda.

</span>
<span class="ltx_bibblock">A Novel Framework for Efficient Offloading of Communication
Operations to Bluefield SmartNICs.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Parallel &amp; Distributed
Processing Symposium (IPDPS)</span>, pages 123–133, May 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Xingda Wei, Rongxin Cheng, Yuhan Yang, Rong Chen, and Haibo Chen.

</span>
<span class="ltx_bibblock">Characterizing Off-path SmartNIC for Accelerating Distributed
Systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the USENIX Symposium on Operating Systems
Design and Implementation (OSDI)</span>, pages 987–1004, July 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.06560" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.06561" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.06561">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.06561" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.06562" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 17:55:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
