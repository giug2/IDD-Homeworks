<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents</title>
<!--Generated on Mon Sep 23 22:47:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.15594v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S1" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S2" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>SyncLLM</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.SS1" title="In 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Latency tolerant interaction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.SS2" title="In 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Token sequence format</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S4" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.SS1" title="In 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Semantic evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.SS2" title="In 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Naturalness evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.SS3" title="In 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.SS4" title="In 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Full-duplex interaction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S6" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S7" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations and Risks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A1" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional training details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A1.SS1" title="In Appendix A Additional training details ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A1.SS2" title="In Appendix A Additional training details ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Benchmarking interleaving strategies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A2" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Naturalness-MOS Instructions</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A2.SS1" title="In Appendix B Naturalness-MOS Instructions ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>N-MOS &amp; M-MOS</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A2.SS1.SSS1" title="In B.1 N-MOS &amp; M-MOS ‣ Appendix B Naturalness-MOS Instructions ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1.1 </span>Meaningfulness-MOS</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A3" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Effect of latency on full-duplex interaction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A4" title="In Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Turn-taking event correlation between prompt and generation</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Beyond Turn-Based Interfaces: 
<br class="ltx_break"/>Synchronous LLMs as Full-Duplex Dialogue Agents</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Bandhav Veluri<sup class="ltx_sup" id="id8.8.id1"><span class="ltx_text ltx_font_italic" id="id8.8.id1.1">1,2</span></sup>, Benjamin N Peloquin<sup class="ltx_sup" id="id9.9.id2">1</sup>, Bokai Yu<sup class="ltx_sup" id="id10.10.id3">1</sup>, 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id4.4.1">Hongyu Gong<sup class="ltx_sup" id="id4.4.1.1"><span class="ltx_text ltx_font_medium" id="id4.4.1.1.1">1</span></sup>,</span> <span class="ltx_text ltx_font_bold" id="id5.5.2">Shyamnath Gollakota<sup class="ltx_sup" id="id5.5.2.1"><span class="ltx_text ltx_font_medium" id="id5.5.2.1.1">2</span></sup></span>
<br class="ltx_break"/> <sup class="ltx_sup" id="id11.11.id4">1</sup>Meta AI, <sup class="ltx_sup" id="id12.12.id5">2</sup>University of Washington 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id13.13.id6">{bandhav,gshyam}@cs.washington.edu  hygong@meta.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">Despite broad interest in modeling spoken dialogue agents, most approaches are inherently “half-duplex” – restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is “full-duplex” allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of “time”. To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model’s ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240ms. Webpage: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://syncllm.cs.washington.edu/" style="color:#0000FF;" title="">https://syncllm.cs.washington.edu/</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.7">
<p class="ltx_p" id="p1.7.8"><span class="ltx_text ltx_font_bold" id="p1.7.8.1">Beyond Turn-Based Interfaces: 
<br class="ltx_break"/>Synchronous LLMs as Full-Duplex Dialogue Agents</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.7.7" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.7.7.7" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.7.7.7.7">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.3.3.3.3.3">
<span class="ltx_td ltx_align_center" id="p1.3.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="p1.3.3.3.3.3.3.3">
Bandhav Veluri<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.3.3.3.3.3.3.3.1.1">1,2</span></sup>, Benjamin N Peloquin<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.2"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.2.1">1</span></sup>, Bokai Yu<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.3"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.3.1">1</span></sup>,</span></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.5.2"><span class="ltx_text ltx_font_bold" id="p1.4.4.4.4.4.1.1">Hongyu Gong<sup class="ltx_sup" id="p1.4.4.4.4.4.1.1.1"><span class="ltx_text ltx_font_medium" id="p1.4.4.4.4.4.1.1.1.1">1</span></sup>,</span> <span class="ltx_text ltx_font_bold" id="p1.5.5.5.5.5.2.2">Shyamnath Gollakota<sup class="ltx_sup" id="p1.5.5.5.5.5.2.2.1"><span class="ltx_text ltx_font_medium" id="p1.5.5.5.5.5.2.2.1.1">2</span></sup></span></span></span>
<span class="ltx_tr" id="p1.7.7.7.7.7">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.7.2"><sup class="ltx_sup" id="p1.7.7.7.7.7.2.1">1</sup>Meta AI, <sup class="ltx_sup" id="p1.7.7.7.7.7.2.2">2</sup>University of Washington</span></span>
<span class="ltx_tr" id="p1.7.7.7.7.8.1">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.8.1.1"><span class="ltx_text ltx_font_typewriter" id="p1.7.7.7.7.8.1.1.1">{bandhav,gshyam}@cs.washington.edu  hygong@meta.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Existing spoken dialogue models are predominantly turn-based interfaces that are half-duplex in nature <cite class="ltx_cite ltx_citemacro_cite">Lakhotia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib18" title="">2021</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib37" title="">2023a</a>); Hassid et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib11" title="">2024</a>); Borsos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib3" title="">2023</a>)</cite>. To achieve a change of turn, these systems rely on either explicit user inputs or pauses at the end of a user’s utterance <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib37" title="">2023a</a>)</cite>. Human spoken dialogue, by contrast, does not rely on silence as its primary turn-taking cue <cite class="ltx_cite ltx_citemacro_cite">Levinson and Torreira (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib19" title="">2015a</a>); Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib25" title="">2022</a>)</cite>. Research indicates that in human conversations intra-turn pauses (pauses within a speaker’s turn) are usually longer than the intervals between turns across speakers <cite class="ltx_cite ltx_citemacro_cite">Heldner and Edlund (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib12" title="">2010</a>); Brady (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib4" title="">1968</a>); ten Bosch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib34" title="">2005</a>)</cite>. English speakers often begin their turns without waiting for pauses, using grammatical, prosodic, and pragmatic cues to seamlessly initiate their next turn while minimizing overlaps and gaps <cite class="ltx_cite ltx_citemacro_cite">Stivers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib33" title="">2009</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Human spoken dialogue is inherently full-duplex, allowing for seamless, bi-directional communication where both parties can simultaneously speak and listen. This mode of interaction enables immediate feedback, interruptions for clarification, and real-time adjustments in information flow <cite class="ltx_cite ltx_citemacro_cite">Reece et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib29" title="">2023</a>); Levinson and Torreira (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib20" title="">2015b</a>)</cite>.
Unlike half-duplex systems that process text or speech based on full utterances in each turn, human dialogue frequently contains verbal backchannels – short, overlapping phrases such as "yeah" or "uh-huh" – signals from the listener to the speaker that they understand and that the speaker may continue. Such synchronous dynamics allow the interaction to flow smoothly and create a rhythm absent in written text <cite class="ltx_cite ltx_citemacro_cite">Heldner and Edlund (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib12" title="">2010</a>)</cite>.
While humans learn turn-taking cues from infancy to minimize speech overlaps and silence duration <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib27" title="">2021</a>)</cite>, overlapping speech as well as long silences are common in human spoken dialogue as they enrich conversations providing additional pragmatic cues. For example, overlapping speech and frequent backchanneling often signifies engaged listening. Similarly the length of silences can vary across cultures and is influenced by the promptness of responses <cite class="ltx_cite ltx_citemacro_cite">Stivers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib33" title="">2009</a>); Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib25" title="">2022</a>)</cite>. In both cases, these dynamics make conversation sound more “human.”</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="333" id="S1.F1.g1" src="x1.png" width="656"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S1.F1.3.1">SyncLLM as a full-duplex dialogue agent.</span> At current time step (chunk N in the figure), SyncLLM’s context contains interleaved chunks of the LLM’s speech until the current chunk, and the user’s speech corresponding to all but the current chunk. To be in synchrony with the user, the LLM must generate its next chunk (chunk N+1) before the end of the current chunk. As a result, SyncLLM first generates an <em class="ltx_emph ltx_font_italic" id="S1.F1.4.2">estimated user’s chunk</em>, which is in-turn appended to the context and used to predict its next chunk.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Developing a full-duplex spoken dialog agent is challenging for four reasons: 1) Understanding and generating turn-taking cues in spoken dialogue requires the model to have a common reference clock with the real-world. However, current LLMs do not have such a sense of “time”. 2) Compared to text-based chat datasets, spoken dialogue data is limited. A combination of all significant spoken dialogue datasets <cite class="ltx_cite ltx_citemacro_cite">Cieri et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib7" title="">2004</a>); Godfrey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib9" title="">1992</a>); Reece et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib29" title="">2023</a>)</cite> would still result in only <math alttext="\sim" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><csymbol cd="latexml" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">∼</annotation></semantics></math>3k hours of spoken dialogue data. 3) Full-duplex dialogue entails model to be always listening and should always be ready to speak, because back-channels or overlaps could occur at arbitrary points in time. This requires the model to be streaming for the duration of the dialogue. 4) Since the spoken dialogue agent might run on cloud infrastructure, it must address the fundamental latency inherent in Internet transmissions. Thus, the model may not have immediate access to the current tokens or speech generated by the user and must operate with delayed input (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we make multiple contributions to develop a full-duplex dialogue agent:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce Synchronous LLMs, in short SyncLLM, for full-duplex spoken dialogue. SyncLLM achieves synchrony modeling by
integrating time information into LLMs so that they can run synchronously with the real-world clock. We generate a periodic synchronization token to provide a common time frame for both sides of the dialogue. This however requires us to address duplicate tokens, caused by silence within and across utterances. Duplicate tokens can adversely affect the semantic capability of spoken dialogue model <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib25" title="">2022</a>)</cite>. Instead, we train our model to predict deduplicated token sequences, with timing information maintained by our periodic synchronization tokens.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Human voice interactions rely on the ability to model the other person’s response on the short-term. We can take turns with gaps as small as 200ms, while language generation latency is around 600ms <cite class="ltx_cite ltx_citemacro_cite">Levinson and Torreira (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib20" title="">2015b</a>)</cite>. This implies we anticipate the next few words of what the other person would say and respond appropriately. We use this insight to predict speech units for both speakers, into the future, in chunk sizes of 160-240 ms. This ensures resiliency to Internet latencies of up to 240 ms.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We propose a three-stage training recipe that leverages synthetic spoken dialogue generated from text dialogue data to mitigate the limited availability of real-world spoken dialogue data. Specifically, we use 212k hours of synthetic spoken dialogue data and just 2k hours of real-world spoken dialogue data to develop a model that generates meaningful spoken dialogue with naturalistic turn-taking, overlaps, and backchannels.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">With an experimental setup based on Llama3-8b <cite class="ltx_cite ltx_citemacro_cite">at Meta (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib2" title="">2024</a>)</cite> and extensive user-study (n=32), we show that our method achieves +2.2-point Mean Opinion Score (MOS) improvement in dialogue content <span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.1">Meaningfulness</span> over state-of-the-art full-duplex voice model dGSLM <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib25" title="">2022</a>)</cite>, while maintaining turn-taking <span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.2">Naturalness</span>. Further, our results show that our model fine-tuned on the Fisher training set <cite class="ltx_cite ltx_citemacro_cite">Cieri et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib7" title="">2004</a>)</cite> can generalize to the out-of-distribution Candor testset <cite class="ltx_cite ltx_citemacro_cite">Reece et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib29" title="">2023</a>)</cite>, while preserving both dialog content meaningfulness and naturalness.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">Finally, by simulating full-duplex dialogue between two finetuned Llama3-8b models, we show how this approach can enable latency-tolerant and streaming full-duplex voice interfaces. Further, SyncLLM can perform a coherent conversation even when the user’s side of the conversation is generated by a model trained with a different dataset.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Multimodal language models</span>. The success of text language models like GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib28" title="">2023</a>)</cite>, <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.2">Llama</span> <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib35" title="">2023</a>)</cite>, and Mistral <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib15" title="">2023</a>)</cite> has inspired explorations into multimodal models. Here, we focus our discussion on speech and text modalities. Initialization from a pretrained text LLM has been shown to benefit multimodal training <cite class="ltx_cite ltx_citemacro_cite">Hassid et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib10" title="">2023</a>)</cite>. Recent works have proposed extending the vocabulary of text LLMs with discrete speech tokens to enable the model to handle speech inputs and outputs <cite class="ltx_cite ltx_citemacro_cite">Rubenstein et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib30" title="">2023</a>)</cite>. Models are trained with cross-modal knowledge from aligned speech-text data, including tasks like automatic speech recognition (ASR), text-to-speech synthesis (TTS), speech-to-text (S2T), and speech-to-speech translation (S2ST). Multitask learning with these tasks has been adopted by VioLA <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib36" title="">2023</a>)</cite>, AudioPaLM <cite class="ltx_cite ltx_citemacro_cite">Rubenstein et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib30" title="">2023</a>)</cite>, VoxtLM <cite class="ltx_cite ltx_citemacro_cite">Maiti et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib22" title="">2023</a>)</cite>, and SUTLM <cite class="ltx_cite ltx_citemacro_cite">Chou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib6" title="">2023</a>)</cite>. SpiRit-LM <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib26" title="">2024</a>)</cite> interleaves speech and text tokens and trains the model with next token prediction, demonstrating both speech understanding and generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Spoken dialogue models</span>. Prior work on spoken dialogue research covers various topics such as dialogue state tracking <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib38" title="">2023b</a>)</cite>, turn-taking prediction <cite class="ltx_cite ltx_citemacro_cite">Skantze (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib32" title="">2021</a>); Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib21" title="">2022</a>)</cite>, and response generation <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib39" title="">2020</a>)</cite>. Recent works leverage LLMs in dialogue systems <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib40" title="">2020</a>)</cite>.
Initialized from <span class="ltx_text ltx_font_smallcaps" id="S2.p2.1.2">Llama</span>, SpeechGPT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib37" title="">2023a</a>)</cite> is finetuned sequentially on speech-only data and multimodal instruction sets to perform spoken question answering (QA) tasks. USDM <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib17" title="">2024</a>)</cite> continues pretraining Mistral with interleaved speech-text data to capture multimodal semantics. For dialogue finetuning, it constructs templates using both speech and transcripts of user input as instruction data. Unlike models that use speech tokens, Spectron <cite class="ltx_cite ltx_citemacro_cite">Nachmani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib23" title="">2023</a>)</cite> directly manipulates spectrograms for tasks such as spoken QA and speech continuation. However, these prior works are limited to the turn-taking setting, where the dialogue model is explicitly prompted to speak in its own turn. Human spoken dialogue is more complex, involving implicit turn-taking cues and overlapping speech, such as interruptions and backchanneling <cite class="ltx_cite ltx_citemacro_cite">Schegloff (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib31" title="">2000</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The closest work to ours is dGSLM <cite class="ltx_cite ltx_citemacro_cite">Lakhotia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib18" title="">2021</a>)</cite>, which models simultaneous dialogue using a dual-tower Transformer that attends to two channels.
It demonstrates superior performance than cascaded architecture which consists of automatic speech recognition (ASR), text LLM and text-to-speech (TTS).
One weakness of dGSLM is its reliance on speech-only training, which does not fully utilize textual knowledge. In contrast, our work leverages the generative intelligence of language models, equipping them with multimodal and synchronous capabilities. Moreover, in its empirical study, dGSLM does not consider delays in real-life scenarios and assumes that the hidden states of one interlocutor are immediately accessible to the other. In contrast, we explicitly discuss how our model handles delayed responses in spoken dialogue.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>SyncLLM</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">SyncLLM is an auto-regressive transformer decoder architecture, that natively models discrete speech units in a wall-clock synchronous fashion. SyncLLM is trained to predict interleaving chunks of speech units corresponding to both sides of the dialogue as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">1</span></a>. In each time step, the model predicts speech units corresponding to a fixed duration, referred to as the model’s <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">chunk size</em>, for its side of the dialogue followed by speech units corresponding to user’s side of the dialogue. With this approach, the model is capable of generating two streams of speech synchronized with a real-world clock. This allows our method to model all conversational cues such as backchannels, overlaps, interruptions etc. Furthermore, since we use the same architecture as existing LLMs, our approach can leverage large scale pre-training of LLMs.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The model trained to predict interleaved chunks of token sequences can be used for full-duplex voice interaction if we could replace one of the two token streams, with that corresponding to the real-world user. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">1</span></a>, purple boxes correspond to token sequences of the LLM’s side of the conversation in each time chunk and the green boxes correspond to the user’s side of the dialogue. We achieve full-duplex LLM-user voice interaction by discarding the LLM’s predictions of user’s response and replace it with the user’s speech.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="220" id="S3.F2.g1" src="x2.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>SyncLLM’s token sequence format visualized with a chunk size of 160 ms. (Top row) We represent spoken dialogue as interleaved chunks of HuBERT tokens, where the chunk size determines the frequency of the synchronization token [S0]. (Middle row) We train SyncLLM to generate interleaved chunks of deduplicated HuBERT tokens along with periodic synchronization tokens. (Bottom row) We interpolate deduplicated tokens in each chunk to obtain spoken dialogue sequence in the original format.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Latency tolerant interaction</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">1</span></a>, consider the Nth time chunk to be current time step. We could interleave the LLM’s output speech chunks until the Nth chunk, with the user’s input chunks corresponding to only N-1 chunks. The reasoning here is that the user’s input for the Nth chunk is not available until the end of Nth time step. To handle this intrinsic latency, similar to the way humans anticipate the next few words of what the other person taking part in the dialogue would say <cite class="ltx_cite ltx_citemacro_cite">Levinson and Torreira (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib20" title="">2015b</a>)</cite>, the LLM’s output for the next chunk (N+1) is computed by first estimating the user’s response for the Nth time chunk (depicted in the figure with green boxes with dotted border). We then append this estimated chunk to the LLM’s context to generate the LLM’s next chunk (N+1). For generating subsequent chunks (N+2, N+3, …), we discard the estimated user’s chunk for Nth time step and replace that with the user’s real-world input, thus grounding the subsequent interaction with actual input from the user.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Token sequence format</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Following prior works in spoken language modeling <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib25" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib26" title="">2024</a>)</cite>, we use HuBERT <cite class="ltx_cite ltx_citemacro_cite">Hsu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib13" title="">2021</a>)</cite> to represent speech. We use the tokenization parameters from <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib26" title="">2024</a>)</cite>, with a token sampling rate of 25 Hz – resulting in one token for every 40 ms of audio – and a vocabulary size of 501. To model dialog between two speakers 0 &amp; 1, we define two special tokens <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p1.1.1">[S0]</code> and <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p1.1.2">[S1]</code>, referred to as speaker tags, specifying the start of each speaker’s token sequence, respectively. We represent dialogue as two parallel speech streams, one for each speaker, interleaved, as shown in the top row of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.F2" title="Figure 2 ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a>. For each stream, we embed a periodic speaker tag, with the time period equal to chunk size of the model.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="475" id="S3.F3.g1" src="x3.png" width="705"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Tokens required for representing a second of speech with/without deduplication. Histogram computed over 15 hr of dialog data in the Fisher dataset <cite class="ltx_cite ltx_citemacro_cite">Cieri et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib7" title="">2004</a>)</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Deduplication.</span> The fixed time period of HuBERT tokens is useful for modeling time in the full-duplex dialogue. However, raw HuBERT sequences consist of significant repeated tokens, mainly caused by silence within and across utterances. The number of repetitions of each unique token denote the duration of the acoustic unit represented by the token. The semantic content, however, can be modeled by only considering unique tokens while deduplicating the token sequence <cite class="ltx_cite ltx_citemacro_cite">Kharitonov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib16" title="">2022</a>); Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib25" title="">2022</a>)</cite>. Duplicate token sequences can adversely affect the semantic capability of the final spoken dialogue model <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib25" title="">2022</a>)</cite>, because as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.F3" title="Figure 3 ‣ 3.2 Token sequence format ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">3</span></a>, they contain <math alttext="\sim 50\%" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml"></mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">∼</mo><mrow id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mn id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">50</mn><mo id="S3.SS2.p2.1.m1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">absent</csymbol><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.1">percent</csymbol><cn id="S3.SS2.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.3.2">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\sim 50\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">∼ 50 %</annotation></semantics></math> lower semantic content per token compared to deduplicated sequences.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">So, instead, SyncLLM is trained to predict deduplicated HuBERT sequences, with coarse timing information maintained by periodically interleaved special tokens, <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p3.1.1">[S0]</code> and <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p3.1.2">[S1]</code>, as in the second row of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.F2" title="Figure 2 ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a>. In the first chunk of the example in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.F2" title="Figure 2 ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a>, the two speaker streams contained 4 repetitions of <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p3.1.3">[75]</code> and <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p3.1.4">[89]</code>, respectively. After deduplication, the interleaved token sequence corresponding to the first chunk would be <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p3.1.5">[S0][75][S1][89]</code>. In the second chunk, speaker 0 has 2 new tokens (<code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p3.1.6">[17]</code> &amp; <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p3.1.7">[338]</code>), but speaker 1 tokens are just a repetition of the last token in the previous chunk, <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p3.1.8">[89]</code>. So, the second chunk’s token sequence would just be <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p3.1.9">[S0][17][338]</code>. Note that when a chunk contains no novel tokens corresponding to speaker 1, we exclude speaker 1’s special token <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p3.1.10">[S1]</code> as well. However, this is not the case for speaker 0, as we need one of the speaker’s special token to be present in all chunks to unambiguously distinguish chunks. This is shown in the third chunk of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.F2" title="Figure 2 ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Interpolation.</span> While deduplicated token sequences are beneficial for auto-regressive modeling, to generate token sequences suitable for speech synthesis, we need periodic HuBERT tokens in the original format. Since the speaker tag <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.p4.1.2">[S0]</code> maintains the timing information, we know the number of tokens removed after deduplication within each chunk. We use this to interpolate the deduplicated token to match the expected number of token in each chunk. For example, in the first chunk of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.F2" title="Figure 2 ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a>, speaker 0’s stream only has one token after deduplication. But since chunk size in that case is 160ms, each chunk would contain 160/40 = 4 tokens. So as shown in the third row of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.F2" title="Figure 2 ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a>, we repeat the deduplicated token thrice to reconstruct the chunk. If a chunk has multiple deduplicated tokens, like the second in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.F2" title="Figure 2 ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a>, we repeat each token by an equal amount. We note this approach could result in an error because the original chunk may not follow this heuristic. We observed that the effect of this is imperceptible even with a chunk size of 240 ms, likely because the error in the predicted duration of each token is upper bounded by the chunk size. Further, in chunks with more novel tokens, the error would be even smaller.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Training</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We use Llama3-8b <cite class="ltx_cite ltx_citemacro_cite">at Meta (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib2" title="">2024</a>)</cite> as our base model and employ a three stage training procedure that uses synthetic spoken dialogue data predominantly and relatively small amount of real-world spoken dialogue data to develop a full-duplex voice agent.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Data used for training in different stages. We convert text based data to speech using TTS.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Stage</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">Speech</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.2.2.1"></th>
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.2.2.2"></th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.2.3.1">modality</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.2.4.1">(hrs)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.3.3.1">Supervised</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.3.3.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.3.3">Text</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.3.4">193k</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.4.1">finetuning (SFT)</th>
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.4.4.2"></th>
<td class="ltx_td" id="S4.T1.1.4.4.3"></td>
<td class="ltx_td" id="S4.T1.1.4.4.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.5.1">Dialogue</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.5.5.2">2</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.3">Text</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.4">20k</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T1.1.6.6.1">Spoken dialogue</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T1.1.6.6.2">3</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.6.6.3">Speech</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.6.6.4">1927</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Stage 1: Turn-based spoken dialogue model with synthetic speech data.</span>
Given the limited spoken dialogue data, we generate synthetic speech data from large-scale text dialogue datasets.
We use supervised finetuning (SFT) datasets, as our source text-dialogue datasets. We used Bark TTS <cite class="ltx_cite ltx_citemacro_cite">AI (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib1" title="">2023</a>)</cite> model to generate spoken versions of text-dialogue datasets, with its 10 speaker presets.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Since Llama3-8b is a text-only LLM, in the first stage, we aim to achieve text-speech alignment in the context of dialogues. Given a spoken question, we train the model to generate a spoken response. We expand the vocabulary of Llama3 to include 501 HuBERT tokens, in addition to the speaker tags, <code class="ltx_verbatim ltx_font_typewriter" id="S4.p3.1.1">[S0]</code> and <code class="ltx_verbatim ltx_font_typewriter" id="S4.p3.1.2">[S1]</code>. A turn-based dialog could be defined as made of turns, which in turn are made of sentences. We finetuned Llama3 with dialog sequences in the following format:</p>
</div>
<div class="ltx_para" id="S4.p4">
<pre class="ltx_verbatim ltx_font_typewriter" id="S4.p4.1">
[S1]&lt;sent0&gt;[S0]&lt;sent0&gt;&lt;sent1&gt;[S1]..
</pre>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="372" id="S4.F4.g1" src="x4.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>We sample speech percentages from truncated normal distribution, so we obtain samples with all possible combinations of text-speech interleaving throughout the training process, with a bias for higher speech percentages as the training progresses. This resulted in stable training when starting out with a text-only LLM.</figcaption>
</figure>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Each sentence is randomly chosen to either be text or deduplicated speech token sequences during training. For each training sample, we sample the percentage of speech sentences in the training sequence from the truncated normal distribution (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S4.F4" title="Figure 4 ‣ 4 Training ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">4</span></a>). Training only with fully speech sequences or step-wise increment of speech percentage resulted in unstable training. Sentence level text-speech interleaving not only trains the model to be capable of performing dialog, but also achieves text/speech alignment in the context of dialog.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.p6.1.1">Stage 2: Full-duplex dialogue assuming no overlaps.</span> Turn-based spoken dialogue is special case of full-duplex dialogue with no overlaps. Based on this observation, we could treat synthetic spoken dialogue data as full-duplex spoken dialogue data where during one speaker’s turn, other speaker is completely silent. In this stage, we create synthetic spoken dialogue data from text-dialogue data similarly to the previous stage with one main difference: From each turn in the dialogue, we generate a speech utterance corresponding to one speaker and silence of equal duration corresponding to the other speaker. We then tokenize the parallel speech dialog data in the format shown in the second row of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.F2" title="Figure 2 ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a>. This way, we can further leverage text-dialogue data for help our model learn the token sequence format in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.F2" title="Figure 2 ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a>. This stage of finetuning models timing within an utterance. The model cannot learn turn-taking cues such as back-channeling or overlaps between two speakers yet.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">For the the previous stage, most samples in SFT datasetswould contain one speaker (user of the LLM) taking a short turn and the other speaker (the LLM) giving a long response. Spoken dialogues however contain more frequent turn-taking taking with short utterances. Therefore for this stage, we use text-dialogue datasets comprising of shorter turns, equivalent to around <math alttext="20" class="ltx_Math" display="inline" id="S4.p7.1.m1.1"><semantics id="S4.p7.1.m1.1a"><mn id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><cn id="S4.p7.1.m1.1.1.cmml" type="integer" xref="S4.p7.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="S4.p7.1.m1.1d">20</annotation></semantics></math>k hrs of synthetic spoken dialogue.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of Pearson correlation of turn-taking event durations between generations and ground-truth continuations, given same set of prompts. SyncLLM’s chunk sizes are shown in parenthesis.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Fisher (in-distribution)</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S4.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1">Candor (out-of-distribution)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.1.2.2.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.2.1">ipu</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.3.1">pause</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.4.1">fto</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.5.1">Average</span></td>
<td class="ltx_td" id="S4.T2.1.2.2.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.7.1">ipu</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.8"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.8.1">pause</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.9"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.9.1">fto</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.10"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.10.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.3.3.1">dGSLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.2">0.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.3">0.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.4">0.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.5">0.33</td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.3.3.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.7">0.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.8">0.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.9">0.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.10">0.14</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.4.1">SyncLLM-F (160 ms)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.2">0.60</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.3">0.50</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.4">0.20</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.5">0.43</td>
<td class="ltx_td" id="S4.T2.1.4.4.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.7">0.45</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.8">0.09</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.9">0.14</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.10">0.23</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.5.1">SyncLLM-F (200 ms)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.2">0.60</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.3">0.49</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.4">0.19</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.5">0.43</td>
<td class="ltx_td" id="S4.T2.1.5.5.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.7">0.44</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.8">0.28</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.9">0.14</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.10">0.29</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.6.1">SyncLLM-F (240 ms)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.2">0.58</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.3">0.40</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.4">0.25</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.5">0.41</td>
<td class="ltx_td" id="S4.T2.1.6.6.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.7">0.45</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.8">0.27</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.9">0.21</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.10">0.31</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.7.7.1">Prompt</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.7.2">0.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.7.3">0.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.7.4">0.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.7.5">0.52</td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.7.7.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.7.7">0.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.7.8">0.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.7.9">0.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.7.7.10">0.32</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.1.8.8.1">Resynth-GT</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.8.8.2">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.8.8.3">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.8.8.4">0.53</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.8.8.5">0.79</td>
<td class="ltx_td ltx_border_b" id="S4.T2.1.8.8.6"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.8.8.7">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.8.8.8">0.86</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.8.8.9">0.37</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.8.8.10">0.71</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p8">
<p class="ltx_p" id="S4.p8.1"><span class="ltx_text ltx_font_bold" id="S4.p8.1.1">Stage 3: Modeling with real-world spoken dialogue data.</span> Finally, we finetune the model to learn turn-taking cues from real-world spoken dialogue data. We use the Fisher <cite class="ltx_cite ltx_citemacro_cite">Cieri et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib7" title="">2004</a>)</cite> dataset with 2000 hours of spoken dialogues, where each speaker’s speech in a dialogue is separated into independent audio channels. We split the dataset into train, val and test split with 98:1:1 ratio, respectively. Each audio channel in the dialogue is separately tokenized and interleaved in the full-duplex dialogue format used in the previous stage. In this stage in addition to learning timing within utterances, the model learns effective turn-taking, conversational cues like accurate distribution of pauses between turn and backchanneling.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="439" id="S5.F5.g1" src="x5.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Perplexity of transcriptions of spoken dialogues generated by different models. Perplexity is measured with respect to a text dialogue model’s predictions.</figcaption>
</figure>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We evaluate SyncLLM in both continuation and interaction settings. In the continuation setting, given a spoken dialogue prompt, the model generates both sides of the dialogue. For interaction setting, we simulate interaction between two instances of SyncLLM as described in §<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S3.SS1" title="3.1 Latency tolerant interaction ‣ 3 SyncLLM ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">3.1</span></a>. We denote SyncLLM trained on Fisher in continuation setting as SyncLLM-F and use dGSLM as the continuation setting baseline. Both dGSLM and SyncLLM-F use Fisher as the only real-world spoken dialogue dataset for training. We denote SyncLLM trained on Fisher interacting with an instance trained on Fisher as SyncLLM-F-F, and SyncLLM trained on Fisher interacting with an instance trained on CANDOR <cite class="ltx_cite ltx_citemacro_cite">Reece et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib29" title="">2023</a>)</cite> as SyncLLM-F-C.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Semantic evaluation</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We evaluate the semantics of SyncLLM in the text domain by converting spoken generations to text using ASR. We transcribe the generated spoken dialogues into turn-based text dialogues ignoring any overlapping speech. We then compute perplexity of transcribed dialogues generated with 10 second spoken dialogue prompts, with respect a text-only dialogue model. To account for outliers (samples with abnormally high perplexities), we consider median perplexity over the testset.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.2">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.F5" title="Figure 5 ‣ 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">5</span></a> compares the semantic quality of spoken dialogues generated by SyncLLM with different chunk sizes to the prior state-of-the-art full-duplex dGSLM model <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib25" title="">2022</a>)</cite> and ground-truth continuations. We find that dGSLM has a perplexity drop of <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mo id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">∼</annotation></semantics></math>70 relative to the ground-truth, while SyncLLM only has a drop of <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mo id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><csymbol cd="latexml" id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">∼</annotation></semantics></math>15. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.F6" title="Figure 6 ‣ 5.1 Semantic evaluation ‣ 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">6</span></a> also compares median perplexities measured with prompts sampled from Fisher and Candor test splits separately, with all models trained only on Fisher training split. Here, Candor test split is an out-of-distribution testset.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="439" id="S5.F6.g1" src="x6.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>In-distribution and out-of-distribution testing.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">These evaluations show that our approach of using the standard auto-regressive architecture, thus leveraging vast text pre-training, results in much more semantically coherent spoken dialogue model, compared to a custom architecture proposed for speech-only training. Furthermore, our three-stage training approach leveraging large amount of synthetic spoken dialogue data generated from text dialogues, allows us to converge much faster on limited real-world dual-channel spoken dialogue data. This results in a general model that has superior out-of-distribution (ood) performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Naturalness evaluation</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Appropriate timing of pauses, speaker transitions and overlaps are integral part of spoken-dialogue which convey essential information required for natural spoken conversation. To evaluate these aspect of our generated spoken dialogues, we consider the turn-taking events proposed in <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib25" title="">2022</a>)</cite> that evaluate overall naturalness of generated spoken dialogues: inter-pausal units (IPUs), pauses, and floor-transfer offset (FTO). FTO is the duration of between turn-transitions, which is a combination of overlaps and gaps – negative FTOs represent overlaps and positive FTOs represent gaps.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Similar to dGSLM’s setup, we use 30s prompts sampled from the test splits and generate 90s dialogues with different model configurations. We then compute pair-wise correlation of turn-taking event durations between the dialogue generations and ground-truth continuations, given the same prompt. We first compute voice activities of each side of dialogue (generated in separate audio channels) using the <code class="ltx_verbatim ltx_font_typewriter" id="S5.SS2.p2.1.1">pyannote.audio</code> library <cite class="ltx_cite ltx_citemacro_cite">Bredin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib5" title="">2020</a>)</cite>. We then measure the start and end timestamps for each turn-taking event. We measure the average duration of the turn-taking events in generated dialogues and then compute the Pearson correlation between the average durations observed in generations of different models and those in the ground-truth.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Table. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S4.T2" title="Table 2 ‣ 4 Training ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a> compares this correlation with in-distribution Fisher <cite class="ltx_cite ltx_citemacro_cite">Cieri et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib7" title="">2004</a>)</cite> test-split and out-of-distribution Candor test-split. We observe that, generations with our models achieve better turn-taking event correlation with ground-truth continuations compared to dGSLM for both in-distribution and out-of-distribution testsets. In addition to this, we provide turn-taking event correlation with prompts and re-synthesized ground-truth continuations (Resynth-GT). Resynth-GT is obtained by re-synthesizing the tokenized ground-truth continuation. Resynth-GT does not perfectly correlate with ground-truth owing to variance in timing introduced by the tokenization process, and serves as a topline for our method.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Human Evaluation</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We conduct an evaluation study with 32 annotators recruited via a third party vendor with the requirement that they had native-level English proficiency.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Meaningfulness (Meaning.) and Naturalness (Nat.) (scores 1-5) mean estimates and standard errors (in parentheses), aggregated overall and for Fisher and CANDOR subsets. We use a 160ms chunk size for this study.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.6.7.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T3.6.7.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T3.6.7.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.6.7.1.2.1">Overall</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T3.6.7.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.6.7.1.3.1">Fisher</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T3.6.7.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.6.7.1.4.1">CANDOR</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S5.T3.6.6.7"><span class="ltx_text ltx_font_bold" id="S5.T3.6.6.7.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1">Meaning. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.2.2.2">Nat. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.2.2.2.m1.1"><semantics id="S5.T3.2.2.2.m1.1a"><mo id="S5.T3.2.2.2.m1.1.1" stretchy="false" xref="S5.T3.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.3.3">Meaning. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.3.3.3.m1.1"><semantics id="S5.T3.3.3.3.m1.1a"><mo id="S5.T3.3.3.3.m1.1.1" stretchy="false" xref="S5.T3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.m1.1b"><ci id="S5.T3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4">Nat. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.4.4.4.m1.1"><semantics id="S5.T3.4.4.4.m1.1a"><mo id="S5.T3.4.4.4.m1.1.1" stretchy="false" xref="S5.T3.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.m1.1b"><ci id="S5.T3.4.4.4.m1.1.1.cmml" xref="S5.T3.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.5.5.5">Meaning. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.5.5.5.m1.1"><semantics id="S5.T3.5.5.5.m1.1a"><mo id="S5.T3.5.5.5.m1.1.1" stretchy="false" xref="S5.T3.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.m1.1b"><ci id="S5.T3.5.5.5.m1.1.1.cmml" xref="S5.T3.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.5.5.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.6.6.6">Nat. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.6.6.6.m1.1"><semantics id="S5.T3.6.6.6.m1.1a"><mo id="S5.T3.6.6.6.m1.1.1" stretchy="false" xref="S5.T3.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.m1.1b"><ci id="S5.T3.6.6.6.m1.1.1.cmml" xref="S5.T3.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.6.6.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.6.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.6.8.1.1">dGSLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.8.1.2">1.55 (0.06)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.8.1.3">3.95 (0.08)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.8.1.4">1.67 (0.09)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.8.1.5">4.21 (0.08)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.8.1.6">1.43 (0.08)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.8.1.7">3.70 (0.12)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.9.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.6.9.2.1">SyncLLM-C</th>
<td class="ltx_td ltx_align_center" id="S5.T3.6.9.2.2">3.40 (0.07)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.9.2.3">3.96 (0.06)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.9.2.4">3.14 (0.10)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.9.2.5">3.97 (0.08)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.9.2.6">3.66 (0.08)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.9.2.7">3.94 (0.08)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.10.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.6.10.3.1">SyncLLM-F</th>
<td class="ltx_td ltx_align_center" id="S5.T3.6.10.3.2">3.74 (0.06)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.10.3.3">3.90 (0.06)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.10.3.4">3.82 (0.08)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.10.3.5">3.98 (0.08)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.10.3.6">3.67 (0.09)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.10.3.7">3.82 (0.10)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.11.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.6.11.4.1">Re-synth</th>
<td class="ltx_td ltx_align_center" id="S5.T3.6.11.4.2">3.87 (0.06)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.11.4.3">4.03 (0.05)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.11.4.4">4.04 (0.08)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.11.4.5">4.14 (0.08)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.11.4.6">3.69 (0.07)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.11.4.7">3.91 (0.06)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.12.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T3.6.12.5.1">GT</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.6.12.5.2">4.96 (0.02)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.6.12.5.3">4.96 (0.02)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.6.12.5.4">4.96 (0.03)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.6.12.5.5">4.94 (0.04)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.6.12.5.6">4.97 (0.02)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.6.12.5.7">4.98 (0.02)</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">We adapt the Mean Opinion Score (MOS) protocol (a 5-pt Likert scale) <cite class="ltx_cite ltx_citemacro_cite">ITU-T Recommendation P.808 (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib14" title="">2018</a>)</cite> to evaluate <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">Naturalness</span> (N-MOS) of turn-taking and <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">Meaningfulness</span> (M-MOS) of dialogue content. For both N-MOS and M-MOS, annotators are presented with the prompt- and continuation-audio. Annotators are instructed to first read the descriptions of N-MOS and M-MOS, listen to the prompt audio, then listen to the continuation audio. Finally, they are asked to provide a rating considering the quality of the continuation audio relative to the information contained in the prompt. Each annotator assigned to a given prompt / continuation pair provides a rating for both N-MOS and M-MOS (see §<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A2.SS1" title="B.1 N-MOS &amp; M-MOS ‣ Appendix B Naturalness-MOS Instructions ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">B.1</span></a>).</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.4">In total, <math alttext="n_{annot}=32" class="ltx_Math" display="inline" id="S5.SS3.p3.1.m1.1"><semantics id="S5.SS3.p3.1.m1.1a"><mrow id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml"><msub id="S5.SS3.p3.1.m1.1.1.2" xref="S5.SS3.p3.1.m1.1.1.2.cmml"><mi id="S5.SS3.p3.1.m1.1.1.2.2" xref="S5.SS3.p3.1.m1.1.1.2.2.cmml">n</mi><mrow id="S5.SS3.p3.1.m1.1.1.2.3" xref="S5.SS3.p3.1.m1.1.1.2.3.cmml"><mi id="S5.SS3.p3.1.m1.1.1.2.3.2" xref="S5.SS3.p3.1.m1.1.1.2.3.2.cmml">a</mi><mo id="S5.SS3.p3.1.m1.1.1.2.3.1" xref="S5.SS3.p3.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.1.m1.1.1.2.3.3" xref="S5.SS3.p3.1.m1.1.1.2.3.3.cmml">n</mi><mo id="S5.SS3.p3.1.m1.1.1.2.3.1a" xref="S5.SS3.p3.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.1.m1.1.1.2.3.4" xref="S5.SS3.p3.1.m1.1.1.2.3.4.cmml">n</mi><mo id="S5.SS3.p3.1.m1.1.1.2.3.1b" xref="S5.SS3.p3.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.1.m1.1.1.2.3.5" xref="S5.SS3.p3.1.m1.1.1.2.3.5.cmml">o</mi><mo id="S5.SS3.p3.1.m1.1.1.2.3.1c" xref="S5.SS3.p3.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.1.m1.1.1.2.3.6" xref="S5.SS3.p3.1.m1.1.1.2.3.6.cmml">t</mi></mrow></msub><mo id="S5.SS3.p3.1.m1.1.1.1" xref="S5.SS3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS3.p3.1.m1.1.1.3" xref="S5.SS3.p3.1.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><apply id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1"><eq id="S5.SS3.p3.1.m1.1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1.1"></eq><apply id="S5.SS3.p3.1.m1.1.1.2.cmml" xref="S5.SS3.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p3.1.m1.1.1.2.1.cmml" xref="S5.SS3.p3.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS3.p3.1.m1.1.1.2.2.cmml" xref="S5.SS3.p3.1.m1.1.1.2.2">𝑛</ci><apply id="S5.SS3.p3.1.m1.1.1.2.3.cmml" xref="S5.SS3.p3.1.m1.1.1.2.3"><times id="S5.SS3.p3.1.m1.1.1.2.3.1.cmml" xref="S5.SS3.p3.1.m1.1.1.2.3.1"></times><ci id="S5.SS3.p3.1.m1.1.1.2.3.2.cmml" xref="S5.SS3.p3.1.m1.1.1.2.3.2">𝑎</ci><ci id="S5.SS3.p3.1.m1.1.1.2.3.3.cmml" xref="S5.SS3.p3.1.m1.1.1.2.3.3">𝑛</ci><ci id="S5.SS3.p3.1.m1.1.1.2.3.4.cmml" xref="S5.SS3.p3.1.m1.1.1.2.3.4">𝑛</ci><ci id="S5.SS3.p3.1.m1.1.1.2.3.5.cmml" xref="S5.SS3.p3.1.m1.1.1.2.3.5">𝑜</ci><ci id="S5.SS3.p3.1.m1.1.1.2.3.6.cmml" xref="S5.SS3.p3.1.m1.1.1.2.3.6">𝑡</ci></apply></apply><cn id="S5.SS3.p3.1.m1.1.1.3.cmml" type="integer" xref="S5.SS3.p3.1.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">n_{annot}=32</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.1.m1.1d">italic_n start_POSTSUBSCRIPT italic_a italic_n italic_n italic_o italic_t end_POSTSUBSCRIPT = 32</annotation></semantics></math> annotators provided ratings for <math alttext="n_{items}=180" class="ltx_Math" display="inline" id="S5.SS3.p3.2.m2.1"><semantics id="S5.SS3.p3.2.m2.1a"><mrow id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml"><msub id="S5.SS3.p3.2.m2.1.1.2" xref="S5.SS3.p3.2.m2.1.1.2.cmml"><mi id="S5.SS3.p3.2.m2.1.1.2.2" xref="S5.SS3.p3.2.m2.1.1.2.2.cmml">n</mi><mrow id="S5.SS3.p3.2.m2.1.1.2.3" xref="S5.SS3.p3.2.m2.1.1.2.3.cmml"><mi id="S5.SS3.p3.2.m2.1.1.2.3.2" xref="S5.SS3.p3.2.m2.1.1.2.3.2.cmml">i</mi><mo id="S5.SS3.p3.2.m2.1.1.2.3.1" xref="S5.SS3.p3.2.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.2.m2.1.1.2.3.3" xref="S5.SS3.p3.2.m2.1.1.2.3.3.cmml">t</mi><mo id="S5.SS3.p3.2.m2.1.1.2.3.1a" xref="S5.SS3.p3.2.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.2.m2.1.1.2.3.4" xref="S5.SS3.p3.2.m2.1.1.2.3.4.cmml">e</mi><mo id="S5.SS3.p3.2.m2.1.1.2.3.1b" xref="S5.SS3.p3.2.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.2.m2.1.1.2.3.5" xref="S5.SS3.p3.2.m2.1.1.2.3.5.cmml">m</mi><mo id="S5.SS3.p3.2.m2.1.1.2.3.1c" xref="S5.SS3.p3.2.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.2.m2.1.1.2.3.6" xref="S5.SS3.p3.2.m2.1.1.2.3.6.cmml">s</mi></mrow></msub><mo id="S5.SS3.p3.2.m2.1.1.1" xref="S5.SS3.p3.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS3.p3.2.m2.1.1.3" xref="S5.SS3.p3.2.m2.1.1.3.cmml">180</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><apply id="S5.SS3.p3.2.m2.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1"><eq id="S5.SS3.p3.2.m2.1.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1.1"></eq><apply id="S5.SS3.p3.2.m2.1.1.2.cmml" xref="S5.SS3.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p3.2.m2.1.1.2.1.cmml" xref="S5.SS3.p3.2.m2.1.1.2">subscript</csymbol><ci id="S5.SS3.p3.2.m2.1.1.2.2.cmml" xref="S5.SS3.p3.2.m2.1.1.2.2">𝑛</ci><apply id="S5.SS3.p3.2.m2.1.1.2.3.cmml" xref="S5.SS3.p3.2.m2.1.1.2.3"><times id="S5.SS3.p3.2.m2.1.1.2.3.1.cmml" xref="S5.SS3.p3.2.m2.1.1.2.3.1"></times><ci id="S5.SS3.p3.2.m2.1.1.2.3.2.cmml" xref="S5.SS3.p3.2.m2.1.1.2.3.2">𝑖</ci><ci id="S5.SS3.p3.2.m2.1.1.2.3.3.cmml" xref="S5.SS3.p3.2.m2.1.1.2.3.3">𝑡</ci><ci id="S5.SS3.p3.2.m2.1.1.2.3.4.cmml" xref="S5.SS3.p3.2.m2.1.1.2.3.4">𝑒</ci><ci id="S5.SS3.p3.2.m2.1.1.2.3.5.cmml" xref="S5.SS3.p3.2.m2.1.1.2.3.5">𝑚</ci><ci id="S5.SS3.p3.2.m2.1.1.2.3.6.cmml" xref="S5.SS3.p3.2.m2.1.1.2.3.6">𝑠</ci></apply></apply><cn id="S5.SS3.p3.2.m2.1.1.3.cmml" type="integer" xref="S5.SS3.p3.2.m2.1.1.3">180</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">n_{items}=180</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.2.m2.1d">italic_n start_POSTSUBSCRIPT italic_i italic_t italic_e italic_m italic_s end_POSTSUBSCRIPT = 180</annotation></semantics></math> items divided evenly between the CANDOR and Fisher datasets. Each sample received a rating from <math alttext="1-Bad,...,5-Excellent" class="ltx_Math" display="inline" id="S5.SS3.p3.3.m3.3"><semantics id="S5.SS3.p3.3.m3.3a"><mrow id="S5.SS3.p3.3.m3.3.3.2" xref="S5.SS3.p3.3.m3.3.3.3.cmml"><mrow id="S5.SS3.p3.3.m3.2.2.1.1" xref="S5.SS3.p3.3.m3.2.2.1.1.cmml"><mn id="S5.SS3.p3.3.m3.2.2.1.1.2" xref="S5.SS3.p3.3.m3.2.2.1.1.2.cmml">1</mn><mo id="S5.SS3.p3.3.m3.2.2.1.1.1" xref="S5.SS3.p3.3.m3.2.2.1.1.1.cmml">−</mo><mrow id="S5.SS3.p3.3.m3.2.2.1.1.3" xref="S5.SS3.p3.3.m3.2.2.1.1.3.cmml"><mi id="S5.SS3.p3.3.m3.2.2.1.1.3.2" xref="S5.SS3.p3.3.m3.2.2.1.1.3.2.cmml">B</mi><mo id="S5.SS3.p3.3.m3.2.2.1.1.3.1" xref="S5.SS3.p3.3.m3.2.2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.3.m3.2.2.1.1.3.3" xref="S5.SS3.p3.3.m3.2.2.1.1.3.3.cmml">a</mi><mo id="S5.SS3.p3.3.m3.2.2.1.1.3.1a" xref="S5.SS3.p3.3.m3.2.2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.3.m3.2.2.1.1.3.4" xref="S5.SS3.p3.3.m3.2.2.1.1.3.4.cmml">d</mi></mrow></mrow><mo id="S5.SS3.p3.3.m3.3.3.2.3" xref="S5.SS3.p3.3.m3.3.3.3.cmml">,</mo><mi id="S5.SS3.p3.3.m3.1.1" mathvariant="normal" xref="S5.SS3.p3.3.m3.1.1.cmml">…</mi><mo id="S5.SS3.p3.3.m3.3.3.2.4" xref="S5.SS3.p3.3.m3.3.3.3.cmml">,</mo><mrow id="S5.SS3.p3.3.m3.3.3.2.2" xref="S5.SS3.p3.3.m3.3.3.2.2.cmml"><mn id="S5.SS3.p3.3.m3.3.3.2.2.2" xref="S5.SS3.p3.3.m3.3.3.2.2.2.cmml">5</mn><mo id="S5.SS3.p3.3.m3.3.3.2.2.1" xref="S5.SS3.p3.3.m3.3.3.2.2.1.cmml">−</mo><mrow id="S5.SS3.p3.3.m3.3.3.2.2.3" xref="S5.SS3.p3.3.m3.3.3.2.2.3.cmml"><mi id="S5.SS3.p3.3.m3.3.3.2.2.3.2" xref="S5.SS3.p3.3.m3.3.3.2.2.3.2.cmml">E</mi><mo id="S5.SS3.p3.3.m3.3.3.2.2.3.1" xref="S5.SS3.p3.3.m3.3.3.2.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.3.m3.3.3.2.2.3.3" xref="S5.SS3.p3.3.m3.3.3.2.2.3.3.cmml">x</mi><mo id="S5.SS3.p3.3.m3.3.3.2.2.3.1a" xref="S5.SS3.p3.3.m3.3.3.2.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.3.m3.3.3.2.2.3.4" xref="S5.SS3.p3.3.m3.3.3.2.2.3.4.cmml">c</mi><mo id="S5.SS3.p3.3.m3.3.3.2.2.3.1b" xref="S5.SS3.p3.3.m3.3.3.2.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.3.m3.3.3.2.2.3.5" xref="S5.SS3.p3.3.m3.3.3.2.2.3.5.cmml">e</mi><mo id="S5.SS3.p3.3.m3.3.3.2.2.3.1c" xref="S5.SS3.p3.3.m3.3.3.2.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.3.m3.3.3.2.2.3.6" xref="S5.SS3.p3.3.m3.3.3.2.2.3.6.cmml">l</mi><mo id="S5.SS3.p3.3.m3.3.3.2.2.3.1d" xref="S5.SS3.p3.3.m3.3.3.2.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.3.m3.3.3.2.2.3.7" xref="S5.SS3.p3.3.m3.3.3.2.2.3.7.cmml">l</mi><mo id="S5.SS3.p3.3.m3.3.3.2.2.3.1e" xref="S5.SS3.p3.3.m3.3.3.2.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.3.m3.3.3.2.2.3.8" xref="S5.SS3.p3.3.m3.3.3.2.2.3.8.cmml">e</mi><mo id="S5.SS3.p3.3.m3.3.3.2.2.3.1f" xref="S5.SS3.p3.3.m3.3.3.2.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.3.m3.3.3.2.2.3.9" xref="S5.SS3.p3.3.m3.3.3.2.2.3.9.cmml">n</mi><mo id="S5.SS3.p3.3.m3.3.3.2.2.3.1g" xref="S5.SS3.p3.3.m3.3.3.2.2.3.1.cmml">⁢</mo><mi id="S5.SS3.p3.3.m3.3.3.2.2.3.10" xref="S5.SS3.p3.3.m3.3.3.2.2.3.10.cmml">t</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.3.m3.3b"><list id="S5.SS3.p3.3.m3.3.3.3.cmml" xref="S5.SS3.p3.3.m3.3.3.2"><apply id="S5.SS3.p3.3.m3.2.2.1.1.cmml" xref="S5.SS3.p3.3.m3.2.2.1.1"><minus id="S5.SS3.p3.3.m3.2.2.1.1.1.cmml" xref="S5.SS3.p3.3.m3.2.2.1.1.1"></minus><cn id="S5.SS3.p3.3.m3.2.2.1.1.2.cmml" type="integer" xref="S5.SS3.p3.3.m3.2.2.1.1.2">1</cn><apply id="S5.SS3.p3.3.m3.2.2.1.1.3.cmml" xref="S5.SS3.p3.3.m3.2.2.1.1.3"><times id="S5.SS3.p3.3.m3.2.2.1.1.3.1.cmml" xref="S5.SS3.p3.3.m3.2.2.1.1.3.1"></times><ci id="S5.SS3.p3.3.m3.2.2.1.1.3.2.cmml" xref="S5.SS3.p3.3.m3.2.2.1.1.3.2">𝐵</ci><ci id="S5.SS3.p3.3.m3.2.2.1.1.3.3.cmml" xref="S5.SS3.p3.3.m3.2.2.1.1.3.3">𝑎</ci><ci id="S5.SS3.p3.3.m3.2.2.1.1.3.4.cmml" xref="S5.SS3.p3.3.m3.2.2.1.1.3.4">𝑑</ci></apply></apply><ci id="S5.SS3.p3.3.m3.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1">…</ci><apply id="S5.SS3.p3.3.m3.3.3.2.2.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2"><minus id="S5.SS3.p3.3.m3.3.3.2.2.1.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.1"></minus><cn id="S5.SS3.p3.3.m3.3.3.2.2.2.cmml" type="integer" xref="S5.SS3.p3.3.m3.3.3.2.2.2">5</cn><apply id="S5.SS3.p3.3.m3.3.3.2.2.3.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3"><times id="S5.SS3.p3.3.m3.3.3.2.2.3.1.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3.1"></times><ci id="S5.SS3.p3.3.m3.3.3.2.2.3.2.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3.2">𝐸</ci><ci id="S5.SS3.p3.3.m3.3.3.2.2.3.3.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3.3">𝑥</ci><ci id="S5.SS3.p3.3.m3.3.3.2.2.3.4.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3.4">𝑐</ci><ci id="S5.SS3.p3.3.m3.3.3.2.2.3.5.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3.5">𝑒</ci><ci id="S5.SS3.p3.3.m3.3.3.2.2.3.6.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3.6">𝑙</ci><ci id="S5.SS3.p3.3.m3.3.3.2.2.3.7.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3.7">𝑙</ci><ci id="S5.SS3.p3.3.m3.3.3.2.2.3.8.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3.8">𝑒</ci><ci id="S5.SS3.p3.3.m3.3.3.2.2.3.9.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3.9">𝑛</ci><ci id="S5.SS3.p3.3.m3.3.3.2.2.3.10.cmml" xref="S5.SS3.p3.3.m3.3.3.2.2.3.10">𝑡</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.3.m3.3c">1-Bad,...,5-Excellent</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.3.m3.3d">1 - italic_B italic_a italic_d , … , 5 - italic_E italic_x italic_c italic_e italic_l italic_l italic_e italic_n italic_t</annotation></semantics></math> by three unique raters. We compute item-level scores by taking the median score per item. To compute system-level scores we take the mean of item scores for a given system. We compute 95% confidence intervals via bootstrapping, resampling at the item level for <math alttext="n_{b}=1000" class="ltx_Math" display="inline" id="S5.SS3.p3.4.m4.1"><semantics id="S5.SS3.p3.4.m4.1a"><mrow id="S5.SS3.p3.4.m4.1.1" xref="S5.SS3.p3.4.m4.1.1.cmml"><msub id="S5.SS3.p3.4.m4.1.1.2" xref="S5.SS3.p3.4.m4.1.1.2.cmml"><mi id="S5.SS3.p3.4.m4.1.1.2.2" xref="S5.SS3.p3.4.m4.1.1.2.2.cmml">n</mi><mi id="S5.SS3.p3.4.m4.1.1.2.3" xref="S5.SS3.p3.4.m4.1.1.2.3.cmml">b</mi></msub><mo id="S5.SS3.p3.4.m4.1.1.1" xref="S5.SS3.p3.4.m4.1.1.1.cmml">=</mo><mn id="S5.SS3.p3.4.m4.1.1.3" xref="S5.SS3.p3.4.m4.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.4.m4.1b"><apply id="S5.SS3.p3.4.m4.1.1.cmml" xref="S5.SS3.p3.4.m4.1.1"><eq id="S5.SS3.p3.4.m4.1.1.1.cmml" xref="S5.SS3.p3.4.m4.1.1.1"></eq><apply id="S5.SS3.p3.4.m4.1.1.2.cmml" xref="S5.SS3.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p3.4.m4.1.1.2.1.cmml" xref="S5.SS3.p3.4.m4.1.1.2">subscript</csymbol><ci id="S5.SS3.p3.4.m4.1.1.2.2.cmml" xref="S5.SS3.p3.4.m4.1.1.2.2">𝑛</ci><ci id="S5.SS3.p3.4.m4.1.1.2.3.cmml" xref="S5.SS3.p3.4.m4.1.1.2.3">𝑏</ci></apply><cn id="S5.SS3.p3.4.m4.1.1.3.cmml" type="integer" xref="S5.SS3.p3.4.m4.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.4.m4.1c">n_{b}=1000</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.4.m4.1d">italic_n start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT = 1000</annotation></semantics></math> iterations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">Overall results.</span> The two left-most columns of Table. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.T3" title="Table 3 ‣ 5.3 Human Evaluation ‣ 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">3</span></a> indicate that nearly all models are at parity in perceived <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.2">Naturalness</span> (N-MOS) of turn-taking, while close to re-synthesized ground-truth values. On the perceived <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.3">Meaningfulness</span> (M-MOS) of the dialogue content, SyncLLM-based models significantly outperform dGSLM, approaching re-synthesized ground-truth values. Resynth-GT here accounts for the tokenization process and is the topline number for the implementation of our method using the HuBERT tokenizer.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p5.1.1">In-distribution and OOD.</span> Table. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.T3" title="Table 3 ‣ 5.3 Human Evaluation ‣ 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">3</span></a> also highlights the difference between in-distribution (Fisher) and OOD (CANDOR) between dGSLM and Fisher-trained SyncLLM-F. While dGSLM suffers from significant degradation OOD (dropping -0.24 and -0.51 in M-MOS and N-MOS ratings), these declines are reduced in SyncLLM-F only dropping -0.15 and -0.16 moving OOD. SyncLLM trained on CANDOR dataset (SyncLLM-C) shows a decline OOD on M-MOS (-0.52), but not N-MOS (+0.03). We note that dGSLM <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib25" title="">2022</a>)</cite> uses speech representations fine-tuned on the Fisher dataset, while our method uses general-purpose speech representations for all domains of speech. This results in our method outperforming the baseline on the out-of-distribution Candor testset in naturalness, as judged by human evaluators in Table. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.T3" title="Table 3 ‣ 5.3 Human Evaluation ‣ 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="439" id="S5.F7.g1" src="x7.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Comparison of ASR perplexity between continuation mode and interaction-mode.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Full-duplex interaction</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We simulate LLM-user interaction using LLM-LLM interaction with one-chunk latency. We evaluate our model trained with different chunk sizes, thus simulating different latencies. We also train a version of SyncLLM with Candor training split in the third training stage, and simulate its interaction with the original model trained with only Fisher.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.F7" title="Figure 7 ‣ 5.3 Human Evaluation ‣ 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">7</span></a>, we compare median perplexities obtained with prompts sampled from Fisher and Candor test splits. We also show the perplexity of ground-truth and samples generated in the dialog continuation setting for reference. We find that SyncLLM in the LLM-LLM interaction setting is able to closely match the performance of the continuation setting, and perform significantly better than dGSLM in continuation setting. Furthermore, we find that interaction between instances of SyncLLM trained with Fisher and Candor datasets, respectively is are almost the same signifying that SyncLLM can perform a coherent conversation even when user’s side of the conversation is generated by a model trained with a different dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">Human evaluation.</span>
Table. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.T4" title="Table 4 ‣ 5.4 Full-duplex interaction ‣ 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">4</span></a> shows ratings for dGSLM, the Fisher-trained continuation model, and LLM-LLM interactions. Results corroborate findings in §<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.SS4" title="5.4 Full-duplex interaction ‣ 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">5.4</span></a> – LLM-LLM interactions outperform dGSLM on M-MOS, but are slightly worse compared to the single model continuation setting.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Human evaluation results for Meaningfulness (Meaning.) and Naturalness (Nat.) mean estimates and standard errors (in parentheses) across all data.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.2.2.3"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.3.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1">Meaning. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.m1.1.1" stretchy="false" xref="S5.T4.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2">Nat. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.2.2.2.m1.1"><semantics id="S5.T4.2.2.2.m1.1a"><mo id="S5.T4.2.2.2.m1.1.1" stretchy="false" xref="S5.T4.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><ci id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.2.3.1.1">dGSLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.3.1.2">1.55 (0.06)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.3.1.3">3.95 (0.08)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.2.4.2.1">SyncLLM-F</th>
<td class="ltx_td ltx_align_center" id="S5.T4.2.4.2.2">3.74 (0.06)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.4.2.3">3.90 (0.06)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.2.5.3.1">SyncLLM-F-C</th>
<td class="ltx_td ltx_align_center" id="S5.T4.2.5.3.2">3.39 (0.06)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.5.3.3">3.78 (0.06)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.2.6.4.1">SyncLLM-F-F</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.2.6.4.2">3.47 (0.06)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.2.6.4.3">3.72 (0.06)</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We present Synchronous LLMs, a novel post-training framework that converts an auto-regressive LLM into a full-duplex spoken dialogue agent. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining turn-taking naturalness. Finally, by simulating full-duplex dialogue between two agents, we show robustness to delayed input from Internet-scale latencies, where the agents do not have immediate access to the
speech generated by their users.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations and Risks</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1"><span class="ltx_text ltx_font_bold" id="S7.p1.1.1">Limitations.</span> The performance of Synchronous LLMs could be further improved in terms of speech quality. Currently, we use a simple HiFi-GAN vocoder for speech synthesis, and higher-quality speech could be synthesized from semantic units with a more advanced speech generator. Moreover, we have not studied expressivity and non-verbal sounds in dialogue such as laughter, which could make the spoken dialogue more human-like. Another limitation is the context length; synchronous LLMs are initialized from Llama-3, and therefore have the same sequence length limit which constrained the long-context modeling in dialogue as well as the use of more expressive multi-codebook tokenizers like EnCodec <cite class="ltx_cite ltx_citemacro_cite">Défossez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib8" title="">2022</a>)</cite> that have higher token rate.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1">Ethical considerations.</span> The proposed model is intended for spoken dialogue agents. In case of failure, the system might generate inappropriate responses and toxicity mitigation may be needed for speech outputs. As for unintended use, one example is that bad actors misuse the model for online scams. Speech watermarking is one potential approach to counter abuse of the technology.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The University of Washington researchers are partly supported by the Meta AI Mentorship program, Moore Inventor Fellow award #10617, UW CoMotion fund, and the NSF.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2023)</span>
<span class="ltx_bibblock">
Suno AI. 2023.

</span>
<span class="ltx_bibblock">Bark tts.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/suno-ai/bark" title="">https://github.com/suno-ai/bark</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">at Meta (2024)</span>
<span class="ltx_bibblock">
AT at Meta. 2024.

</span>
<span class="ltx_bibblock">Meta llama 3.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/meta-llama/llama3" title="">https://github.com/meta-llama/llama3</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borsos et al. (2023)</span>
<span class="ltx_bibblock">
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2209.03143" title="">Audiolm: a language modeling approach to audio generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Preprint</em>, arXiv:2209.03143.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brady (1968)</span>
<span class="ltx_bibblock">
Paul T. Brady. 1968.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:62739009" title="">A statistical analysis of on-off patterns in 16 conversations</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Bell System Technical Journal</em>, 47:73–91.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bredin et al. (2020)</span>
<span class="ltx_bibblock">
Hervé Bredin, Ruiqing Yin, Juan Manuel Coria, Gregory Gelly, Pavel Korshunov, Marvin Lavechin, Diego Fustes, Hadrien Titeux, Wassim Bouaziz, and Marie-Philippe Gill. 2020.

</span>
<span class="ltx_bibblock">pyannote.audio: neural building blocks for speaker diarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, Barcelona, Spain.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chou et al. (2023)</span>
<span class="ltx_bibblock">
Ju-Chieh Chou, Chung-Ming Chien, Wei-Ning Hsu, Karen Livescu, Arun Babu, Alexis Conneau, Alexei Baevski, and Michael Auli. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.08715" title="">Toward joint language modeling for speech units and text</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Preprint</em>, arXiv:2310.08715.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cieri et al. (2004)</span>
<span class="ltx_bibblock">
Christopher Cieri, David Miller, and Kevin Walker. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:8414900" title="">The fisher corpus: a resource for the next generations of speech-to-text</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">International Conference on Language Resources and Evaluation</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Défossez et al. (2022)</span>
<span class="ltx_bibblock">
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2022.

</span>
<span class="ltx_bibblock">High fidelity neural audio compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2210.13438</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Godfrey et al. (1992)</span>
<span class="ltx_bibblock">
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICASSP.1992.225858" title="">Switchboard: telephone speech corpus for research and development</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">[Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, volume 1, pages 517–520 vol.1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassid et al. (2023)</span>
<span class="ltx_bibblock">
Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Défossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, and Yossi Adi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2023/hash/c859b99b5d717c9035e79d43dfd69435-Abstract-Conference.html" title="">Textually pretrained speech language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassid et al. (2024)</span>
<span class="ltx_bibblock">
Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, and Yossi Adi. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.13009" title="">Textually pretrained speech language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Preprint</em>, arXiv:2305.13009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heldner and Edlund (2010)</span>
<span class="ltx_bibblock">
Mattias Heldner and Jens Edlund. 2010.

</span>
<span class="ltx_bibblock">Pauses, gaps and overlaps in conversations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Journal of Phonetics</em>, 38(4):555–568.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2106.07447" title="">Hubert: Self-supervised speech representation learning by masked prediction of hidden units</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Preprint</em>, arXiv:2106.07447.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ITU-T Recommendation P.808 (2018)</span>
<span class="ltx_bibblock">
ITU-T Recommendation P.808. 2018.

</span>
<span class="ltx_bibblock">Subjective evaluation of speech quality with a crowdsourcing approach.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, abs/2310.06825.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kharitonov et al. (2022)</span>
<span class="ltx_bibblock">
Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu-Anh Nguyen, Morgane Rivière, Abdelrahman Mohamed, Emmanuel Dupoux, and Wei-Ning Hsu. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2109.03264" title="">Text-free prosody-aware generative spoken language modeling</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Preprint</em>, arXiv:2109.03264.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2024)</span>
<span class="ltx_bibblock">
Heeseung Kim, Soonshin Seo, Kyeongseok Jeong, Ohsung Kwon, Jungwhan Kim, Jaehong Lee, Eunwoo Song, Myungwoo Oh, Sungroh Yoon, and Kang Min Yoo. 2024.

</span>
<span class="ltx_bibblock">Unified speech-text pretraining for spoken dialog modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CoRR</em>, abs/2402.05706.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lakhotia et al. (2021)</span>
<span class="ltx_bibblock">
Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman Mohamed, and Emmanuel Dupoux. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2102.01192" title="">Generative spoken language modeling from raw audio</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Preprint</em>, arXiv:2102.01192.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levinson and Torreira (2015a)</span>
<span class="ltx_bibblock">
Stephen C. Levinson and Francisco Torreira. 2015a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3389/fpsyg.2015.00731" title="">Timing in turn-taking and its implications for processing models of language</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Frontiers in Psychology</em>, 6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levinson and Torreira (2015b)</span>
<span class="ltx_bibblock">
Stephen C Levinson and Francisco Torreira. 2015b.

</span>
<span class="ltx_bibblock">Timing in turn-taking and its implications for processing models of language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Frontiers in psychology</em>, 6:731.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Ting-En Lin, Yuchuan Wu, Fei Huang, Luo Si, Jian Sun, and Yongbin Li. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3534678.3539209" title="">Duplex conversation: Towards human-like interaction in spoken dialogue systems</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, KDD ’22. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maiti et al. (2023)</span>
<span class="ltx_bibblock">
Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, and Shinji Watanabe. 2023.

</span>
<span class="ltx_bibblock">Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">CoRR</em>, abs/2309.07937.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nachmani et al. (2023)</span>
<span class="ltx_bibblock">
Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. 2023.

</span>
<span class="ltx_bibblock">Spoken question answering and speech continuation using spectrogram-powered llm.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2020)</span>
<span class="ltx_bibblock">
Tu Anh Nguyen, Maureen de Seyssel, Patricia Rozé, Morgane Rivière, Evgeny Kharitonov, Alexei Baevski, Ewan Dunbar, and Emmanuel Dupoux. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2011.11588" title="">The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language modeling</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Preprint</em>, arXiv:2011.11588.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2022)</span>
<span class="ltx_bibblock">
Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, and Emmanuel Dupoux. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2203.16502" title="">Generative spoken dialogue language modeling</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Preprint</em>, arXiv:2203.16502.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2024)</span>
<span class="ltx_bibblock">
Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Gabriel Synnaeve, Juan Pino, Benoit Sagot, and Emmanuel Dupoux. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.05755" title="">Spirit-lm: Interleaved spoken and written language model</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Preprint</em>, arXiv:2402.05755.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2021)</span>
<span class="ltx_bibblock">
Vivian T Nguyen, Otto Versyp, Christopher Cox, and Riccardo Fusaroli. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:247547959" title="">A systematic review and bayesian meta-analysis of the development of turn taking in adult-child vocal interactions.</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Child development</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2303.08774" title="">GPT-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">CoRR</em>, abs/2303.08774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reece et al. (2023)</span>
<span class="ltx_bibblock">
Andrew Reece, Gus Cooney, Peter Bull, Christine Chung, Bryn Dawson, Casey Fitzpatrick, Tamara Glazer, Dean Knox, Alex Liebscher, and Sebastian Marin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1126/sciadv.adf3197" title="">The candor corpus: Insights from a large multimodal dataset of naturalistic conversation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Science Advances</em>, 9(13):eadf3197.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubenstein et al. (2023)</span>
<span class="ltx_bibblock">
Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimirović, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, and Christian Frank. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2306.12925" title="">Audiopalm: A large language model that can speak and listen</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Preprint</em>, arXiv:2306.12925.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schegloff (2000)</span>
<span class="ltx_bibblock">
Emanuel A Schegloff. 2000.

</span>
<span class="ltx_bibblock">Overlapping talk and the organization of turn-taking for conversation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Language in society</em>, 29(1):1–63.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skantze (2021)</span>
<span class="ltx_bibblock">
Gabriel Skantze. 2021.

</span>
<span class="ltx_bibblock">Turn-taking in conversational systems and human-robot interaction: A review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Comput. Speech Lang.</em>, 67:101178.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stivers et al. (2009)</span>
<span class="ltx_bibblock">
Tanya Stivers, Nick J. Enfield, Penelope Brown, Christina Englert, Makoto Hayashi, Trine Heinemann, Gertie Hoymann, Federico Rossano, Jan Peter De Ruiter, Kyung-Eun Yoon, Stephen C. Levinson, Paul Kay, and Krishna Y. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:10200647" title="">Universals and cultural variation in turn-taking in conversation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the National Academy of Sciences</em>, 106:10587 – 10592.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ten Bosch et al. (2005)</span>
<span class="ltx_bibblock">
Louis ten Bosch, Nelleke Oostdijk, and Lou Boves. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:44754316" title="">On temporal aspects of turn taking in conversational dialogues</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Speech Commun.</em>, 47:80–86.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">CoRR</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock">Viola: Unified codec language models for speech recognition, synthesis, and translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">CoRR</em>, abs/2305.16107.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023a.

</span>
<span class="ltx_bibblock">Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 15757–15773. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Haoning Zhang, Junwei Bao, Haipeng Sun, Youzheng Wu, Wenye Li, Shuguang Cui, and Xiaodong He. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2211.05503" title="">Monet: Tackle state momentum via noise-enhanced training for dialogue state tracking</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Preprint</em>, arXiv:2211.05503.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020.

</span>
<span class="ltx_bibblock">Dialogpt: Large-scale generative pre-training for conversational response generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">ACL, system demonstration</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2020)</span>
<span class="ltx_bibblock">
Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, and Rui Yan. 2020.

</span>
<span class="ltx_bibblock">Knowledge-grounded dialogue generation with pre-trained language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, pages 3377–3390. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional training details</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Hyperparameters</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.3">We trained SyncLLM with the Llama3-8b’s original sequence length 8192. In the first stage, we train with a per-gpu batch size of 1 on 128 A100 GPUs, equivalent to a total batch of 8192 x 128 = 1M tokens. We use a learning rate of <math alttext="3\times{10}^{-5}" class="ltx_Math" display="inline" id="A1.SS1.p1.1.m1.1"><semantics id="A1.SS1.p1.1.m1.1a"><mrow id="A1.SS1.p1.1.m1.1.1" xref="A1.SS1.p1.1.m1.1.1.cmml"><mn id="A1.SS1.p1.1.m1.1.1.2" xref="A1.SS1.p1.1.m1.1.1.2.cmml">3</mn><mo id="A1.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p1.1.m1.1.1.1.cmml">×</mo><msup id="A1.SS1.p1.1.m1.1.1.3" xref="A1.SS1.p1.1.m1.1.1.3.cmml"><mn id="A1.SS1.p1.1.m1.1.1.3.2" xref="A1.SS1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A1.SS1.p1.1.m1.1.1.3.3" xref="A1.SS1.p1.1.m1.1.1.3.3.cmml"><mo id="A1.SS1.p1.1.m1.1.1.3.3a" xref="A1.SS1.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="A1.SS1.p1.1.m1.1.1.3.3.2" xref="A1.SS1.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.1.m1.1b"><apply id="A1.SS1.p1.1.m1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1"><times id="A1.SS1.p1.1.m1.1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1.1"></times><cn id="A1.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.SS1.p1.1.m1.1.1.2">3</cn><apply id="A1.SS1.p1.1.m1.1.1.3.cmml" xref="A1.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A1.SS1.p1.1.m1.1.1.3.1.cmml" xref="A1.SS1.p1.1.m1.1.1.3">superscript</csymbol><cn id="A1.SS1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A1.SS1.p1.1.m1.1.1.3.2">10</cn><apply id="A1.SS1.p1.1.m1.1.1.3.3.cmml" xref="A1.SS1.p1.1.m1.1.1.3.3"><minus id="A1.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="A1.SS1.p1.1.m1.1.1.3.3"></minus><cn id="A1.SS1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="A1.SS1.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.1.m1.1c">3\times{10}^{-5}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.1.m1.1d">3 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, with 500 step warmup and train for 40k iterations. In the second stage, we reduce the batch size to 512k tokens, learning rate to <math alttext="2.2\times{10}^{-5}" class="ltx_Math" display="inline" id="A1.SS1.p1.2.m2.1"><semantics id="A1.SS1.p1.2.m2.1a"><mrow id="A1.SS1.p1.2.m2.1.1" xref="A1.SS1.p1.2.m2.1.1.cmml"><mn id="A1.SS1.p1.2.m2.1.1.2" xref="A1.SS1.p1.2.m2.1.1.2.cmml">2.2</mn><mo id="A1.SS1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p1.2.m2.1.1.1.cmml">×</mo><msup id="A1.SS1.p1.2.m2.1.1.3" xref="A1.SS1.p1.2.m2.1.1.3.cmml"><mn id="A1.SS1.p1.2.m2.1.1.3.2" xref="A1.SS1.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="A1.SS1.p1.2.m2.1.1.3.3" xref="A1.SS1.p1.2.m2.1.1.3.3.cmml"><mo id="A1.SS1.p1.2.m2.1.1.3.3a" xref="A1.SS1.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="A1.SS1.p1.2.m2.1.1.3.3.2" xref="A1.SS1.p1.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.2.m2.1b"><apply id="A1.SS1.p1.2.m2.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1"><times id="A1.SS1.p1.2.m2.1.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1.1"></times><cn id="A1.SS1.p1.2.m2.1.1.2.cmml" type="float" xref="A1.SS1.p1.2.m2.1.1.2">2.2</cn><apply id="A1.SS1.p1.2.m2.1.1.3.cmml" xref="A1.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="A1.SS1.p1.2.m2.1.1.3.1.cmml" xref="A1.SS1.p1.2.m2.1.1.3">superscript</csymbol><cn id="A1.SS1.p1.2.m2.1.1.3.2.cmml" type="integer" xref="A1.SS1.p1.2.m2.1.1.3.2">10</cn><apply id="A1.SS1.p1.2.m2.1.1.3.3.cmml" xref="A1.SS1.p1.2.m2.1.1.3.3"><minus id="A1.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="A1.SS1.p1.2.m2.1.1.3.3"></minus><cn id="A1.SS1.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="A1.SS1.p1.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.2.m2.1c">2.2\times{10}^{-5}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.2.m2.1d">2.2 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> and warmup steps to 200, and train for 6000 iterations. In the last stage, we train with a batch size of 256k tokens, with a learning rate of <math alttext="1.5\times{10}^{-5}" class="ltx_Math" display="inline" id="A1.SS1.p1.3.m3.1"><semantics id="A1.SS1.p1.3.m3.1a"><mrow id="A1.SS1.p1.3.m3.1.1" xref="A1.SS1.p1.3.m3.1.1.cmml"><mn id="A1.SS1.p1.3.m3.1.1.2" xref="A1.SS1.p1.3.m3.1.1.2.cmml">1.5</mn><mo id="A1.SS1.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p1.3.m3.1.1.1.cmml">×</mo><msup id="A1.SS1.p1.3.m3.1.1.3" xref="A1.SS1.p1.3.m3.1.1.3.cmml"><mn id="A1.SS1.p1.3.m3.1.1.3.2" xref="A1.SS1.p1.3.m3.1.1.3.2.cmml">10</mn><mrow id="A1.SS1.p1.3.m3.1.1.3.3" xref="A1.SS1.p1.3.m3.1.1.3.3.cmml"><mo id="A1.SS1.p1.3.m3.1.1.3.3a" xref="A1.SS1.p1.3.m3.1.1.3.3.cmml">−</mo><mn id="A1.SS1.p1.3.m3.1.1.3.3.2" xref="A1.SS1.p1.3.m3.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.3.m3.1b"><apply id="A1.SS1.p1.3.m3.1.1.cmml" xref="A1.SS1.p1.3.m3.1.1"><times id="A1.SS1.p1.3.m3.1.1.1.cmml" xref="A1.SS1.p1.3.m3.1.1.1"></times><cn id="A1.SS1.p1.3.m3.1.1.2.cmml" type="float" xref="A1.SS1.p1.3.m3.1.1.2">1.5</cn><apply id="A1.SS1.p1.3.m3.1.1.3.cmml" xref="A1.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="A1.SS1.p1.3.m3.1.1.3.1.cmml" xref="A1.SS1.p1.3.m3.1.1.3">superscript</csymbol><cn id="A1.SS1.p1.3.m3.1.1.3.2.cmml" type="integer" xref="A1.SS1.p1.3.m3.1.1.3.2">10</cn><apply id="A1.SS1.p1.3.m3.1.1.3.3.cmml" xref="A1.SS1.p1.3.m3.1.1.3.3"><minus id="A1.SS1.p1.3.m3.1.1.3.3.1.cmml" xref="A1.SS1.p1.3.m3.1.1.3.3"></minus><cn id="A1.SS1.p1.3.m3.1.1.3.3.2.cmml" type="integer" xref="A1.SS1.p1.3.m3.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.3.m3.1c">1.5\times{10}^{-5}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.3.m3.1d">1.5 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> and 100 warmup steps, for 2000 iterations.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span><span class="ltx_text ltx_font_bold" id="A1.T5.6.1">Ablation evaluations over interleaving level.</span> WUGGY, BLIMP, Topic-StoryCloze, and StoryCloze assess the knowledge and capacity of the model in lexical, syntactical, and semantic levels respectively. We report the accuracy based on negative-log-likelihood – normalized by the number of tokens – minimization prediction. The tasks are evaluated in the zero-shot setting.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T5.4.4.5"><span class="ltx_text ltx_font_bold" id="A1.T5.4.4.5.1">Interleaving</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1">WUGGY<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.1.1.1.1.m1.1"><semantics id="A1.T5.1.1.1.1.m1.1a"><mo id="A1.T5.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.m1.1b"><ci id="A1.T5.1.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T5.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T5.2.2.2.1">BLIMP<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.2.2.2.1.m1.1"><semantics id="A1.T5.2.2.2.1.m1.1a"><mo id="A1.T5.2.2.2.1.m1.1.1" stretchy="false" xref="A1.T5.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.1.m1.1b"><ci id="A1.T5.2.2.2.1.m1.1.1.cmml" xref="A1.T5.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T5.3.3.3"><span class="ltx_text ltx_font_bold" id="A1.T5.3.3.3.1">Topic-StoryCloze<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.3.3.3.1.m1.1"><semantics id="A1.T5.3.3.3.1.m1.1a"><mo id="A1.T5.3.3.3.1.m1.1.1" stretchy="false" xref="A1.T5.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.3.1.m1.1b"><ci id="A1.T5.3.3.3.1.m1.1.1.cmml" xref="A1.T5.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T5.4.4.4"><span class="ltx_text ltx_font_bold" id="A1.T5.4.4.4.1">StoryCloze<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.4.4.4.1.m1.1"><semantics id="A1.T5.4.4.4.1.m1.1a"><mo id="A1.T5.4.4.4.1.m1.1.1" stretchy="false" xref="A1.T5.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.4.4.4.1.m1.1b"><ci id="A1.T5.4.4.4.1.m1.1.1.cmml" xref="A1.T5.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.4.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.4.4.4.1.m1.1d">↑</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.4.5.1.1">Turn-level</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.5.1.2">63.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.5.1.3">56.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.5.1.4">76.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.5.1.5">55.1</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T5.4.6.2.1">Sentence-level</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.4.6.2.2">70.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.4.6.2.3">56.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.4.6.2.4">83.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.4.6.2.5">61.8</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Benchmarking interleaving strategies</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">We explore two text-speech interleaving strategies in stage 1 of our training: i) Sentence-level interleaving: each sentence is chosen randomly to be either text modality or speech modality. ii) Turn-level interleaving: each turn is chosen randomly to be either text modality or speech modality, resulting in consistent modality for all the sentences within the turn. We compare them by evaluating on a set of spoken language understanding benchmarks proposed in <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#bib.bib24" title="">2020</a>)</cite>. We report these results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A1.T5" title="Table 5 ‣ A.1 Hyperparameters ‣ Appendix A Additional training details ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">5</span></a>. On these tasks, we observe that sentence-level interleaving outperforms turn-level interleaving across all benchmarks.</p>
</div>
<figure class="ltx_table" id="A1.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison of average Pearson correlation of turn-taking event durations between generation and ground-truth continuation with SyncLLM in the two-model interaction setting. Measured on testsets comprising both Fisher and Candor testsets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T6.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.1">Latency</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.2.1">SyncLLM-F-F</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.3.1">SyncLLM-F-C</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.2.1.1">160 ms</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.2.1.2">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.2.1.3">0.36</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.3.2">
<td class="ltx_td ltx_align_center" id="A1.T6.1.3.2.1">200 ms</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.3.2.2">0.31</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.3.2.3">0.35</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T6.1.4.3.1">240 ms</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T6.1.4.3.2">0.28</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T6.1.4.3.3">0.32</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="A1.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Comparison of Pearson correlation of turn-taking event durations between prompt and generation.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="A1.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.2.1">Fisher (in-distribution)</span></td>
<td class="ltx_td ltx_border_t" id="A1.T7.1.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="A1.T7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.4.1">Candor (out-of-distribution)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="A1.T7.1.2.2.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T7.1.2.2.2.1">ipu</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A1.T7.1.2.2.3.1">pause</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A1.T7.1.2.2.4.1">fto</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A1.T7.1.2.2.5.1">Average</span></td>
<td class="ltx_td" id="A1.T7.1.2.2.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.2.7"><span class="ltx_text ltx_font_bold" id="A1.T7.1.2.2.7.1">ipu</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.2.8"><span class="ltx_text ltx_font_bold" id="A1.T7.1.2.2.8.1">pause</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.2.9"><span class="ltx_text ltx_font_bold" id="A1.T7.1.2.2.9.1">fto</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.2.10"><span class="ltx_text ltx_font_bold" id="A1.T7.1.2.2.10.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T7.1.3.3.1">dGSLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.3.3.2">0.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.3.3.3">0.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.3.3.4">0.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.3.3.5">0.39</td>
<td class="ltx_td ltx_border_t" id="A1.T7.1.3.3.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.3.3.7">0.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.3.3.8">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.3.3.9">0.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.3.3.10">0.24</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.4.4.1">SyncLLM-F (160 ms)</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.4.2">0.69</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.4.3">0.34</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.4.4">0.35</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.4.5">0.46</td>
<td class="ltx_td" id="A1.T7.1.4.4.6"></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.4.7">0.64</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.4.8">0.12</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.4.9">0.24</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.4.10">0.33</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.5.5.1">SyncLLM-F (200 ms)</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.5.2">0.57</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.5.3">0.49</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.5.4">0.29</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.5.5">0.45</td>
<td class="ltx_td" id="A1.T7.1.5.5.6"></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.5.7">0.61</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.5.8">0.34</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.5.9">0.13</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.5.10">0.36</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.6.6.1">SyncLLM-F (240 ms)</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.6.2">0.63</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.6.3">0.49</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.6.4">0.33</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.6.5">0.48</td>
<td class="ltx_td" id="A1.T7.1.6.6.6"></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.6.7">0.59</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.6.8">0.23</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.6.9">0.19</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.6.10">0.34</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="A1.T7.1.7.7.1">GT</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.1.7.7.2">0.72</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.1.7.7.3">0.53</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.1.7.7.4">0.31</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.1.7.7.5">0.52</td>
<td class="ltx_td ltx_border_b ltx_border_t" id="A1.T7.1.7.7.6"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.1.7.7.7">0.54</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.1.7.7.8">0.30</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.1.7.7.9">0.12</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.1.7.7.10">0.32</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Naturalness-MOS Instructions</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Naturalistic turn-taking between two people is characterized by smooth transitions where each participant listens to the other, responds appropriately, and allows for pauses or silences, creating a balanced and dynamic interaction. Typically, the participants try to avoid overlapping speech, although this may occur especially when one participant provides information that they understood the other by using words like “yeah” or “uh-huh.” Hesitations, pausing, silence, and repairs are also natural events that occur in a conversation between two people.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">Here, you will listen to a dialogue between two people and provide a rating for how natural the turn-taking sounds regardless of its content (the meaning of the words used) and the clarity of voices.</p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">Some of the samples are generated by an AI model, some are actual recordings of humans in conversation, and some are actual recordings of people, but with AI generated voices overlayed. Please try to assess the naturalness of the turn-taking without taking into consideration the sound of the voices.</p>
</div>
<div class="ltx_para" id="A2.p4">
<p class="ltx_p" id="A2.p4.1">To begin, first listen to the “prompt” audio in its entirety. This is the first part of the conversation. Then listen to the “continuation” audio in its entirety. This is the second part of the conversation. Note that in many cases the voices in the prompt may differ from the voices in the continuation (including the perceived gender of the speakers). Your rating should reflect how natural the “continuation” audio sounds given the turn-taking characteristics you observe in the “prompt.”</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>N-MOS &amp; M-MOS</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">We provide the complete protocol used for human evaluation of turn-taking <span class="ltx_text ltx_font_italic" id="A2.SS1.p1.1.1">Naturalness</span> and dialogue content <span class="ltx_text ltx_font_italic" id="A2.SS1.p1.1.2">Meaningfulness</span>.</p>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1"><span class="ltx_text ltx_font_italic" id="A2.SS1.p2.1.1">Audios presented</span></p>
</div>
<div class="ltx_para" id="A2.SS1.p3">
<p class="ltx_p" id="A2.SS1.p3.1">Please base your rating on the impression you have that two people are talking and listening naturally with one-another in the “continuation” audio.
<span class="ltx_ERROR undefined" id="A2.SS1.p3.1.1">{etaremune}</span>[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]</p>
</div>
<div class="ltx_para" id="A2.SS1.p4">
<p class="ltx_p" id="A2.SS1.p4.1">Excellent - basically indistinguishable from human-like turn-taking</p>
</div>
<div class="ltx_para" id="A2.SS1.p5">
<p class="ltx_p" id="A2.SS1.p5.1">Good - minor differences from human-like turn-taking</p>
</div>
<div class="ltx_para" id="A2.SS1.p6">
<p class="ltx_p" id="A2.SS1.p6.1">Fair - substantial differences from human-like turn-taking</p>
</div>
<div class="ltx_para" id="A2.SS1.p7">
<p class="ltx_p" id="A2.SS1.p7.1">Poor - very little in common with human-like turn-taking</p>
</div>
<div class="ltx_para" id="A2.SS1.p8">
<p class="ltx_p" id="A2.SS1.p8.1">Bad - essentially nothing in common with human-like turn-taking</p>
</div>
<section class="ltx_subsubsection" id="A2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.1.1 </span>Meaningfulness-MOS</h4>
<div class="ltx_para" id="A2.SS1.SSS1.p1">
<p class="ltx_p" id="A2.SS1.SSS1.p1.1">In this task you will listen to a dialogue between two people and provide a rating for how meaningful their conversation is. By meaningful we mean the degree to which the content of the conversation is coherent and plausible (can you understand the intent of the speakers and does it sound like something people would reasonably talk about). Just as in everyday conversations, the content may or may not be perfectly grammatical, but must be understandable in the context of the conversation.</p>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p2">
<p class="ltx_p" id="A2.SS1.SSS1.p2.1">To begin, first listen to the “prompt” audio in its entirety. This is the first part of the conversation. Then listen to the “continuation” audio in its entirety. This is the second part of the conversation. Note that in many cases the voices in the prompt may differ from the voices in the continuation (including the perceived gender of the speakers). Your rating should reflect how meaningful the “continuation” audio is, given the “prompt.”</p>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p3">
<p class="ltx_p" id="A2.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_italic" id="A2.SS1.SSS1.p3.1.1">Audios presented</span></p>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p4">
<p class="ltx_p" id="A2.SS1.SSS1.p4.1">Please base your rating on the impression you have that the continuation is a meaningful “continuation” of the prompt audio - that it represents a plausible direction the conversation would go and is coherent.</p>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p5">
<span class="ltx_ERROR undefined" id="A2.SS1.SSS1.p5.1">{etaremune}</span>
<p class="ltx_p" id="A2.SS1.SSS1.p5.2">[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]</p>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p6">
<p class="ltx_p" id="A2.SS1.SSS1.p6.1">Excellent - all of the conversation content is plausible and coherent</p>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p7">
<p class="ltx_p" id="A2.SS1.SSS1.p7.1">Good - most of the conversation content is plausible and coherent</p>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p8">
<p class="ltx_p" id="A2.SS1.SSS1.p8.1">Fair - some of the conversation content is plausible and coherent</p>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p9">
<p class="ltx_p" id="A2.SS1.SSS1.p9.1">Poor - little of the conversation content is plausible and coherent</p>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p10">
<p class="ltx_p" id="A2.SS1.SSS1.p10.1">Bad - basically none of the conversation content is plausible and coherent</p>
</div>
<figure class="ltx_figure" id="A2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A2.F8.g1" src="x8.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Effect of latency on two-model interaction.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Effect of latency on full-duplex interaction</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A2.F8" title="Figure 8 ‣ B.1.1 Meaningfulness-MOS ‣ B.1 N-MOS &amp; M-MOS ‣ Appendix B Naturalness-MOS Instructions ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">8</span></a>, we compare the performance in the interaction setting with different latencies. We find that our method is robust to a latency as much as 200 ms, but the performance drops with latency greater than that. Similar to our naturalness evaluation in the continuation setting in §<a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S5.SS2" title="5.2 Naturalness evaluation ‣ 5 Experiments ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">5.2</span></a>, to evaluate turn-taking capability of SyncLLM in interaction setting, we compare Pearson correlation of the duration of turn-taking events in generation and ground-truth continuations. In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A1.T6" title="Table 6 ‣ A.2 Benchmarking interleaving strategies ‣ Appendix A Additional training details ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">6</span></a>, we observe that on a combined test set of in-distribution and out-of-distribution prompts, performance in the interaction setting closely matches with latencies 160 ms and 200 ms, but drops with 240 ms.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Turn-taking event correlation between prompt and generation</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">Similar to the naturalness evaluation in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#S4.T2" title="Table 2 ‣ 4 Training ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">2</span></a>, where we consider ground-truth continuation as the reference for turn-taking event statistics, we could also consider prompt as the reference. In a way, this measures style consistency between prompt and the continuation. In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15594v1#A1.T7" title="Table 7 ‣ A.2 Benchmarking interleaving strategies ‣ Appendix A Additional training details ‣ Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"><span class="ltx_text ltx_ref_tag">7</span></a>, we compare turn-taking event correlation of generations of our method in continuation setting, with that of dGSLM method. We observed that our method demonstrates better turn-taking correlation with the prompts as well.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 23 22:47:38 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
