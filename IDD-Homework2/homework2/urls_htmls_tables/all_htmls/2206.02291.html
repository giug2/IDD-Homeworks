<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2206.02291] Pretrained Models for Multilingual Federated Learning</title><meta property="og:description" content="Since the advent of Federated Learning (FL), research has applied these methods to natural language processing (NLP) tasks.
Despite a plethora of papers in FL for NLP, no previous works have studied how multilingual te…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Pretrained Models for Multilingual Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Pretrained Models for Multilingual Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2206.02291">

<!--Generated on Mon Mar 11 19:32:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Pretrained Models for Multilingual Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Orion Weller*, Marc Marone*, 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_bold">Vladimir Braverman, Dawn Lawrie, Benjamin Van Durme</span> 
<br class="ltx_break">Johns Hopkins University 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">oweller@cs.jhu.edu,mmarone1@jhu.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Since the advent of Federated Learning (FL), research has applied these methods to natural language processing (NLP) tasks.
Despite a plethora of papers in FL for NLP, no previous works have studied how multilingual text impacts FL algorithms.
Furthermore, multilingual text provides an interesting avenue to examine the impact of non-IID text (e.g. different languages) on FL in naturally occurring data.
We explore three multilingual language tasks, language modeling, machine translation, and text classification using differing federated and non-federated learning algorithms.
Our results show that using pretrained models reduces the negative effects of FL, helping them to perform near or better than centralized (no privacy) learning, even when using non-IID partitioning.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Our code and data are made publicly available at <a target="_blank" href="https://github.com/orionw/Multilingual-Federated-Learning" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/orionw/Multilingual-Federated-Learning</a></span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>* Authors contributed equally</span></span></span>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) is a machine learning technique that trains a model across multiple distributed clients holding local data samples, without ever storing client data in a central location <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al., <a href="#bib.bib13" title="" class="ltx_ref">2016</a>; McMahan et al., <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>.
These techniques are appealing for those who wish to learn from data in a privacy-preserving way, without ever transmitting the data off of a client device.
FL becomes essential when data is especially sensitive, as is the case at hospitals, legal firms, financial institutions, or in countries that enact legislation concerning data privacy (such as the EU’s GDPR or the US’s HIPAA).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2206.02291/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="166" height="123" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A depiction of different learning strategies with Federated Learning (FL) and multilingual data, with 4 clients and 16 instances from En, Fr, Ru, and Zh in this toy example. Black lines indicate gradient flow. Centralized learning is the standard training method (no privacy), FL with IID data partitions the data into IID data subsets for each client, while FL with non-IID data has the languages separated across clients.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">FL has been applied to problems in natural language processing (NLP) since its inception, particularly in use of the language modeling task <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib39" title="" class="ltx_ref">2018</a>; Hard et al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>; Ramaswamy et al., <a href="#bib.bib29" title="" class="ltx_ref">2019</a>; Chen et al., <a href="#bib.bib4" title="" class="ltx_ref">2019a</a>; Ji et al., <a href="#bib.bib10" title="" class="ltx_ref">2019</a>; Stremmel and Singh, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>. Another large area of FL research is focused on analyzing performance when the data is non identically independently distributed (non-IID). In these cases, many works have shown that FL performance is sub-par with respect to centralized learning methods <cite class="ltx_cite ltx_citemacro_cite">Konečnỳ et al. (<a href="#bib.bib13" title="" class="ltx_ref">2016</a>); Hard et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>); Lin et al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite the large amount of research in FL for NLP, how different languages impact the FL training process has yet to be explored <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>. Furthermore, multilingual FL provides an interesting and natural setting to explore non-IID data, of which different languages are an obvious example.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we explore multilingual federated learning across three multilingual language tasks and different stages of model pretraining. Our results show that fine-tuning pretrained models with FL can perform similarly to pretrained models fine-tuned with the standard centralized method (the no privacy setting), despite having completely non-IID language partitioned data. This finding shows that pretrained models provide an effective way for practitioners (and consumers) of multilingual data to gain the privacy benefits of FL at little or no cost to the final task performance.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The term <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">Federated Learning</span> was first proposed in <cite class="ltx_cite ltx_citemacro_citet">McMahan et al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>, who applied the <span id="S2.p1.1.2" class="ltx_text ltx_font_typewriter">FederatedAveraging</span> algorithm to the tasks of language modeling and image classification.
Since then, much of the theoretical and applied work in FL (e.g. <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019b</a>); Wu et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020</a>)</cite> among many others) has considered language modeling as a key task or benchmark.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Concurrent with the growing interest in Federated Learning, NLP has rapidly shifted toward the use of pretrained language models (PLMs) (e.g., BERT <cite class="ltx_cite ltx_citemacro_citet">Devlin et al. <a href="#bib.bib6" title="" class="ltx_ref">2019</a></cite>; T5 <cite class="ltx_cite ltx_citemacro_citet">Raffel et al. <a href="#bib.bib28" title="" class="ltx_ref">2019</a></cite>; GPT-3 <cite class="ltx_cite ltx_citemacro_citet">Brown et al. <a href="#bib.bib2" title="" class="ltx_ref">2020</a></cite>).
These PLMs are used for both the core task of next word prediction and as a starting point for learning other downstream NLP tasks. This <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">pretrain-and-fine-tune</span> paradigm has since become ubiquitous in modern NLP and has inspired a large and active area of research in model pretraining.
Multilingual versions of these pretrained models have since been developed and are often used with transfer learning techniques to increase performance for tasks where data is limited (e.g. mBERT from <cite class="ltx_cite ltx_citemacro_citet">Devlin et al. <a href="#bib.bib6" title="" class="ltx_ref">2019</a></cite>).</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The intersection of distributed learning from private data partitions
and PLMs is still a nascent area.
Several works have explored more efficient methods of federated communication with the purpose of enabling these larger NLP models for production situations <cite class="ltx_cite ltx_citemacro_cite">Sui et al. (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>); Wu et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>.
Our work is orthogonal to these (and could be combined in future work), as we explore the effects of multilingual data on PLM FL, rather than creating methods to enable their use.
Other papers focus on the gap between federated learning performance and centralized performance, evaluating on a wide variety of English NLP tasks <cite class="ltx_cite ltx_citemacro_cite">Liu and Miller (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>); Lin et al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>); Chen et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>.
Although they focus on differential privacy (DP) rather than FL, <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite> find that direct PLM training is difficult with standard DP methods, but that fine-tuning PLMs on English data is possible with private learning techniques.
We differ from all these works by studying private learning, specifically FL, for PLMs in the novel multilingual setting.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Design</h2>

<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2206.02291/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="159" height="78" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2206.02291/assets/x3.png" id="S3.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="159" height="77" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of the language modeling results. Bars indicate the average language perplexity (PPL) over 8 languages for the Europarl dataset and 6 languages for the UN corpus. Lower is better.</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="9"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Europarl</span></td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="7"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">UN</span></td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">M</span></th>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">En</span></td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.3.1" class="ltx_text ltx_font_bold">Cs</span></td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.4.1" class="ltx_text ltx_font_bold">Lt</span></td>
<td id="S3.T1.1.2.2.5" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.5.1" class="ltx_text ltx_font_bold">Es</span></td>
<td id="S3.T1.1.2.2.6" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.6.1" class="ltx_text ltx_font_bold">Pl</span></td>
<td id="S3.T1.1.2.2.7" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.7.1" class="ltx_text ltx_font_bold">Fi</span></td>
<td id="S3.T1.1.2.2.8" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.8.1" class="ltx_text ltx_font_bold">Pt</span></td>
<td id="S3.T1.1.2.2.9" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.9.1" class="ltx_text ltx_font_bold">De</span></td>
<td id="S3.T1.1.2.2.10" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T1.1.2.2.10.1" class="ltx_text ltx_font_bold">Avg</span></td>
<td id="S3.T1.1.2.2.11" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.11.1" class="ltx_text ltx_font_bold">En</span></td>
<td id="S3.T1.1.2.2.12" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.12.1" class="ltx_text ltx_font_bold">Es</span></td>
<td id="S3.T1.1.2.2.13" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.13.1" class="ltx_text ltx_font_bold">Fr</span></td>
<td id="S3.T1.1.2.2.14" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.14.1" class="ltx_text ltx_font_bold">Ru</span></td>
<td id="S3.T1.1.2.2.15" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.15.1" class="ltx_text ltx_font_bold">Zh</span></td>
<td id="S3.T1.1.2.2.16" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.16.1" class="ltx_text ltx_font_bold">Ar</span></td>
<td id="S3.T1.1.2.2.17" class="ltx_td ltx_align_right"><span id="S3.T1.1.2.2.17.1" class="ltx_text ltx_font_bold">Avg</span></td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<th id="S3.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">B</th>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_right ltx_border_t">26.2</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t">34.8</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t">40.1</td>
<td id="S3.T1.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t">20.0</td>
<td id="S3.T1.1.3.3.6" class="ltx_td ltx_align_right ltx_border_t">20.0</td>
<td id="S3.T1.1.3.3.7" class="ltx_td ltx_align_right ltx_border_t">26.6</td>
<td id="S3.T1.1.3.3.8" class="ltx_td ltx_align_right ltx_border_t">25.5</td>
<td id="S3.T1.1.3.3.9" class="ltx_td ltx_align_right ltx_border_t">22.1</td>
<td id="S3.T1.1.3.3.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">26.9</td>
<td id="S3.T1.1.3.3.11" class="ltx_td ltx_align_right ltx_border_t">22.3</td>
<td id="S3.T1.1.3.3.12" class="ltx_td ltx_align_right ltx_border_t">15.0</td>
<td id="S3.T1.1.3.3.13" class="ltx_td ltx_align_right ltx_border_t">17.2</td>
<td id="S3.T1.1.3.3.14" class="ltx_td ltx_align_right ltx_border_t">9.8</td>
<td id="S3.T1.1.3.3.15" class="ltx_td ltx_align_right ltx_border_t">18.1</td>
<td id="S3.T1.1.3.3.16" class="ltx_td ltx_align_right ltx_border_t">14.7</td>
<td id="S3.T1.1.3.3.17" class="ltx_td ltx_align_right ltx_border_t">16.2</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<th id="S3.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">C</th>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.2.1" class="ltx_text ltx_font_bold">19.3</span></td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.3.1" class="ltx_text ltx_font_bold">4.5</span></td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.4.1" class="ltx_text ltx_font_bold">3.9</span></td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.5.1" class="ltx_text ltx_font_bold">8.3</span></td>
<td id="S3.T1.1.4.4.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.6.1" class="ltx_text ltx_font_bold">4.7</span></td>
<td id="S3.T1.1.4.4.7" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.7.1" class="ltx_text ltx_font_bold">4.9</span></td>
<td id="S3.T1.1.4.4.8" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.8.1" class="ltx_text ltx_font_bold">7.0</span></td>
<td id="S3.T1.1.4.4.9" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.9.1" class="ltx_text ltx_font_bold">10.8</span></td>
<td id="S3.T1.1.4.4.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.4.4.10.1" class="ltx_text ltx_font_bold">7.9</span></td>
<td id="S3.T1.1.4.4.11" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.11.1" class="ltx_text ltx_font_bold">9.0</span></td>
<td id="S3.T1.1.4.4.12" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.12.1" class="ltx_text ltx_font_bold">5.2</span></td>
<td id="S3.T1.1.4.4.13" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.4.4.13.1" class="ltx_text ltx_font_bold">8.2</span></td>
<td id="S3.T1.1.4.4.14" class="ltx_td ltx_align_right ltx_border_t">*3.9</td>
<td id="S3.T1.1.4.4.15" class="ltx_td ltx_align_right ltx_border_t">*4.3</td>
<td id="S3.T1.1.4.4.16" class="ltx_td ltx_align_right ltx_border_t">*4.6</td>
<td id="S3.T1.1.4.4.17" class="ltx_td ltx_align_right ltx_border_t">*5.9</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<th id="S3.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">I</th>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_right">26.6</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_right">5.4</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_right">*4.3</td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_right">11.2</td>
<td id="S3.T1.1.5.5.6" class="ltx_td ltx_align_right">5.8</td>
<td id="S3.T1.1.5.5.7" class="ltx_td ltx_align_right">5.7</td>
<td id="S3.T1.1.5.5.8" class="ltx_td ltx_align_right">8.9</td>
<td id="S3.T1.1.5.5.9" class="ltx_td ltx_align_right">15.1</td>
<td id="S3.T1.1.5.5.10" class="ltx_td ltx_align_right ltx_border_r">10.4</td>
<td id="S3.T1.1.5.5.11" class="ltx_td ltx_align_right">*9.1</td>
<td id="S3.T1.1.5.5.12" class="ltx_td ltx_align_right"><span id="S3.T1.1.5.5.12.1" class="ltx_text ltx_font_bold">5.2</span></td>
<td id="S3.T1.1.5.5.13" class="ltx_td ltx_align_right">*8.4</td>
<td id="S3.T1.1.5.5.14" class="ltx_td ltx_align_right"><span id="S3.T1.1.5.5.14.1" class="ltx_text ltx_font_bold">3.7</span></td>
<td id="S3.T1.1.5.5.15" class="ltx_td ltx_align_right"><span id="S3.T1.1.5.5.15.1" class="ltx_text ltx_font_bold">3.9</span></td>
<td id="S3.T1.1.5.5.16" class="ltx_td ltx_align_right"><span id="S3.T1.1.5.5.16.1" class="ltx_text ltx_font_bold">4.5</span></td>
<td id="S3.T1.1.5.5.17" class="ltx_td ltx_align_right"><span id="S3.T1.1.5.5.17.1" class="ltx_text ltx_font_bold">5.8</span></td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<th id="S3.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">N</th>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_right">50.6</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_right">7.1</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_right">11.9</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_right">16.0</td>
<td id="S3.T1.1.6.6.6" class="ltx_td ltx_align_right">17.7</td>
<td id="S3.T1.1.6.6.7" class="ltx_td ltx_align_right">12.1</td>
<td id="S3.T1.1.6.6.8" class="ltx_td ltx_align_right">35.6</td>
<td id="S3.T1.1.6.6.9" class="ltx_td ltx_align_right">21.7</td>
<td id="S3.T1.1.6.6.10" class="ltx_td ltx_align_right ltx_border_r">21.6</td>
<td id="S3.T1.1.6.6.11" class="ltx_td ltx_align_right">12.8</td>
<td id="S3.T1.1.6.6.12" class="ltx_td ltx_align_right">11.5</td>
<td id="S3.T1.1.6.6.13" class="ltx_td ltx_align_right">14.6</td>
<td id="S3.T1.1.6.6.14" class="ltx_td ltx_align_right">9.3</td>
<td id="S3.T1.1.6.6.15" class="ltx_td ltx_align_right">8.2</td>
<td id="S3.T1.1.6.6.16" class="ltx_td ltx_align_right">8.3</td>
<td id="S3.T1.1.6.6.17" class="ltx_td ltx_align_right">10.8</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<th id="S3.T1.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">C</th>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_right ltx_border_t">12.1</td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.7.7.3.1" class="ltx_text ltx_font_bold">3.7</span></td>
<td id="S3.T1.1.7.7.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.7.7.4.1" class="ltx_text ltx_font_bold">3.3</span></td>
<td id="S3.T1.1.7.7.5" class="ltx_td ltx_align_right ltx_border_t">13.9</td>
<td id="S3.T1.1.7.7.6" class="ltx_td ltx_align_right ltx_border_t">4.7</td>
<td id="S3.T1.1.7.7.7" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.7.7.7.1" class="ltx_text ltx_font_bold">4.0</span></td>
<td id="S3.T1.1.7.7.8" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.7.7.8.1" class="ltx_text ltx_font_bold">4.8</span></td>
<td id="S3.T1.1.7.7.9" class="ltx_td ltx_align_right ltx_border_t">*6.8</td>
<td id="S3.T1.1.7.7.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">6.7</td>
<td id="S3.T1.1.7.7.11" class="ltx_td ltx_align_right ltx_border_t">*7.0</td>
<td id="S3.T1.1.7.7.12" class="ltx_td ltx_align_right ltx_border_t">*4.1</td>
<td id="S3.T1.1.7.7.13" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.7.7.13.1" class="ltx_text ltx_font_bold">4.9</span></td>
<td id="S3.T1.1.7.7.14" class="ltx_td ltx_align_right ltx_border_t">*2.9</td>
<td id="S3.T1.1.7.7.15" class="ltx_td ltx_align_right ltx_border_t">*3.3</td>
<td id="S3.T1.1.7.7.16" class="ltx_td ltx_align_right ltx_border_t">*3.6</td>
<td id="S3.T1.1.7.7.17" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.7.7.17.1" class="ltx_text ltx_font_bold">4.3</span></td>
</tr>
<tr id="S3.T1.1.8.8" class="ltx_tr">
<th id="S3.T1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">I</th>
<td id="S3.T1.1.8.8.2" class="ltx_td ltx_align_right">10.5</td>
<td id="S3.T1.1.8.8.3" class="ltx_td ltx_align_right">*4.0</td>
<td id="S3.T1.1.8.8.4" class="ltx_td ltx_align_right">4.2</td>
<td id="S3.T1.1.8.8.5" class="ltx_td ltx_align_right">*6.1</td>
<td id="S3.T1.1.8.8.6" class="ltx_td ltx_align_right"><span id="S3.T1.1.8.8.6.1" class="ltx_text ltx_font_bold">3.8</span></td>
<td id="S3.T1.1.8.8.7" class="ltx_td ltx_align_right">4.5</td>
<td id="S3.T1.1.8.8.8" class="ltx_td ltx_align_right">*5.6</td>
<td id="S3.T1.1.8.8.9" class="ltx_td ltx_align_right">*6.9</td>
<td id="S3.T1.1.8.8.10" class="ltx_td ltx_align_right ltx_border_r">*5.7</td>
<td id="S3.T1.1.8.8.11" class="ltx_td ltx_align_right"><span id="S3.T1.1.8.8.11.1" class="ltx_text ltx_font_bold">6.5</span></td>
<td id="S3.T1.1.8.8.12" class="ltx_td ltx_align_right"><span id="S3.T1.1.8.8.12.1" class="ltx_text ltx_font_bold">3.9</span></td>
<td id="S3.T1.1.8.8.13" class="ltx_td ltx_align_right">5.7</td>
<td id="S3.T1.1.8.8.14" class="ltx_td ltx_align_right"><span id="S3.T1.1.8.8.14.1" class="ltx_text ltx_font_bold">2.8</span></td>
<td id="S3.T1.1.8.8.15" class="ltx_td ltx_align_right"><span id="S3.T1.1.8.8.15.1" class="ltx_text ltx_font_bold">3.2</span></td>
<td id="S3.T1.1.8.8.16" class="ltx_td ltx_align_right"><span id="S3.T1.1.8.8.16.1" class="ltx_text ltx_font_bold">3.5</span></td>
<td id="S3.T1.1.8.8.17" class="ltx_td ltx_align_right"><span id="S3.T1.1.8.8.17.1" class="ltx_text ltx_font_bold">4.3</span></td>
</tr>
<tr id="S3.T1.1.9.9" class="ltx_tr">
<th id="S3.T1.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">N</th>
<td id="S3.T1.1.9.9.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T1.1.9.9.2.1" class="ltx_text ltx_font_bold">8.8</span></td>
<td id="S3.T1.1.9.9.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T1.1.9.9.3.1" class="ltx_text ltx_font_bold">3.7</span></td>
<td id="S3.T1.1.9.9.4" class="ltx_td ltx_align_right ltx_border_bb">3.9</td>
<td id="S3.T1.1.9.9.5" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T1.1.9.9.5.1" class="ltx_text ltx_font_bold">6.0</span></td>
<td id="S3.T1.1.9.9.6" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T1.1.9.9.6.1" class="ltx_text ltx_font_bold">3.8</span></td>
<td id="S3.T1.1.9.9.7" class="ltx_td ltx_align_right ltx_border_bb">*4.4</td>
<td id="S3.T1.1.9.9.8" class="ltx_td ltx_align_right ltx_border_bb">*5.6</td>
<td id="S3.T1.1.9.9.9" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T1.1.9.9.9.1" class="ltx_text ltx_font_bold">6.7</span></td>
<td id="S3.T1.1.9.9.10" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r"><span id="S3.T1.1.9.9.10.1" class="ltx_text ltx_font_bold">5.4</span></td>
<td id="S3.T1.1.9.9.11" class="ltx_td ltx_align_right ltx_border_bb">*7.1</td>
<td id="S3.T1.1.9.9.12" class="ltx_td ltx_align_right ltx_border_bb">4.5</td>
<td id="S3.T1.1.9.9.13" class="ltx_td ltx_align_right ltx_border_bb">6.2</td>
<td id="S3.T1.1.9.9.14" class="ltx_td ltx_align_right ltx_border_bb">*3.2</td>
<td id="S3.T1.1.9.9.15" class="ltx_td ltx_align_right ltx_border_bb">4.2</td>
<td id="S3.T1.1.9.9.16" class="ltx_td ltx_align_right ltx_border_bb">4.0</td>
<td id="S3.T1.1.9.9.17" class="ltx_td ltx_align_right ltx_border_bb">*4.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results for FL experiments on the LM task. Bold scores indicate the best in the column for the given section. Scores are measured in perplexity (lower is better). The top row (B) is a baseline using the pretrained model with no fine-tuning. The middle rows are trained from <span id="S3.T1.3.1" class="ltx_text ltx_font_italic">randomly-initialized</span> models while the bottom rows tune the pretrained model on task data. Due to space we abbreviate: C for Centralized, I for IID FL, and N for non-IID FL.
We sample the mask distribution with 5 seeds and report the mean (standard deviations can be found in the Appendix, Tables <a href="#A4.T4" title="Table 4 ‣ Appendix D Full LM Results ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#A4.T5" title="Table 5 ‣ Appendix D Full LM Results ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). Asterisks indicate scores within 2 standard deviations of the best.
</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Federated Learning Methods</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">FederatedAveraging</span> as the primary learning algorithm <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>.
<span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">FederatedAveraging</span> was introduced alongside the term Federated Learning and has been studied in both learning theory research <cite class="ltx_cite ltx_citemacro_cite">Stich (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> and applied work <cite class="ltx_cite ltx_citemacro_cite">Hard et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>); Lin et al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>.
In this algorithm, each client runs stochastic gradient descent (SGD) on its local data.
After a specified number of steps, the client transmits its local model to the server, which averages these updates into a single centralized set of parameters. The server then broadcasts the centralized parameters to each client and the process repeats.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Client Partitioning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We consider three different training settings: standard training with no FL (e.g. <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">centralized</span> or <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">C</span>), FL with IID data (<span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">FL IID</span> or <span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_italic">I</span>), where the data for each client is sampled randomly from all data, and FL with non-IID data (<span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_italic">FL non-IID</span> or <span id="S3.SS2.p1.1.6" class="ltx_text ltx_font_italic">N</span>) where each client only sees data for one language (or for MT, one direction). See <a href="#S1.F1" title="In 1 Introduction ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> for a visual depiction of these three client partitioning schemes.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We study three multilingual language tasks, due to their common use in the community: language modeling (LM), machine translation (MT), and text classification (TC). We note that the data we use for training is relatively small; however, this mirrors pratical FL, as each client will not have a large amount of data. We measure scores using perplexity (PPL) for LM,
BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a href="#bib.bib25" title="" class="ltx_ref">2002</a>)</cite> for MT, and accuracy for TC.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Europarl</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">We use the Europarl corpus <cite class="ltx_cite ltx_citemacro_citep">(Koehn et al., <a href="#bib.bib12" title="" class="ltx_ref">2005</a>)</cite> taken from transcripts of European Union meetings. We sample data from eight languages: English, Spanish, Portuguese, French, German, Finnish, Polish, Lithuanian, and Czech. We sample 20k of each language for training and 5k for validation/testing, and use it for the LM task.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">MTNT</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.2" class="ltx_p">We use the Machine Translation of Noisy Text (MTNT) dataset <cite class="ltx_cite ltx_citemacro_cite">Michel and Neubig (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>, which was the testset for the 2019 WMT robustness challenge.
MTNT was gathered from user comments on Reddit discussion threads and contains noisy text including typos, casual language, and niche terminology. The dataset contains two non-English languages that we use: En <math id="S3.SS3.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.1.m1.1a"><mo stretchy="false" id="S3.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.1.m1.1c">\rightarrow</annotation></semantics></math> Fr and En <math id="S3.SS3.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.2.m2.1a"><mo stretchy="false" id="S3.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.2.m2.1c">\rightarrow</annotation></semantics></math> Ja.
This dataset has been used to test MT systems for robustness to domain shift <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> and is suitable for our experiments since FL deals with client data that is uniquely shifted from centralized data.
For more details on MTNT data preprocessing for M2M-100, see <a href="#A3" title="Appendix C MTNT Data Preprocessing for M2M-100 ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">UN Corpus</h4>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px3.p1.3" class="ltx_p">The UN Corpus <cite class="ltx_cite ltx_citemacro_cite">Ziemski et al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite> consists of official records from the UN proceedings over the years 1990 to 2014, in six languages: English, French, Spanish, Russian, Chinese, and Arabic. We use this data for LM (with 50k instances of training data per language and 5k for validation/testing) as well as three MT directions covering 6 languages (En <math id="S3.SS3.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.1.m1.1a"><mo stretchy="false" id="S3.SS3.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.1.m1.1c">\rightarrow</annotation></semantics></math> Fr, Ar <math id="S3.SS3.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.2.m2.1a"><mo stretchy="false" id="S3.SS3.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.2.m2.1c">\rightarrow</annotation></semantics></math> Es, Ru <math id="S3.SS3.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.3.m3.1a"><mo stretchy="false" id="S3.SS3.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px3.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.3.m3.1c">\rightarrow</annotation></semantics></math> Zh). Following previous work in MT adaption (see MTNT above) we sample 10k in each direction for training and 5k each for evaluation sets.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">NC Corpus</h4>

<div id="S3.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px4.p1.1" class="ltx_p">For text classification we use the News Classification (NC) dataset from the XGLUE benchmark for cross-lingual language understanding <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>. This is a classification problem with 10 classes across 5 languages: English, Spanish, French, German, and Russian. We predict the article category given the article title and body (e.g. finance, sports, travel). Since only 10k annotated examples are available for each language (excluding the official test set), we sample 8k instances for training and 1k for evaluation sets. Note that although XGLUE is made for cross-lingual evaluation, we use it for multilingual evaluation.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">MTNT</span></td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">UN</span></td>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T2.1.2.2.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S3.T2.1.2.2.2" class="ltx_td ltx_align_right"><span id="S3.T2.1.2.2.2.1" class="ltx_text ltx_font_bold">En-Fr</span></td>
<td id="S3.T2.1.2.2.3" class="ltx_td ltx_align_right"><span id="S3.T2.1.2.2.3.1" class="ltx_text ltx_font_bold">En-Ja</span></td>
<td id="S3.T2.1.2.2.4" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T2.1.2.2.4.1" class="ltx_text ltx_font_bold">Avg</span></td>
<td id="S3.T2.1.2.2.5" class="ltx_td ltx_align_right"><span id="S3.T2.1.2.2.5.1" class="ltx_text ltx_font_bold">En-Fr</span></td>
<td id="S3.T2.1.2.2.6" class="ltx_td ltx_align_right"><span id="S3.T2.1.2.2.6.1" class="ltx_text ltx_font_bold">Ar-Es</span></td>
<td id="S3.T2.1.2.2.7" class="ltx_td ltx_align_right"><span id="S3.T2.1.2.2.7.1" class="ltx_text ltx_font_bold">Ru-Zh</span></td>
<td id="S3.T2.1.2.2.8" class="ltx_td ltx_align_right"><span id="S3.T2.1.2.2.8.1" class="ltx_text ltx_font_bold">Avg</span></td>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<td id="S3.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No Training</td>
<td id="S3.T2.1.3.3.2" class="ltx_td ltx_align_right ltx_border_t">30.7</td>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t">14.1</td>
<td id="S3.T2.1.3.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">22.4</td>
<td id="S3.T2.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t">31.4</td>
<td id="S3.T2.1.3.3.6" class="ltx_td ltx_align_right ltx_border_t">27.4</td>
<td id="S3.T2.1.3.3.7" class="ltx_td ltx_align_right ltx_border_t">27.9</td>
<td id="S3.T2.1.3.3.8" class="ltx_td ltx_align_right ltx_border_t">28.9</td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<td id="S3.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Centralized</td>
<td id="S3.T2.1.4.4.2" class="ltx_td ltx_align_right ltx_border_t">31.8</td>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_right ltx_border_t">*15.4</td>
<td id="S3.T2.1.4.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">23.6</td>
<td id="S3.T2.1.4.4.5" class="ltx_td ltx_align_right ltx_border_t">37.3</td>
<td id="S3.T2.1.4.4.6" class="ltx_td ltx_align_right ltx_border_t">35.9</td>
<td id="S3.T2.1.4.4.7" class="ltx_td ltx_align_right ltx_border_t">34.1</td>
<td id="S3.T2.1.4.4.8" class="ltx_td ltx_align_right ltx_border_t">35.8</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<td id="S3.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r">IID FL</td>
<td id="S3.T2.1.5.5.2" class="ltx_td ltx_align_right"><span id="S3.T2.1.5.5.2.1" class="ltx_text ltx_font_bold">33.1</span></td>
<td id="S3.T2.1.5.5.3" class="ltx_td ltx_align_right"><span id="S3.T2.1.5.5.3.1" class="ltx_text ltx_font_bold">15.6</span></td>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_right ltx_border_r"><span id="S3.T2.1.5.5.4.1" class="ltx_text ltx_font_bold">24.4</span></td>
<td id="S3.T2.1.5.5.5" class="ltx_td ltx_align_right"><span id="S3.T2.1.5.5.5.1" class="ltx_text ltx_font_bold">38.6</span></td>
<td id="S3.T2.1.5.5.6" class="ltx_td ltx_align_right"><span id="S3.T2.1.5.5.6.1" class="ltx_text ltx_font_bold">36.9</span></td>
<td id="S3.T2.1.5.5.7" class="ltx_td ltx_align_right">*35.6</td>
<td id="S3.T2.1.5.5.8" class="ltx_td ltx_align_right"><span id="S3.T2.1.5.5.8.1" class="ltx_text ltx_font_bold">37.0</span></td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<td id="S3.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">non-IID FL</td>
<td id="S3.T2.1.6.6.2" class="ltx_td ltx_align_right ltx_border_bb">*32.9</td>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T2.1.6.6.3.1" class="ltx_text ltx_font_bold">15.6</span></td>
<td id="S3.T2.1.6.6.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">24.3</td>
<td id="S3.T2.1.6.6.5" class="ltx_td ltx_align_right ltx_border_bb">37.9</td>
<td id="S3.T2.1.6.6.6" class="ltx_td ltx_align_right ltx_border_bb">*36.6</td>
<td id="S3.T2.1.6.6.7" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T2.1.6.6.7.1" class="ltx_text ltx_font_bold">35.7</span></td>
<td id="S3.T2.1.6.6.8" class="ltx_td ltx_align_right ltx_border_bb">36.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results for FL experiments on the Machine Translation task. Bold scores indicate the best in the column, while asterisks indicate scores that are statistically similar to the best according to a paired bootstrap resampling test. Scores are measured with <span id="S3.T2.3.1" class="ltx_text ltx_font_typewriter">sacreBLEU</span> <cite class="ltx_cite ltx_citemacro_cite">Post (<a href="#bib.bib27" title="" class="ltx_ref">2018</a>)</cite>, higher is better.</figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.3.1.1" class="ltx_tr">
<th id="S3.T3.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T3.3.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S3.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.3.1.1.2.1" class="ltx_text ltx_font_bold">En</span></th>
<th id="S3.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.3.1.1.3.1" class="ltx_text ltx_font_bold">Es</span></th>
<th id="S3.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.3.1.1.4.1" class="ltx_text ltx_font_bold">Fr</span></th>
<th id="S3.T3.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.3.1.1.5.1" class="ltx_text ltx_font_bold">De</span></th>
<th id="S3.T3.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.3.1.1.6.1" class="ltx_text ltx_font_bold">Ru</span></th>
<th id="S3.T3.3.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.3.1.1.7.1" class="ltx_text ltx_font_bold">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.3.2.1" class="ltx_tr">
<th id="S3.T3.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Centralized</th>
<td id="S3.T3.3.2.1.2" class="ltx_td ltx_align_right ltx_border_t">86.6 ± 0.3</td>
<td id="S3.T3.3.2.1.3" class="ltx_td ltx_align_right ltx_border_t">77.5 ± 1.2</td>
<td id="S3.T3.3.2.1.4" class="ltx_td ltx_align_right ltx_border_t">74.9 ± 1.6</td>
<td id="S3.T3.3.2.1.5" class="ltx_td ltx_align_right ltx_border_t">*82.3 ± 1.6</td>
<td id="S3.T3.3.2.1.6" class="ltx_td ltx_align_right ltx_border_t">80.7 ± 0.7</td>
<td id="S3.T3.3.2.1.7" class="ltx_td ltx_align_right ltx_border_t">80.4 ± 0.6</td>
</tr>
<tr id="S3.T3.3.3.2" class="ltx_tr">
<th id="S3.T3.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">IID FL</th>
<td id="S3.T3.3.3.2.2" class="ltx_td ltx_align_right">
<span id="S3.T3.3.3.2.2.1" class="ltx_text ltx_font_bold">88.0</span> ± 0.6</td>
<td id="S3.T3.3.3.2.3" class="ltx_td ltx_align_right">
<span id="S3.T3.3.3.2.3.1" class="ltx_text ltx_font_bold">79.8</span> ± 0.5</td>
<td id="S3.T3.3.3.2.4" class="ltx_td ltx_align_right">
<span id="S3.T3.3.3.2.4.1" class="ltx_text ltx_font_bold">76.4</span> ± 0.6</td>
<td id="S3.T3.3.3.2.5" class="ltx_td ltx_align_right">
<span id="S3.T3.3.3.2.5.1" class="ltx_text ltx_font_bold">82.6</span> ± 0.6</td>
<td id="S3.T3.3.3.2.6" class="ltx_td ltx_align_right">
<span id="S3.T3.3.3.2.6.1" class="ltx_text ltx_font_bold">82.5</span> ± 0.4</td>
<td id="S3.T3.3.3.2.7" class="ltx_td ltx_align_right">
<span id="S3.T3.3.3.2.7.1" class="ltx_text ltx_font_bold">81.8</span> ± 0.3</td>
</tr>
<tr id="S3.T3.3.4.3" class="ltx_tr">
<th id="S3.T3.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">non-IID FL</th>
<td id="S3.T3.3.4.3.2" class="ltx_td ltx_align_right">81.0 ± 0.9</td>
<td id="S3.T3.3.4.3.3" class="ltx_td ltx_align_right">69.3 ± 1.6</td>
<td id="S3.T3.3.4.3.4" class="ltx_td ltx_align_right">73.7 ± 1.0</td>
<td id="S3.T3.3.4.3.5" class="ltx_td ltx_align_right">76.0 ± 0.3</td>
<td id="S3.T3.3.4.3.6" class="ltx_td ltx_align_right">71.9 ± 1.1</td>
<td id="S3.T3.3.4.3.7" class="ltx_td ltx_align_right">74.4 ± 0.5</td>
</tr>
<tr id="S3.T3.3.5.4" class="ltx_tr">
<th id="S3.T3.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Centralized</th>
<td id="S3.T3.3.5.4.2" class="ltx_td ltx_align_right ltx_border_t">93.5 ± 0.7</td>
<td id="S3.T3.3.5.4.3" class="ltx_td ltx_align_right ltx_border_t">*86.3 ± 0.5</td>
<td id="S3.T3.3.5.4.4" class="ltx_td ltx_align_right ltx_border_t">
<span id="S3.T3.3.5.4.4.1" class="ltx_text ltx_font_bold">82.9</span> ± 0.3</td>
<td id="S3.T3.3.5.4.5" class="ltx_td ltx_align_right ltx_border_t">
<span id="S3.T3.3.5.4.5.1" class="ltx_text ltx_font_bold">89.6</span> ± 0.1</td>
<td id="S3.T3.3.5.4.6" class="ltx_td ltx_align_right ltx_border_t">*88.5 ± 0.4</td>
<td id="S3.T3.3.5.4.7" class="ltx_td ltx_align_right ltx_border_t">*88.1 ± 0.2</td>
</tr>
<tr id="S3.T3.3.6.5" class="ltx_tr">
<th id="S3.T3.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">IID FL</th>
<td id="S3.T3.3.6.5.2" class="ltx_td ltx_align_right">
<span id="S3.T3.3.6.5.2.1" class="ltx_text ltx_font_bold">94.0</span> ± 0.2</td>
<td id="S3.T3.3.6.5.3" class="ltx_td ltx_align_right">
<span id="S3.T3.3.6.5.3.1" class="ltx_text ltx_font_bold">86.9</span> ± 1.1</td>
<td id="S3.T3.3.6.5.4" class="ltx_td ltx_align_right">82.1 ± 0.7</td>
<td id="S3.T3.3.6.5.5" class="ltx_td ltx_align_right">
<span id="S3.T3.3.6.5.5.1" class="ltx_text ltx_font_bold">89.6</span> ± 0.2</td>
<td id="S3.T3.3.6.5.6" class="ltx_td ltx_align_right">
<span id="S3.T3.3.6.5.6.1" class="ltx_text ltx_font_bold">89.1</span> ± 1.2</td>
<td id="S3.T3.3.6.5.7" class="ltx_td ltx_align_right">
<span id="S3.T3.3.6.5.7.1" class="ltx_text ltx_font_bold">88.3</span> ± 0.3</td>
</tr>
<tr id="S3.T3.3.7.6" class="ltx_tr">
<th id="S3.T3.3.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">non-IID FL</th>
<td id="S3.T3.3.7.6.2" class="ltx_td ltx_align_right ltx_border_bb">92.5 ± 0.1</td>
<td id="S3.T3.3.7.6.3" class="ltx_td ltx_align_right ltx_border_bb">*86.1 ± 0.6</td>
<td id="S3.T3.3.7.6.4" class="ltx_td ltx_align_right ltx_border_bb">81.4 ± 0.3</td>
<td id="S3.T3.3.7.6.5" class="ltx_td ltx_align_right ltx_border_bb">88.8 ± 0.1</td>
<td id="S3.T3.3.7.6.6" class="ltx_td ltx_align_right ltx_border_bb">84.5 ± 0.7</td>
<td id="S3.T3.3.7.6.7" class="ltx_td ltx_align_right ltx_border_bb">86.7 ± 0.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results for FL experiments on the Text Classification task. Bold scores indicate the best in the column, while asterisks indicate scores within two standard deviations of the best. Scores are the mean of training with 3 different seeds, <math id="S3.T3.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T3.2.m1.1b"><mo id="S3.T3.2.m1.1.1" xref="S3.T3.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.m1.1c"><csymbol cd="latexml" id="S3.T3.2.m1.1.1.cmml" xref="S3.T3.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.m1.1d">\pm</annotation></semantics></math> denotes the standard deviation. Scores are measured with accuracy, higher is better. The top rows are trained from random initialization while the bottom rows initialize from the pretrained model.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Modeling</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">For language modeling and text classification, we examine two different initialization settings: (1) fine-tuning from a pretrained multilingual model or (2) training the same multilingual model architecture but doing so with randomly initialized weights. For the MT experiments, we omit the <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">randomly-initialized</span> results as MT systems generally need large amounts of data to produce good results (see <a href="#A2" title="Appendix B Randomly Initialized MT ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a> for more details).</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Our base model for the LM task is a distilled version of the mBERT model (134M parameters), shown to perform well across many languages <cite class="ltx_cite ltx_citemacro_citep">(Sanh et al., <a href="#bib.bib30" title="" class="ltx_ref">2019</a>; Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite> while being smaller than the full mBERT.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We note that mBERT uses masked language modeling (MLM) instead of standard language modeling, however, for the purposes of our analysis (as we do not seek to compare direct scores to previous work) MLM suffices. Furthermore, most multilingual PLMs train via some version of MLM.</span></span></span> For MT, we use the M2M-100 model <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> with 418M parameters, a many-to-many MT model that can translate between any pairing of 100 languages. For text classification, we use the XLM-RoBERTa base sized model (270M parameters).
We note that although there are other PLMs to consider, we focus on testing a varied set of commonly used, high-performing PLMs.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Training</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">We use the Flower framework <cite class="ltx_cite ltx_citemacro_cite">Beutel et al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> for federated training and evaluation due to its ease of use and strong community support. We use Hugging Face’s <span id="S3.SS5.p1.1.1" class="ltx_text ltx_font_italic">transformers</span> library <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite> for loading pretrained models and PyTorch as the underlying differentiation framework <cite class="ltx_cite ltx_citemacro_cite">Paszke et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>. We train each LM model for 100 epochs if pretrained or 200 epochs if randomly initialized. For MT, we train for 25 epochs and for TC we train for 10 epochs if pretrained and 50 epochs if randomly initialized. For other hyperparameters and compute settings, see <a href="#A1" title="Appendix A Hyperparameters ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Language Modeling</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">In <a href="#S3.F2" title="In 3 Experimental Design ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> we see the overall results of the language modeling task across the two datasets. As expected, the randomly initialized models perform much worse than the pretrained models. The gap between between FL and centralized methods is smaller when using pretrained models, indicating that pretrained models are an effective initialization for federated learning.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p">In <a href="#S3.T1" title="In 3 Experimental Design ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> we show results broken down by language. Since the fine-tuning task is the same as the pretraining objective (masked language modeling), we can use the pretrained model as a baseline (top row, B).
In the randomly initialized category, the centralized model is the same or better than the FL methods in every single language, across both datasets.
In the pretrained section the results are more mixed, with the centralized model winning or tying in 5 of the 8 Europarl languages and obtaining similar scores on the UN corpus.
We also see that the randomly initialized non-IID model appears to diverge for some of the Europarl languages.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p3.1" class="ltx_p">Examining the difference between IID FL and non-IID FL, we see that IID FL performs better on average in three of the four settings. However, when initializing with a pretrained model, the performance gap narrows.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Machine Translation</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p"><a href="#S3.T2" title="In NC Corpus ‣ 3.3 Data ‣ 3 Experimental Design ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> exhibits results on tuning a machine translation model on a domain specific dataset. We see that on the MTNT dataset, both FL algorithms actually outperform centralized learning (24.4 avg. BLEU for IID FL vs 23.6 for Centralized). The scores on Japanese are very similar for all models, possibly reflecting the difficulty of the task. On the UN corpus, we see again that the IID FL model performs best.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p2.1" class="ltx_p">Since the fine-tuning task matches the original M2M-100 task, we can use the pretrained model directly as a baseline. In all cases, fine-tuning shows an improvement (first row, No Training baseline). Note that our scores are not directly comparable to other work as we use a smaller training set.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text Classification</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p"><a href="#S3.T3" title="In NC Corpus ‣ 3.3 Data ‣ 3 Experimental Design ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows results on text classification. We see that when initialized randomly, non-IID FL shows a large drop in performance compared to the two other methods (i.e. more than 5 points worse than the Centralized method).
Initializing with the pretrained model yields a modest though consistent improvement for all three models (80.4% accuracy vs 88.3% accuracy for Centralized).<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Note that although the setups are not the same (e.g. XGLUE is cross-lingual rather than multilingual) our scores are slightly higher than those reported in the original paper.</span></span></span> Furthermore, with a pretrained initialization the non-IID FL method scores become significantly closer to the other two methods, with less than a two point difference between them (86.7% non-IID FL vs 88.3% IID FL).</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Discussion</h4>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p1.1" class="ltx_p">Our examination of multilingual FL indicates that performance is similar when pretrained models are used.
Despite the fact that local models are averaged together, non-IID data partitioning (where each client sees only one language) has only a small impact on final multilingual performance, when using pretrained models.
These findings suggest that, when possible, practitioners who need multilingual federated learning should employ pretrained models in order to gain the privacy benefits of federated learning, without taking much (if any) of a performance loss to do so.</p>
</div>
<div id="S4.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p2.1" class="ltx_p">In several cases, we found that IID FL or non-IID FL could even outperform centralized learning. We leave investigation of this phenomena for future work but note a couple of possible explanations.
First, FL with <span id="S4.SS0.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_typewriter">FederatedAveraging</span> may have similar implicit regularization effects to checkpoint averaging, a common technique when using transformer models (noted in <cite class="ltx_cite ltx_citemacro_citet">Vaswani et al. <a href="#bib.bib35" title="" class="ltx_ref">2017</a></cite>, <cite class="ltx_cite ltx_citemacro_citet">Edunov et al. <a href="#bib.bib7" title="" class="ltx_ref">2018</a></cite>, etc.).
Furthermore, there may be other regularization effects during federated fine-tuning, as transformer training is known to be unstable and sensitive to optimization choices (<cite class="ltx_cite ltx_citemacro_citet">Mosbach et al. <a href="#bib.bib23" title="" class="ltx_ref">2020</a></cite>, <cite class="ltx_cite ltx_citemacro_citet">Nguyen and Salazar <a href="#bib.bib24" title="" class="ltx_ref">2019</a></cite>).</p>
</div>
<div id="S4.SS0.SSS0.Px4.p3" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p3.1" class="ltx_p">Overall, our analysis shows that our conclusions hold for different multilingual models, on disparate NLP tasks, and across 13 different languages. We acknowledge that the languages used in this study are generally considered higher-resource, but expect that these conclusions will continue to hold as long as the pretrained model is effective on the target language (or language pairs, for MT).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work we provided the first analysis of multilingual language data on federated learning algorithms.
We found that fine-tuning a pretrained model with FL methods can yield similar performance to centralized learning, even when clients are partitioned by language (non-IID FL).</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">However, models trained from random initializations still show a large gap between centralized and federated learning.
Our results suggest that learning on private partitioned data is possible without having to incur a large performance penalty.
We hope that these results will aid practitioners in using FL (and also downstream consumers) and inspire the broader community to consider multilingual data in future federated learning research for natural language processing.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel et al. (2020)</span>
<span class="ltx_bibblock">
Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet,
Pedro PB de Gusmão, and Nicholas D Lane. 2020.

</span>
<span class="ltx_bibblock">Flower: A friendly federated learning research framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.14390</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.14165</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. 2021.

</span>
<span class="ltx_bibblock">Fedmatch: Federated learning over heterogeneous question answering
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on
Information &amp; Knowledge Management</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019a)</span>
<span class="ltx_bibblock">
Mingqing Chen, Rajiv Mathews, Tom Ouyang, and Françoise Beaufays.
2019a.

</span>
<span class="ltx_bibblock">Federated learning of out-of-vocabulary words.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.10635</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019b)</span>
<span class="ltx_bibblock">
Mingqing Chen, Ananda Theertha Suresh, Rajiv Mathews, Adeline Wong, Cyril
Allauzen, Françoise Beaufays, and Michael Riley. 2019b.

</span>
<span class="ltx_bibblock">Federated learning of n-gram language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd Conference on Computational Natural
Language Learning (CoNLL)</em>, pages 121–130.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">NAACL-HLT (1)</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov et al. (2018)</span>
<span class="ltx_bibblock">
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1045" title="" class="ltx_ref ltx_href">Understanding
back-translation at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 489–500, Brussels, Belgium. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2020)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,
Siddharth Goyal, Mandeep Baines, Onur Çelebi, Guillaume Wenzek, Vishrav
Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov,
Edouard Grave, Michael Auli, and Armand Joulin. 2020.

</span>
<span class="ltx_bibblock">Beyond english-centric multilingual machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2010.11125.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al. (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise
Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel
Ramage. 2018.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2019)</span>
<span class="ltx_bibblock">
Shaoxiong Ji, Shirui Pan, Guodong Long, Xue Li, Jing Jiang, and Zi Huang. 2019.

</span>
<span class="ltx_bibblock">Learning private neural language modeling with attentive aggregation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2019 International Joint Conference on Neural Networks
(IJCNN)</em>, pages 1–8. IEEE.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2014)</span>
<span class="ltx_bibblock">
Diederik P Kingma and Jimmy Ba. 2014.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn et al. (2005)</span>
<span class="ltx_bibblock">
Philipp Koehn et al. 2005.

</span>
<span class="ltx_bibblock">Europarl: A parallel corpus for statistical machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">MT summit</em>, volume 5, pages 79–86. Citeseer.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al. (2016)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon. 2016.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Xian Li, Paul Michel, Antonios Anastasopoulos, Yonatan Belinkov, Nadir Durrani,
Orhan Firat, Philipp Koehn, Graham Neubig, Juan Pino, and Hassan Sajjad.
2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W19-5303" title="" class="ltx_ref ltx_href">Findings of the first
shared task on machine translation robustness</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourth Conference on Machine Translation
(Volume 2: Shared Task Papers, Day 1)</em>, pages 91–102, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. 2021.

</span>
<span class="ltx_bibblock">Large language models can be strong differentially private learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.05679</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2020)</span>
<span class="ltx_bibblock">
Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong,
Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Bruce Zhang, Rahul
Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen,
Winnie Wu, Shuguang Liu, Fan Yang, Daniel Fernando Campos, Rangan Majumder,
and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock">Xglue: A new benchmark datasetfor cross-lingual pre-training,
understanding and generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2021)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Chaoyang He, ZiHang Zeng, Hulin Wang, Yufen Huang,
M. Soltanolkotabi, Xiang Ren, and S. Avestimehr. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2104.08815" title="" class="ltx_ref ltx_href">Fednlp: A research platform
for federated learning in natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv cs.CL 2104.08815</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Miller (2020)</span>
<span class="ltx_bibblock">
Dianbo Liu and Tim Miller. 2020.

</span>
<span class="ltx_bibblock">Federated pretraining and fine tuning of bert using clinical notes
from multiple silos.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2002.08562.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Ming Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, and He Zhang. 2021.

</span>
<span class="ltx_bibblock">Federated learning meets natural language processing: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.12603</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2017)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2017.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.05101</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>, pages 1273–1282.
PMLR.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michel and Neubig (2018)</span>
<span class="ltx_bibblock">
Paul Michel and Graham Neubig. 2018.

</span>
<span class="ltx_bibblock">Mtnt: A testbed for machine translation of noisy text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mosbach et al. (2020)</span>
<span class="ltx_bibblock">
Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2020.

</span>
<span class="ltx_bibblock">On the stability of fine-tuning bert: Misconceptions, explanations,
and strong baselines.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen and Salazar (2019)</span>
<span class="ltx_bibblock">
Toan Q Nguyen and Julian Salazar. 2019.

</span>
<span class="ltx_bibblock">Transformers without tears: Improving the normalization of
self-attention.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.05895</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_href">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" title="" class="ltx_ref ltx_href">Pytorch: An imperative style, high-performance deep learning library</a>.

</span>
<span class="ltx_bibblock">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 32</em>, pages 8024–8035. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock">A call for clarity in reporting bleu scores.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.08771</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2019)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.10683</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramaswamy et al. (2019)</span>
<span class="ltx_bibblock">
Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Françoise Beaufays.
2019.

</span>
<span class="ltx_bibblock">Federated learning for emoji prediction in a mobile keyboard.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.04329</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2019)</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.

</span>
<span class="ltx_bibblock">Distilbert, a distilled version of bert: smaller, faster, cheaper and
lighter.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.01108</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stich (2019)</span>
<span class="ltx_bibblock">
Sebastian Urban Stich. 2019.

</span>
<span class="ltx_bibblock">Local sgd converges fast and communicates little.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ICLR 2019-International Conference on Learning
Representations</em>, CONF.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stickland et al. (2020)</span>
<span class="ltx_bibblock">
Asa Cooper Stickland, Xian Li, and Marjan Ghazvininejad. 2020.

</span>
<span class="ltx_bibblock">Recipes for adapting pre-trained monolingual and multilingual models
to machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.14911</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stremmel and Singh (2020)</span>
<span class="ltx_bibblock">
Joel Stremmel and Arjun Singh. 2020.

</span>
<span class="ltx_bibblock">Pretraining federated text models for next word prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.04828</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sui et al. (2020)</span>
<span class="ltx_bibblock">
Dianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuantao Xie, and Weijian Sun.
2020.

</span>
<span class="ltx_bibblock">Feded: Federated learning via ensemble distillation for medical
relation extraction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
5998–6008.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2019.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language
processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03771</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2021)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Ruixuan Liu, Lingjuan Lyu, Yongfeng Huang, and Xing
Xie. 2021.

</span>
<span class="ltx_bibblock">Fedkd: Communication efficient federated learning via knowledge
distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2108.13323.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2020)</span>
<span class="ltx_bibblock">
Xing Wu, Zhaowang Liang, and Jianjia Wang. 2020.

</span>
<span class="ltx_bibblock">Fedmed: A federated learning framework for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, 20(14):4048.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Françoise Beaufays. 2018.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google keyboard query
suggestions.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.02903</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziemski et al. (2016)</span>
<span class="ltx_bibblock">
Michał Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. 2016.

</span>
<span class="ltx_bibblock">The united nations parallel corpus v1. 0.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC’16)</em>, pages 3530–3534.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Hyperparameters</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Each LM experiment ran for approximately a day each on a 6 GPU cluster of RTX 6000 GPUs with 24GB of memory per GPU. The MT experiments took approximately 12 hours each and the TC experiments took around 3 hours each, all on the same cluster.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">We use the AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>); Kingma and Ba (<a href="#bib.bib11" title="" class="ltx_ref">2014</a>)</cite> for all experiments (shown to be effective for FL in <cite class="ltx_cite ltx_citemacro_citet">Lin et al. <a href="#bib.bib17" title="" class="ltx_ref">2021</a></cite>). Each client goes through a full epoch of local learning before synchronizing with the server.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">For MT, we report results using the 5e-5 learning rate, as we found in initial results (as have others also, see Appendix B of <cite class="ltx_cite ltx_citemacro_citet">Stickland et al. (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> as one example) that MT experiments are generally consistent over learning rates when fine-tuning.
For language modeling and text classification, we use three different learning rates (1e-4, 5e-5, 1e-5). All models were selected using the best performing version on the validation set, for the given model and training setting.
For both tasks, we use early stopping (5 epochs of no improvement for MT and TC, 10 epochs for LM).</p>
</div>
<div id="A1.p4" class="ltx_para">
<p id="A1.p4.1" class="ltx_p">We use the standard <span id="A1.p4.1.1" class="ltx_text ltx_font_typewriter">sacreBLEU</span> settings: nrefs:1, mixed case, eff:no, tok:13a, smooth:exp, and version 2.0.0. For Ja and Zh we use their respective tokenizers.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Randomly Initialized MT</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We do not report results for randomly initialized training of MT systems, as large neural MT systems generally need large amounts of data to be effective. We ran experiments for the MTNT dataset from random initializations, running for twice as many epochs. Resulting models appeared to converge by loss but had extremely low BLEU scores. Thus, we only include pretrained results in <a href="#S3.T2" title="In NC Corpus ‣ 3.3 Data ‣ 3 Experimental Design ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>MTNT Data Preprocessing for M2M-100</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.2" class="ltx_p">M2M-100 was trained using scripts that removed input with “excess punctuation." We follow this in preparing MTNT training data. We use all En <math id="A3.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A3.p1.1.m1.1a"><mo stretchy="false" id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><ci id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">\rightarrow</annotation></semantics></math> Ja data (consisting of approximately 6k instances) and take the corresponding En <math id="A3.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A3.p1.2.m2.1a"><mo stretchy="false" id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><ci id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">\rightarrow</annotation></semantics></math> Fr instances, randomly sampling additional instances until there are the same number of instances in each direction. We sample an equal number of training instances as we are testing the effects of multilingual data, rather than unequal dataset sizes. We then remove the training instances with excess punctuation (or sentences less than 3 characters) following the M2M-100 script. This leaves 5605 instances in each direction for training. We use the standard MTNT dev and test sets, as-is, consisting of approximately 1k data points.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Full LM Results</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">We show the full results of the LM experiments, with standard deviations over five random seeds in Tables <a href="#A4.T4" title="Table 4 ‣ Appendix D Full LM Results ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#A4.T5" title="Table 5 ‣ Appendix D Full LM Results ‣ Pretrained Models for Multilingual Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="A4.T4" class="ltx_table">
<table id="A4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T4.1.1.1" class="ltx_tr">
<th id="A4.T4.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="A4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="9"><span id="A4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Europarl</span></th>
</tr>
<tr id="A4.T4.1.2.2" class="ltx_tr">
<th id="A4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="A4.T4.1.2.2.1.1" class="ltx_text ltx_font_bold">M</span></th>
<th id="A4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T4.1.2.2.2.1" class="ltx_text ltx_font_bold">En</span></th>
<th id="A4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T4.1.2.2.3.1" class="ltx_text ltx_font_bold">Cs</span></th>
<th id="A4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T4.1.2.2.4.1" class="ltx_text ltx_font_bold">Lt</span></th>
<th id="A4.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T4.1.2.2.5.1" class="ltx_text ltx_font_bold">Es</span></th>
<th id="A4.T4.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T4.1.2.2.6.1" class="ltx_text ltx_font_bold">Pl</span></th>
<th id="A4.T4.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T4.1.2.2.7.1" class="ltx_text ltx_font_bold">Fi</span></th>
<th id="A4.T4.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T4.1.2.2.8.1" class="ltx_text ltx_font_bold">Pt</span></th>
<th id="A4.T4.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T4.1.2.2.9.1" class="ltx_text ltx_font_bold">De</span></th>
<th id="A4.T4.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T4.1.2.2.10.1" class="ltx_text ltx_font_bold">Avg</span></th>
</tr>
<tr id="A4.T4.1.3.3" class="ltx_tr">
<th id="A4.T4.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">B</th>
<th id="A4.T4.1.3.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">26.2 ± 2.4</th>
<th id="A4.T4.1.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">34.8 ± 1.8</th>
<th id="A4.T4.1.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">40.1 ± 2.3</th>
<th id="A4.T4.1.3.3.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">20.0 ± 1.3</th>
<th id="A4.T4.1.3.3.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">20.0 ± 1.3</th>
<th id="A4.T4.1.3.3.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">26.6 ± 1.4</th>
<th id="A4.T4.1.3.3.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">25.5 ± 2.0</th>
<th id="A4.T4.1.3.3.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">22.1 ± 1.8</th>
<th id="A4.T4.1.3.3.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">26.9 ± 1.8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T4.1.4.1" class="ltx_tr">
<th id="A4.T4.1.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">C</th>
<td id="A4.T4.1.4.1.2" class="ltx_td ltx_align_right ltx_border_t">19.3 ± 1.5</td>
<td id="A4.T4.1.4.1.3" class="ltx_td ltx_align_right ltx_border_t">4.5 ± 0.4</td>
<td id="A4.T4.1.4.1.4" class="ltx_td ltx_align_right ltx_border_t">3.9 ± 0.3</td>
<td id="A4.T4.1.4.1.5" class="ltx_td ltx_align_right ltx_border_t">8.3 ± 0.7</td>
<td id="A4.T4.1.4.1.6" class="ltx_td ltx_align_right ltx_border_t">4.7 ± 0.3</td>
<td id="A4.T4.1.4.1.7" class="ltx_td ltx_align_right ltx_border_t">4.9 ± 0.3</td>
<td id="A4.T4.1.4.1.8" class="ltx_td ltx_align_right ltx_border_t">7.0 ± 0.6</td>
<td id="A4.T4.1.4.1.9" class="ltx_td ltx_align_right ltx_border_t">10.8 ± 0.8</td>
<td id="A4.T4.1.4.1.10" class="ltx_td ltx_align_right ltx_border_t">7.9 ± 0.6</td>
</tr>
<tr id="A4.T4.1.5.2" class="ltx_tr">
<th id="A4.T4.1.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">I</th>
<td id="A4.T4.1.5.2.2" class="ltx_td ltx_align_right">26.6 ± 1.7</td>
<td id="A4.T4.1.5.2.3" class="ltx_td ltx_align_right">5.4 ± 0.4</td>
<td id="A4.T4.1.5.2.4" class="ltx_td ltx_align_right">4.3 ± 0.3</td>
<td id="A4.T4.1.5.2.5" class="ltx_td ltx_align_right">11.2 ± 0.9</td>
<td id="A4.T4.1.5.2.6" class="ltx_td ltx_align_right">5.8 ± 0.4</td>
<td id="A4.T4.1.5.2.7" class="ltx_td ltx_align_right">5.7 ± 0.3</td>
<td id="A4.T4.1.5.2.8" class="ltx_td ltx_align_right">8.9 ± 0.7</td>
<td id="A4.T4.1.5.2.9" class="ltx_td ltx_align_right">15.1 ± 1.1</td>
<td id="A4.T4.1.5.2.10" class="ltx_td ltx_align_right">10.4 ± 0.7</td>
</tr>
<tr id="A4.T4.1.6.3" class="ltx_tr">
<th id="A4.T4.1.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">N</th>
<td id="A4.T4.1.6.3.2" class="ltx_td ltx_align_right">50.6 ± 2.9</td>
<td id="A4.T4.1.6.3.3" class="ltx_td ltx_align_right">7.1 ± 0.5</td>
<td id="A4.T4.1.6.3.4" class="ltx_td ltx_align_right">11.9 ± 0.9</td>
<td id="A4.T4.1.6.3.5" class="ltx_td ltx_align_right">16.0 ± 1.2</td>
<td id="A4.T4.1.6.3.6" class="ltx_td ltx_align_right">17.7 ± 1.2</td>
<td id="A4.T4.1.6.3.7" class="ltx_td ltx_align_right">12.1 ± 0.7</td>
<td id="A4.T4.1.6.3.8" class="ltx_td ltx_align_right">35.6 ± 2.8</td>
<td id="A4.T4.1.6.3.9" class="ltx_td ltx_align_right">21.7 ± 1.4</td>
<td id="A4.T4.1.6.3.10" class="ltx_td ltx_align_right">21.6 ± 1.4</td>
</tr>
<tr id="A4.T4.1.7.4" class="ltx_tr">
<th id="A4.T4.1.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">C</th>
<td id="A4.T4.1.7.4.2" class="ltx_td ltx_align_right ltx_border_t">12.1 ± 0.9</td>
<td id="A4.T4.1.7.4.3" class="ltx_td ltx_align_right ltx_border_t">3.7 ± 0.3</td>
<td id="A4.T4.1.7.4.4" class="ltx_td ltx_align_right ltx_border_t">3.3 ± 0.2</td>
<td id="A4.T4.1.7.4.5" class="ltx_td ltx_align_right ltx_border_t">13.9 ± 1.2</td>
<td id="A4.T4.1.7.4.6" class="ltx_td ltx_align_right ltx_border_t">4.7 ± 0.4</td>
<td id="A4.T4.1.7.4.7" class="ltx_td ltx_align_right ltx_border_t">4.0 ± 0.2</td>
<td id="A4.T4.1.7.4.8" class="ltx_td ltx_align_right ltx_border_t">4.8 ± 0.4</td>
<td id="A4.T4.1.7.4.9" class="ltx_td ltx_align_right ltx_border_t">6.8 ± 0.6</td>
<td id="A4.T4.1.7.4.10" class="ltx_td ltx_align_right ltx_border_t">6.7 ± 0.5</td>
</tr>
<tr id="A4.T4.1.8.5" class="ltx_tr">
<th id="A4.T4.1.8.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">I</th>
<td id="A4.T4.1.8.5.2" class="ltx_td ltx_align_right">10.5 ± 0.9</td>
<td id="A4.T4.1.8.5.3" class="ltx_td ltx_align_right">4.0 ± 0.3</td>
<td id="A4.T4.1.8.5.4" class="ltx_td ltx_align_right">4.2 ± 0.3</td>
<td id="A4.T4.1.8.5.5" class="ltx_td ltx_align_right">6.1 ± 0.5</td>
<td id="A4.T4.1.8.5.6" class="ltx_td ltx_align_right">3.8 ± 0.3</td>
<td id="A4.T4.1.8.5.7" class="ltx_td ltx_align_right">4.5 ± 0.3</td>
<td id="A4.T4.1.8.5.8" class="ltx_td ltx_align_right">5.6 ± 0.4</td>
<td id="A4.T4.1.8.5.9" class="ltx_td ltx_align_right">6.9 ± 0.5</td>
<td id="A4.T4.1.8.5.10" class="ltx_td ltx_align_right">5.7 ± 0.4</td>
</tr>
<tr id="A4.T4.1.9.6" class="ltx_tr">
<th id="A4.T4.1.9.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">N</th>
<td id="A4.T4.1.9.6.2" class="ltx_td ltx_align_right ltx_border_bb">8.8 ± 0.6</td>
<td id="A4.T4.1.9.6.3" class="ltx_td ltx_align_right ltx_border_bb">3.7 ± 0.3</td>
<td id="A4.T4.1.9.6.4" class="ltx_td ltx_align_right ltx_border_bb">3.9 ± 0.3</td>
<td id="A4.T4.1.9.6.5" class="ltx_td ltx_align_right ltx_border_bb">6.0 ± 0.5</td>
<td id="A4.T4.1.9.6.6" class="ltx_td ltx_align_right ltx_border_bb">3.8 ± 0.3</td>
<td id="A4.T4.1.9.6.7" class="ltx_td ltx_align_right ltx_border_bb">4.4 ± 0.3</td>
<td id="A4.T4.1.9.6.8" class="ltx_td ltx_align_right ltx_border_bb">5.6 ± 0.4</td>
<td id="A4.T4.1.9.6.9" class="ltx_td ltx_align_right ltx_border_bb">6.7 ± 0.5</td>
<td id="A4.T4.1.9.6.10" class="ltx_td ltx_align_right ltx_border_bb">5.4 ± 0.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results for the LM FL experiments on the Europarl Corpus. Bold scores indicate the best in the column for the given section. Scores are measured in perplexity (lower is better). The top row (B) is a baseline using the pretrained model with no further tuning. The middle rows are trained from <span id="A4.T4.3.1" class="ltx_text ltx_font_italic">randomly-initialized</span> models while the bottom rows tune the pretrained model on task data. Due to space we abbreviate: C for Centralized, I for IID FL, and N for non-IID FL.
</figcaption>
</figure>
<figure id="A4.T5" class="ltx_table">
<table id="A4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T5.1.1.1" class="ltx_tr">
<th id="A4.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="A4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="7"><span id="A4.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">UN</span></th>
</tr>
<tr id="A4.T5.1.2.2" class="ltx_tr">
<th id="A4.T5.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="A4.T5.1.2.2.1.1" class="ltx_text ltx_font_bold">M</span></th>
<th id="A4.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T5.1.2.2.2.1" class="ltx_text ltx_font_bold">En</span></th>
<th id="A4.T5.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T5.1.2.2.3.1" class="ltx_text ltx_font_bold">Es</span></th>
<th id="A4.T5.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T5.1.2.2.4.1" class="ltx_text ltx_font_bold">Fr</span></th>
<th id="A4.T5.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T5.1.2.2.5.1" class="ltx_text ltx_font_bold">Ru</span></th>
<th id="A4.T5.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T5.1.2.2.6.1" class="ltx_text ltx_font_bold">Zh</span></th>
<th id="A4.T5.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T5.1.2.2.7.1" class="ltx_text ltx_font_bold">Ar</span></th>
<th id="A4.T5.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A4.T5.1.2.2.8.1" class="ltx_text ltx_font_bold">Avg</span></th>
</tr>
<tr id="A4.T5.1.3.3" class="ltx_tr">
<th id="A4.T5.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">B</th>
<th id="A4.T5.1.3.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">22.3 ± 2.0</th>
<th id="A4.T5.1.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">15.0 ± 0.9</th>
<th id="A4.T5.1.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">17.2 ± 1.1</th>
<th id="A4.T5.1.3.3.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">9.8 ± 0.6</th>
<th id="A4.T5.1.3.3.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">18.1 ± 1.0</th>
<th id="A4.T5.1.3.3.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">14.7 ± 0.8</th>
<th id="A4.T5.1.3.3.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">16.2 ± 1.0</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T5.1.4.1" class="ltx_tr">
<th id="A4.T5.1.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">C</th>
<td id="A4.T5.1.4.1.2" class="ltx_td ltx_align_right ltx_border_t">9.0 ± 0.6</td>
<td id="A4.T5.1.4.1.3" class="ltx_td ltx_align_right ltx_border_t">5.2 ± 0.3</td>
<td id="A4.T5.1.4.1.4" class="ltx_td ltx_align_right ltx_border_t">8.2 ± 0.5</td>
<td id="A4.T5.1.4.1.5" class="ltx_td ltx_align_right ltx_border_t">3.9 ± 0.3</td>
<td id="A4.T5.1.4.1.6" class="ltx_td ltx_align_right ltx_border_t">4.3 ± 0.2</td>
<td id="A4.T5.1.4.1.7" class="ltx_td ltx_align_right ltx_border_t">4.6 ± 0.3</td>
<td id="A4.T5.1.4.1.8" class="ltx_td ltx_align_right ltx_border_t">5.9 ± 0.4</td>
</tr>
<tr id="A4.T5.1.5.2" class="ltx_tr">
<th id="A4.T5.1.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">I</th>
<td id="A4.T5.1.5.2.2" class="ltx_td ltx_align_right">9.1 ± 0.7</td>
<td id="A4.T5.1.5.2.3" class="ltx_td ltx_align_right">5.2 ± 0.3</td>
<td id="A4.T5.1.5.2.4" class="ltx_td ltx_align_right">8.4 ± 0.4</td>
<td id="A4.T5.1.5.2.5" class="ltx_td ltx_align_right">3.7 ± 0.3</td>
<td id="A4.T5.1.5.2.6" class="ltx_td ltx_align_right">3.9 ± 0.2</td>
<td id="A4.T5.1.5.2.7" class="ltx_td ltx_align_right">4.5 ± 0.2</td>
<td id="A4.T5.1.5.2.8" class="ltx_td ltx_align_right">5.8 ± 0.3</td>
</tr>
<tr id="A4.T5.1.6.3" class="ltx_tr">
<th id="A4.T5.1.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">N</th>
<td id="A4.T5.1.6.3.2" class="ltx_td ltx_align_right">12.8 ± 0.9</td>
<td id="A4.T5.1.6.3.3" class="ltx_td ltx_align_right">11.5 ± 0.7</td>
<td id="A4.T5.1.6.3.4" class="ltx_td ltx_align_right">14.6 ± 0.8</td>
<td id="A4.T5.1.6.3.5" class="ltx_td ltx_align_right">9.3 ± 0.7</td>
<td id="A4.T5.1.6.3.6" class="ltx_td ltx_align_right">8.2 ± 0.5</td>
<td id="A4.T5.1.6.3.7" class="ltx_td ltx_align_right">8.3 ± 0.4</td>
<td id="A4.T5.1.6.3.8" class="ltx_td ltx_align_right">10.8 ± 0.6</td>
</tr>
<tr id="A4.T5.1.7.4" class="ltx_tr">
<th id="A4.T5.1.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">C</th>
<td id="A4.T5.1.7.4.2" class="ltx_td ltx_align_right ltx_border_t">7.0 ± 0.5</td>
<td id="A4.T5.1.7.4.3" class="ltx_td ltx_align_right ltx_border_t">4.1 ± 0.2</td>
<td id="A4.T5.1.7.4.4" class="ltx_td ltx_align_right ltx_border_t">4.9 ± 0.3</td>
<td id="A4.T5.1.7.4.5" class="ltx_td ltx_align_right ltx_border_t">2.9 ± 0.2</td>
<td id="A4.T5.1.7.4.6" class="ltx_td ltx_align_right ltx_border_t">3.3 ± 0.2</td>
<td id="A4.T5.1.7.4.7" class="ltx_td ltx_align_right ltx_border_t">3.6 ± 0.2</td>
<td id="A4.T5.1.7.4.8" class="ltx_td ltx_align_right ltx_border_t">4.3 ± 0.3</td>
</tr>
<tr id="A4.T5.1.8.5" class="ltx_tr">
<th id="A4.T5.1.8.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">I</th>
<td id="A4.T5.1.8.5.2" class="ltx_td ltx_align_right">6.5 ± 0.5</td>
<td id="A4.T5.1.8.5.3" class="ltx_td ltx_align_right">3.9 ± 0.2</td>
<td id="A4.T5.1.8.5.4" class="ltx_td ltx_align_right">5.7 ± 0.3</td>
<td id="A4.T5.1.8.5.5" class="ltx_td ltx_align_right">2.8 ± 0.2</td>
<td id="A4.T5.1.8.5.6" class="ltx_td ltx_align_right">3.2 ± 0.2</td>
<td id="A4.T5.1.8.5.7" class="ltx_td ltx_align_right">3.5 ± 0.2</td>
<td id="A4.T5.1.8.5.8" class="ltx_td ltx_align_right">4.3 ± 0.3</td>
</tr>
<tr id="A4.T5.1.9.6" class="ltx_tr">
<th id="A4.T5.1.9.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">N</th>
<td id="A4.T5.1.9.6.2" class="ltx_td ltx_align_right ltx_border_bb">7.1 ± 0.5</td>
<td id="A4.T5.1.9.6.3" class="ltx_td ltx_align_right ltx_border_bb">4.5 ± 0.3</td>
<td id="A4.T5.1.9.6.4" class="ltx_td ltx_align_right ltx_border_bb">6.2 ± 0.3</td>
<td id="A4.T5.1.9.6.5" class="ltx_td ltx_align_right ltx_border_bb">3.2 ± 0.2</td>
<td id="A4.T5.1.9.6.6" class="ltx_td ltx_align_right ltx_border_bb">4.2 ± 0.2</td>
<td id="A4.T5.1.9.6.7" class="ltx_td ltx_align_right ltx_border_bb">4.0 ± 0.2</td>
<td id="A4.T5.1.9.6.8" class="ltx_td ltx_align_right ltx_border_bb">4.9 ± 0.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Results for the LM FL experiments on the UN Corpus. Bold scores indicate the best in the column for the given section. Scores are measured in perplexity (lower is better). The top row (B) is a baseline using the pretrained model with no further tuning. The middle rows are trained from <span id="A4.T5.3.1" class="ltx_text ltx_font_italic">randomly-initialized</span> models while the bottom rows tune the pretrained model on task data. Due to space we abbreviate: C for Centralized, I for IID FL, and N for non-IID FL.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2206.02290" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2206.02291" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2206.02291">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2206.02291" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2206.02292" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 19:32:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
