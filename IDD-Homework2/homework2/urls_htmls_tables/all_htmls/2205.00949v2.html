<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.00949] Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks</title><meta property="og:description" content="We present Answer-Me, a task-aware multi-task framework which unifies a variety of question answering tasks, such as, visual question answering, visual entailment, visual reasoning. In contrast to previous works using …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.00949">

<!--Generated on Mon Mar 11 15:52:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Google Research</span></span></span>
<h1 class="ltx_title ltx_title_document">Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">AJ Piergiovanni
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Wei Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Weicheng Kuo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Mohammad Saffar
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Fred Bertsch and Anelia Angelova
</span></span>
</div>

<h1 class="ltx_title ltx_title_document">Multi-Task Learning for Generalization to Many Question-Answering Tasks </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">AJ Piergiovanni
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Wei Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Weicheng Kuo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Mohammad Saffar
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Fred Bertsch and Anelia Angelova
</span></span>
</div>

<h1 class="ltx_title ltx_title_document">Multi-Task Generalization for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">AJ Piergiovanni
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Wei Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Weicheng Kuo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Mohammad Saffar
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Fred Bertsch and Anelia Angelova
</span></span>
</div>

<h1 class="ltx_title ltx_title_document">Answer-Me: Multi-Task Open-Vocabulary Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">AJ Piergiovanni
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Wei Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Weicheng Kuo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Mohammad Saffar
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Fred Bertsch and Anelia Angelova
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">We present Answer-Me, a task-aware multi-task framework which unifies a variety of question answering tasks, such as, visual question answering, visual entailment, visual reasoning. In contrast to previous works using contrastive or generative captioning training, we propose a novel and simple recipe to pre-train a vision-language joint model, which is multi-task as well. The pre-training uses only noisy image captioning data, and is formulated to use the entire architecture end-to-end with both a strong language encoder and decoder. Our results show state-of-the-art performance, zero-shot generalization, robustness to forgetting, and competitive single-task results across a variety of question answering tasks. Our multi-task mixture training learns from tasks of various question intents and thus generalizes better, including on zero-shot vision-language tasks. We conduct experiments in the challenging multi-task and open-vocabulary settings and across a variety of datasets and tasks, such as VQA2.0, SNLI-VE, NLVR2, GQA. We observe that the proposed approach is able to generalize to unseen tasks and that more diverse mixtures lead to higher accuracy in both known and novel tasks.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The ability to understand both visual and textual cues is crucial for interactions grounded in the rich visual world.
Questions are a natural way for users to articulate information needs,
and a variety of tasks in which a question is posed towards an image, such as Visual Question Answering (VQA), visual commonsense reasoning, visual entailment and others, have been created <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>.
Question answering with an image is a challenging task as it involves deeper visual understanding, for example ‘What is the child holding?’ requires visual recognition, whereas ‘Why is the person smiling?’ requires additional reasoning, beyond just recognition.
We build a multi-task model which can use natural language to ask questions about an image and respond in free-form text, enabling the model to answer many questions, even out-of-domain ones.
</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2205.00949/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="139" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples comparing zero-shot performances between a pretrained (PT) model on a large dataset, our multi-task learning on 4 tasks, and our multi-task learning on 8 tasks, evaluated on VQA and visual entailment (VE) tasks. All are zero-shot performances with respect to the new question and answering task: while pretrained models are powerful they lack understanding of the question intent and are not able to respond to questions as adequately as our task aware multi-task setup.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Multi-modal image-language approaches have made great strides recently, showing tremendous promise in joint visuo-linguistic understanding.
Most commonly, they use pretraining on large generic datasets, and then fine-tuning on multiple downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Pretraining is appealing because it can be done once and adapted to multiple tasks.
However pretraining is <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">task agnostic</span>, i.e., is not tuned to the tasks’ goals, and finetuning is often done for each individual task independently, creating multiple, different copies of the model.
Importantly, the intent of the input text is not captured by many pretrained models, as they are trained on datasets for image captioning or
with weak image-language correlations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, which might not learn the right interactions between image and language to capture the goal of the question. For example, pretraining with image captioning does not train a text encoder, so the model will not be able to understand questions. Instead, we need a model which is able to take advantage of the language-image training and learn the corresponding interactions which reflect the intent of the question or task.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Furthermore, it is desirable that image-language models are able to generalize to other tasks with natural questions and answers, which are not seen during training. This is needed because models fine-tuned on a specific task tend to demonstrate larger rates of catastrophic forgetting on new tasks. For example, GPV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> demonstrated that fine-tuning on one task leads to a loss of 50% precision on another task, after adding only about a hundred examples. This overfitting and forgetting is observed even if very large datasets are used for pretraining; we also observe it here (Section <a href="#S5.SS3" title="5.3 Answer-Me prevents catastrophic forgetting ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Lastly, architecture complexity has been prevalent, with complex box proposal mechanisms used to obtain the initial image features making the architectures cumbersome to train and support. This limits the generalization abilities of the model in cases where objects are not detected (e.g., out-of-domain objects) and prevents models from general adoption, dissemination and makes them harder to scale.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We propose ‘Answer-Me’ which unifies visual question answering tasks and aims to answer a variety of natural language questions towards an image from a wide range of tasks. The gist of the method is multi-task, task-aware training, which is able to take in the question intent. This is combined with our novel pretraining which learns the encoder, decoder and image-language feature interaction end-to-end, and is also multi-task itself. The pretraining method is simple; it uses only image captioning data in multiple ways in order to train both the image and language encoders and the text decoder model components. This allows for natural language questions and free-form answers that recognize intent and answer accordingly, without additional prompts.
We evaluate our multi-task approach in the open-vocabulary setting which is more challenging, as the generated text has to match exactly the ground truth to be counted as accurate, as opposed to other approaches which use classification as a proxy, choosing among a pre-defined set of possible answers. We note that open-vocabulary text generation is compounded by the multi-task setting, as the method does not have the opportunity to fine-tune on specific tasks, thus it cannot easily memorize specific answers within a dataset. While challenging, this is a more realistic scenario and thus more valuable to sustain in evaluations.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The approach is simple and easily extensible to more tasks and datasets and is shown to generalize to novel tasks. It performs well compared to the state-of-the-art (SOTA), largely outperforming prior multi-task methods and in some cases pre-trained and fine-tuned ones, despite the more challenging open-vocabulary evaluation setting.
</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We find the following advantages of the proposed multi-task training:
(i) semantically related tasks benefit from multi-task training as visual linguistic concepts are shared, helping their co-training;
(ii) pooling tasks increases diversity of the questions, allows the model to react to different question types, leading to improved generalization to novels tasks;
(iii) training multiple tasks improves performance,
reducing catastrophic forgetting and overfitting.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">We make the following contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A multi-task task-aware general framework which can serve multiple diverse Visual Question Answering tasks and is able to generalize to many tasks. It uses a simple, easily scalable and extensible architecture and considers the VQA tasks in the more-challenging open-vocabulary generative setting.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A pretraining method to train the entire encoder-decoder vision-language model simultaneously using only noisy captioning data that results in a strong pretrained model.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Answer-Me is able to perform well on zero-shot (novel task) generalization settings, is also robust to overfiting and forgetting, and performs well on a new ‘detection’ task which requires semantic object recognition.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Multi-modal image-language learning.</span>
Multi-modal image-language learning has garnered large interest in recent years, encompassing tasks, such as VQA, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, visual commonsense reasoning  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, visual dialog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, cross-modality retrieval i.e., image-to-text and text-to-image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, referring expressions comprehension <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and many others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.
Some tasks use joint image and language inputs, such as captioning or VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, while
some use image and language learning to joint space which enables cross-modal retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>.
Additional modalities are also often incorporated in such models, for example, video, audio, or additional text such as transcriptions or video captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Transfer learning for image-language models.</span>
A common approach to leveraging image and text datasets for multiple tasks is to pretrain on a related dataset, and then fine-tune on downstream datasets and tasks. Pretraining from large data has shown impressive performance gains: e.g., ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> pretrain on the Conceptual Captions dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and with some modifications to the architecture, fine-tune on a number of tasks.
Other works also show successful training on downstream image-and-language tasks: VLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, UNITER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, Oscar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, SOHO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, Unified VLP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, VisualBERT,  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>,
ViLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite></p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Pretraining techniques, as such, also vary. In some cases pretraining is done by training on a single task e.g., captioning and a large dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, others use different pretraining objectives depending on the downstream task, for example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> uses captioning and matching losses for pretraining, where the former is used for downstream VQA or captioning tasks, and the latter for retrieval tasks. Contrastive pretraining is particularly useful for retrieval tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
Masked modeling objectives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and its derivatives, is one of the most popular form of pretraining for joint image and language learning. Following the same idea for text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, VirTex <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and LocTex <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> also show that pretraining on both image and language modalities jointly benefits vision-only tasks.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Image language learning.</span>
Many image-language interaction learning methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> are based on the the popular Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>.
For example, co-attention with Transformer in VilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> proposed attention-based image-language approach and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> use it for localization.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Multi-task learning for vision and language.</span>
Our work is most aligned with the image-language multi-task
approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. In early work, Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> combine three vision and language tasks by incrementally adding tasks to the training. Following the success of unified multi-task learning for text, e.g., T5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, MT-DNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, a number of vision+language multi-task approaches have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
Cho et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> unify several vision and language tasks as a text-based generation task. GPV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> combines tasks with diverging outputs, e.g., VQA, captioning, localization. UniT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> proposes multi-task learning which spans many tasks. 12-in-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> train jointly several diverse vision and language tasks. Many of the multi-task approaches mentioned above rely on off-the-shelf object bounding boxes during training, or detection-specific pretraining  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which might create burdensome architectures and additional complexities in data gathering and training.
12-in-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> utilizes FasterRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> for detection, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> utilize DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, whereas UniT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> uses a ResNet architecture, pretrained on detection. Further <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> relies on ground truth boxes for many of the training tasks. Our work uses simple architectures without bounding boxes and relies instead on learnable image-language interactions.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">In complement to the above-mentioned multi-task learning approaches,
our main motivation for this work is creating a general visual representation which can be directly utilized by multiple diverse image-language tasks. Our model is able to respond naturally depending on the intent and the implied task from the question.
It is easily extensible to many tasks in the mix and new unseen tasks, it is also easily scalable as it does not rely on complex mechanisms.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Multi-task Learning for Visual Question Answering</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Tasks and query intent</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Each of the image-language question-answering tasks contain a specific intent in the question, for example, counting the number of objects, answering a visual entailment question, or reasoning about an image.
Answer-Me combines the tasks in joint training using a mixture, enabling the model to acquire new capabilities, and generalize to other classes of questions without losing accuracy on tasks.
</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2205.00949/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="322" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Model overview. The model processes an image and text, fuses them together, and uses transformer layers to generate the output text. The pretraining task mix, shown in different colors at the bottom right (see Section <a href="#S3.SS3" title="3.3 Pretraining for multi-task learning ‣ 3 Multi-task Learning for Visual Question Answering ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) allows all parts of the model to be well-trained, and is better suited for the subsequent multi-task training.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In contrast to prior work which leverages large data, task-agnostic pretraining, but uses only partially pretrained models, Answer-Me both 1) pretrains specifically to exercise all components of the model with a related image-language task, and 2) trains with multiple datasets with various question intents, specifically to increase generalization.
For example, even with very large pretraining, when finetuning on a single task, e.g., captioning or VQA, the model tends to overfit to the task itself and does not generalize to other tasks or question types. By mixing tasks together, the model is able to better generalize to other datasets and tasks, even in a zero-shot setting.
Section <a href="#S3.SS2" title="3.2 Main architecture ‣ 3 Multi-task Learning for Visual Question Answering ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> describes the overall architecture, Section <a href="#S3.SS3" title="3.3 Pretraining for multi-task learning ‣ 3 Multi-task Learning for Visual Question Answering ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> describes the pretraining strategy which trains the model end-to-end to exercise different components.
Section <a href="#S3.SS4" title="3.4 Multi-task training ‣ 3 Multi-task Learning for Visual Question Answering ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> details the multi-task training.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Main architecture</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our model consists of an image encoder (ResNet) and text encoder (T5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>). These components are easily and independently scalable. Our experiments are based on a ResNet-50 and T5-base model, and we scale it 3x by using ResNet-101 and T5-large. The image and language features are provided to the fusion module, described below. The output of the fusion module is used as input to the text decoder, which produces free-form text for all Answer-Me tasks. The architecture is designed to be modular and easily scalable, as shown in our experiments. <a href="#S3.F2" title="Figure 2 ‣ 3.1 Tasks and query intent ‣ 3 Multi-task Learning for Visual Question Answering ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> shows an overview.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image-language fusion.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.2" class="ltx_p">The image and language features are first fused by concatenation.
We use the ResNet feature map to get <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="H*W" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">∗</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><times id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1"></times><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">𝐻</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">H*W</annotation></semantics></math> features and concatenate with the <math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">L</annotation></semantics></math> (number of tokens) text features. We apply a relative position bias to both image and text separately. We then apply the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> on the concatenated features, combining both sources. While existing works have proposed similar fusion methods,
(e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>), the pretraining method and generalisation abilities without forgetting are new, further only using raw images instead of region proposals.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Pretraining for multi-task learning</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In order to enable the model to address new tasks, i.e., to respond to unseen question types and answer adequately, we take advantage of a unique pretraining designed to train all the components of a model. We pretrain on the Conceptual Captions 12 million (CC12m) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Unlike previous work, this pretraining strategy is targeted towards training the entire encoder-decoder model, and differs by the following components:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">It is designed to train all components of the model end-to-end. Thus the image encoder, text encoder, image-text fusion module and text decoder are all trained together.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Pretraining exercises various pathways in the model, which makes it suitable for various question-answering and description tasks. We construct a mix of captioning, caption completion and matching tasks for that purpose.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Taken together, these tasks allow the encoder and decoder to see all, part of, or none of the caption, and experimentally we find this pretraining method results in a stronger model than any individual pretraining task (see <a href="#S5.T7" title="Table 7 ‣ 5.5 Ablations ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 7</span></a>).</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">We pretrain the model as follows. For each sample, we have an (image, text) pair, obtained from the CC12m dataset. To train all parts of the model, we design four tasks (shown in <a href="#S3.F2" title="Figure 2 ‣ 3.1 Tasks and query intent ‣ 3 Multi-task Learning for Visual Question Answering ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>):
<span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">(1) image captioning (IC).</span> Here the input text is ‘caption the image’ and the target text is the caption. This task mostly trains the text decoder and fusion layers.
<span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_bold">(2) caption completion (CMP).</span> Here the input is 10-40% of the caption text and the target text is the remaining caption. This encourages training of the entire model.
<span id="S3.SS3.p3.1.3" class="ltx_text ltx_font_bold">(3) text MLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</span> Here the input is the caption with 25% of the words masked out, the target text is the missing words<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We also tried generating the entire caption and it performed similarly.</span></span></span>. This trains the entire model.
<span id="S3.SS3.p3.1.4" class="ltx_text ltx_font_bold">(4) image text matching (ITM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</span> Here the input is either the image caption or a random caption and the target text is ‘true’ or ‘false’ depending on if the caption matches the image or not. This primarily trains the encoder and fusion layers.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">When training together, these tasks allow the model components, e.g., encoder, decoder, etc., to see all, part of, or none of the caption, and thus prepare the final model to address a variety of image-text tasks.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Another key advantage of this approach is that the training loss is simple: cross entropy over the tokens. All tasks use the same loss, including target tasks such as VQA, visual entailment, etc.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multi-task training</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.2" class="ltx_p">The multi-task training is done by taking a set of <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">N</annotation></semantics></math> tasks and mixing them together, sampled so that a batch consists of an equal amount of each dataset, i.e., batch size/<math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mi id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">N</annotation></semantics></math> samples from each task. Since we use a text generation setting for the tasks, the loss is computed over the tokens, all using the same vocabulary. We use the same vocabulary as T5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> with 32,000 tokens for all our experiments.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental setup</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and tasks</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We use the following datasets to address a number of question-answering tasks:</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Visual Question Answering (VQA2.0).</span> We use the popular Visual Question Answering dataset VQA2.0 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which has about 400k training question/answer pairs. Here we use the words as the output tokens.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Visual Entailment (SNLI-VE).</span> SNLI-VE  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> is a Visual Entailement (VE) dataset with <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\sim</annotation></semantics></math>500k samples. The VE task involves reasoning about the image and concluding whether a proposed statement is ‘entailment’, a ‘contradiction’, or ‘neutral’, in the context of the image. We use ‘true’, ‘false’ and ‘neutral’ as the output words corresponding to entailment, contradiction or neutral.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Natural Language for Visual Reasoning (NLVR2).</span> NLVR2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> dataset requires reasoning about two images simultaneously. Challenging questions include counting if the left image contains n times more objects of a certain type. We use the NLVR2 version which contains real images, 107,292 text annotations. Here we use ‘true’ and ‘false’ as the target words.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Visual Genome-Question Answering (VG-QA).</span> The VG-QA dataset is based on Visual Genome. It contains 1.7M question-answer pairs, with <math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><mo id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">\sim</annotation></semantics></math>17 question-answers per image. We use the ground truth answers as the tokens.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p"><span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_bold">GQA.</span> The GQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is also based on the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and introduces more complex compositional questions based on scene object relationships. It consists of <math id="S4.SS1.p6.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.p6.1.m1.1a"><mo id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.p6.1.m1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">\sim</annotation></semantics></math>22M question-answer pairs for 113k images.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p"><span id="S4.SS1.p7.1.1" class="ltx_text ltx_font_bold">CC12m.</span> The Conceptual Captions 12m dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is an image captioning dataset collected from the web, containing 12M image-text pairs.
This is our only pretraining dataset.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.1" class="ltx_p">The datatsets considered in this work are collected from various sources e.g., web images, Flickr, personal mobile phones, etc.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For each of the above-mentioned datasets above, we follow the evaluation protocols established in prior work. We also use standard well adopted metrics to measure performance. For example, accuracy on SNLI-VE, NLVR2 and GQA, the VQA2.0 accuracy metric on VQA2.0 (dev set). Notably, instead of training a classification layer for each dataset, we use a large, open vocabulary and generate text answers which are used to compute the metrics.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Zero-Shot Evaluation</span>: In our experiments below we conduct Zero-Shot (ZS) evaluation across tasks. We use the standard evaluation metrics per each dataset, where in the ZS setting the model is trained on different datasets. In some cases, datasets might stem from the same source e.g., GQA and VG-QA or have any overlap, in which case we ensure training and validation/test sets do not contain overlapping images across all our experiments.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Here we show the main results of the Answer-Me multi-task training.
We evaluate the approach in a zero-shot (ZS) setting (Section <a href="#S5.SS1" title="5.1 Zero-shot results ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>), and on multiple question-answering tasks. We then train and evaluate with multiple task mixtures (Section <a href="#S5.SS2" title="5.2 Multi-task results ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>), and demonstrate Answer-Me is beneficial against catastrophic forgetting when using multiple datasets (Section <a href="#S5.SS3" title="5.3 Answer-Me prevents catastrophic forgetting ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>).
We also evaluate Answer-Me on an entirely new task of detection conveyed via text (Section <a href="#S5.SS4" title="5.4 Novel semantic ‘detection’ task ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>), which does not fit any of the question-answering tasks and demonstrate Answer-Me generalization capabilities in both zero-shot and within-task settings.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Experimental results summary.</span>
In our main experiments we evaluate the performance on Answer-Me when 4 visual question-answering tasks are combined and trained together, and when 8 tasks are combined in training. Across both zero-shot and standard multi-task experiments, we observe that the mixtures perform well on novel tasks and within-tasks, and that a mixture of more tasks results in better accuracies across all datasets tested. We further demonstrate the performance of our model when scaled 3x (has 3 times more parameters), which is easy to do with our model. Here again, across evaluation settings and across datasets, Answer-Me brings in large improvements in performance.
We demonstrate that the Answer-Me mixtures are much more robust to catastrophic forgetting and that the Answer-Me mixtures perform well across multiple tasks, instead of having higher accuracy on a single task only.
We note that these results are also obtained with the open-vocabulary outputs from the Answer-Me model, unlike all prior work, which to our knowledge uses smaller and fixed vocabulary to evaluate their performance.
Furthermore, on the new task of detection conveyed via text, which is not a question answering task, we observe the utility of the approach on this unique and novel task, showing its ability to recognize a variety of objects, even in a zero-shot setting.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S5.T1.2.1" class="ltx_text ltx_font_bold">Zero-shot</span> performance of multi-task Answer-Me for mixtures of tasks. The mixtures do not include the task that is being tested on (we make sure there is no ‘leakage’ to the test set for each experiment). As seen, the mixture improves zero-shot performance over pretraining. Increasing the Answer-Me mixture set (number of tasks in the mixture) leads to better ZS performance. Scaling the model (last row) is additionally beneficial across all tasks.</figcaption>
<div id="S5.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:296.7pt;height:89.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.5pt,0.4pt) scale(0.99,0.99) ;">
<table id="S5.T1.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.3.1.1.1" class="ltx_tr">
<th id="S5.T1.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Approach</th>
<th id="S5.T1.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VQA</th>
<th id="S5.T1.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">NLVR2</th>
<th id="S5.T1.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SNLI-VE</th>
<th id="S5.T1.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">GQA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.3.1.2.1" class="ltx_tr">
<th id="S5.T1.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Pretrained only</th>
<td id="S5.T1.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">25.3</td>
<td id="S5.T1.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">32.5</td>
<td id="S5.T1.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">22.7</td>
<td id="S5.T1.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">40.9</td>
</tr>
<tr id="S5.T1.3.1.3.2" class="ltx_tr">
<th id="S5.T1.3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Answer-Me 4-ZS-tasks</th>
<td id="S5.T1.3.1.3.2.2" class="ltx_td ltx_align_center">30.0</td>
<td id="S5.T1.3.1.3.2.3" class="ltx_td ltx_align_center">42.5</td>
<td id="S5.T1.3.1.3.2.4" class="ltx_td ltx_align_center">34.1</td>
<td id="S5.T1.3.1.3.2.5" class="ltx_td ltx_align_center">42.3</td>
</tr>
<tr id="S5.T1.3.1.4.3" class="ltx_tr">
<th id="S5.T1.3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Answer-Me 8-ZS-tasks</th>
<td id="S5.T1.3.1.4.3.2" class="ltx_td ltx_align_center"><span id="S5.T1.3.1.4.3.2.1" class="ltx_text ltx_font_bold">35.0</span></td>
<td id="S5.T1.3.1.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T1.3.1.4.3.3.1" class="ltx_text ltx_font_bold">44.7</span></td>
<td id="S5.T1.3.1.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T1.3.1.4.3.4.1" class="ltx_text ltx_font_bold">37.3</span></td>
<td id="S5.T1.3.1.4.3.5" class="ltx_td ltx_align_center"><span id="S5.T1.3.1.4.3.5.1" class="ltx_text ltx_font_bold">44.2</span></td>
</tr>
<tr id="S5.T1.3.1.5.4" class="ltx_tr">
<th id="S5.T1.3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Answer-Me 8-ZS-tasks, 3x</th>
<td id="S5.T1.3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.3.1.5.4.2.1" class="ltx_text ltx_font_bold">39.2</span></td>
<td id="S5.T1.3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.3.1.5.4.3.1" class="ltx_text ltx_font_bold">48.3</span></td>
<td id="S5.T1.3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.3.1.5.4.4.1" class="ltx_text ltx_font_bold">41.1</span></td>
<td id="S5.T1.3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.3.1.5.4.5.1" class="ltx_text ltx_font_bold">47.2</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experiments comparing Answer-Me multi-task learning, evaluated on several datasets. As seen, more tasks improve performance across all datasets; scaling the model, which is easy in our framework, brings in further consistent improvements. Results from models, fine-tuned to individual tasks, are shown in the top portion of the table, multi-task models (a single one per evaluation), are shown in the bottom portion.
</figcaption>
<div id="S5.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:386.0pt;height:142.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.9pt,0.7pt) scale(0.99,0.99) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Approach</th>
<td id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Num Models</td>
<td id="S5.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">VQA2.0</td>
<td id="S5.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">NLVR2</td>
<td id="S5.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">SNLI-VE</td>
<td id="S5.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">GQA</td>
</tr>
<tr id="S5.T2.1.1.2.2" class="ltx_tr">
<th id="S5.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Single-Task (random init)</th>
<td id="S5.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mult</td>
<td id="S5.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.05</td>
<td id="S5.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.5</td>
<td id="S5.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.1</td>
<td id="S5.T2.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">68.9</td>
</tr>
<tr id="S5.T2.1.1.3.3" class="ltx_tr">
<th id="S5.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Single-task, pretrained (PT)</th>
<td id="S5.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">Mult</td>
<td id="S5.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">65.2</td>
<td id="S5.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">70.2</td>
<td id="S5.T2.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r">77.72</td>
<td id="S5.T2.1.1.3.3.6" class="ltx_td ltx_align_center">73.03</td>
</tr>
<tr id="S5.T2.1.1.4.4" class="ltx_tr">
<th id="S5.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Single-Task, PT, 3x scaled</th>
<td id="S5.T2.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">Mult</td>
<td id="S5.T2.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">71.2</td>
<td id="S5.T2.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">72.0</td>
<td id="S5.T2.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.1.1.4.4.5.1" class="ltx_text ltx_font_bold">85.8</span></td>
<td id="S5.T2.1.1.4.4.6" class="ltx_td ltx_align_center">77.2</td>
</tr>
<tr id="S5.T2.1.1.5.5" class="ltx_tr">
<th id="S5.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Answer-Me, PT, Zero-shot</th>
<td id="S5.T2.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Single</td>
<td id="S5.T2.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25.3</td>
<td id="S5.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.5</td>
<td id="S5.T2.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.7</td>
<td id="S5.T2.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">40.9</td>
</tr>
<tr id="S5.T2.1.1.6.6" class="ltx_tr">
<th id="S5.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Answer-Me, PT, 4 tasks</th>
<td id="S5.T2.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">Single</td>
<td id="S5.T2.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">64.8</td>
<td id="S5.T2.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">71.5</td>
<td id="S5.T2.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r">77.2</td>
<td id="S5.T2.1.1.6.6.6" class="ltx_td ltx_align_center">72.1</td>
</tr>
<tr id="S5.T2.1.1.7.7" class="ltx_tr">
<th id="S5.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Answer-Me, PT, 8 tasks</th>
<td id="S5.T2.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r">Single</td>
<td id="S5.T2.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">65.1</td>
<td id="S5.T2.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">71.7</td>
<td id="S5.T2.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r">77.5</td>
<td id="S5.T2.1.1.7.7.6" class="ltx_td ltx_align_center">72.8</td>
</tr>
<tr id="S5.T2.1.1.8.8" class="ltx_tr">
<th id="S5.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Answer-Me, PT, 8 tasks, 3x</th>
<td id="S5.T2.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Single</td>
<td id="S5.T2.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T2.1.1.8.8.3.1" class="ltx_text ltx_font_bold">73.6</span></td>
<td id="S5.T2.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T2.1.1.8.8.4.1" class="ltx_text ltx_font_bold">73.9</span></td>
<td id="S5.T2.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T2.1.1.8.8.5.1" class="ltx_text ltx_font_bold">85.8</span></td>
<td id="S5.T2.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.1.1.8.8.6.1" class="ltx_text ltx_font_bold">77.5</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparing mixture training vs. individual task FT.</figcaption>
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:273.8pt;height:124.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.4pt,0.6pt) scale(0.99,0.99) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Tasks</th>
<th id="S5.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VQA2.0</th>
<th id="S5.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">NLVR2</th>
<th id="S5.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SNLI-VE</th>
<th id="S5.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">GQA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.2.1" class="ltx_tr">
<th id="S5.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">VQA2.0-only</th>
<td id="S5.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.2.1.2.1" class="ltx_text ltx_font_bold">65.2</span></td>
<td id="S5.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">34.2</td>
<td id="S5.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">24.3</td>
<td id="S5.T3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">41.3</td>
</tr>
<tr id="S5.T3.1.1.3.2" class="ltx_tr">
<th id="S5.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">NLVR2-only</th>
<td id="S5.T3.1.1.3.2.2" class="ltx_td ltx_align_center">23.4</td>
<td id="S5.T3.1.1.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.3.2.3.1" class="ltx_text ltx_font_bold">70.2</span></td>
<td id="S5.T3.1.1.3.2.4" class="ltx_td ltx_align_center">21.3</td>
<td id="S5.T3.1.1.3.2.5" class="ltx_td ltx_align_center">36.7</td>
</tr>
<tr id="S5.T3.1.1.4.3" class="ltx_tr">
<th id="S5.T3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SNLI-VE-only</th>
<td id="S5.T3.1.1.4.3.2" class="ltx_td ltx_align_center">24.3</td>
<td id="S5.T3.1.1.4.3.3" class="ltx_td ltx_align_center">34.6</td>
<td id="S5.T3.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.4.3.4.1" class="ltx_text ltx_font_bold">77.7</span></td>
<td id="S5.T3.1.1.4.3.5" class="ltx_td ltx_align_center">38.5</td>
</tr>
<tr id="S5.T3.1.1.5.4" class="ltx_tr">
<th id="S5.T3.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GQA-only</th>
<td id="S5.T3.1.1.5.4.2" class="ltx_td ltx_align_center">28.6</td>
<td id="S5.T3.1.1.5.4.3" class="ltx_td ltx_align_center">33.5</td>
<td id="S5.T3.1.1.5.4.4" class="ltx_td ltx_align_center">23.4</td>
<td id="S5.T3.1.1.5.4.5" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.5.4.5.1" class="ltx_text ltx_font_bold">73.0</span></td>
</tr>
<tr id="S5.T3.1.1.6.5" class="ltx_tr">
<th id="S5.T3.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VG-QA-only (ZS)</th>
<td id="S5.T3.1.1.6.5.2" class="ltx_td ltx_align_center">29.4</td>
<td id="S5.T3.1.1.6.5.3" class="ltx_td ltx_align_center">33.7</td>
<td id="S5.T3.1.1.6.5.4" class="ltx_td ltx_align_center">27.5</td>
<td id="S5.T3.1.1.6.5.5" class="ltx_td ltx_align_center">40.8</td>
</tr>
<tr id="S5.T3.1.1.7.6" class="ltx_tr">
<th id="S5.T3.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">5-task mix</th>
<td id="S5.T3.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">65.1</td>
<td id="S5.T3.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">71.5</td>
<td id="S5.T3.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">77.4</td>
<td id="S5.T3.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">72.8</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Zero-shot results</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In <a href="#S5.T1" title="Table 1 ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>, we evaluate Answer-Me when training on a mix of tasks in a Zero-Shot (ZS) setting, i.e., on unseen tasks. This is important as it is not always feasible to obtain full labeling of examples for new tasks. The mixture training is able to give better answers than just the pretrained model. Our results also indicate that larger mixtures lead to better zero-shot results, which shows an important ability to perform new skills.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Using the five VQA datasets above (VQA2.0, GQA, SNLI-VE and VG-QA) in the mixture setting, we mix by picking four for training and one for evaluation. The dataset evaluated on is not used in training. We further make sure any evaluation images are excluded from the training data.
For example, when evaluating on VQA2.0, we train on GQA, SNLI-VE and VG-QA with the VQA2.0 validation images removed from those datasets. Similarly when evaluating on SNLI-VE, we train on VQA2.0, GQA and VG-QA.
For the 8-task mixture, we additionally add Coco Captioning data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Localized Narratives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, Visual Genome region descriptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, and ImageNet matching (e.g., ‘Is this X?’ with a ‘yes/no’ output, generated from ImageNet classes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>). We similarly remove overlapping images from the evaluation data.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">We evaluate in zero-shot manner on VQA2.0, SNLI-VE, GQA and NLVR2. NLVR2 needs two images as input, unlike all other datasets, so here it is only used as a zero-shot evaluation. <a href="#S5.T1" title="Table 1 ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a> shows the zero-shot results, using the pretrained only, 4-task and 8-task model. The 8-task mix improves by almost 10% on VQA2.0.
We also find that 8 tasks mixture produces consistently better results across all datasets than 4 tasks, even though the additional 4 tasks are less related to VQA data. When scaling, we observe even higher results on VQA and consistent improvements on all datasets.
For comparison, a single model from scratch trained on VQA2.0 only achieves 49% accuracy. The multi-task learning by leveraging other image-text datasets is able to reduce the gap to supervised training. This is very promising as it demonstrates a level of generalization to novel/unseen tasks. This further improves upon the results in Frozen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> which got 29.5% for ZS on VQA.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Multi-task results</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this section we test the capabilities of the Answer-Me models and their potential for skill transfer. I.e., we compare how a model performs when a task is included in the training mix vs. a task outside the mix. <a href="#S5.T2" title="Table 2 ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a> compares Answer-Me trained on single tasks vs. different task mixtures. We observe that the mixtures provide competitive results to single pretrained and fine-tuned (FT) models, and that more tasks in the mixture improve the performance across all datasets. When scaling the model, we see consistent improvements. In <a href="#S5.T3" title="Table 3 ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a> we compare single-task vs. multi-task training, then evaluating on all 4 different VQA datasets. We see that when fine-tuning on a single task, the model does well on that task, but poorly on the others, but when trained on a mix of all 5 VQA datasets in the table, the model does nearly the same as the single task training. Together, these tables show robustness to forgetting and generalization ability to a variety of question types.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">In <a href="#S5.T4" title="Table 4 ‣ 5.2 Multi-task results ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a> we compare to the state-of-the-art (SOTA) results.
We compare to other multi-task state-of-the-art methods, such as UniT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, 12-in-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, GPV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and others.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">We note that our results are obtained in the open-vocabulary setting which is more challenging, as (in our case) the generated text has to match exactly the ground truth to be counted as accurate. This is additionally compounded by the multi-task setting as the method is less encouraged to memorize answers within a task or datasets. While more challenging, this is a more realistic scenario and we hope that other works commence adopting it.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Despite the more challenging setting, our results largely outperform other SOTA approaches and for some datasets, e.g. GQA, SNLI-VE, with large margins. Only 12-in-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> outperforms our approach and only on the NLVR2 dataset. This demonstrates the generalization advantages of our multi-task approach.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">Furthermore, our approach compares well even with additionally fine-tuned methods, which are further advantaged by fine-tuning to a specific task. Specifically, Answer-Me outperforms all prior fine-tuned approaches on GQA datasets,
and outperforms some of them on all datasets, e.g.
VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>,
ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
SimVLM-Huge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> which is a 1.5 Billion parameter model trained on the 1.1 Billion image-language Align dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> largely performs best among these. Note that our multi-task open-vocabulary model is close in performance to SimVLM-Huge and outperforms SimVLM-Large on SNLI-VE, despite using smaller data and smaller model, which further demonstrates the power of multi-task diverse training for VQA.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p">As seen in <a href="#S5.T1" title="Table 1 ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>, <a href="#S5.T2" title="Table 2 ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a> and <a href="#S5.T4" title="Table 4 ‣ 5.2 Multi-task results ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>, there is progressive improvement with adding more tasks in the mixture and scaling the model, here 3X, produces consistently higher accuracies.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Experiments comparing to SOTA: specialized models (top section), and multi-task models, including ours (middle large section). Answer-Me largely outperforms other multi-task models despite working in the open-vocabulary generative setting. For reference we include pre-trained and fine-tuned models which are further advantaged by fine-tuning to each individual dataset (bottom section). As seen, Answer-Me still outperforms these on GQA and it even outperforms the Large version of SimVLM model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> and it is close to its SimVLM-Huge on SNLI-VE.
The best results among the pre-trained fine-tuned models are marked in italics.</figcaption>
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:400.4pt;height:373.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.2pt,20.7pt) scale(0.9,0.9) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Approach</td>
<td id="S5.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">VQA2.0 (dev)</td>
<td id="S5.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">NLVR2</td>
<td id="S5.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">SNLI-VE</td>
<td id="S5.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">GQA</td>
</tr>
<tr id="S5.T4.1.1.2.2" class="ltx_tr">
<td id="S5.T4.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">DFAF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S5.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.22</td>
<td id="S5.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T4.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T4.1.1.3.3" class="ltx_tr">
<td id="S5.T4.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r">Specialized from  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S5.T4.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">60.1</td>
<td id="S5.T4.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.3.3.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.1.4.4" class="ltx_tr">
<td id="S5.T4.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r">Suhr et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S5.T4.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">53.5</td>
<td id="S5.T4.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.4.4.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.1.5.5" class="ltx_tr">
<td id="S5.T4.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r">Xie et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
<td id="S5.T4.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">71.56</td>
<td id="S5.T4.1.1.5.5.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.1.6.6" class="ltx_tr">
<td id="S5.T4.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_r">Hudson &amp; Manning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S5.T4.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.6.6.5" class="ltx_td ltx_align_center">57.5</td>
</tr>
<tr id="S5.T4.1.1.7.7" class="ltx_tr">
<td id="S5.T4.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Frozen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> (VQA+pretraining)</td>
<td id="S5.T4.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.4</td>
<td id="S5.T4.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T4.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T4.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T4.1.1.8.8" class="ltx_tr">
<td id="S5.T4.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Nguyen et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> (VQA2.0+VG)</td>
<td id="S5.T4.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.35</td>
<td id="S5.T4.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T4.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T4.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T4.1.1.9.9" class="ltx_tr">
<td id="S5.T4.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_r">Multi-task GPV  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S5.T4.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r">62.5</td>
<td id="S5.T4.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.9.9.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.1.10.10" class="ltx_tr">
<td id="S5.T4.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_r">VL-BART <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S5.T4.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r">71.3</td>
<td id="S5.T4.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r">70.3</td>
<td id="S5.T4.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.10.10.5" class="ltx_td ltx_align_center">60.5</td>
</tr>
<tr id="S5.T4.1.1.11.11" class="ltx_tr">
<td id="S5.T4.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_r">VL-T5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S5.T4.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r">70.3</td>
<td id="S5.T4.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">73.6</td>
<td id="S5.T4.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.11.11.5" class="ltx_td ltx_align_center">60.8</td>
</tr>
<tr id="S5.T4.1.1.12.12" class="ltx_tr">
<td id="S5.T4.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_r">12-in-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S5.T4.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r">72.57</td>
<td id="S5.T4.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.12.12.3.1" class="ltx_text ltx_font_bold">78.4</span></td>
<td id="S5.T4.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r">76.78</td>
<td id="S5.T4.1.1.12.12.5" class="ltx_td ltx_align_center">60.12</td>
</tr>
<tr id="S5.T4.1.1.13.13" class="ltx_tr">
<td id="S5.T4.1.1.13.13.1" class="ltx_td ltx_align_left ltx_border_r">UniT (Coco init.) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S5.T4.1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r">66.97</td>
<td id="S5.T4.1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r">73.16</td>
<td id="S5.T4.1.1.13.13.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.1.14.14" class="ltx_tr">
<td id="S5.T4.1.1.14.14.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S5.T4.1.1.14.14.1.1" class="ltx_text ltx_font_bold">Answer-Me</span>, 8 tasks (Ours)</td>
<td id="S5.T4.1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.1</td>
<td id="S5.T4.1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.7</td>
<td id="S5.T4.1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.5</td>
<td id="S5.T4.1.1.14.14.5" class="ltx_td ltx_align_center ltx_border_t">72.8</td>
</tr>
<tr id="S5.T4.1.1.15.15" class="ltx_tr">
<td id="S5.T4.1.1.15.15.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S5.T4.1.1.15.15.1.1" class="ltx_text ltx_font_bold">Answer-Me</span>, 8 tasks, 3x scaling (Ours)</td>
<td id="S5.T4.1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.15.15.2.1" class="ltx_text ltx_font_bold">73.6</span></td>
<td id="S5.T4.1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r">73.9</td>
<td id="S5.T4.1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.15.15.4.1" class="ltx_text ltx_font_bold">85.8</span></td>
<td id="S5.T4.1.1.15.15.5" class="ltx_td ltx_align_center"><span id="S5.T4.1.1.15.15.5.1" class="ltx_text ltx_font_bold">77.5</span></td>
</tr>
<tr id="S5.T4.1.1.16.16" class="ltx_tr">
<td id="S5.T4.1.1.16.16.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">VisualBERT (pretr+FT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S5.T4.1.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">67.36</td>
<td id="S5.T4.1.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">66.7</td>
<td id="S5.T4.1.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">75.69</td>
<td id="S5.T4.1.1.16.16.5" class="ltx_td ltx_align_center ltx_border_tt">-</td>
</tr>
<tr id="S5.T4.1.1.17.17" class="ltx_tr">
<td id="S5.T4.1.1.17.17.1" class="ltx_td ltx_align_left ltx_border_r">ViLBERT (pretr+FT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="S5.T4.1.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r">70.55</td>
<td id="S5.T4.1.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.17.17.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.17.17.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.1.18.18" class="ltx_tr">
<td id="S5.T4.1.1.18.18.1" class="ltx_td ltx_align_left ltx_border_r">LXMERT (pretr+FT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S5.T4.1.1.18.18.2" class="ltx_td ltx_align_center ltx_border_r">69.9</td>
<td id="S5.T4.1.1.18.18.3" class="ltx_td ltx_align_center ltx_border_r">74.9</td>
<td id="S5.T4.1.1.18.18.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.18.18.5" class="ltx_td ltx_align_center">60.0</td>
</tr>
<tr id="S5.T4.1.1.19.19" class="ltx_tr">
<td id="S5.T4.1.1.19.19.1" class="ltx_td ltx_align_left ltx_border_r">Oscar (pretr+FT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S5.T4.1.1.19.19.2" class="ltx_td ltx_align_center ltx_border_r">73.61</td>
<td id="S5.T4.1.1.19.19.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.19.19.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.19.19.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.1.20.20" class="ltx_tr">
<td id="S5.T4.1.1.20.20.1" class="ltx_td ltx_align_left ltx_border_r">Uniter (pretr+FT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S5.T4.1.1.20.20.2" class="ltx_td ltx_align_center ltx_border_r">73.82</td>
<td id="S5.T4.1.1.20.20.3" class="ltx_td ltx_align_center ltx_border_r">79.12</td>
<td id="S5.T4.1.1.20.20.4" class="ltx_td ltx_align_center ltx_border_r">79.39</td>
<td id="S5.T4.1.1.20.20.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.1.21.21" class="ltx_tr">
<td id="S5.T4.1.1.21.21.1" class="ltx_td ltx_align_left ltx_border_r">VinVL (pretr+FT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S5.T4.1.1.21.21.2" class="ltx_td ltx_align_center ltx_border_r">75.95</td>
<td id="S5.T4.1.1.21.21.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.21.21.3.1" class="ltx_text ltx_font_italic">82.05</span></td>
<td id="S5.T4.1.1.21.21.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T4.1.1.21.21.5" class="ltx_td ltx_align_center"><span id="S5.T4.1.1.21.21.5.1" class="ltx_text ltx_font_italic">65.05</span></td>
</tr>
<tr id="S5.T4.1.1.22.22" class="ltx_tr">
<td id="S5.T4.1.1.22.22.1" class="ltx_td ltx_align_left ltx_border_r">SimVLM (Large) (pretr+FT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S5.T4.1.1.22.22.2" class="ltx_td ltx_align_center ltx_border_r">79.32</td>
<td id="S5.T4.1.1.22.22.3" class="ltx_td ltx_align_center ltx_border_r">84.13</td>
<td id="S5.T4.1.1.22.22.4" class="ltx_td ltx_align_center ltx_border_r">85.68</td>
<td id="S5.T4.1.1.22.22.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.1.23.23" class="ltx_tr">
<td id="S5.T4.1.1.23.23.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">SimVLM (Huge) (pretr+FT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S5.T4.1.1.23.23.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T4.1.1.23.23.2.1" class="ltx_text ltx_font_italic">80.03</span></td>
<td id="S5.T4.1.1.23.23.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T4.1.1.23.23.3.1" class="ltx_text ltx_font_italic">84.53</span></td>
<td id="S5.T4.1.1.23.23.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T4.1.1.23.23.4.1" class="ltx_text ltx_font_italic">86.21</span></td>
<td id="S5.T4.1.1.23.23.5" class="ltx_td ltx_align_center ltx_border_bb">-</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>
The pretrained base model trained on the 3 VQA (GQA, VG-QA) mix plus either VQA2.0 or SNLI-VE. This is evaluated on VQA2.0 and SNLI-VE. We further train the model on one of the tasks and repeat the evaluations. The results show that fine tuned models tend to forget (first/second rows), even if original mix shows good within-data and out-of sample generalization (first rows). Additional fine-tuning seems to recover the losses within a task (first/third rows), but costs <math id="S5.T5.2.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.T5.2.m1.1b"><mi id="S5.T5.2.m1.1.1" xref="S5.T5.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.T5.2.m1.1c"><ci id="S5.T5.2.m1.1.1.cmml" xref="S5.T5.2.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.m1.1d">N</annotation></semantics></math> times the cost in training, and performance on the other task deteriorates again. Interestingly, this model performs even worse than the original out-of-sample mixture on the second task.
Training on many tasks in the mix maintains performance (last row).
</figcaption>
<div id="S5.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:380.5pt;height:160.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.9pt,0.8pt) scale(0.99,0.99) ;">
<table id="S5.T5.3.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.3.1.1.1" class="ltx_tr">
<td id="S5.T5.3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Approach</td>
<td id="S5.T5.3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Num</td>
<td id="S5.T5.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">VQA2.0</td>
<td id="S5.T5.3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">SNLI-VE</td>
</tr>
<tr id="S5.T5.3.1.2.2" class="ltx_tr">
<td id="S5.T5.3.1.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3-task + VQA2.0 (ours)</td>
<td id="S5.T5.3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Single</td>
<td id="S5.T5.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">64.3</td>
<td id="S5.T5.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">33.8</td>
</tr>
<tr id="S5.T5.3.1.3.3" class="ltx_tr">
<td id="S5.T5.3.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r">3-task + VQAv, FT on SNLI</td>
<td id="S5.T5.3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">Multiple</td>
<td id="S5.T5.3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_rr">35.5</td>
<td id="S5.T5.3.1.3.3.4" class="ltx_td ltx_align_center">76.9</td>
</tr>
<tr id="S5.T5.3.1.4.4" class="ltx_tr">
<td id="S5.T5.3.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r">3-task + VQA2.0, FT on VQA2.0 (ours)</td>
<td id="S5.T5.3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">Multiple</td>
<td id="S5.T5.3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_rr">65.2</td>
<td id="S5.T5.3.1.4.4.4" class="ltx_td ltx_align_center">26.7</td>
</tr>
<tr id="S5.T5.3.1.5.5" class="ltx_tr">
<td id="S5.T5.3.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3-task + SNLI-VE (ours)</td>
<td id="S5.T5.3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Single</td>
<td id="S5.T5.3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">33.2</td>
<td id="S5.T5.3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">76.5</td>
</tr>
<tr id="S5.T5.3.1.6.6" class="ltx_tr">
<td id="S5.T5.3.1.6.6.1" class="ltx_td ltx_align_left ltx_border_r">3-task + SNLI-VE, FT on VQA2.0</td>
<td id="S5.T5.3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">Multiple</td>
<td id="S5.T5.3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_rr">64.8</td>
<td id="S5.T5.3.1.6.6.4" class="ltx_td ltx_align_center">24.5</td>
</tr>
<tr id="S5.T5.3.1.7.7" class="ltx_tr">
<td id="S5.T5.3.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r">3-task + SNLI-VE, FT on SNLI (ours)</td>
<td id="S5.T5.3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r">Multiple</td>
<td id="S5.T5.3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_rr">29.4</td>
<td id="S5.T5.3.1.7.7.4" class="ltx_td ltx_align_center">77.2</td>
</tr>
<tr id="S5.T5.3.1.8.8" class="ltx_tr">
<td id="S5.T5.3.1.8.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Multi-task (w/o VQA2.0 or SN) Zero-Shot (ours)</td>
<td id="S5.T5.3.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Single</td>
<td id="S5.T5.3.1.8.8.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">27.3</td>
<td id="S5.T5.3.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">24.2</td>
</tr>
<tr id="S5.T5.3.1.9.9" class="ltx_tr">
<td id="S5.T5.3.1.9.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">5-task (ours)</td>
<td id="S5.T5.3.1.9.9.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Single</td>
<td id="S5.T5.3.1.9.9.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr">65.1</td>
<td id="S5.T5.3.1.9.9.4" class="ltx_td ltx_align_center ltx_border_bb">77.4</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Answer-Me prevents catastrophic forgetting</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">While pretraining and fine-tuning, as is customarily done in previous works, produces accurate models, it tends to overfit to the new data and to immediately forget other datasets or tasks, even when it was previously trained on them. We show that Answer-Me, through the mixture training, is more robust, as it is able to sustain good performance across tasks. In <a href="#S5.T5" title="Table 5 ‣ 5.2 Multi-task results ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 5</span></a>, we find that when training on one task, then finetuning on a second (as commonly done in previous works), the model does well on the new task, but performs poorly on the first as it ‘forgets’, achieving accuracies close to the zero-shot model; additional fine-tuning on the first task actually makes the performance on the second one even worse than zero-shot. In contrast, when using the mixture training for Answer-Me, the model maintains the single-task performance for both datasets.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Novel semantic ‘detection’ task</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">We also examine Answer-Me’s ability to ‘detect’ objects through language. This is quite different from the other tasks presented in the paper, and it shows the model’s ability to understand the whole image and many objects present in it, even in a zero-shot setting. The task is to output as text the names of all the objects in the image. If an object appears multiple times, e.g., 3 times, then the model should output the name 3 times. We evaluate this on the standard MsCoco detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and compute the precision, recall and <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><msub id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mi id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml">F</mi><mn id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS4.p1.1.m1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2">𝐹</ci><cn type="integer" id="S5.SS4.p1.1.m1.1.1.3.cmml" xref="S5.SS4.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">F_{1}</annotation></semantics></math> values based on how many output object names match the ground truth objects. This is challenging as it requires localization awareness of the models, but without extra box regression layers and training that are not part of Answer-Me. It is also unique with respect to other question-answering tasks. We here evaluate this task in a zero-shot and full mixture scenarios. <a href="#S5.T6" title="Table 6 ‣ 5.4 Novel semantic ‘detection’ task ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 6</span></a> shows that the Answer-Me model and training is able to do well both in zero-shot and FT settings.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>‘Detection’ task, conveyed by text, where the desired outcome is to list (in text) all objects present in the image. This evaluation is on the MsCoco detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. As seen, Answer-Me mixture of question-answering tasks helps boost this new detection task beyond MsCoco. The Zero-Shot portion does not use MsCoco data for training. The (A) and (B) task mix are different 4-task mixtures; they both have VQA2.0, SNLI-VE, GQA, 4-task(A) has VG-QA, whereas 4-task(B) has VG region descriptions instead, which is clearly much more advantageous.
</figcaption>
<div id="S5.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:250.7pt;height:160.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.3pt,0.8pt) scale(0.99,0.99) ;">
<table id="S5.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T6.1.1.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Train Data</th>
<td id="S5.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Recall</td>
<td id="S5.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Precision</td>
<td id="S5.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">F1</td>
</tr>
<tr id="S5.T6.1.1.2.2" class="ltx_tr">
<th id="S5.T6.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MsCoco (from scratch)</th>
<td id="S5.T6.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">0.25</td>
<td id="S5.T6.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">0.21</td>
<td id="S5.T6.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">0.22</td>
</tr>
<tr id="S5.T6.1.1.3.3" class="ltx_tr">
<th id="S5.T6.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MsCoco</th>
<td id="S5.T6.1.1.3.3.2" class="ltx_td ltx_align_center">0.31</td>
<td id="S5.T6.1.1.3.3.3" class="ltx_td ltx_align_center">0.24</td>
<td id="S5.T6.1.1.3.3.4" class="ltx_td ltx_align_center">0.27</td>
</tr>
<tr id="S5.T6.1.1.4.4" class="ltx_tr">
<th id="S5.T6.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">4-task (A) mix + MsCoco</th>
<td id="S5.T6.1.1.4.4.2" class="ltx_td ltx_align_center">0.34</td>
<td id="S5.T6.1.1.4.4.3" class="ltx_td ltx_align_center">0.30</td>
<td id="S5.T6.1.1.4.4.4" class="ltx_td ltx_align_center">0.31</td>
</tr>
<tr id="S5.T6.1.1.5.5" class="ltx_tr">
<th id="S5.T6.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">4-task (B) mix + MsCoco</th>
<td id="S5.T6.1.1.5.5.2" class="ltx_td ltx_align_center"><span id="S5.T6.1.1.5.5.2.1" class="ltx_text ltx_font_bold">0.55</span></td>
<td id="S5.T6.1.1.5.5.3" class="ltx_td ltx_align_center"><span id="S5.T6.1.1.5.5.3.1" class="ltx_text ltx_font_bold">0.52</span></td>
<td id="S5.T6.1.1.5.5.4" class="ltx_td ltx_align_center"><span id="S5.T6.1.1.5.5.4.1" class="ltx_text ltx_font_bold">0.53</span></td>
</tr>
<tr id="S5.T6.1.1.6.6" class="ltx_tr">
<th id="S5.T6.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="3">Zero-shot</th>
<td id="S5.T6.1.1.6.6.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T6.1.1.7.7" class="ltx_tr">
<th id="S5.T6.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">pretraining</th>
<td id="S5.T6.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">0.03</td>
<td id="S5.T6.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">0.02</td>
<td id="S5.T6.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">0.02</td>
</tr>
<tr id="S5.T6.1.1.8.8" class="ltx_tr">
<th id="S5.T6.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">4-task (A) mix</th>
<td id="S5.T6.1.1.8.8.2" class="ltx_td ltx_align_center">0.06</td>
<td id="S5.T6.1.1.8.8.3" class="ltx_td ltx_align_center">0.04</td>
<td id="S5.T6.1.1.8.8.4" class="ltx_td ltx_align_center">0.05</td>
</tr>
<tr id="S5.T6.1.1.9.9" class="ltx_tr">
<th id="S5.T6.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">4-task (B) mix</th>
<td id="S5.T6.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T6.1.1.9.9.2.1" class="ltx_text ltx_font_bold">0.20</span></td>
<td id="S5.T6.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T6.1.1.9.9.3.1" class="ltx_text ltx_font_bold">0.14</span></td>
<td id="S5.T6.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T6.1.1.9.9.4.1" class="ltx_text ltx_font_bold">0.16</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2205.00949/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="266" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example Answer-Me results, both successful and unsuccessful. We note that training for question-answering tends to tweak otherwise well sounding image captions to answer the questions more adequately.
</figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Ablations</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p"><a href="#S5.T7" title="Table 7 ‣ 5.5 Ablations ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 7</span></a> shows the performance of various tasks for pretraining. We find that using all 4 tasks provides the best model, as it is able to better train all parts of the model, including the decoder which is often ignored in contrastive-style pretraining and the encoder ignored in captioning pretraining. Adding these tasks is simple as they all use the same loss, just slightly different preprocessing.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Study on different tasks for pretraining (individually fine-tuned). Using all four pretraining tasks is best, and outperforms any one of them used alone, in some cases by large margins.</figcaption>
<div id="S5.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:185.0pt;height:106.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.9pt,0.5pt) scale(0.99,0.99) ;">
<table id="S5.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.1.1.1.1" class="ltx_tr">
<th id="S5.T7.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">PT Task</th>
<th id="S5.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VQA2.0</th>
<th id="S5.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SNLI-VE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.1.1.2.1" class="ltx_tr">
<th id="S5.T7.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Captioning</th>
<td id="S5.T7.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">62.3</td>
<td id="S5.T7.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">75.2</td>
</tr>
<tr id="S5.T7.1.1.3.2" class="ltx_tr">
<th id="S5.T7.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CapCompletion</th>
<td id="S5.T7.1.1.3.2.2" class="ltx_td ltx_align_center">60.1</td>
<td id="S5.T7.1.1.3.2.3" class="ltx_td ltx_align_center">73.8</td>
</tr>
<tr id="S5.T7.1.1.4.3" class="ltx_tr">
<th id="S5.T7.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ITM</th>
<td id="S5.T7.1.1.4.3.2" class="ltx_td ltx_align_center">54.5</td>
<td id="S5.T7.1.1.4.3.3" class="ltx_td ltx_align_center">74.2</td>
</tr>
<tr id="S5.T7.1.1.5.4" class="ltx_tr">
<th id="S5.T7.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MLM</th>
<td id="S5.T7.1.1.5.4.2" class="ltx_td ltx_align_center">58.3</td>
<td id="S5.T7.1.1.5.4.3" class="ltx_td ltx_align_center">72.4</td>
</tr>
<tr id="S5.T7.1.1.6.5" class="ltx_tr">
<th id="S5.T7.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">All 4</th>
<td id="S5.T7.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T7.1.1.6.5.2.1" class="ltx_text ltx_font_bold">65.2</span></td>
<td id="S5.T7.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T7.1.1.6.5.3.1" class="ltx_text ltx_font_bold">77.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">We also experiment with an alternative architectural choice for fusion in which a transformer encoder/decoder layer is used instead of concatenation and transformer encoders. In this alternative approach, the queries come from the text input and the key/value comes from the image input.
<a href="#S5.T8" title="Table 8 ‣ 5.5 Ablations ‣ 5 Experimental results ‣ Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 8</span></a> shows that using the encoder/decoder fusion style has little to no effect on the benchmark metrics but the number of parameters is higher compared to the encoder-only fusion. We therefore opted to use concatenation and transformer encoders for fusion.</p>
</div>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Study on different fusion layer types. Both approaches behave similarly but encoder fusion has fewer parameters. The experiment is done with no pretraining, so the numbers are lower. </figcaption>
<div id="S5.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:262.0pt;height:53.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.3pt,0.3pt) scale(0.99,0.99) ;">
<table id="S5.T8.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T8.1.1.1.1" class="ltx_tr">
<th id="S5.T8.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Fusion Approach</th>
<th id="S5.T8.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VQA2.0</th>
<th id="S5.T8.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SNLI-VE</th>
<th id="S5.T8.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Num Params</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T8.1.1.2.1" class="ltx_tr">
<th id="S5.T8.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Encoder</th>
<td id="S5.T8.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">49.05</td>
<td id="S5.T8.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">73.1</td>
<td id="S5.T8.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">332M</td>
</tr>
<tr id="S5.T8.1.1.3.2" class="ltx_tr">
<th id="S5.T8.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Encoder/Decoder</th>
<td id="S5.T8.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">48.9</td>
<td id="S5.T8.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">73.3</td>
<td id="S5.T8.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">346M</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p"><span id="S5.SS5.p3.1.1" class="ltx_text ltx_font_bold">Limitations.</span> One limitation of our approach is that generated text outputs might not always be best, this is likely because we use data and models of modest sizes.
It is also unable to read text, which can be remedied from the inclusion of TextVQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> or large diverse datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. Other limitations are related to measuring the performance of these models. The evaluation can be further refined and extended, by including human judgement.</p>
</div>
<div id="S5.SS5.p4" class="ltx_para">
<p id="S5.SS5.p4.1" class="ltx_p"><span id="S5.SS5.p4.1.1" class="ltx_text ltx_font_bold">Societal impact.</span>
Multi-modal image-language models in conjunction with large datasets can potentially use training examples with biased or offensive text or images. While best efforts have been applied in the collection of the Conceptual Captions dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, which we use as a pretraining dataset, we are aware of the potential pitfalls of large image-language corpora with regards to bias, fairness and safety.
Our mitigation strategy is to not share, release or otherwise deploy models trained on this data
and use these results for evaluating the capabilities of the proposed methods and technologies, to facilitate future research in this direction.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We introduce Answer-Me, a framework for multi-task training and pretraining, for answering natural language questions towards an image with natural responses for multiple tasks, such as VQA, visual reasoning, and others. It can answer questions from multiple diverse tasks seamlessly, understanding the intent of the question without specific task specification or additional prompts, and can generalize to novel tasks.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">We show Answer-Me is capable of zero-shot QA tasks and is robust to forgetting on a variety of tasks. The model is simple, has an easily scalable and extensible architecture which learns the image-language interactions across many tasks.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Answer-Me performs well with respect to SOTA, largely outperforming prior multi-task models, and in some cases the fine-tuned (and sometimes larger) models. It is being evaluated in the open-vocabulary setting which is a more challenging one, demonstrating the power of our approach for more practical scenarios. We hope that other methods adopt the open-vocabulary generative setting in evaluation in future work, as well.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Agrawal, A., Lu, J., Antol, S., Mitchell, M., Zitnick, C.L., Batra, D., Parikh,
D.: Vqa: Visual question answering. In: ICCV (2015)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
L.: Bottom-up and top-down attention for image captioning and visual question
answering. In: CVPR (2018)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bowman, S.R., Angeli, G., Potts, C., Manning, C.D.: A large annotated corpus
for learning natural language inference. In: EMNLP (2015)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.:
End-to-end object detection with transformers. In:
https://arxiv.org/abs/2005.12872 (2020)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Changpinyo, S., Pang, B., Sharma, P., Soricut, R.: Decoupled box proposal and
featurization with ultrafine-grained semantic labels improve image captioning
and visual question answering. In: Conference on Empirical Methods in Natural
Language Processing (EMNLP) (2019)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Changpinyo, S., Pont-Tuset, J., Ferrari, V., Soricut, R.: Telling the what
while pointing to the where: Multimodal queries for image retrieval. In:
Arxiv 2102.04980 (2021)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing
web-scale image-text pre-training to recognize long-tail visual concepts. In:
CVPR (2021)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollar, P., Zitnick,
C.L.: Microsoft coco captions: Data collection and evaluation server. In:
https://arxiv.org/abs/1504.00325 (2015)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chen, Y.C., Li, L., Yu, L., Kholy, A.E., Ahmed, F., Gan, Z., Cheng, Y., Liu,
J.: Uniter: Universal image-text representation learning. In: ECCV (2020)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Cho, J., Lei, J., Tan, H., Bansal, M.: Unifying vision-and-language tasks via
text generation. In: Arxiv 2102.02779 (2021)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M.F., Parikh,
D., Batra, D.: Visual dialog. In: CVPR (2017)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Deng, C., Chen, S., Chen, D., He, Y., Wu, Q.: Sketch, ground, and refine:
Top-down dense video captioning. In: CVPR (2021)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A
large-scale hierarchical image database. In: CVPR (2009)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Deng, J., Yang, Z., Chen, T., Zhou, W., Li, H.: Transvg: End-to-end visual
grounding with transformers. ICCV (2021)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Desai, K., Johnson, J.: Virtex: Learning visual representations from textual
annotations. In: CVPR (2021)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 (2018)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Gan, Z., Chen, Y.C., Li, L., Zhu, C., Cheng, Y., Liu, J.: Large-scale
adversarial training for vision-and-language representation learning. In:
NeurIPS (2020)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in
VQA matter: Elevating the role of image understanding in visual
question answering. In: CVPR (2017)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Gupta, T., Kamath, A., Kembhavi, A., Hoiem, D.: Towards general purpose vision
systems. In: arxiv.org/abs/2104.00743 (2021)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Gurari, D., Li, Q., Stangl, A.J., Guo, A., Luo, C.L.K.G.J., Bigham, J.P.:
VizWiz grand challenge: Answering visual questions from blind people. In:
CVPR (2018)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Hu, R., Singh, A.: Unit: Multimodal multitask learning with a unified
transformer. In: arxiv.org/abs/2102.10772 (2021)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Huang, G.,  , B.P., Zhu, Z., Rivera, C., Soricut, R.: Multimodal pretraining
for dense video captioning. In: AACL-IJCNLP (2020)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Huang, Z., Zeng, Z., Huang, Y., Liu, B., Fu, D., Fu, J.: Seeing out of the
box:end-to-end pre-training for vision-language representation learning. In:
CVPR (2021)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Hudson, D.A., Manning, C.D.: Gqa: a new dataset for compositional question
answering over realworld images. In: CVPR (2019)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.V., Sung,
Y., Li, Z., Duerig, T.: Scaling up visual and vision-language representation
learning with noisy text supervision. In: ICML (2021)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Jiang, H., Misra, I., Rohrbach, M., Learned-Miller, E., Chen, X.: In defense of
grid features for visual question answering. In: CVPR (2020)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Kamath, A., Singh, M., LeCun, Y., Misra, I., Synnaeve, G., Carion, N.: Mdetr -
modulated detection for end-to-end multi-modal understanding. In:
https://arxiv.org/abs/2104.12763 (2021)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Kim, W., Son, B., Kim, I.: Vilt: Vision-and-language transformer without
convolution or region supervision. In: ICML (2021)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Kottur, S., Moura, J.M.F., Parikh, D., Batra, D., Rohrbach, M.: Visual
coreference resolution in visual dialog using neural module networks. In:
ECCV (2018)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual
genome: Connecting language and vision using crowdsourced dense image
annotations (2016), <a target="_blank" href="https://arxiv.org/abs/1602.07332" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1602.07332</a>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple
and performant baseline for vision and language. arXiv preprint
arXiv:1908.03557 (2019)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong,
L., Wei, F., Choi, Y., Gao, J.: Oscar: Object-semantics aligned pre-training
for vision-language tasks. In: ECCV (2020)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Liao, Y., Liu, S., Li, G., Wang, F., Chen, Y., Qian, C., Li, B.: A real-time
cross-modality correlation filtering method for referring expression
comprehension. In: CVPR. pp. 10880–10889 (2020)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar,
P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In:
ECCV (2014)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Lin, X., Bertasius, G., Wang, J., Chang, S.F., Parikh, D.: Vx2text: End-to-end
learning of video-based text generation from multimodal inputs. In: CVPR
(2021)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Liu, D., Zhang, H., Wu, F., Zha, Z.J.: Learning to assemble neural module tree
networks for visual grounding. In: ICCV (2019)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Liu, X., He, P., Chen, W., Gao, J.: Multi-task deep neural networks for natural
language understanding. In: In Proceedings of ACL (2019)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Liu, Y., Huang, L., Song, L., Wang, B., Zhang, Y., Pan, P.: Enhancing textual
cues in multi-modal transformers for VQA. In: VizWiz Challenge 2021
(2021)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Liu, Z., Stent, S., Li, J., Gideon, J., Han, S.: Loctex: Learning
data-efficient visual representations from localized textual supervision. In:
ICCV (2021)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic
visiolinguistic representations for vision-and-language tasks. In: CVPR
(2019)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Lu, J., Goswami, V., Rohrbach, M., Parikh, D., Lee, S.: 12-in-1: Multi-task
vision and language representation learning. In: CVPR (2020)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A., Murphy, K.: Generation
and comprehension of unambiguous object descriptions. In: CVPR (2016)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Margffoy-Tuay, E., Perez, J.C., Botero, E., Arbelaez, P.: Dynamic multimodal
instance segmentation guided by natural language queries. In: ECCV (2018)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Nguyen, D.K., Okatani, T.: Multi-task learning of hierarchical vision-language
representation. In: CVPR (2019)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Peng, G., Jiang, Z., You, H., Lu, P., Hoi, S., Wang, X., Li, H.: Dynamic fusion
with intra- and inter- modality attention flow for visual question answering.
In: CVPR (2019)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Plummer, B.A., Shih, K.J., Li, Y., Xu, K., Lazebnik, S., Sclaroff, S., Saenko,
K.: Revisiting image-language networks for open-ended phrase detection. In:
TPAMI (2020)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J.,
Lazebnik, S.: Flickr30k entities: Collecting region-to-phrase correspondences
for richer image-to-sentence models. In: International Journal of Computer
Vision (2017)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Pont-Tuset, J., Uijlings, J., Changpinyo, S., Soricut, R., Ferrari, V.:
Connecting vision and language with localized narratives. In: ECCV (2020)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Pramanik, S., Agrawal, P., Hussain, A.: Omninet: A unified architecture for
multi-modal multi-task learning. In: arxiv.org/abs/1907.07804 (2019)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Qiao, Y., Deng, C., Wu, Q.: Referring expression comprehension: A survey of
methods and datasets. In: IEEE Transactions on Multimedia (2020)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning
transferable visual models from natural language supervision. In: ICML (2021)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a
unified text-to-text transformer. In: Journal of Machine Learning Research
(2020)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object
detection with region proposal networks. In: Advances in Neural Information
Processing Systems (2015)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Rohrbach, A., Rohrbach, M., Hu, R., Darrell, T., Schiele, B.: Grounding of
textual phrases in images by reconstruction. In: ECCV (2016)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Rohrbach, A., Rohrbach, M., Tandon, N., Schiele, B.: A dataset for movie
description. In: CVPR (2015)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Sariyildiz, M.B., Perez, J., Larlus, D.: Learning visual representations with
caption annotations. In: ECCV (2020)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In: ACL
(2018)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Shuster, K., Ju, D., Roller, S., Dinan, E., Boureau, Y.L., Weston, J.: The
dialogue dodecathlon: Open-domain knowledge and image grounded conversational
agents. In: ACL

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Singh, A., Natarjan, V., Shah, M., Jiang, Y., Chen, X., Parikh, D., Rohrbach,
M.: Towards vqa models that can read. In: CVPR (2019)

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Srinivasan, K., Raman, K., Chen, J., Bendersky, M., Najork, M.: Wit:
Wikipedia-based image text dataset for multimodal multilingual machine
learning. In: arXiv:2103.01913 (2021)

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Suhr, A., Lewis, M., Yeh, J., Artzi, Y.: A corpus of natural language for
visual reasoning. In: Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (2017)

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Sun, C., Myers, A., Vondrick, C., Murphy, K., Schmid, C.: Videobert: A joint
model for video and language representation learning. In: ICCV (2019)

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Tan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations
from transformers. In: EMNLP (2019)

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Tsimpoukelli, M., Menick, J., Cabi, S., Eslami, S.M.A., Vinyals, O., Hill, F.:
Multimodal few-shot learning with frozen language models (2021),
<a target="_blank" href="https://arxiv.org/abs/2106.13884" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2106.13884</a>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Wang, Z., Yu, J., Yu, A.W., Dai, Z., Tsvetkov, Y., Cao, Y.: Simvlm: Simple
visual language model pretraining with weak supervision. In:
https://arxiv.org/pdf/2108.10904.pdf (2021)

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Weijie Su, Xizhou Zhu, Y.C.B.L.L.L.F.W.J.D.: Vl-bert: Pre-training of generic
visual-linguistic representations. In: ICLR (2020)

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Whitehead, S., Wu, H., Ji, H., Feris, R., Saenko, K.: Separating skills and
concepts for novel visual question answering. In: CVPR (2021)

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Xie, N., Lai, F., Doran, D., Kadav, A.: Visual entailment: A novel task for
fine-grained image understanding. In: https://arxiv.org/abs/1901.06706 (2019)

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset
for bridging video and language. In: CVPR (2016)

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Yang, Z., Chen, T., Wang, L., Luo, J.: Improving one-stage visual grounding by
recursive sub-query construction. In: ECCV (2020)

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Yang, Z., Gong, B., Wang, L., Huang, W., Yu, D., Luo, J.: A fast and accurate
one-stage approach to visual grounding. In: ICCV (2019)

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Yin, D., Li, L.H., Hu, Z., Peng, N., Chang, K.W.: Broaden the vision:
Geo-diverse visual commonsense reasoning. In: EMNLP (2021)

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Yu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H., Wang, H.: Ernie-vil:
Knowledge enhanced vision-language representations through scene graph. In:
AAAI (2021)

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., Berg, T.L.: Mattnet:
Modular attention network for referring expression comprehension. In: CVPR
(2021)

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in
referring expressions. In: ECCV (2016)

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Zellers, R., Bisk, Y., Farhadi, A., Choi, Y.: From recognition to cognition:
Visual commonsense reasoning. In: CVPR (June 2019)

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., Gao, J.:
Vinvl: Revisiting visual representations in vision-language models. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 5579–5588 (2021)

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J.J., Gao, J.: Unified
vision-language pre-training for image captioning and vqa. In: AAAI (2020)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.00948" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.00949" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.00949">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.00949" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.00950" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 15:52:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
