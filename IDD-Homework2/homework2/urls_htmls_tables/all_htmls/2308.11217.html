<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.11217] Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this.</title><meta property="og:description" content="With the tremendous success of large language models represented by ChatGPT, artificial intelligence has ushered in a new wave and entered the era of big models. Multimodal data, which can comprehensively perceive and â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.11217">

<!--Generated on Wed Feb 28 12:18:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
multimodal model,  federated learning,  big model
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models
<br class="ltx_break"><span id="id16.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>Identify applicable funding agency here. If none, delete this.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zengxiang Li<sup id="id17.16.id1" class="ltx_sup"><span id="id17.16.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Zhaoxiang Hou<sup id="id18.17.id2" class="ltx_sup"><span id="id18.17.id2.1" class="ltx_text ltx_font_italic">1</span></sup>, Hui Liu<sup id="id19.18.id3" class="ltx_sup"><span id="id19.18.id3.1" class="ltx_text ltx_font_italic">1</span></sup>, Ying Wang<sup id="id20.19.id4" class="ltx_sup"><span id="id20.19.id4.1" class="ltx_text ltx_font_italic">1</span></sup>,Tongzhi Li<sup id="id21.20.id5" class="ltx_sup"><span id="id21.20.id5.1" class="ltx_text ltx_font_italic">1</span></sup>, Longfei Xie<sup id="id22.21.id6" class="ltx_sup"><span id="id22.21.id6.1" class="ltx_text ltx_font_italic">1</span></sup>, Chao Shi<sup id="id23.22.id7" class="ltx_sup"><span id="id23.22.id7.1" class="ltx_text ltx_font_italic">1</span></sup>,Chengyi Yang<sup id="id24.23.id8" class="ltx_sup"><span id="id24.23.id8.1" class="ltx_text ltx_font_italic">1</span></sup>, 
<br class="ltx_break">Weishan Zhang<sup id="id25.24.id9" class="ltx_sup"><span id="id25.24.id9.1" class="ltx_text ltx_font_italic">2</span></sup>, Zelei Liu<sup id="id26.25.id10" class="ltx_sup"><span id="id26.25.id10.1" class="ltx_text ltx_font_italic">3</span></sup>, Liang Xu<sup id="id27.26.id11" class="ltx_sup"><span id="id27.26.id11.1" class="ltx_text ltx_font_italic">4</span></sup> 
<br class="ltx_break"><sup id="id28.27.id12" class="ltx_sup"><span id="id28.27.id12.1" class="ltx_text ltx_font_italic">1</span></sup>ENN Group Co.,Ltd., China
<br class="ltx_break"><sup id="id29.28.id13" class="ltx_sup"><span id="id29.28.id13.1" class="ltx_text ltx_font_italic">2</span></sup>China University of Petroleum, China
<br class="ltx_break"><sup id="id30.29.id14" class="ltx_sup"><span id="id30.29.id14.1" class="ltx_text ltx_font_italic">3</span></sup>Unicom (Shanghai) Industrial Internet Co.,Ltd., China
<br class="ltx_break"><sup id="id31.30.id15" class="ltx_sup"><span id="id31.30.id15.1" class="ltx_text ltx_font_italic">4</span></sup>Qingdao Windaka Technology Co.,Ltd., China
<br class="ltx_break">e-mail:{lizengxiang,houzhaoxiang,liuhuiau,wangyingbj,litongzhi,
xielongfei,shichaog,yangchengyia}@enn.cn, 
<br class="ltx_break">zhw@s.upc.edu.cn, liuzl231@chinaunicom.cn, xuliang.upc.edu@gmail.com
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id32.id1" class="ltx_p">With the tremendous success of large language models represented by ChatGPT, artificial intelligence has ushered in a new wave and entered the era of big models. Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient coordination of the federated learning platform, technical innovations on data quality improvement based on large model capabilities and efficient joint fine-tuning approaches. Preliminary experiments show that enterprises can enhance and accumulate intelligent capabilities through multimodal model federated learning, thereby jointly creating an smart city model that provides high-quality intelligent services covering energy infrastructure safety, residential community security, and urban operation management. The established federated learning cooperation ecosystem is expected to further aggregate industry, academia, and research resources, realize large models in multiple vertical domains, and promote the large-scale industrial application of artificial intelligence and cutting-edge research on multimodal federated learning.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
multimodal model, federated learning, big model

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Thanks to several decades of rapid development in data, computing power, and algorithms, artificial intelligence has entered a new era of enthusiasm. Large-scale language models such as ChatGPT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> have achieved incredible effects, particularly in understanding human intentions, context learning, and zero-shot learning for general tasks. The achievements have swiftly influenced the field of computer vision as well. Works like SAM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and UniDetector<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> have demonstrated capabilities in open-world object recognition and segmentation. This surge in research breakthroughs and astonishing intelligent abilities has
instilled the hope of achieving universal artificial intelligence.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To realize general artificial intelligence, intelligent agents need to be capable of processing and comprehending multimodal information while interacting with the real worldÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Multimodality has an innate advantage in information fusion to cover everything from traditional sensory recognition to complex cognitive reasoning and decision making tasks. For instance, by adding lip motion image modal data, some vague pronunciations can be accurately recognized, in the autonomous driving domain, by comprehensively utilizing sensor and camera signals, the perception of road conditions is enhanced, in the field of robot control, RGB-D and 3D point clouds, due to the introduction of depth information, have given intelligent entities a concept of â€distanceâ€ in their perception of the environment.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Multimodal data, which includes text, image, video, 3D point clouds, audio, vibration signals, and more, inherently possess characteristics such as heterogeneity, interactivity, and correlations. Effectively combining diverse sources of multimodal data using cutting-edge large-scale model techniques and deeply exploring the relationships among various modalities to integrate information for intelligent emergent effects has rapidly become one of the most prominent research topics. The mainstream approaches includes fusing multimodal feature through cross-attentions such as CLIP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>,ALBEF<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and BLIP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and incorporating multimodal information into a powerful large language model such as OFA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Flamingo<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, Palm-E<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> .</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.11217/assets/FL.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Private data fine-tuning and federated learning amongst multiple companies are essential to transform a general large model to domain-specific model for real-world industrial application adoption.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Most of the multimodal large models provided by AI giants and start-ups are based on public datasets. They could achieve high performance on the validation datasets in experimental environments and could adapt to some application scenarios requiring common sense only. However, these models often fall short of expectations in specific industrial applications. To enhance their performance, fine-tuning of these general large models with enterprise private data, alongside the integration of domain expertise, becomes essential to a successful adoption. This process enables the creation of domain-specific large models, enriching domain understanding and producing more specialized and accurate outputs. The method of fine-tuning based on large models can integrate domain knowledge on top of world knowledge. Compared with the traditional small models trained end-to-end in special domains, it has better generalization capability and can quickly adapt to open industrial application environments. This is conducive to the adoption of industrial intelligence in large amounts of application scenarios with lower cost.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">As shown in Fig <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, by fine-tuning the general large model based on its proprietary data, a company can enhance model intelligent capabilities with integrated domain-knowledge, such as special device recognition and safety inspection rules, in its existing application scenarios. However, the application scenarios, proprietary data and expert knowledge of an individual company is often insufficient to cover the entire industry domain. Only through ecological cooperation, a number of companies could contribute complementary and multimodal data, as well as different aspects of expert knowledge, and thus jointly train a domain-specific large model covering as many as scenarios in the industry domain. This model can support various intelligent tasks complementing and stacking each other, and have stable and reliable performance in various practical application environments. For instance, models jointly trained by energy companies, household appliance companies, and healthcare companies can provide household customers with intelligent services for low-carbon and healthy living.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2308.11217/assets/framework.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="260" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Federated learning framework in big model era: enables companies collaboratively train domain-specific models by harnessing the capabilities of an open general large model while utilizing their private multimodal data.</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Due to data privacy and asset value concerns, companies are reluctant to share data with each other. Federated learning(FL)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> provides a new AI paradigm to enable companies to jointly train a model while keeping data within their local computing resources. Traditional federated learning leverages similar or relevant data from multiple companies to train a global model from scratch, for the purpose of improving higher accuracy within the specific application scenarios. In big model era, the concept and definition of federated learning should be extended and reconsidered to enable aforementioned ecological cooperation aiming at domain specific large models for industrial applications. It starts by leveraging the power of pre-trained large models using vast public datasets as the intelligence foundation, and jointly fine-tuning the foundation model using multimodal data from various companies. The multimodal large model generated by federated learning not only has the intelligence capabilities of each participant, but also integrates and fuses their intelligence. As a result, it enables all-encompassing intelligent capabilities across industrial scenarios and expanding the intelligent application boundaries for individual participants. As shown in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, thanks to the collaborative intelligence among a city operation management company, a residential security company and an energy company, the multimodal model could provide informative description in terms of the environment background (bicycle lane along a arterial road), nearby energy infrastructure (e.g., natural gas pipeline and well cover), as well as peopleâ€™s feature and behaviour (e.g, a woman in read squatting and selling vegetables).</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In this paper, we propose a foundational technical framework for multimodal federated learning that enables multiple enterprises to collaboratively train large-scale models in vertical domains using proprietary data sources. The paper is organized as follows, SectionÂ <a href="#S2" title="II Federated Learning Framework for Domain-Specific Multimodal Large Models â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> illustrates the details of federated learning multimodal model framework and corresponding research directions to explore. SectionÂ <a href="#S3" title="III Case Study: Multimodal Large Model for Urban Operation Management â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> presents a smart city vision-language model case study, including federated learning framework deployment and data enhancement methods. SectionÂ <a href="#S4" title="IV Conclusion â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Federated Learning Framework for Domain-Specific Multimodal Large Models</span>
</h2>

<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of Federated Learning in Small Model Era and Large Model Era</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S2.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.1.1.1" class="ltx_p" style="width:43.4pt;">Dimension</span>
</span>
</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S2.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.2.1.1" class="ltx_p" style="width:173.4pt;">Federated Learning in Small Model Era</span>
</span>
</th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S2.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.3.1.1" class="ltx_p" style="width:173.4pt;">Federated Learning in Large Model Era</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.1.1.1" class="ltx_p" style="width:43.4pt;">Intelligence Foundation</span>
</span>
</td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.2.1.1" class="ltx_p" style="width:173.4pt;">Training models from scratch or randomly initiated parameters.</span>
</span>
</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.3.1.1" class="ltx_p" style="width:173.4pt;">Fine-tuning based on pre-trained general large models.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.1.1.1" class="ltx_p" style="width:43.4pt;">Intelligent 
<br class="ltx_break">Objectives</span>
</span>
</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.2.1.1" class="ltx_p" style="width:173.4pt;">Multiple companies collaboratively training global models to enhance accuracy in specific scenarios within closed environment.</span>
</span>
</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.3.1.1" class="ltx_p" style="width:173.4pt;">Multiple companies collaboratively train domain-specific large models, supporting comprehensive intelligent services for industry applications and open environment.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.1.1.1" class="ltx_p" style="width:43.4pt;">Federated Data</span>
</span>
</td>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.2.1.1" class="ltx_p" style="width:173.4pt;">Single modality, spatiotemporal alignment, similarity/correlation data with statistical heterogeneity.</span>
</span>
</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.3.1.1" class="ltx_p" style="width:173.4pt;">Diverse modalities and heterogeneous forms, challenging sample and spatiotemporal alignment, significant data quality and integrity variations.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.1.1.1" class="ltx_p" style="width:43.4pt;">Federated 
<br class="ltx_break">Models</span>
</span>
</td>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.2.1.1" class="ltx_p" style="width:173.4pt;">High-frequency synchronized aggregation and iterative refinement. Data-similar clustering, model-adaptive aggregation algorithms to address data heterogeneity and improve aggregation model performance.</span>
</span>
</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.3.1.1" class="ltx_p" style="width:173.4pt;">Participants contribute advantageous modules and small portions of parameters in large model structure, through asynchronous aggregation, iterative updates, or continuous learning, fine-tuning general large models collaboratively.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.1.1.1" class="ltx_p" style="width:43.4pt;">Performance Optimization</span>
</span>
</td>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.2.1.1" class="ltx_p" style="width:173.4pt;">Using techniques such as model quantization compression, adaptive aggregation module selection, and knowledge distillation to reduce communication overhead.</span>
</span>
</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.3.1.1" class="ltx_p" style="width:173.4pt;">Strategies for large model collaborative fine-tuning need to adapt to ecosystem partnersâ€™ variations in data and computational resources, balancing model performance and cost overhead.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.1.1.1" class="ltx_p" style="width:43.4pt;">Privacy 
<br class="ltx_break">Protection</span>
</span>
</td>
<td id="S2.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.2.1.1" class="ltx_p" style="width:173.4pt;">Employing techniques like multi-party secure computation, homomorphic encryption, differential privacy, and trusted execution environments to prevent leakage of sensitive information during model transmission and aggregation.</span>
</span>
</td>
<td id="S2.T1.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.3.1.1" class="ltx_p" style="width:173.4pt;">Privacy leakage risks exist in both model transmission and large model output. Requires careful and comprehensive balance between security levels and additional costs.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.8.7" class="ltx_tr">
<td id="S2.T1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S2.T1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.7.1.1.1" class="ltx_p" style="width:43.4pt;">Incentive Mechanism</span>
</span>
</td>
<td id="S2.T1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S2.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.7.2.1.1" class="ltx_p" style="width:173.4pt;">Evaluating data quality and contribution to the model by each party as the foundation for fair distribution and stimulating ecosystem development.</span>
</span>
</td>
<td id="S2.T1.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S2.T1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.7.3.1.1" class="ltx_p" style="width:173.4pt;">Evaluating specific characteristics, real-time nature, and professionalism of participant data, as well as their collaborative contributions to improving large model performance in vertical domains.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">TableÂ <a href="#S2.T1" title="TABLE I â€£ II Federated Learning Framework for Domain-Specific Multimodal Large Models â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> makes the comparison between traditional federated learning(i.e., in small model era) and that in big model era. Traditional federated learning jointly train a global model from scratch for the purpose of enhance model accuracy within a predefined application scenarios. Nowadays, federated learning aims at a domain-specific model based on the foundational intelligence of a general large model. Since the application target and generation method of intelligent product have been fundamentally changed in big model era, federated learning faces new challenges to tackled.</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Data Multi-modality and Heterogeneity: Companies own data in different modalities, exhibiting diverse data format and statistical distributions, considerable differences in data quality and completeness and the data samples and features could not be aligned explicitly.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Model Structure and Size: The amount of model parameters grows exponentially, involving the various deep learning networks and complex structures. Traditional federated learning aggregates updates of the entire model which leads to substantial computational and network communication costs. However, a company with limited local data usually affects a small portion of the well-structured general large model. Hence, the aggregation strategy should decide â€œwhat to aggregateâ€ and â€œhow to aggregateâ€ taking the trade-off between model training cost and performance into account.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Data Privacy and Security: According to large model aggregation strategy, the potential privacy leakage risks should be re-assessed and cost-effective privacy preserving computation should be adopted. In addition, measurements should be taken to prevent the potential exposure of company private data through generative outputs of large models.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">Incentive Mechanisms and Collaboration: Due to increasing model size and complexity, it becomes even more challenging to evaluate data value and model contributions of participants. Data uniqueness and domain knowledge become more important, which bring added values transforming general large model to domain-specific model.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In this paper, we propose a novel federated learning framework, as illustrated in Fig <a href="#S1.F2" title="Figure 2 â€£ I Introduction â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, which enables multiple companies to collaboratively train domain-specific large models using their proprietary data sources. Federated learning participants have private data of different modalities. The client can use various methods to implement heterogeneous models, such as using the LoRA method to train a small amount of network parameters for the model, training only a specific expert module (MOE), transferring feature maps through knowledge distillation, or training bridge networks for different modal encoders.The server fuses the heterogeneous models from other participants using a flexible model aggregation method and distributes the fused model for iterative training. With the help of the fused modelâ€™s capability, the data quality of the participants is continuously improved. This forms a positive feedback loop of data and model performance. In this way, we obtain a large-scale pre-trained model for a specific domain. This model can support multiple downstream intelligent tasks such as visual question answering, image captioning, image classification, image generation, etc.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S2.F3.1" class="ltx_logical-block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_center ltx_align_middle" style="width:433.6pt;">
<figure id="S2.F3.sf1" class="ltx_figure"><img src="/html/2308.11217/assets/A.png" id="S2.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="293" height="150" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S2.F3.sf1.3.2" class="ltx_text" style="font-size:80%;">Adapter Finetuning</span></figcaption>
</figure>
<figure id="S2.F3.sf2" class="ltx_figure"><img src="/html/2308.11217/assets/B.png" id="S2.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="293" height="150" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S2.F3.sf2.3.2" class="ltx_text" style="font-size:80%;"> Feature Map Knowledge Distillation</span></figcaption>
</figure>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S2.F3.2" class="ltx_logical-block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_center ltx_align_middle" style="width:433.6pt;">
<figure id="S2.F3.sf3" class="ltx_figure"><img src="/html/2308.11217/assets/C.png" id="S2.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="293" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S2.F3.sf3.3.2" class="ltx_text" style="font-size:80%;">Representation Space Allignment</span></figcaption>
</figure>
<figure id="S2.F3.sf4" class="ltx_figure"><img src="/html/2308.11217/assets/D.png" id="S2.F3.sf4.g1" class="ltx_graphics ltx_img_landscape" width="293" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf4.2.1.1" class="ltx_text" style="font-size:80%;">(d)</span> </span><span id="S2.F3.sf4.3.2" class="ltx_text" style="font-size:80%;">Network Bridging and Connection</span></figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The types of aggregation for the federated multimodal model.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Heterogeneous Model Fusion</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Multimodal large-scale models trained on massive public datasets are becoming increasingly large, while the participants in federated learning have relatively limited and unimodal private data, which can lead to model forgetting and overfitting, as well as high resource and time costs, when fully fine-tuning the large models. Federated multimodal large-scale models do not need to aggregate all the parameters, and can decide what to aggregate based on the following methods,as shown in Fig <a href="#S2.F3" title="Figure 3 â€£ II Federated Learning Framework for Domain-Specific Multimodal Large Models â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><math id="S2.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="\boldsymbol{Adapter\ Fine\-tuning:}" display="inline"><semantics id="S2.I2.i1.p1.1.m1.1a"><mrow id="S2.I2.i1.p1.1.m1.1.1" xref="S2.I2.i1.p1.1.m1.1.1.cmml"><mrow id="S2.I2.i1.p1.1.m1.1.1.2" xref="S2.I2.i1.p1.1.m1.1.1.2.cmml"><mi id="S2.I2.i1.p1.1.m1.1.1.2.2" xref="S2.I2.i1.p1.1.m1.1.1.2.2.cmml">ğ‘¨</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.3" xref="S2.I2.i1.p1.1.m1.1.1.2.3.cmml">ğ’…</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1a" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.4" xref="S2.I2.i1.p1.1.m1.1.1.2.4.cmml">ğ’‚</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1b" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.5" xref="S2.I2.i1.p1.1.m1.1.1.2.5.cmml">ğ’‘</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1c" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.6" xref="S2.I2.i1.p1.1.m1.1.1.2.6.cmml">ğ’•</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1d" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.7" xref="S2.I2.i1.p1.1.m1.1.1.2.7.cmml">ğ’†</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1e" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.8" xref="S2.I2.i1.p1.1.m1.1.1.2.8.cmml">ğ’“</mi><mo lspace="0.500em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1f" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.9" xref="S2.I2.i1.p1.1.m1.1.1.2.9.cmml">ğ‘­</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1g" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.10" xref="S2.I2.i1.p1.1.m1.1.1.2.10.cmml">ğ’Š</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1h" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.11" xref="S2.I2.i1.p1.1.m1.1.1.2.11.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1i" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.12" xref="S2.I2.i1.p1.1.m1.1.1.2.12.cmml">ğ’†</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1j" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.13" xref="S2.I2.i1.p1.1.m1.1.1.2.13.cmml">ğ’•</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1k" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.14" xref="S2.I2.i1.p1.1.m1.1.1.2.14.cmml">ğ’–</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1l" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.15" xref="S2.I2.i1.p1.1.m1.1.1.2.15.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1m" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.16" xref="S2.I2.i1.p1.1.m1.1.1.2.16.cmml">ğ’Š</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1n" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.17" xref="S2.I2.i1.p1.1.m1.1.1.2.17.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.2.1o" xref="S2.I2.i1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i1.p1.1.m1.1.1.2.18" xref="S2.I2.i1.p1.1.m1.1.1.2.18.cmml">ğ’ˆ</mi></mrow><mo class="ltx_mathvariant_bold" lspace="0.278em" mathvariant="bold" rspace="0.278em" id="S2.I2.i1.p1.1.m1.1.1.1" xref="S2.I2.i1.p1.1.m1.1.1.1.cmml">:</mo><mi id="S2.I2.i1.p1.1.m1.1.1.3" xref="S2.I2.i1.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.1.m1.1b"><apply id="S2.I2.i1.p1.1.m1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1"><ci id="S2.I2.i1.p1.1.m1.1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1.1">bold-:</ci><apply id="S2.I2.i1.p1.1.m1.1.1.2.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2"><times id="S2.I2.i1.p1.1.m1.1.1.2.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.1"></times><ci id="S2.I2.i1.p1.1.m1.1.1.2.2.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.2">ğ‘¨</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.3.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.3">ğ’…</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.4.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.4">ğ’‚</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.5.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.5">ğ’‘</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.6.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.6">ğ’•</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.7.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.7">ğ’†</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.8.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.8">ğ’“</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.9.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.9">ğ‘­</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.10.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.10">ğ’Š</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.11.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.11">ğ’</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.12.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.12">ğ’†</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.13.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.13">ğ’•</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.14.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.14">ğ’–</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.15.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.15">ğ’</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.16.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.16">ğ’Š</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.17.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.17">ğ’</ci><ci id="S2.I2.i1.p1.1.m1.1.1.2.18.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2.18">ğ’ˆ</ci></apply><csymbol cd="latexml" id="S2.I2.i1.p1.1.m1.1.1.3.cmml" xref="S2.I2.i1.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.1.m1.1c">\boldsymbol{Adapter\ Fine\-tuning:}</annotation></semantics></math> By freezing the local open-source general large-scale models and fine-tuning the adapters, the local training computational overhead can be significantly reduced, as well as the amount of model parameters transferred between the participants and the server. For example: LORA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> freezes the parameters of the pre-trained model and adds matrices A and B, and only updates A and B when fine-tuning the downstream tasks.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><math id="S2.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="\boldsymbol{Feature\ Map\ Knowledge\ Distillation:}" display="inline"><semantics id="S2.I2.i2.p1.1.m1.1a"><mrow id="S2.I2.i2.p1.1.m1.1.1" xref="S2.I2.i2.p1.1.m1.1.1.cmml"><mrow id="S2.I2.i2.p1.1.m1.1.1.2" xref="S2.I2.i2.p1.1.m1.1.1.2.cmml"><mi id="S2.I2.i2.p1.1.m1.1.1.2.2" xref="S2.I2.i2.p1.1.m1.1.1.2.2.cmml">ğ‘­</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.3" xref="S2.I2.i2.p1.1.m1.1.1.2.3.cmml">ğ’†</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1a" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.4" xref="S2.I2.i2.p1.1.m1.1.1.2.4.cmml">ğ’‚</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1b" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.5" xref="S2.I2.i2.p1.1.m1.1.1.2.5.cmml">ğ’•</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1c" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.6" xref="S2.I2.i2.p1.1.m1.1.1.2.6.cmml">ğ’–</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1d" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.7" xref="S2.I2.i2.p1.1.m1.1.1.2.7.cmml">ğ’“</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1e" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.8" xref="S2.I2.i2.p1.1.m1.1.1.2.8.cmml">ğ’†</mi><mo lspace="0.500em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1f" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.9" xref="S2.I2.i2.p1.1.m1.1.1.2.9.cmml">ğ‘´</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1g" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.10" xref="S2.I2.i2.p1.1.m1.1.1.2.10.cmml">ğ’‚</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1h" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.11" xref="S2.I2.i2.p1.1.m1.1.1.2.11.cmml">ğ’‘</mi><mo lspace="0.500em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1i" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.12" xref="S2.I2.i2.p1.1.m1.1.1.2.12.cmml">ğ‘²</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1j" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.13" xref="S2.I2.i2.p1.1.m1.1.1.2.13.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1k" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.14" xref="S2.I2.i2.p1.1.m1.1.1.2.14.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1l" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.15" xref="S2.I2.i2.p1.1.m1.1.1.2.15.cmml">ğ’˜</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1m" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.16" xref="S2.I2.i2.p1.1.m1.1.1.2.16.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1n" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.17" xref="S2.I2.i2.p1.1.m1.1.1.2.17.cmml">ğ’†</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1o" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.18" xref="S2.I2.i2.p1.1.m1.1.1.2.18.cmml">ğ’…</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1p" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.19" xref="S2.I2.i2.p1.1.m1.1.1.2.19.cmml">ğ’ˆ</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1q" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.20" xref="S2.I2.i2.p1.1.m1.1.1.2.20.cmml">ğ’†</mi><mo lspace="0.500em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1r" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.21" xref="S2.I2.i2.p1.1.m1.1.1.2.21.cmml">ğ‘«</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1s" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.22" xref="S2.I2.i2.p1.1.m1.1.1.2.22.cmml">ğ’Š</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1t" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.23" xref="S2.I2.i2.p1.1.m1.1.1.2.23.cmml">ğ’”</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1u" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.24" xref="S2.I2.i2.p1.1.m1.1.1.2.24.cmml">ğ’•</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1v" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.25" xref="S2.I2.i2.p1.1.m1.1.1.2.25.cmml">ğ’Š</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1w" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.26" xref="S2.I2.i2.p1.1.m1.1.1.2.26.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1x" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.27" xref="S2.I2.i2.p1.1.m1.1.1.2.27.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1y" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.28" xref="S2.I2.i2.p1.1.m1.1.1.2.28.cmml">ğ’‚</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1z" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.29" xref="S2.I2.i2.p1.1.m1.1.1.2.29.cmml">ğ’•</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1aa" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.30" xref="S2.I2.i2.p1.1.m1.1.1.2.30.cmml">ğ’Š</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1ab" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.31" xref="S2.I2.i2.p1.1.m1.1.1.2.31.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i2.p1.1.m1.1.1.2.1ac" xref="S2.I2.i2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i2.p1.1.m1.1.1.2.32" xref="S2.I2.i2.p1.1.m1.1.1.2.32.cmml">ğ’</mi></mrow><mo class="ltx_mathvariant_bold" lspace="0.278em" mathvariant="bold" rspace="0.278em" id="S2.I2.i2.p1.1.m1.1.1.1" xref="S2.I2.i2.p1.1.m1.1.1.1.cmml">:</mo><mi id="S2.I2.i2.p1.1.m1.1.1.3" xref="S2.I2.i2.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.1.m1.1b"><apply id="S2.I2.i2.p1.1.m1.1.1.cmml" xref="S2.I2.i2.p1.1.m1.1.1"><ci id="S2.I2.i2.p1.1.m1.1.1.1.cmml" xref="S2.I2.i2.p1.1.m1.1.1.1">bold-:</ci><apply id="S2.I2.i2.p1.1.m1.1.1.2.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2"><times id="S2.I2.i2.p1.1.m1.1.1.2.1.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.1"></times><ci id="S2.I2.i2.p1.1.m1.1.1.2.2.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.2">ğ‘­</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.3.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.3">ğ’†</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.4.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.4">ğ’‚</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.5.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.5">ğ’•</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.6.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.6">ğ’–</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.7.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.7">ğ’“</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.8.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.8">ğ’†</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.9.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.9">ğ‘´</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.10.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.10">ğ’‚</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.11.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.11">ğ’‘</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.12.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.12">ğ‘²</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.13.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.13">ğ’</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.14.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.14">ğ’</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.15.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.15">ğ’˜</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.16.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.16">ğ’</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.17.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.17">ğ’†</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.18.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.18">ğ’…</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.19.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.19">ğ’ˆ</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.20.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.20">ğ’†</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.21.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.21">ğ‘«</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.22.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.22">ğ’Š</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.23.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.23">ğ’”</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.24.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.24">ğ’•</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.25.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.25">ğ’Š</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.26.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.26">ğ’</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.27.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.27">ğ’</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.28.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.28">ğ’‚</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.29.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.29">ğ’•</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.30.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.30">ğ’Š</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.31.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.31">ğ’</ci><ci id="S2.I2.i2.p1.1.m1.1.1.2.32.cmml" xref="S2.I2.i2.p1.1.m1.1.1.2.32">ğ’</ci></apply><csymbol cd="latexml" id="S2.I2.i2.p1.1.m1.1.1.3.cmml" xref="S2.I2.i2.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.1.m1.1c">\boldsymbol{Feature\ Map\ Knowledge\ Distillation:}</annotation></semantics></math>. Using federated knowledge distillation, we can handle the heterogeneity of model architectures and data modalities among the participants, while leveraging the single-modal or multi-modal data on the clients to train a larger server-side model. By using additional public datasets as the medium for knowledge distillation, we transfer the feature map of the public data between the server and the clients, which complement the missing modalities for the single-modal clients, constrain the clients to learn a global consensus, and do not expose the private models and data of the clients.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><math id="S2.I2.i3.p1.1.m1.1" class="ltx_Math" alttext="\boldsymbol{Representation\ Space\ Allignment:}" display="inline"><semantics id="S2.I2.i3.p1.1.m1.1a"><mrow id="S2.I2.i3.p1.1.m1.1.1" xref="S2.I2.i3.p1.1.m1.1.1.cmml"><mrow id="S2.I2.i3.p1.1.m1.1.1.2" xref="S2.I2.i3.p1.1.m1.1.1.2.cmml"><mi id="S2.I2.i3.p1.1.m1.1.1.2.2" xref="S2.I2.i3.p1.1.m1.1.1.2.2.cmml">ğ‘¹</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.3" xref="S2.I2.i3.p1.1.m1.1.1.2.3.cmml">ğ’†</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1a" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.4" xref="S2.I2.i3.p1.1.m1.1.1.2.4.cmml">ğ’‘</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1b" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.5" xref="S2.I2.i3.p1.1.m1.1.1.2.5.cmml">ğ’“</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1c" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.6" xref="S2.I2.i3.p1.1.m1.1.1.2.6.cmml">ğ’†</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1d" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.7" xref="S2.I2.i3.p1.1.m1.1.1.2.7.cmml">ğ’”</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1e" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.8" xref="S2.I2.i3.p1.1.m1.1.1.2.8.cmml">ğ’†</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1f" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.9" xref="S2.I2.i3.p1.1.m1.1.1.2.9.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1g" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.10" xref="S2.I2.i3.p1.1.m1.1.1.2.10.cmml">ğ’•</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1h" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.11" xref="S2.I2.i3.p1.1.m1.1.1.2.11.cmml">ğ’‚</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1i" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.12" xref="S2.I2.i3.p1.1.m1.1.1.2.12.cmml">ğ’•</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1j" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.13" xref="S2.I2.i3.p1.1.m1.1.1.2.13.cmml">ğ’Š</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1k" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.14" xref="S2.I2.i3.p1.1.m1.1.1.2.14.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1l" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.15" xref="S2.I2.i3.p1.1.m1.1.1.2.15.cmml">ğ’</mi><mo lspace="0.500em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1m" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.16" xref="S2.I2.i3.p1.1.m1.1.1.2.16.cmml">ğ‘º</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1n" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.17" xref="S2.I2.i3.p1.1.m1.1.1.2.17.cmml">ğ’‘</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1o" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.18" xref="S2.I2.i3.p1.1.m1.1.1.2.18.cmml">ğ’‚</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1p" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.19" xref="S2.I2.i3.p1.1.m1.1.1.2.19.cmml">ğ’„</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1q" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.20" xref="S2.I2.i3.p1.1.m1.1.1.2.20.cmml">ğ’†</mi><mo lspace="0.500em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1r" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.21" xref="S2.I2.i3.p1.1.m1.1.1.2.21.cmml">ğ‘¨</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1s" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.22" xref="S2.I2.i3.p1.1.m1.1.1.2.22.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1t" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.23" xref="S2.I2.i3.p1.1.m1.1.1.2.23.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1u" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.24" xref="S2.I2.i3.p1.1.m1.1.1.2.24.cmml">ğ’Š</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1v" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.25" xref="S2.I2.i3.p1.1.m1.1.1.2.25.cmml">ğ’ˆ</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1w" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.26" xref="S2.I2.i3.p1.1.m1.1.1.2.26.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1x" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.27" xref="S2.I2.i3.p1.1.m1.1.1.2.27.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1y" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.28" xref="S2.I2.i3.p1.1.m1.1.1.2.28.cmml">ğ’†</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1z" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.29" xref="S2.I2.i3.p1.1.m1.1.1.2.29.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i3.p1.1.m1.1.1.2.1aa" xref="S2.I2.i3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.I2.i3.p1.1.m1.1.1.2.30" xref="S2.I2.i3.p1.1.m1.1.1.2.30.cmml">ğ’•</mi></mrow><mo class="ltx_mathvariant_bold" lspace="0.278em" mathvariant="bold" rspace="0.278em" id="S2.I2.i3.p1.1.m1.1.1.1" xref="S2.I2.i3.p1.1.m1.1.1.1.cmml">:</mo><mi id="S2.I2.i3.p1.1.m1.1.1.3" xref="S2.I2.i3.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i3.p1.1.m1.1b"><apply id="S2.I2.i3.p1.1.m1.1.1.cmml" xref="S2.I2.i3.p1.1.m1.1.1"><ci id="S2.I2.i3.p1.1.m1.1.1.1.cmml" xref="S2.I2.i3.p1.1.m1.1.1.1">bold-:</ci><apply id="S2.I2.i3.p1.1.m1.1.1.2.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2"><times id="S2.I2.i3.p1.1.m1.1.1.2.1.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.1"></times><ci id="S2.I2.i3.p1.1.m1.1.1.2.2.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.2">ğ‘¹</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.3.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.3">ğ’†</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.4.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.4">ğ’‘</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.5.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.5">ğ’“</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.6.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.6">ğ’†</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.7.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.7">ğ’”</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.8.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.8">ğ’†</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.9.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.9">ğ’</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.10.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.10">ğ’•</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.11.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.11">ğ’‚</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.12.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.12">ğ’•</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.13.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.13">ğ’Š</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.14.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.14">ğ’</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.15.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.15">ğ’</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.16.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.16">ğ‘º</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.17.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.17">ğ’‘</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.18.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.18">ğ’‚</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.19.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.19">ğ’„</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.20.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.20">ğ’†</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.21.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.21">ğ‘¨</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.22.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.22">ğ’</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.23.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.23">ğ’</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.24.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.24">ğ’Š</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.25.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.25">ğ’ˆ</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.26.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.26">ğ’</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.27.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.27">ğ’</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.28.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.28">ğ’†</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.29.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.29">ğ’</ci><ci id="S2.I2.i3.p1.1.m1.1.1.2.30.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2.30">ğ’•</ci></apply><csymbol cd="latexml" id="S2.I2.i3.p1.1.m1.1.1.3.cmml" xref="S2.I2.i3.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i3.p1.1.m1.1c">\boldsymbol{Representation\ Space\ Allignment:}</annotation></semantics></math> Ideally, a federated representation space with different types of data can enable the model to learn the information of other modalities. The textual representations learned from large-scale web data can serve as the targets for learning different modalities features, and the federated partners are selected to maximize the diversity of data, which improves the coverage ratio in the label space, and aligns other modalities such as images, sounds, and videos with the textual modality as the â€œmediatorâ€.</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I2.i4.p1" class="ltx_para">
<p id="S2.I2.i4.p1.1" class="ltx_p"><math id="S2.I2.i4.p1.1.m1.1" class="ltx_Math" alttext="\boldsymbol{Network\ Bridging\ and\ Connection}" display="inline"><semantics id="S2.I2.i4.p1.1.m1.1a"><mrow id="S2.I2.i4.p1.1.m1.1.1" xref="S2.I2.i4.p1.1.m1.1.1.cmml"><mi id="S2.I2.i4.p1.1.m1.1.1.2" xref="S2.I2.i4.p1.1.m1.1.1.2.cmml">ğ‘µ</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.3" xref="S2.I2.i4.p1.1.m1.1.1.3.cmml">ğ’†</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1a" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.4" xref="S2.I2.i4.p1.1.m1.1.1.4.cmml">ğ’•</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1b" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.5" xref="S2.I2.i4.p1.1.m1.1.1.5.cmml">ğ’˜</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1c" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.6" xref="S2.I2.i4.p1.1.m1.1.1.6.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1d" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.7" xref="S2.I2.i4.p1.1.m1.1.1.7.cmml">ğ’“</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1e" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.8" xref="S2.I2.i4.p1.1.m1.1.1.8.cmml">ğ’Œ</mi><mo lspace="0.500em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1f" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.9" xref="S2.I2.i4.p1.1.m1.1.1.9.cmml">ğ‘©</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1g" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.10" xref="S2.I2.i4.p1.1.m1.1.1.10.cmml">ğ’“</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1h" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.11" xref="S2.I2.i4.p1.1.m1.1.1.11.cmml">ğ’Š</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1i" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.12" xref="S2.I2.i4.p1.1.m1.1.1.12.cmml">ğ’…</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1j" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.13" xref="S2.I2.i4.p1.1.m1.1.1.13.cmml">ğ’ˆ</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1k" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.14" xref="S2.I2.i4.p1.1.m1.1.1.14.cmml">ğ’Š</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1l" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.15" xref="S2.I2.i4.p1.1.m1.1.1.15.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1m" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.16" xref="S2.I2.i4.p1.1.m1.1.1.16.cmml">ğ’ˆ</mi><mo lspace="0.500em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1n" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.17" xref="S2.I2.i4.p1.1.m1.1.1.17.cmml">ğ’‚</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1o" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.18" xref="S2.I2.i4.p1.1.m1.1.1.18.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1p" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.19" xref="S2.I2.i4.p1.1.m1.1.1.19.cmml">ğ’…</mi><mo lspace="0.500em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1q" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.20" xref="S2.I2.i4.p1.1.m1.1.1.20.cmml">ğ‘ª</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1r" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.21" xref="S2.I2.i4.p1.1.m1.1.1.21.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1s" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.22" xref="S2.I2.i4.p1.1.m1.1.1.22.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1t" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.23" xref="S2.I2.i4.p1.1.m1.1.1.23.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1u" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.24" xref="S2.I2.i4.p1.1.m1.1.1.24.cmml">ğ’†</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1v" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.25" xref="S2.I2.i4.p1.1.m1.1.1.25.cmml">ğ’„</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1w" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.26" xref="S2.I2.i4.p1.1.m1.1.1.26.cmml">ğ’•</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1x" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.27" xref="S2.I2.i4.p1.1.m1.1.1.27.cmml">ğ’Š</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1y" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.28" xref="S2.I2.i4.p1.1.m1.1.1.28.cmml">ğ’</mi><mo lspace="0em" rspace="0em" id="S2.I2.i4.p1.1.m1.1.1.1z" xref="S2.I2.i4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I2.i4.p1.1.m1.1.1.29" xref="S2.I2.i4.p1.1.m1.1.1.29.cmml">ğ’</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i4.p1.1.m1.1b"><apply id="S2.I2.i4.p1.1.m1.1.1.cmml" xref="S2.I2.i4.p1.1.m1.1.1"><times id="S2.I2.i4.p1.1.m1.1.1.1.cmml" xref="S2.I2.i4.p1.1.m1.1.1.1"></times><ci id="S2.I2.i4.p1.1.m1.1.1.2.cmml" xref="S2.I2.i4.p1.1.m1.1.1.2">ğ‘µ</ci><ci id="S2.I2.i4.p1.1.m1.1.1.3.cmml" xref="S2.I2.i4.p1.1.m1.1.1.3">ğ’†</ci><ci id="S2.I2.i4.p1.1.m1.1.1.4.cmml" xref="S2.I2.i4.p1.1.m1.1.1.4">ğ’•</ci><ci id="S2.I2.i4.p1.1.m1.1.1.5.cmml" xref="S2.I2.i4.p1.1.m1.1.1.5">ğ’˜</ci><ci id="S2.I2.i4.p1.1.m1.1.1.6.cmml" xref="S2.I2.i4.p1.1.m1.1.1.6">ğ’</ci><ci id="S2.I2.i4.p1.1.m1.1.1.7.cmml" xref="S2.I2.i4.p1.1.m1.1.1.7">ğ’“</ci><ci id="S2.I2.i4.p1.1.m1.1.1.8.cmml" xref="S2.I2.i4.p1.1.m1.1.1.8">ğ’Œ</ci><ci id="S2.I2.i4.p1.1.m1.1.1.9.cmml" xref="S2.I2.i4.p1.1.m1.1.1.9">ğ‘©</ci><ci id="S2.I2.i4.p1.1.m1.1.1.10.cmml" xref="S2.I2.i4.p1.1.m1.1.1.10">ğ’“</ci><ci id="S2.I2.i4.p1.1.m1.1.1.11.cmml" xref="S2.I2.i4.p1.1.m1.1.1.11">ğ’Š</ci><ci id="S2.I2.i4.p1.1.m1.1.1.12.cmml" xref="S2.I2.i4.p1.1.m1.1.1.12">ğ’…</ci><ci id="S2.I2.i4.p1.1.m1.1.1.13.cmml" xref="S2.I2.i4.p1.1.m1.1.1.13">ğ’ˆ</ci><ci id="S2.I2.i4.p1.1.m1.1.1.14.cmml" xref="S2.I2.i4.p1.1.m1.1.1.14">ğ’Š</ci><ci id="S2.I2.i4.p1.1.m1.1.1.15.cmml" xref="S2.I2.i4.p1.1.m1.1.1.15">ğ’</ci><ci id="S2.I2.i4.p1.1.m1.1.1.16.cmml" xref="S2.I2.i4.p1.1.m1.1.1.16">ğ’ˆ</ci><ci id="S2.I2.i4.p1.1.m1.1.1.17.cmml" xref="S2.I2.i4.p1.1.m1.1.1.17">ğ’‚</ci><ci id="S2.I2.i4.p1.1.m1.1.1.18.cmml" xref="S2.I2.i4.p1.1.m1.1.1.18">ğ’</ci><ci id="S2.I2.i4.p1.1.m1.1.1.19.cmml" xref="S2.I2.i4.p1.1.m1.1.1.19">ğ’…</ci><ci id="S2.I2.i4.p1.1.m1.1.1.20.cmml" xref="S2.I2.i4.p1.1.m1.1.1.20">ğ‘ª</ci><ci id="S2.I2.i4.p1.1.m1.1.1.21.cmml" xref="S2.I2.i4.p1.1.m1.1.1.21">ğ’</ci><ci id="S2.I2.i4.p1.1.m1.1.1.22.cmml" xref="S2.I2.i4.p1.1.m1.1.1.22">ğ’</ci><ci id="S2.I2.i4.p1.1.m1.1.1.23.cmml" xref="S2.I2.i4.p1.1.m1.1.1.23">ğ’</ci><ci id="S2.I2.i4.p1.1.m1.1.1.24.cmml" xref="S2.I2.i4.p1.1.m1.1.1.24">ğ’†</ci><ci id="S2.I2.i4.p1.1.m1.1.1.25.cmml" xref="S2.I2.i4.p1.1.m1.1.1.25">ğ’„</ci><ci id="S2.I2.i4.p1.1.m1.1.1.26.cmml" xref="S2.I2.i4.p1.1.m1.1.1.26">ğ’•</ci><ci id="S2.I2.i4.p1.1.m1.1.1.27.cmml" xref="S2.I2.i4.p1.1.m1.1.1.27">ğ’Š</ci><ci id="S2.I2.i4.p1.1.m1.1.1.28.cmml" xref="S2.I2.i4.p1.1.m1.1.1.28">ğ’</ci><ci id="S2.I2.i4.p1.1.m1.1.1.29.cmml" xref="S2.I2.i4.p1.1.m1.1.1.29">ğ’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i4.p1.1.m1.1c">\boldsymbol{Network\ Bridging\ and\ Connection}</annotation></semantics></math>. To improve the efficiency and effectiveness of training, the clients can use the pre-trained image encoder and text encoder weights to map images and texts to a better semantic space. Then, an additional bridge network is added, which is responsible for aligning the image and text spaces. Multiple participants jointly train the bridge network to achieve faster modality alignment and represent domain specific knowledge.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Flexible Model Aggregation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Transformer-based deep-learning network has been verified to be more robust than those with a traditional CNN architecture, through prior research and numerous experiments<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. They can alleviate catastrophic forgetting of data and enhance model convergence. In federated learning, networks with a Transformers architecture achieve higher similarity and synchronization efficiency among the participants<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Therefore, multimodal networks using Transformer as the backbone can fuse multi-source heterogeneous data more efficiently and prevent catastrophic forgetting. In the era of big models, federated learning does not require the participants to frequently and iteratively update and aggregate the model. Multiple enterprises can adopt asynchronous<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, chained<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, or continual learning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and obtain the expected model performance. This more relaxed collaboration mechanism can substantially reduce the challenges and costs of actual deployment, as well as ease the participantsâ€™ concerns about network security and other issues.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Data Quality Improvement</span>
</h3>

<figure id="S2.F4" class="ltx_figure"><img src="/html/2308.11217/assets/data_qualit.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Cross-enterprise data cleaning method.</figcaption>
</figure>
<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">One of the crucial factors for developing effective multimodal large models in the industrial internet domain is the availability and quality of multimodal data. Increasing the size of the dataset and including more comprehensive and varied samples from different modalities can enhance the performance of multimodal large models. However, in the industrial internet domain, data from different industries and modalities are distributed among different enterprises, and they cannot be aggregated and processed due to confidentiality and other constraints. To address this challenge, we propose a cross-enterprise data cleaning method that utilizes the power of large models and federated learning technology,as illustrated in Fig <a href="#S2.F4" title="Figure 4 â€£ II-C Data Quality Improvement â€£ II Federated Learning Framework for Domain-Specific Multimodal Large Models â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.The proposed data cleaning method consists of the following steps:</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<ol id="S2.I3" class="ltx_enumerate">
<li id="S2.I3.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S2.I3.ix1.p1" class="ltx_para">
<p id="S2.I3.ix1.p1.1" class="ltx_p">An initial model is trained by federated learning on the data from N partners. However, due to the heterogeneous and noisy nature of the training data, the initial model fails to achieve satisfactory performance for practical use.</p>
</div>
</li>
<li id="S2.I3.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S2.I3.ix2.p1" class="ltx_para">
<p id="S2.I3.ix2.p1.1" class="ltx_p">The initial model is then used to screen the data from N participants, and eliminate the low-quality data, retaining only the high-quality data. This enhances the quality of the participant data.</p>
</div>
</li>
<li id="S2.I3.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S2.I3.ix3.p1" class="ltx_para">
<p id="S2.I3.ix3.p1.1" class="ltx_p">The high-quality data obtained in (2) are used to further train a more effective model by federated learning.</p>
</div>
</li>
<li id="S2.I3.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S2.I3.ix4.p1" class="ltx_para">
<p id="S2.I3.ix4.p1.1" class="ltx_p">Steps (2) and (3) are iterated until the performance of the trained model meets the expected criteria.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.4.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.5.2" class="ltx_text ltx_font_italic">Data Privacy Preserving</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Based on the strategy of aggregating large models, itâ€™s crucial to re-evaluate potential risks related to privacy leakage. Itâ€™s also essential to implement cost-efficient methods to preserve privacy. Some effective solutions include:</p>
<ul id="S2.I4" class="ltx_itemize">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p id="S2.I4.i1.p1.1" class="ltx_p">Engaging with Trusted Execution Environments, which offer hardware-based isolation, enabling the refinement of general models using industry-specific private data on a reliable platform.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I4.i2.p1" class="ltx_para">
<p id="S2.I4.i2.p1.1" class="ltx_p">Splitting Learning and Knowledge Distillation, which permits part of the model training locally and then sending the feature map (or embedding) to a cloud platform for the main large model training phase.</p>
</div>
</li>
<li id="S2.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I4.i3.p1" class="ltx_para">
<p id="S2.I4.i3.p1.1" class="ltx_p">Leveraging cryptography methods, such as Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential Privacy (DP). These involve considerable overheads in communication and computation, as well as introduce data noise for added security.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Additionally, measures must be taken to minimize the risk of unintentionally revealing business-sensitive information via the outputs of large models.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<ul id="S2.I5" class="ltx_itemize">
<li id="S2.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I5.i1.p1" class="ltx_para">
<p id="S2.I5.i1.p1.1" class="ltx_p">Sanitizing Training Data: Itâ€™s vital to ensure that the model learns from data free from private or delicate details, like user IDs or precise addresses.</p>
</div>
</li>
<li id="S2.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I5.i2.p1" class="ltx_para">
<p id="S2.I5.i2.p1.1" class="ltx_p">Security Fence and Filtering: A proactive strategy for preventing large model leaking privacy is to filter and monitor their results. This includes blacklisting specific words or phrases, integrating post-processing protocols, and using conditional generation techniques.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS5.4.1.1" class="ltx_text">II-E</span> </span><span id="S2.SS5.5.2" class="ltx_text ltx_font_italic">Incentive Mechanism</span>
</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Shapley Value (SV) is a well-known approach to evaluate individualâ€™s marginal contribution in a coalition, but the canonical SV calculation and its available variants are very costly. Our proposed WTDP-Shapley method for FL contribution measurement has been proven to be effective and efficient in a real city-gas household inspection scenario <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Using WTDP-Shapley allows for a quick assessment of each participantâ€™s contribution with a guaranteed low error rate.However, in big model era, there are several challenges in applying the WTDP-Shapley method for measuring sharing.</p>
<ul id="S2.I6" class="ltx_itemize">
<li id="S2.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I6.i1.p1" class="ltx_para">
<p id="S2.I6.i1.p1.1" class="ltx_p">Although the performance has been improved dramatically, the cost is still un-affordable to generate and evaluate a group of large models with billions of parameters.</p>
</div>
</li>
<li id="S2.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I6.i2.p1" class="ltx_para">
<p id="S2.I6.i2.p1.1" class="ltx_p">Contribution evaluation should decompose the model structure, and consider the corresponding data modality and model blocks. We can address these challenges by employing a dynamic masking approach, which involves masking different block weights to identify the block that play a significant role in the prediction at each time step. This approach highlights the important block that vary over time, allowing us to assess their contributions<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
</li>
<li id="S2.I6.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I6.i3.p1" class="ltx_para">
<p id="S2.I6.i3.p1.1" class="ltx_p">Since the foundation large model has been pre-trained based on public datasets with general knowledge, data uniqueness and domain knowledge would become important to bring added values for domain-specific model.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Case Study: Multimodal Large Model for Urban Operation Management</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Urban operation management requires a holistic solution, based on various data sources (e.g., surveillance video and inspection photo) and knowledge (e.g., reports and regulations) from different companies and government agencies. Therefore, a domain-specific multimodal large model is desired to inspect safety hazardous in any corner of the city regarding to infrastructure and human behaviours. This comprehensive large model helps to improve the urban safety and efficiency. By integrating visual, textual, auditory, and other modality data, the model can perform real-time monitoring and early warning, and thus help the urban management departments to quickly respond and handle various incidents, ensuring the safety of residentsâ€™ lives and property. In addition, it is also very helpful for city management and planners to improve facility layout and resource scheduling for energy saving, as well as living convenience and quality.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">As urban operation management is a complex and dynamic process, various data sources and domain expertise are required to train a multimodal large model covering the entire application scenarios. To solve this problem, ENN Group, WiNDAKA, and UNICOM(Shanghai) have collaborated to establish a collaborative ecosystem. As illustrated in TableÂ <a href="#S3.T2" title="TABLE II â€£ III Case Study: Multimodal Large Model for Urban Operation Management â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, the three companies have expert knowledge and rich experience in energy infrastructure safety, residential community security, and urban management. They have accumulated large amounts of data over the years to support a vision-language large model training for urban operation and management vertical domain. The multi-party collaboration is essential for covering smart city intelligence tasks. The parties share and integrate complementary data and knowledge, forming a data aggregation effect, thereby establishing a more comprehensive and accurate multimodal large model.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Overview of Enterprise Data and Scenarios</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.1.1.1" class="ltx_p" style="width:65.0pt;">Company</span>
</span>
</th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.2.1.1" class="ltx_p" style="width:130.1pt;">Scenario Description</span>
</span>
</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.3.1.1" class="ltx_p" style="width:86.7pt;">Data Type and Quantity</span>
</span>
</th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T2.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.4.1.1" class="ltx_p" style="width:65.0pt;">Annotation Present</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.1.1.1" class="ltx_p" style="width:65.0pt;">ENN Group</span>
</span>
</td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.2.1.1" class="ltx_p" style="width:130.1pt;">Energy Security: Hazard Identification and Descriptive Data</span>
</span>
</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.3.1.1" class="ltx_p" style="width:86.7pt;">Visual and Textual, 260k+</span>
</span>
</td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.4.1.1" class="ltx_p" style="width:65.0pt;">Yes</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.1.1.1" class="ltx_p" style="width:65.0pt;">Unicom (Shanghai)</span>
</span>
</td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.2.1.1" class="ltx_p" style="width:130.1pt;">Urban Management: City Streets and Shop Management</span>
</span>
</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.3.1.1" class="ltx_p" style="width:86.7pt;">Visual and Textual, 150k+</span>
</span>
</td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.4.1.1" class="ltx_p" style="width:65.0pt;">Partial</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.1.1.1" class="ltx_p" style="width:65.0pt;">WiNDAKA</span>
</span>
</td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.2.1.1" class="ltx_p" style="width:130.1pt;">Community Security: Attributes of People and Vehicles</span>
</span>
</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.3.1.1" class="ltx_p" style="width:86.7pt;">Image, 110k+</span>
</span>
</td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.4.1.1" class="ltx_p" style="width:65.0pt;">Yes</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">However, challenges should be tackled to establish a multimodal large model successfully in federated learning manner:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Instead of training from scratch, we need to choose a suitable pre-trained vision-language general model as the foundation. It is able to fuse multimodal data relevant to smart city, support mainstream visual-text tasks, and have strong Chinese understanding and output capabilities.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">A efficient and robust federated learning framework should be deployed securely and quickly amongst multiple parties, supporting various algorithms for model aggregation, data security and privacy protection.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">The quality of multimodal data in each party should be improved. For instance, the existing image caption is non-informative and incomplete. However, professional expert annotation cost is very high to improve data quality manually.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">As the model size has been increase significantly, federated learning requires huge communication bandwidth, which is very costly especially when the companies must be connected via VPN for security and fault-tolerance considerations.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">To address the four challenges mentioned above, we propose corresponding solutions in following subsections.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Unified multimodal network</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Multimodal learning primarily encompasses two directions, as shown in Fig <a href="#S3.F5" title="Figure 5 â€£ III-A Unified multimodal network â€£ III Case Study: Multimodal Large Model for Urban Operation Management â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>:</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2308.11217/assets/dual.png" id="S3.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="299" height="168" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S3.F5.sf1.3.2" class="ltx_text" style="font-size:80%;">Multimodal feature fusion with multi-tower structure.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2308.11217/assets/LLM.png" id="S3.F5.sf2.g1" class="ltx_graphics ltx_img_square" width="240" height="211" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S3.F5.sf2.3.2" class="ltx_text" style="font-size:80%;">Multimodal information with large language
model.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Two directions of multimodal learning.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Multimodal feature fusion with multi-tower structure. In the early days of self-supervised learning, there were no powerful language models or visual base models. The work at this stage mainly focused on using various self-supervised learning objectives, designing different feature fusion networks, and performing end-to-end pre-training, in order to obtain multimodal pre-trained models. The main representative works include CLIP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>,ALBEF<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, BLIP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, etc.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Incorporating multimodal information into large language models. The main representative works include OFA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Flamingo<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, Palm-E<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, X-LLM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, UnIVAL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> etc., which usually keep the parameters of LLM and pre-trained visual models freezed, and add extra trainable modules to unify a large number of multimodal tasks into generation tasks, and perform pre-training under a unified self-regressive pre-training paradigm.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Alibaba DAMO Academy has introduced a Seq2Seq generative framework OFA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, which unifies modality, task, and structure. OFA shares the same model structure across all tasks, and uses artificially designed instructions to distinguish tasks. It can cover downstream tasks spanning multimodal generation, multimodal understanding, image classification, natural language understanding, text generation and other scenarios, and achieves state-of-the-art results on multiple tasks. At the same time, OFA has been trained on a large amount of multimodal Chinese corpus, and has strong Chinese semantic understanding and output capabilities.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">ENNEW Federated Learning platform</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">ENNEW Federated Learning platform, which was certified by China Academy of Information and Communications Technology in 2022, can meet the federated learning basic capability requirements in terms of scheduling management, data processing, algorithm implementation, effect and performance, security, etc. It adopts TLS encryption technology to ensure data is always protected during transmission and storage, strengthens identity verification and permission management mechanisms to prevent malicious parties from accessing and tampering with other partiesâ€™ data, and introduces multi-party secure computing technology to support multi-party model secure aggregation and prevent model parameters and gradient information from reverse deducing confidential information. In addition, it is flexible in terms of distributed deployment and network policy. For example, the model parameter aggregation server provides RESTFul API and message queue services, and each party does not need to open separate services and network ports.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2308.11217/assets/data_enhance.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Approaches for improving data quality in different cases.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Improving data quality</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In the industrial internet, due to historical management and other reasons, the sample data generation has a lot of randomness and incompleteness, which becomes the bottleneck for each enterprise to achieve industrial digitalization. However, it is very costly in terms of human and time resources to re-annotate all the data with domain experts,as shown in Figure <a href="#S3.F6" title="Figure 6 â€£ III-B ENNEW Federated Learning platform â€£ III Case Study: Multimodal Large Model for Urban Operation Management â€£ Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models Identify applicable funding agency here. If none, delete this." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We adopt a model self-iteration data augmentation scheme, where data and model mutually promote each other, and generate more realistic and comprehensive image captions.For Case A where there is a lot of noise such as address information and sensitive information in the data, we need to perform data cleaning based on the rules. For data with only images and corresponding object labels, we need to use LLM to assist in generating some descriptive text by using prompts.For data with text-image pairs, but the text descriptions are too short to cover the complete scene, we need to add some custom rules to perform text expansion. In different scenarios, multimodal models are first trained locally, and a federated multimodal large model is formed through federated learning, which aggregates data and knowledge from multiple parties. The original local data can be filtered and augmented using a filter-based approach to improve data quality. At the same time, high-quality data can be generated through expert annotation feedback. Through continuous iteration, both the data and model performance can be enhanced.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Communication efficiency</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In the era of large models, the scale of pre-trained large models becomes more larger, and federated learning with full transmission requires a lot of communication bandwidth, making communication a bottleneck and an impossible solution. At the same time, it is also more difficult to perform full fine-tuning. The parameter-efficient fine-tuning method PEFT fixes most of the pre-trained parameters and only fine-tunes a small number or additional model parameters, greatly reducing the computation and storage costs. The PEFT scheme represented by LORA only fine-tunes a small number of parameters in the model, reducing the parameter training overhead to one percent of the original, and achieving very considerable results. We adopt the parameter-efficient fine-tuning technology PEFT, and only upload the fine-tuned parameter part in the federated learning process.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.4.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.5.2" class="ltx_text ltx_font_italic">performance evaluation standards</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Academic performance evaluation standards for large models, such as BLEU, METEOR, ROUGE, CIDEr, etc., measure the performance of models by comparing the difference between the generated image caption and the ground truth. However, federated learning not only enhances the original intelligence capabilities of the parties, but also has the intelligence capabilities of other parties, and through powerful inference capabilities, superimposes and integrates the intelligence capabilities of each party to achieve a higher level of intelligence. Obviously, it is unreasonable to compare the results generated by federated learning with the ground truth of a partyâ€™s limited knowledge. Therefore, it is necessary to form a validation set of large models in vertical domains and re-annotate them in a joint intelligence manner. Since this evaluation method requires re-annotation of the existing validation set, it requires manpower to annotate, which is time-consuming and laborious. Currently, subjective evaluation methods or GPT4 are used to score, and a certain number of validation images are selected to evaluate whether the multimodal large model generated by federated learning has produced joint intelligence.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.4.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.5.2" class="ltx_text ltx_font_italic">Experiment Results</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">To be updated with the latest experimental results, including the adoption of LORA in federated learning framework for cost-saving multimodal large model fine-tuning, the performance evaluation of the federated leaning vision-language model in terms of benchmark metrics (e.g., Rouge) and its advantages on generating professional and informative image captions in various application scenarios.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The evolution of large AI models like ChatGPT highlights the importance of multimodal data for advancing general AI. However, most existing general multimodal large models were trained public datasets and their performance in specific industrial domains are under expectations. In this paper, we have proposed a federated learning framework in big model era, supporting companies to fine-tune a general multimodal model using their proprietary data and knowledge in a collaborative and efficient manner. The framework has been deployed in companies from energy infrastructure, residential community property management and city operation management. The preliminary experimental results has shown that the vision-language large model could learn from diverse and heterogeneous data from companies and integrate their expert knowledge for understanding images better from multiple perspectives. This is essential for establishing a holistic solution to urban operation and management, inspiring novel business model for safety, energy saving, environment-friendly high-quality city living. As the federated learning ecosystem grows, it promises to accelerate collaborative intelligence for industry domain-specific large models and bring researchers together to tackle the challenges of federated learning in big model era, in terms of novel approaches to fuse heterogeneous data and model, ensuring data privacy and security based on the understanding of large models, enhance training efficiency using creative fine-tuning and model aggregation methods, incentive mechanism and IP protection methods for sustainable ecosystem collaboration, and etc.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
L.Â Ouyang, J.Â Wu, X.Â Jiang, D.Â Almeida, C.Â Wainwright, P.Â Mishkin, C.Â Zhang,
S.Â Agarwal, K.Â Slama, A.Â Ray, <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œTraining language models to
follow instructions with human feedback,â€ <span id="bib.bib1.2.2" class="ltx_text ltx_font_italic">Advances in Neural
Information Processing Systems</span>, vol.Â 35, pp.Â 27730â€“27744, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A.Â Kirillov, E.Â Mintun, N.Â Ravi, H.Â Mao, C.Â Rolland, L.Â Gustafson, T.Â Xiao,
S.Â Whitehead, A.Â C. Berg, W.-Y. Lo, <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œSegment anything,â€ <span id="bib.bib2.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.02643</span>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z.Â Wang, Y.Â Li, X.Â Chen, S.-N. Lim, A.Â Torralba, H.Â Zhao, and S.Â Wang,
â€œDetecting everything in the open world: Towards universal object
detection,â€ in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</span>, pp.Â 11433â€“11443, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y.Â Bisk, A.Â Holtzman, J.Â Thomason, J.Â Andreas, Y.Â Bengio, J.Â Chai, M.Â Lapata,
A.Â Lazaridou, J.Â May, A.Â Nisnevich, N.Â Pinto, and J.Â P. Turian, â€œExperience
grounds language,â€ <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol.Â abs/2004.10151, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A.Â Radford, J.Â W. Kim, C.Â Hallacy, A.Â Ramesh, G.Â Goh, S.Â Agarwal, G.Â Sastry,
A.Â Askell, P.Â Mishkin, J.Â Clark, <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œLearning transferable visual
models from natural language supervision,â€ in <span id="bib.bib5.2.2" class="ltx_text ltx_font_italic">International conference
on machine learning</span>, pp.Â 8748â€“8763, PMLR, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J.Â Li, R.Â Selvaraju, A.Â Gotmare, S.Â Joty, C.Â Xiong, and S.Â C.Â H. Hoi, â€œAlign
before fuse: Vision and language representation learning with momentum
distillation,â€ <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
vol.Â 34, pp.Â 9694â€“9705, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J.Â Li, D.Â Li, C.Â Xiong, and S.Â Hoi, â€œBlip: Bootstrapping language-image
pre-training for unified vision-language understanding and generation,â€ in
<span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pp.Â 12888â€“12900, PMLR,
2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P.Â Wang, A.Â Yang, R.Â Men, J.Â Lin, S.Â Bai, Z.Â Li, J.Â Ma, C.Â Zhou, J.Â Zhou, and
H.Â Yang, â€œOfa: Unifying architectures, tasks, and modalities through a
simple sequence-to-sequence learning framework,â€ in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">International
Conference on Machine Learning</span>, pp.Â 23318â€“23340, PMLR, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J.-B. Alayrac, J.Â Donahue, P.Â Luc, A.Â Miech, I.Â Barr, Y.Â Hasson, K.Â Lenc,
A.Â Mensch, K.Â Millican, M.Â Reynolds, <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œFlamingo: a visual
language model for few-shot learning,â€ <span id="bib.bib9.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information
Processing Systems</span>, vol.Â 35, pp.Â 23716â€“23736, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
D.Â Driess, F.Â Xia, M.Â S. Sajjadi, C.Â Lynch, A.Â Chowdhery, B.Â Ichter, A.Â Wahid,
J.Â Tompson, Q.Â Vuong, T.Â Yu, <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œPalm-e: An embodied multimodal
language model,â€ <span id="bib.bib10.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.03378</span>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
B.Â McMahan, E.Â Moore, D.Â Ramage, S.Â Hampson, and B.Â A. yÂ Arcas,
â€œCommunication-efficient learning of deep networks from decentralized
data,â€ in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Artificial intelligence and statistics</span>, pp.Â 1273â€“1282,
PMLR, 2017.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
E.Â J. Hu, Y.Â Shen, P.Â Wallis, Z.Â Allen-Zhu, Y.Â Li, S.Â Wang, L.Â Wang, and
W.Â Chen, â€œLora: Low-rank adaptation of large language models,â€ <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L.Â Qu, Y.Â Zhou, P.Â P. Liang, Y.Â Xia, F.Â Wang, E.Â Adeli, L.Â Fei-Fei, and
D.Â Rubin, â€œRethinking architecture design for tackling data heterogeneity in
federated learning,â€ in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</span>, pp.Â 10061â€“10071, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
D.Â Hendrycks, N.Â Mu, E.Â D. Cubuk, B.Â Zoph, J.Â Gilmer, and B.Â Lakshminarayanan,
â€œAugmix: A simple method to improve robustness and uncertainty under data
shift,â€ in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">International conference on learning representations</span>,
vol.Â 1, p.Â 5, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S.Â Bhojanapalli, A.Â Chakrabarti, D.Â Glasner, D.Â Li, T.Â Unterthiner, and
A.Â Veit, â€œUnderstanding robustness of transformers for image
classification,â€ in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF international
conference on computer vision</span>, pp.Â 10231â€“10241, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y.Â Gao, H.Â Sun, Z.Â Li, and H.Â Yu, â€œThe prospect of enhancing large-scale
heterogeneous federated learning with transformers,â€ 08 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
C.Â Xie, S.Â Koyejo, and I.Â Gupta, â€œAsynchronous federated optimization,â€ <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1903.03934</span>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
K.Â Chang, N.Â Balachandar, C.Â Lam, D.Â Yi, J.Â Brown, A.Â Beers, B.Â Rosen, D.Â L.
Rubin, and J.Â Kalpathy-Cramer, â€œDistributed deep learning networks among
institutions for medical imaging,â€ <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Journal of the American Medical
Informatics Association</span>, vol.Â 25, no.Â 8, pp.Â 945â€“954, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T.Â Lesort, V.Â Lomonaco, A.Â Stoian, D.Â Maltoni, D.Â Filliat, and
N.Â DÃ­az-RodrÃ­guez, â€œContinual learning for robotics: Definition,
framework, learning strategies, opportunities and challenges,â€ <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Information fusion</span>, vol.Â 58, pp.Â 52â€“68, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C.Â Yang, J.Â Liu, H.Â Sun, T.Â Li, and Z.Â Li, â€œWTDP-Shapley: Efficient and
effective incentive mechanism in federated learning for intelligent safety
inspection,â€ <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Big Data</span>, pp.Â 1â€“10, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J.Â CrabbÃ© and M.Â Van DerÂ Schaar, â€œExplaining time series predictions with
dynamic masks,â€ in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>,
pp.Â 2166â€“2177, PMLR, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
F.Â Chen, M.Â Han, H.Â Zhao, Q.Â Zhang, J.Â Shi, S.Â Xu, and B.Â Xu, â€œX-llm:
Bootstrapping advanced large language models by treating multi-modalities as
foreign languages,â€ 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M.Â Shukor, C.Â Dancette, A.Â Rame, and M.Â Cord, â€œUnified model for image, video,
audio and language tasks,â€ 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.11216" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.11217" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.11217">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.11217" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.11218" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 12:18:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
