<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.05213] Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout</title><meta property="og:description" content="Large machine learning models trained on diverse data have recently seen unprecedented success. Federated learning enables training on private data that may otherwise be inaccessible, such as domain-specific datasets d…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.05213">

<!--Generated on Wed Feb 28 06:34:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Federated Learning Under Resource Constraints via Layer-wise Training
<br class="ltx_break">and Depth Dropout</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pengfei Guo  
</span><span class="ltx_author_notes">Work done during an internship at Google Research. 
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Warren Richard Morningstar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Raviteja Vemulapalli
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Karan Singhal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vishal M. Patel
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Philip Andrew Mansfield
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">Large machine learning models trained on diverse data have recently seen unprecedented success. Federated learning enables training on private data that may otherwise be inaccessible, such as domain-specific datasets decentralized across many clients. However, federated learning can be difficult to scale to large models when clients have limited resources. This challenge often results in a trade-off between model size and access to diverse data. To mitigate this issue and facilitate training of large models on edge devices, we introduce a simple yet effective strategy, <span id="id1.1.1" class="ltx_text ltx_font_italic">Federated Layer-wise Learning</span>, to simultaneously reduce per-client memory, computation, and communication costs.
Clients train just a single layer each round, reducing resource costs considerably with minimal performance degradation. We also introduce <span id="id1.1.2" class="ltx_text ltx_font_italic">Federated Depth Dropout</span>, a complementary technique that randomly drops frozen layers during training, to further reduce resource usage. Coupling these two techniques enables us to effectively train significantly larger models on edge devices. Specifically, we reduce training memory usage by 5<math id="id1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation></semantics></math> or more in federated self-supervised representation learning, and demonstrate that performance in downstream tasks is comparable to conventional federated self-supervised learning.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Over the last several years, deep learning has witnessed a rapid paradigm shift towards large foundational models trained on massive datasets <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; Chowdhery et al., <a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>. These models learn representations which often extend to diverse downstream tasks. However, when pre-training data is distributed across a large number of devices, it becomes impractical to train models using centralized learning. In these cases, Federated Learning <cite class="ltx_cite ltx_citemacro_citep">(FL; Konečnỳ et al., <a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite> allows participating clients to train a model together without exchanging raw data. This privacy-preserving property makes FL a popular choice for a range of applications, including face recognition <cite class="ltx_cite ltx_citemacro_citep">(Mei et al., <a href="#bib.bib19" title="" class="ltx_ref">2022a</a>)</cite>, autonomous driving <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>, recommendation systems <cite class="ltx_cite ltx_citemacro_citep">(Ning et al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite>, and self-supervised representation learning <cite class="ltx_cite ltx_citemacro_citep">(Vemulapalli et al., <a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>. In self-supervised learning, SimCLR <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, BYOL <cite class="ltx_cite ltx_citemacro_citep">(Grill et al., <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>, and SimSiam <cite class="ltx_cite ltx_citemacro_citep">(Chen &amp; He, <a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> are widely used approaches that can be adapted for use in FL settings using algorithms like Federated Averaging <cite class="ltx_cite ltx_citemacro_citep">(FedAvg; McMahan et al., <a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite>. Representation learning benefits from large models due to their capacity to learn more nuanced and reliable representations of the data <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib5" title="" class="ltx_ref">2022</a>; Tran et al., <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>. However, in cross-device FL settings, the limited resources of edge devices (including memory, computation capacity, and network bandwidth) impedes the development of large models <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Kairouz et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>. In this work, we focus on federated training of large representation learning models on a large number of edge devices under resource constraints.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Typically in FL, clients’ models share a single global architecture and perform end-to-end training in each communication round <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite>. However, many edge devices (e.g., Internet of Things (IoT) devices, mobile phones, tablets, and personal computers) lack sufficient memory and compute to train most existing large ML models. For example, the Google Pixel 6 has 12 GB of memory, which is insufficient to naively train a multi-billion parameter model. Communication of such a model and its gradient updates during every round of FL is also prohibitively data-intensive and time-consuming. These resource constraints create obstacles for real-world federated learning applications with large-scale models.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Related Work</h4>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">One direction to manage resource constraints for federated learning on edge devices is to carefully select model architecture and hyperparameters <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite> to ensure that it can be trained and run efficiently on edge devices. Another direction is to use techniques such as model compression <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> and pruning <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> to reduce the size and complexity of the model, making it more suited for training and deployment on edge devices. This can be done by removing redundant or unnecessary layers within the model, or by using low-precision arithmetic to reduce the amount of memory and computation required. In both cases, model performance degradation is usually unavoidable. Some methods rely on partially local models to avoid communicating entire models with a central server <cite class="ltx_cite ltx_citemacro_citep">(Singhal et al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>, but these approaches do not reduce local memory usage on edge devices. Other approaches involve retaining part of a model on a central server <cite class="ltx_cite ltx_citemacro_citep">(Augenstein et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>, which can reduce the need for local resource usage and is thus complementary to our work but does not itself enable training larger local models. In addition, FL methods designed for heterogeneous systems <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Horvath et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>; Mei et al., <a href="#bib.bib20" title="" class="ltx_ref">2022b</a>)</cite> are able to construct sub-models at different complexities from one unified base model. One of the early works in this direction is Federated Dropout <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>, which allows users to train using smaller subsets of the global model, reducing the client communication and computation costs. Empirically, this method can drop up to 50% of model parameters, but will degrade model performance. FjORD <cite class="ltx_cite ltx_citemacro_citep">(Horvath et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> improves upon Federated Dropout by introducing an ordered dropout technique that drops adjacent components of the model instead of random neurons. Experiments by <cite class="ltx_cite ltx_citemacro_citet">Horvath et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> show that ordered dropout can bring computational benefits and better model performance. More recently, FLANC <cite class="ltx_cite ltx_citemacro_citep">(Mei et al., <a href="#bib.bib20" title="" class="ltx_ref">2022b</a>)</cite> formulates networks at different capacities as linear combinations of a shared neural basis set, so sub-models can be composed by using capacity-specific coefficients. While these methods can reduce average local resource usage in FL, full model training is still needed for certain clients, and thus the resource usage upper bound is still determined by the base model size.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Our Contributions</h4>

<div id="S1.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S1.SS0.SSS0.Px2.p1.1" class="ltx_p">Full utilization of available resources in cross-device FL remains a challenging task. In this paper, we propose <em id="S1.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">Federated Layer-wise Learning</em>, a strategy for
resource-saving federated training.
In particular, training is divided into several phases. In each phase, we update only one active layer and freeze parameters in fixed layers.
As shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ The Effectiveness of Layer-wise Training ‣ 3 Experiments ‣ Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b), our experimental evaluation demonstrates that Federated Layer-wise Learning (FLL) can significantly reduce the resource usage of a single client compared to federated end-to-end learning (FEL) in all aspects. Specifically, FLL only uses 7–22% memory, 8–39% computation, and 8–54% communication compared to FEL. In addition, we demonstrate that <em id="S1.SS0.SSS0.Px2.p1.1.2" class="ltx_emph ltx_font_italic">Depth Dropout</em> is an effective complementary strategy in federated layer-wise learning, which further reduces resource usage upper bounds without degrading model performance.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2309.05213/assets/figs/overview.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="263" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(a) Overview of Federated Layer-wise Learning. (b) Schematics of training procedure with Depth Dropout for a 5-layer model with a budget of 3 layers.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">We consider the canonical cross-device FL scenario, in which a large distributed population of clients contributes to training a global model using their locally stored data <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>. In such scenarios, the general training process involves the following steps: first, a coordinating server sends the current set of model parameters to each contributing device. Next, each device runs a local training algorithm and sends the result back to the server. Finally, the server aggregates the model updates received from all devices to determine the new set of model parameters and restarts the cycle. As previously discussed, device resource constraints limit real-world large-scale federated learning applications and lead to a trade-off between model complexity and data accessibility.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Federated Layer-wise Learning</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">To address resource constraints when training models in FL, we propose a simple yet effective Federated Layer-wise Learning technique. We motivate and apply the method to self-supervised learning <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> in this work, but this approach is also broadly applicable. In contrast to downstream vision tasks (<span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span>, classification) that require the extraction of compact features (<span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">i.e.</span>, the interpretation of input) from the output of neural networks, contrastive representation learning employs the principle of learning representations of the input data by comparing and contrasting it with other similar and dissimilar examples. Since this loss only refers to layer activations, it can be attached to any encoder layer. When applied to residual networks, we expect the effect of applying the loss on successive layers to be progressive. This motivates our Federated Layer-wise Learning method, as depicted in Fig. <a href="#S2.F1" title="Figure 1 ‣ 2 Methods ‣ Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a).</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">The proposed method divides the holistic training process into several phases and progressively grows the model in an incremental schedule, starting from the shallow layers and moving to deeper layers. Each layer is trained for a predefined number of communication rounds before proceeding to the next layer. We only need to compute gradients and upload them to the server for the active layer, which simultaneously reduces memory usage, compute, and communication costs. We can control resource usage by varying the number of active and fixed layers during training, potentially treating multiple layers as active at a given round. As an aside, in cross-silo FL (where clients participate repeatedly in training) the fact that only one active layer is being trained enables us to avoid communicating the rest of the model to devices on most rounds, further reducing communication.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Depth Dropout</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">While the proposed Federated Layer-wise Learning significantly alleviates resource usage, our target scenario is cross-device FL, in which only a relatively small subset of active clients are selected from a large pool of participants. It is likely that a given client will not be selected twice during the entire federated training process <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>. Thus, it is necessary to download both fixed and active layers from the server to the clients. This can still present a challenge for clients with resource constraints, as downloading a large number of fixed layers and performing forward passes can be computationally intensive at the end of the training process. To this end, we propose <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Depth Dropout</em> to address the increasing resource usage introduced by a large number of fixed layers. Fig. <a href="#S2.F1" title="Figure 1 ‣ 2 Methods ‣ Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b) shows how to apply Depth Dropout to a 5-layer model with a budget of 3 layers. It begins by progressively expanding the model to reach its maximum capacity, which in this case is 3 layers. During the initial three phases, we perform standard layer-wise training. In the last two phases, we randomly remove certain fixed layers. However, the first layer, which includes Transformer patch encoding and position embedding, is never removed. For example, in phase 4, we have the option to remove either layer 1 or layer 2, while in phase 5, we have three candidates to remove. This randomization process is akin to the Dropout technique used in neural networks, and is only applied during training. During inference, the full model with 5 layers is utilized.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets and Implementation</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">We partition the standard CIFAR-100 <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky et al., <a href="#bib.bib16" title="" class="ltx_ref">2009</a>)</cite> training set into 125 clients to simulate a cross-device FL scenario. The original test set in CIFAR-100 <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky et al., <a href="#bib.bib16" title="" class="ltx_ref">2009</a>)</cite> is considered the global test set used to measure performance. ViT-Ti/16 <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> is used as the representation learning backbone. All models are trained using the following settings: SGD optimizer for the server and clients; client learning rate of <math id="S3.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="1\times 10^{-3}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">×</mo><msup id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml"><mo id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3a" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1"><times id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2">1</cn><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2">10</cn><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3"><minus id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">1\times 10^{-3}</annotation></semantics></math>; batch size of 16; 32 active clients per round.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">The Effectiveness of Layer-wise Training</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">Here we compare our approach with federated end-to-end learning on standard benchmarks. Results with different setups on CIFAR-100 are shown in Table <a href="#S3.T1" title="Table 1 ‣ The Effectiveness of Layer-wise Training ‣ 3 Experiments ‣ Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We can make the following observations: (i) Both models pre-trained by Federated Layer-wise Learning and federated end-to-end learning can significantly outperform the model without pre-training, indicating the effectiveness of self-supervised representation learning in federated settings.
(ii) While the Federated Layer-wise Learning approach is an approximation of federated end-to-end learning, it can achieve performance on par with the end-to-end method in downstream evaluation tasks. In particular, the performance gap is less than 1% when using the representation from the last layer (layer 12) of the network. (iii) we found that intermediate representations from the Federated Layer-wise Learning model performed better than those from the federated end-to-end learning model in certain downstream tasks. For example, in a linear downstream task using the representation from layer 3, the Federated Layer-wise Learning model achieved 28.3% accuracy, while the federated end-to-end learning model achieved 23.9%. This trend was also observed in other downstream tasks using different intermediate representations. This superior performance of intermediate representations is due to the contrastive loss being applied to all layers during the layer-wise pre-training process. These results suggest that models trained using the proposed method can easily compose sub-models of varying complexities.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px2.p2.1" class="ltx_p">We conducted additional experiments to further investigate the effect of model size (number of layers) and number of training rounds per layer on the performance of Federated Layer-wise and end-to-end learning. The results of these experiments are shown in Figure <a href="#S3.F2" title="Figure 2 ‣ The Effectiveness of Layer-wise Training ‣ 3 Experiments ‣ Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a). Increasing the number of layers generally led to improved performance for both learning approaches. We also found that the difference in performance between the two approaches was minimal when the number of training rounds per layer was small (4k) but became more pronounced when the number of training rounds per layer was increased (12k). Based on these results, it appears that layer-wise learning may require slightly more training rounds per layer to reach the same performance as end-to-end learning. This may be due to the fact that layer-wise learning is an approximation of end-to-end learning. However, the performance gap between the two approaches is generally less than 1%.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experimental results on CIFAR-100 with different pre-training strategies. For image classification, we report standard Top-1 accuracy (%). </figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr" style="background-color:#FFFFFF;">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.1.1" class="ltx_text" style="background-color:#FFFFFF;">Downstream</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="4"><span id="S3.T1.1.1.1.2.1" class="ltx_text" style="background-color:#FFFFFF;">Linear</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="4"><span id="S3.T1.1.1.1.3.1" class="ltx_text" style="background-color:#FFFFFF;">Finetune</span></th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">Representation From</th>
<th id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.5pt;padding-right:2.5pt;">Layer 1</th>
<th id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.5pt;padding-right:2.5pt;">Layer 3</th>
<th id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.5pt;padding-right:2.5pt;">Layer 6</th>
<th id="S3.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">Layer 12</th>
<th id="S3.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.5pt;padding-right:2.5pt;">Layer 1</th>
<th id="S3.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.5pt;padding-right:2.5pt;">Layer 3</th>
<th id="S3.T1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.5pt;padding-right:2.5pt;">Layer 6</th>
<th id="S3.T1.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:2.5pt;padding-right:2.5pt;">Layer 12</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.3.1" class="ltx_tr" style="background-color:#FFFFFF;">
<td id="S3.T1.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.1.1.1" class="ltx_text" style="background-color:#FFFFFF;">Pre-training Method</span></td>
<td id="S3.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="8"><span id="S3.T1.1.3.1.2.1" class="ltx_text" style="background-color:#FFFFFF;">Federated Layer-wise Learning</span></td>
</tr>
<tr id="S3.T1.1.4.2" class="ltx_tr">
<td id="S3.T1.1.4.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">Accuracy</td>
<td id="S3.T1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">25.3</td>
<td id="S3.T1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">28.3</td>
<td id="S3.T1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">29.2</td>
<td id="S3.T1.1.4.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">29.8</td>
<td id="S3.T1.1.4.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">30.1</td>
<td id="S3.T1.1.4.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">35.6</td>
<td id="S3.T1.1.4.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">37.2</td>
<td id="S3.T1.1.4.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">37.8</td>
</tr>
<tr id="S3.T1.1.5.3" class="ltx_tr" style="background-color:#FFFFFF;">
<td id="S3.T1.1.5.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.3.1.1" class="ltx_text" style="background-color:#FFFFFF;">Pre-training Method</span></td>
<td id="S3.T1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="8"><span id="S3.T1.1.5.3.2.1" class="ltx_text" style="background-color:#FFFFFF;">Federated End-to-end Learning</span></td>
</tr>
<tr id="S3.T1.1.6.4" class="ltx_tr">
<td id="S3.T1.1.6.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">Accuracy</td>
<td id="S3.T1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">18.0</td>
<td id="S3.T1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">23.9</td>
<td id="S3.T1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">27.8</td>
<td id="S3.T1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">30.3</td>
<td id="S3.T1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">25.4</td>
<td id="S3.T1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">32.2</td>
<td id="S3.T1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">35.7</td>
<td id="S3.T1.1.6.4.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">38.6</td>
</tr>
<tr id="S3.T1.1.7.5" class="ltx_tr" style="background-color:#FFFFFF;">
<td id="S3.T1.1.7.5.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.5.1.1" class="ltx_text" style="background-color:#FFFFFF;">Pre-training Method</span></td>
<td id="S3.T1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="8"><span id="S3.T1.1.7.5.2.1" class="ltx_text" style="background-color:#FFFFFF;">Training from scratch (Without Pre-training)</span></td>
</tr>
<tr id="S3.T1.1.8.6" class="ltx_tr">
<td id="S3.T1.1.8.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">Accuracy</td>
<td id="S3.T1.1.8.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">9.2</td>
<td id="S3.T1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">10.0</td>
<td id="S3.T1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">10.5</td>
<td id="S3.T1.1.8.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">11.3</td>
<td id="S3.T1.1.8.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">18.7</td>
<td id="S3.T1.1.8.6.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">24.2</td>
<td id="S3.T1.1.8.6.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">27.6</td>
<td id="S3.T1.1.8.6.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">29.2</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2309.05213/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(a) Comparison between Federated Layer-wise Learning (FLL) and federated end-to-end learning (FEL) under different numbers of training rounds and model sizes. Results are
reported on CIFAR-100 with downstream finetuning evaluation. (b) Resource usage comparison of a client per-round after combining Federated Layer-wise Learning and Depth Dropout. </figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy of depth dropout with Federated Layer-wise Learning, under finetuning downstream evaluation. The budget specifies the max number of layers involved in training. </figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr" style="background-color:#FFFFFF;">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.1.1.1.1" class="ltx_text" style="background-color:#FFFFFF;">Model Size</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.1.1.2.1" class="ltx_text" style="background-color:#FFFFFF;">6 layers</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<table id="S3.T2.1.1.1.3.1" class="ltx_tabular ltx_align_middle" style="background-color:#FFFFFF;">
<tr id="S3.T2.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">6 layers</td>
</tr>
<tr id="S3.T2.1.1.1.3.1.2" class="ltx_tr">
<td id="S3.T2.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">(Budget: 3 layers)</td>
</tr>
</table>
</th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.1.1.4.1" class="ltx_text" style="background-color:#FFFFFF;">3 layers</span></th>
<th id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.1.1.5.1" class="ltx_text" style="background-color:#FFFFFF;">12 layers</span></th>
<th id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<table id="S3.T2.1.1.1.6.1" class="ltx_tabular ltx_align_middle" style="background-color:#FFFFFF;">
<tr id="S3.T2.1.1.1.6.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">12 layers</td>
</tr>
<tr id="S3.T2.1.1.1.6.1.2" class="ltx_tr">
<td id="S3.T2.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">(Budget: 6 layers)</td>
</tr>
</table>
</th>
<th id="S3.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.1.1.7.1" class="ltx_text" style="background-color:#FFFFFF;">6 layers</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr" style="background-color:#FFFFFF;">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.2.1.1.1" class="ltx_text" style="background-color:#FFFFFF;">Accuracy</span></td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.2.1.2.1" class="ltx_text" style="background-color:#FFFFFF;">37.2</span></td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.2.1.3.1" class="ltx_text" style="background-color:#FFFFFF;">37.0</span></td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.2.1.4.1" class="ltx_text" style="background-color:#FFFFFF;">32.8</span></td>
<td id="S3.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.2.1.5.1" class="ltx_text" style="background-color:#FFFFFF;">37.8</span></td>
<td id="S3.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.2.1.6.1" class="ltx_text" style="background-color:#FFFFFF;">37.6</span></td>
<td id="S3.T2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.2.1.7.1" class="ltx_text" style="background-color:#FFFFFF;">37.2</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy of depth dropout with Federated Layer-wise Learning, under linear downstream evaluation. The budget specifies the max number of layers involved in training. </figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr" style="background-color:#FFFFFF;">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.1.1.1.1" class="ltx_text" style="background-color:#FFFFFF;">Model Size</span></th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.1.1.2.1" class="ltx_text" style="background-color:#FFFFFF;">6 layers</span></th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<table id="S3.T3.1.1.1.3.1" class="ltx_tabular ltx_align_middle" style="background-color:#FFFFFF;">
<tr id="S3.T3.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">6 layers</td>
</tr>
<tr id="S3.T3.1.1.1.3.1.2" class="ltx_tr">
<td id="S3.T3.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">(Budget: 3 layers)</td>
</tr>
</table>
</th>
<th id="S3.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.1.1.4.1" class="ltx_text" style="background-color:#FFFFFF;">3 layers</span></th>
<th id="S3.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.1.1.5.1" class="ltx_text" style="background-color:#FFFFFF;">12 layers</span></th>
<th id="S3.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<table id="S3.T3.1.1.1.6.1" class="ltx_tabular ltx_align_middle" style="background-color:#FFFFFF;">
<tr id="S3.T3.1.1.1.6.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">12 layers</td>
</tr>
<tr id="S3.T3.1.1.1.6.1.2" class="ltx_tr">
<td id="S3.T3.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">(Budget: 6 layers)</td>
</tr>
</table>
</th>
<th id="S3.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.1.1.7.1" class="ltx_text" style="background-color:#FFFFFF;">6 layers</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.2.1" class="ltx_tr" style="background-color:#FFFFFF;">
<td id="S3.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.2.1.1.1" class="ltx_text" style="background-color:#FFFFFF;">Accuracy</span></td>
<td id="S3.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.2.1.2.1" class="ltx_text" style="background-color:#FFFFFF;">29.2</span></td>
<td id="S3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.2.1.3.1" class="ltx_text" style="background-color:#FFFFFF;">29.1</span></td>
<td id="S3.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.2.1.4.1" class="ltx_text" style="background-color:#FFFFFF;">28.3</span></td>
<td id="S3.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.2.1.5.1" class="ltx_text" style="background-color:#FFFFFF;">29.8</span></td>
<td id="S3.T3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.2.1.6.1" class="ltx_text" style="background-color:#FFFFFF;">29.7</span></td>
<td id="S3.T3.1.2.1.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T3.1.2.1.7.1" class="ltx_text" style="background-color:#FFFFFF;">29.2</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS0.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px2.p3.1" class="ltx_p"><span id="S3.SS0.SSS0.Px2.p3.1.1" class="ltx_text ltx_font_bold">The Effectiveness of Depth Dropout.</span> We evaluate Depth Dropout with Federated Layer-wise Learning. We conducted two sets of experiments: applying Depth Dropout to a 6-layer model and a 12-layer model, with a fixed dropout rate of 50% (meaning half of the fixed layers were dropped). The results, shown in <a href="#S3.T2" title="In The Effectiveness of Layer-wise Training ‣ 3 Experiments ‣ Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tables</span> <span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S3.T3" title="Table 3 ‣ The Effectiveness of Layer-wise Training ‣ 3 Experiments ‣ Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, demonstrate that Depth Dropout does not significantly impact model performance. For example, the 6-layer model with Depth Dropout achieved 37.0% accuracy after finetuning, while the 6-layer model trained with only Layer-wise Learning achieved 37.2% accuracy. We observed similar results for the 12-layer model with Depth Dropout, which achieved 37.6% accuracy after finetuning, compared to 37.8% for the model trained with normal Layer-wise Learning. Additionally, Depth Dropout significantly reduced resource usage. It is worth noting that the resource usage of the 12-layer model with a budget of 6 layers was equivalent to the resource usage of a 6-layer model without Depth Dropout. As shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ The Effectiveness of Layer-wise Training ‣ 3 Experiments ‣ Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, depth dropout reduced the upper bounds of resource usage in all three categories, especially communication cost. The original upper bound for Layer-wise training was 54%, but it was reduced to 29% when the dropout rate was set to 50%.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Our study presents Federated Layer-wise Learning for devices with limited resources, which simultaneously reduces the demands on memory, computation, and communication for individual clients without significantly compromising performance in comparison to end-to-end training. We demonstrate that our proposed Depth Dropout technique is an effective complement to Federated Layer-wise Learning, as it further reduces resource usage across all categories with minimal loss of performance, even when dropping half of the fixed layers. Future work can evaluate these methods on larger-scale and naturally partitioned datasets, which would enable more realistic analysis of generalization performance across devices <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al., <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite>. Additionally, we intend to investigate the effects of varying dropout rate for the Depth Dropout technique. Furthermore, our method can be integrated with other memory-efficient training techniques, such as model compression <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> and activation paging <cite class="ltx_cite ltx_citemacro_citep">(Patil et al., <a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite> to potentially further reduce resource usage.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Augenstein et al. (2022)</span>
<span class="ltx_bibblock">
Sean Augenstein, Andrew Hard, Lin Ning, Karan Singhal, Satyen Kale, Kurt
Partridge, and Rajiv Mathews.

</span>
<span class="ltx_bibblock">Mixed federated learning: Joint decentralized and centralized
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.13655</em>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. (2018)</span>
<span class="ltx_bibblock">
Sebastian Caldas, Jakub Konečny, H Brendan McMahan, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Expanding the reach of federated learning by reducing client resource
requirements.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.07210</em>, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual
representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pp. 1597–1607. PMLR, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski,
Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer,
et al.

</span>
<span class="ltx_bibblock">Pali: A jointly-scaled multilingual language-image model.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.06794</em>, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen &amp; He (2021)</span>
<span class="ltx_bibblock">
Xinlei Chen and Kaiming He.

</span>
<span class="ltx_bibblock">Exploring simple siamese representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pp.  15750–15758, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2022)</span>
<span class="ltx_bibblock">
Gary Cheng, Zachary Charles, Zachary Garrett, and Keith Rush.

</span>
<span class="ltx_bibblock">Does federated dropout actually work?

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pp.  3387–3395, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2020)</span>
<span class="ltx_bibblock">
Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie.

</span>
<span class="ltx_bibblock">Model compression and hardware acceleration for neural networks: A
comprehensive survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, 108(4):485–532,
2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grill et al. (2020)</span>
<span class="ltx_bibblock">
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan
Guo, Mohammad Gheshlaghi Azar, et al.

</span>
<span class="ltx_bibblock">Bootstrap your own latent-a new approach to self-supervised learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:21271–21284, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horvath et al. (2021)</span>
<span class="ltx_bibblock">
Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos
Venieris, and Nicholas Lane.

</span>
<span class="ltx_bibblock">Fjord: Fair and accurate federated learning under heterogeneous
targets with ordered dropout.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
34:12876–12889, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2022)</span>
<span class="ltx_bibblock">
Yuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K
Leung, and Leandros Tassiulas.

</span>
<span class="ltx_bibblock">Model pruning enables efficient federated learning on edge devices.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>,
2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hubert Eichner, Salim El
Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià
Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaïd
Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu,
Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub
Konečný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
Nock, Ayfer Özgür, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh
Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng
Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu
Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Found. Trends Mach. Learn.</em>, 14(1-2):1–210, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al. (2016)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. (2009)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Geoffrey Hinton, et al.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Yijing Li, Xiaofeng Tao, Xuefei Zhang, Junjie Liu, and Jin Xu.

</span>
<span class="ltx_bibblock">Privacy-preserved federated learning for autonomous driving.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>, pp.  1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mei et al. (2022a)</span>
<span class="ltx_bibblock">
Yiqun Mei, Pengfei Guo, and Vishal M Patel.

</span>
<span class="ltx_bibblock">Escaping data scarcity for high-resolution heterogeneous face
hallucination.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pp.  18676–18686, 2022a.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mei et al. (2022b)</span>
<span class="ltx_bibblock">
Yiqun Mei, Pengfei Guo, Mo Zhou, and Vishal Patel.

</span>
<span class="ltx_bibblock">Resource-adaptive federated learning with all-in-one neural
composition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
2022b.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ning et al. (2021)</span>
<span class="ltx_bibblock">
Lin Ning, Karan Singhal, Ellie X Zhou, and Sushant Prakash.

</span>
<span class="ltx_bibblock">Learning federated representations and recommendations with limited
negatives.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07931</em>, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patil et al. (2022)</span>
<span class="ltx_bibblock">
Shishir G Patil, Paras Jain, Prabal Dutta, Ion Stoica, and Joseph Gonzalez.

</span>
<span class="ltx_bibblock">Poet: Training neural networks on tiny devices with integrated
rematerialization and paging.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp. 17573–17583. PMLR, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al. (2021)</span>
<span class="ltx_bibblock">
Karan Singhal, Hakim Sidahmed, Zachary Garrett, Shanshan Wu, John Rush, and
Sushant Prakash.

</span>
<span class="ltx_bibblock">Federated reconstruction: Partially local federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
34:11220–11232, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. (2022)</span>
<span class="ltx_bibblock">
Dustin Tran, Jeremiah Liu, Michael W Dusenberry, Du Phan, Mark Collier, Jie
Ren, Kehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, et al.

</span>
<span class="ltx_bibblock">Plex: Towards reliability using pretrained large model extensions.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.07411</em>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vemulapalli et al. (2022)</span>
<span class="ltx_bibblock">
Raviteja Vemulapalli, Warren Richard Morningstar, Philip Andrew Mansfield,
Hubert Eichner, Karan Singhal, Arash Afkanpour, and Bradley Green.

</span>
<span class="ltx_bibblock">Federated training of dual encoding models on small non-iid client
datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.00092</em>, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan
Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data,
et al.

</span>
<span class="ltx_bibblock">A field guide to federated optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.06917</em>, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2020)</span>
<span class="ltx_bibblock">
Jinjin Xu, Wenli Du, Yaochu Jin, Wangli He, and Ran Cheng.

</span>
<span class="ltx_bibblock">Ternary compression for communication-efficient federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>,
33(3):1162–1176, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2021)</span>
<span class="ltx_bibblock">
Honglin Yuan, Warren Morningstar, Lin Ning, and Karan Singhal.

</span>
<span class="ltx_bibblock">What do we mean by generalization in federated learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14216</em>, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.05212" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.05213" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.05213">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.05213" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.05214" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 06:34:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
