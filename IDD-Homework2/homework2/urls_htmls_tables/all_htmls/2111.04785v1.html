<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2111.04785] Visual Question Answering based on Formal Logic</title><meta property="og:description" content="Visual question answering (VQA) has been gaining a lot of traction in the machine learning community in the recent years due to the challenges posed in understanding information coming from multiple modalities (i.e., i‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visual Question Answering based on Formal Logic">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visual Question Answering based on Formal Logic">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2111.04785">

<!--Generated on Tue Mar 19 16:37:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Visual Question Answering,  formal logic,  transformers,  interpretable learning
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Visual Question Answering based on Formal Logic</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Muralikrishnna G. Sethuraman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">School of Electrical and Computer Engineering</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">Georgia Institute of Technology
<br class="ltx_break"></span>Atlanta, USA 
<br class="ltx_break">muralikgs@gatech.edu
</span></span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ali Payani
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Cisco</span>
<br class="ltx_break">California, USA 
<br class="ltx_break">apayani@cisco.com
</span></span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Faramarz Fekri
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_font_italic">School of Electrical and Computer Engineering</span>
<br class="ltx_break"><span id="id5.2.id2" class="ltx_text ltx_font_italic">Georgia Institute of Technology
<br class="ltx_break"></span>Atlanta, USA 
<br class="ltx_break">fekri@ece.gatech.edu
</span></span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">J. Clayton Kerce
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.1.id1" class="ltx_text ltx_align_center ltx_font_italic">Georgia Tech Research Institute</span>
<br class="ltx_break">
<br class="ltx_break"><span id="id7.2.id2" class="ltx_text ltx_font_italic">Georgia Institute of Technology
<br class="ltx_break"></span>Atlanta, USA 
<br class="ltx_break">clayton.kerce@gtri.gatech.edu
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Visual question answering (VQA) has been gaining a lot of traction in the machine learning community in the recent years due to the challenges posed in understanding information coming from multiple modalities (i.e., images, language). In VQA, a series of questions are posed based on a set of images and the task at hand is to arrive at the answer. To achieve this, we take a symbolic reasoning based approach using the framework of formal logic. The image and the questions are converted into symbolic representations on which explicit reasoning is performed. We propose a formal logic framework where (i) images are converted to logical background facts with the help of scene graphs, (ii) the questions are translated to first-order predicate logic clauses using a transformer based deep learning model, and (iii) perform satisfiability checks, by using the background knowledge and the grounding of predicate clauses, to obtain the answer. Our proposed method is highly interpretable and each step in the pipeline can be easily analyzed by a human. We validate our approach on the CLEVR and the GQA dataset. We achieve near perfect accuracy of 99.6% on the CLEVR dataset comparable to the state of art models, showcasing that formal logic is a viable tool to tackle visual question answering. Our model is also data efficient, achieving 99.1% accuracy on CLEVR dataset when trained on just 10% of the training data.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Visual Question Answering, formal logic, transformers, interpretable learning

</div>
<div id="p1" class="ltx_para">
<svg id="p1.pic1" class="ltx_picture" height="66.72" overflow="visible" version="1.1" width="604.52"><g transform="translate(0,66.72) matrix(1 0 0 -1 0 0) translate(-122.74,0) translate(0,-14.11) matrix(1.0 0.0 0.0 1.0 127.35 42.63)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignObject width="595.3" height="57.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="p1.pic1.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:430.2pt;">
<span id="p1.pic1.1.1.1.1.1" class="ltx_p"><span id="p1.pic1.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">¬©2021 IEEE. Personal use of this material is permitted.
Permission from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers or
lists, or reuse of any copyrighted component of this work in other works.</span></span>
</span></foreignObject></g></svg>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction.</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the recent years AI research has been moving towards
solving increasingly difficult problems with the goal of building a general purpose intelligence system. To achieve this, these AI systems require an understanding of information
acquired through multiple modalities (visual, audio, language
etc.,) in order to make decisions related to various tasks. Visual
question answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is one such task that integrates
the domains of Computer Vision (CV) and Natural Language
Processing (NLP).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2111.04785/assets/figures/scene_graph.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="351" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of a scene graph taken from the GQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2111.04785/assets/x1.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Samples of images and its associated questions. The first two images from the left are taken from CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> dataset where all the images are rendered and only contains objects of simple shapes. The last two examples are taken from the GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> dataset containing images taken from natural settings.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In VQA, a series of questions is posed based on a scene (image) (Figure <a href="#S1.F2" title="Figure 2 ‚Ä£ I Introduction. ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and the objective is to answer these questions based on the information provided. In the most general case, images can contain a lot of objects making it hard to parse the scene. In some cases the necessary information may not be readily available from the images and may depend some commonsense facts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Additionally, questions posed could be of arbitrary complexity due to convoluted chains of logical reasoning. Moreover, models which directly learn input to output mappings are found to have greater sensitivity towards dataset biases, and fail to learn any underlying reasoning mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. These challenges make VQA a difficult problem to solve.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Nonetheless, there have been several attempts to convert images into a structured format, to aid the process of reasoning. Notably, scene graphs (Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ I Introduction. ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) provide a structured representation of the objects in the scene along with the associated relations in the form of a graph. Here, the vertices of the graph correspond to the objects and the edges depict the relations between the objects. Significant contributions have been made towards converting images to scene graphs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Since a scene graph is a symbolic representation of a scene, this opens up the possibility of performing symbolic reasoning on images. Building on this, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> show how scene graphs can be utilized for image captioning and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> demonstrate the use of scene graphs in their VQA pipeline. In particular, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> make use of Graph Neural Networks on top of the scene graph to create higher level abstractions, making it not very easily trainable. In this paper we directly operate on the scene graph instead of dealing with the images, but do so in the context of formal logic. The scene graphs are converted into a set of background facts about the scene which are easy to interpret and the translation requires very little effort.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In addition to the works on scene graphs, several attempts have been made to model natural language questions as compositions of simple programs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> showcased the use of this approach for VQA by converting the questions into compositions of operations and using neural executors to run them. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> modified <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>‚Äôs approach by introducing curriculum learning training objective and showed significant improvement. By the same token <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> modified this further by adding structural scene representation to scene and managed to obtain really good results on the CLEVR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. These methods validate the symbolic reasoning approach to parsing questions.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">With regards to symbolic reasoning, formal logic has traditionally been used in AI as it provides a platform for constructing rules and performing inference on symbolic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Due to this, formal logic is highly interpretable, but not very effective when it comes to working with noisy data such as images. To circumvent this issue, neural networks are introduced to convert noisy data to a structured format, enabling the use of formal logic to develop solutions that are both robust and interpretable.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this paper, we attempt to solve the Visual Question Answering (VQA) problem utilizing the framework of formal logic. Inspired by the advances in scene graph generation and question to program synthesis, we propose a novel VQA pipeline consisting of: (i) conversion of scene graphs into formal logic facts, (ii) transformer-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> semantic parser for translating questions into formal logic clauses and (iii) a logic inference engine performing satisfiability check on the clauses to produce answers. To elaborate, the logic clauses act as query statements on the background facts. The resulting facts and rules are highly interpretable as they are encoded as human readable formal logic statements, making it easy to analyze the reasoning process. Finally, given the rules and the facts, we utilize prolog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to perform satifiability check. Importantly, the scene graph and questions are processed separately until the final stage of the pipeline, after which they are combined to obtain the answers.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our proposed solution has the following advantanges: (i) The question to rule conversion is highly parallelized due to the use of transformers, thereby making the training of question to rule translation highly efficient on GPUs, (ii) the use of formal logic to represent images and questions makes our intermediate stages interpretable to a great extent, and can easily be analyzed by a human at every stage, (iii) it is highly data efficient with minimal change in performance when trained with fraction of the available data, refer to Section <a href="#S3.SS1" title="III-A CLEVR Dataset ‚Ä£ III Experiments ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>. We demonstrate our proposed solution on the CLEVR dataset and the GQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The remainder of the paper is organized as follows: Section II details the proposed methodology and in Section III, we demonstrate the performance of our model through experiments and state the results. We then end with the discussion and conclusion in Section IV.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section details the overall methodology used to arrive at the answer given the question along with the scene information. To begin with, relevant definitions pertaining to predicate logic framework are provided followed by an overview of the overall proposed pipeline.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2111.04785/assets/x2.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="415" height="156" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The proposed solution has three stages: (i) scene graph to background facts conversion in the form of predicate logic, indicated by the top branch, (ii) question to target predicate logic translation using a transformer (BART <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>) based neural network and finally (iii) the logic inference which checks for the satisfiability of the final target clause. <math id="S2.F3.2.m1.1" class="ltx_Math" alttext="\phi(\cdot)" display="inline"><semantics id="S2.F3.2.m1.1b"><mrow id="S2.F3.2.m1.1.2" xref="S2.F3.2.m1.1.2.cmml"><mi id="S2.F3.2.m1.1.2.2" xref="S2.F3.2.m1.1.2.2.cmml">œï</mi><mo lspace="0em" rspace="0em" id="S2.F3.2.m1.1.2.1" xref="S2.F3.2.m1.1.2.1.cmml">‚Äã</mo><mrow id="S2.F3.2.m1.1.2.3.2" xref="S2.F3.2.m1.1.2.cmml"><mo stretchy="false" id="S2.F3.2.m1.1.2.3.2.1" xref="S2.F3.2.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.F3.2.m1.1.1" xref="S2.F3.2.m1.1.1.cmml">‚ãÖ</mo><mo stretchy="false" id="S2.F3.2.m1.1.2.3.2.2" xref="S2.F3.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.F3.2.m1.1c"><apply id="S2.F3.2.m1.1.2.cmml" xref="S2.F3.2.m1.1.2"><times id="S2.F3.2.m1.1.2.1.cmml" xref="S2.F3.2.m1.1.2.1"></times><ci id="S2.F3.2.m1.1.2.2.cmml" xref="S2.F3.2.m1.1.2.2">italic-œï</ci><ci id="S2.F3.2.m1.1.1.cmml" xref="S2.F3.2.m1.1.1">‚ãÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.2.m1.1d">\phi(\cdot)</annotation></semantics></math> refers to the softmax function</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.F3.3" class="ltx_p ltx_figure_panel ltx_align_center">.</p>
</div>
</div>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Predicate Logic.</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p">Predicate logic framework uses formal logic to denote facts and rules. Typically rules are expressed in the following form</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="a_{H}\leftarrow a_{1},\ldots,a_{m}." display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1" xref="S2.E1.m1.2.2.1.1.cmml"><msub id="S2.E1.m1.2.2.1.1.4" xref="S2.E1.m1.2.2.1.1.4.cmml"><mi id="S2.E1.m1.2.2.1.1.4.2" xref="S2.E1.m1.2.2.1.1.4.2.cmml">a</mi><mi id="S2.E1.m1.2.2.1.1.4.3" xref="S2.E1.m1.2.2.1.1.4.3.cmml">H</mi></msub><mo stretchy="false" id="S2.E1.m1.2.2.1.1.3" xref="S2.E1.m1.2.2.1.1.3.cmml">‚Üê</mo><mrow id="S2.E1.m1.2.2.1.1.2.2" xref="S2.E1.m1.2.2.1.1.2.3.cmml"><msub id="S2.E1.m1.2.2.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.2.cmml">a</mi><mn id="S2.E1.m1.2.2.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.E1.m1.2.2.1.1.2.2.3" xref="S2.E1.m1.2.2.1.1.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">‚Ä¶</mi><mo id="S2.E1.m1.2.2.1.1.2.2.4" xref="S2.E1.m1.2.2.1.1.2.3.cmml">,</mo><msub id="S2.E1.m1.2.2.1.1.2.2.2" xref="S2.E1.m1.2.2.1.1.2.2.2.cmml"><mi id="S2.E1.m1.2.2.1.1.2.2.2.2" xref="S2.E1.m1.2.2.1.1.2.2.2.2.cmml">a</mi><mi id="S2.E1.m1.2.2.1.1.2.2.2.3" xref="S2.E1.m1.2.2.1.1.2.2.2.3.cmml">m</mi></msub></mrow></mrow><mo lspace="0em" id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.1.1.cmml" xref="S2.E1.m1.2.2.1"><ci id="S2.E1.m1.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.3">‚Üê</ci><apply id="S2.E1.m1.2.2.1.1.4.cmml" xref="S2.E1.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.4.1.cmml" xref="S2.E1.m1.2.2.1.1.4">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.4.2.cmml" xref="S2.E1.m1.2.2.1.1.4.2">ùëé</ci><ci id="S2.E1.m1.2.2.1.1.4.3.cmml" xref="S2.E1.m1.2.2.1.1.4.3">ùêª</ci></apply><list id="S2.E1.m1.2.2.1.1.2.3.cmml" xref="S2.E1.m1.2.2.1.1.2.2"><apply id="S2.E1.m1.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.2">ùëé</ci><cn type="integer" id="S2.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.3">1</cn></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">‚Ä¶</ci><apply id="S2.E1.m1.2.2.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.2.2.2.1.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.2">ùëé</ci><ci id="S2.E1.m1.2.2.1.1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.3">ùëö</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">a_{H}\leftarrow a_{1},\ldots,a_{m}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.5" class="ltx_p">The individual elements <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="a_{H}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><msub id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">a</mi><mi id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">ùëé</ci><ci id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">ùêª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">a_{H}</annotation></semantics></math> and the <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><msub id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">a</mi><mi id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">ùëé</ci><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">a_{i}</annotation></semantics></math>‚Äôs of the rule <a href="#S2.E1" title="In II-A Predicate Logic. ‚Ä£ II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> are known as the atoms of the rule. Here <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="a_{H}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><msub id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">a</mi><mi id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">ùëé</ci><ci id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">ùêª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">a_{H}</annotation></semantics></math> is called the head of the rule and <math id="S2.SS1.p1.4.m4.3" class="ltx_Math" alttext="a_{1},\ldots,a_{m}" display="inline"><semantics id="S2.SS1.p1.4.m4.3a"><mrow id="S2.SS1.p1.4.m4.3.3.2" xref="S2.SS1.p1.4.m4.3.3.3.cmml"><msub id="S2.SS1.p1.4.m4.2.2.1.1" xref="S2.SS1.p1.4.m4.2.2.1.1.cmml"><mi id="S2.SS1.p1.4.m4.2.2.1.1.2" xref="S2.SS1.p1.4.m4.2.2.1.1.2.cmml">a</mi><mn id="S2.SS1.p1.4.m4.2.2.1.1.3" xref="S2.SS1.p1.4.m4.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p1.4.m4.3.3.2.3" xref="S2.SS1.p1.4.m4.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">‚Ä¶</mi><mo id="S2.SS1.p1.4.m4.3.3.2.4" xref="S2.SS1.p1.4.m4.3.3.3.cmml">,</mo><msub id="S2.SS1.p1.4.m4.3.3.2.2" xref="S2.SS1.p1.4.m4.3.3.2.2.cmml"><mi id="S2.SS1.p1.4.m4.3.3.2.2.2" xref="S2.SS1.p1.4.m4.3.3.2.2.2.cmml">a</mi><mi id="S2.SS1.p1.4.m4.3.3.2.2.3" xref="S2.SS1.p1.4.m4.3.3.2.2.3.cmml">m</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.3b"><list id="S2.SS1.p1.4.m4.3.3.3.cmml" xref="S2.SS1.p1.4.m4.3.3.2"><apply id="S2.SS1.p1.4.m4.2.2.1.1.cmml" xref="S2.SS1.p1.4.m4.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.2.2.1.1.1.cmml" xref="S2.SS1.p1.4.m4.2.2.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.2.2.1.1.2.cmml" xref="S2.SS1.p1.4.m4.2.2.1.1.2">ùëé</ci><cn type="integer" id="S2.SS1.p1.4.m4.2.2.1.1.3.cmml" xref="S2.SS1.p1.4.m4.2.2.1.1.3">1</cn></apply><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">‚Ä¶</ci><apply id="S2.SS1.p1.4.m4.3.3.2.2.cmml" xref="S2.SS1.p1.4.m4.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.3.3.2.2.1.cmml" xref="S2.SS1.p1.4.m4.3.3.2.2">subscript</csymbol><ci id="S2.SS1.p1.4.m4.3.3.2.2.2.cmml" xref="S2.SS1.p1.4.m4.3.3.2.2.2">ùëé</ci><ci id="S2.SS1.p1.4.m4.3.3.2.2.3.cmml" xref="S2.SS1.p1.4.m4.3.3.2.2.3">ùëö</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.3c">a_{1},\ldots,a_{m}</annotation></semantics></math> is known as the body. Rule of the above form implies that all the atoms in the body have to be true for the head rule to be true. Each atom in the rule is a n-ary Boolean function <math id="S2.SS1.p1.5.m5.3" class="ltx_Math" alttext="p(x_{1},\ldots,x_{n})" display="inline"><semantics id="S2.SS1.p1.5.m5.3a"><mrow id="S2.SS1.p1.5.m5.3.3" xref="S2.SS1.p1.5.m5.3.3.cmml"><mi id="S2.SS1.p1.5.m5.3.3.4" xref="S2.SS1.p1.5.m5.3.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.5.m5.3.3.3" xref="S2.SS1.p1.5.m5.3.3.3.cmml">‚Äã</mo><mrow id="S2.SS1.p1.5.m5.3.3.2.2" xref="S2.SS1.p1.5.m5.3.3.2.3.cmml"><mo stretchy="false" id="S2.SS1.p1.5.m5.3.3.2.2.3" xref="S2.SS1.p1.5.m5.3.3.2.3.cmml">(</mo><msub id="S2.SS1.p1.5.m5.2.2.1.1.1" xref="S2.SS1.p1.5.m5.2.2.1.1.1.cmml"><mi id="S2.SS1.p1.5.m5.2.2.1.1.1.2" xref="S2.SS1.p1.5.m5.2.2.1.1.1.2.cmml">x</mi><mn id="S2.SS1.p1.5.m5.2.2.1.1.1.3" xref="S2.SS1.p1.5.m5.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p1.5.m5.3.3.2.2.4" xref="S2.SS1.p1.5.m5.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">‚Ä¶</mi><mo id="S2.SS1.p1.5.m5.3.3.2.2.5" xref="S2.SS1.p1.5.m5.3.3.2.3.cmml">,</mo><msub id="S2.SS1.p1.5.m5.3.3.2.2.2" xref="S2.SS1.p1.5.m5.3.3.2.2.2.cmml"><mi id="S2.SS1.p1.5.m5.3.3.2.2.2.2" xref="S2.SS1.p1.5.m5.3.3.2.2.2.2.cmml">x</mi><mi id="S2.SS1.p1.5.m5.3.3.2.2.2.3" xref="S2.SS1.p1.5.m5.3.3.2.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S2.SS1.p1.5.m5.3.3.2.2.6" xref="S2.SS1.p1.5.m5.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.3b"><apply id="S2.SS1.p1.5.m5.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3"><times id="S2.SS1.p1.5.m5.3.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3.3"></times><ci id="S2.SS1.p1.5.m5.3.3.4.cmml" xref="S2.SS1.p1.5.m5.3.3.4">ùëù</ci><vector id="S2.SS1.p1.5.m5.3.3.2.3.cmml" xref="S2.SS1.p1.5.m5.3.3.2.2"><apply id="S2.SS1.p1.5.m5.2.2.1.1.1.cmml" xref="S2.SS1.p1.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.2.2.1.1.1.1.cmml" xref="S2.SS1.p1.5.m5.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.5.m5.2.2.1.1.1.2">ùë•</ci><cn type="integer" id="S2.SS1.p1.5.m5.2.2.1.1.1.3.cmml" xref="S2.SS1.p1.5.m5.2.2.1.1.1.3">1</cn></apply><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">‚Ä¶</ci><apply id="S2.SS1.p1.5.m5.3.3.2.2.2.cmml" xref="S2.SS1.p1.5.m5.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.3.3.2.2.2.1.cmml" xref="S2.SS1.p1.5.m5.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.5.m5.3.3.2.2.2.2.cmml" xref="S2.SS1.p1.5.m5.3.3.2.2.2.2">ùë•</ci><ci id="S2.SS1.p1.5.m5.3.3.2.2.2.3.cmml" xref="S2.SS1.p1.5.m5.3.3.2.2.2.3">ùëõ</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.3c">p(x_{1},\ldots,x_{n})</annotation></semantics></math> called a predicate, where the arguments could be variables, constants or even other predicates. A predicate expresses a relation between variables and constants in the framework. Moving forward, all constants are denoted using lower case strings/letters/numbers (e.g., <span id="S2.SS1.p1.5.1" class="ltx_text ltx_font_sansserif">color</span>, <span id="S2.SS1.p1.5.2" class="ltx_text ltx_font_sansserif">blue</span>, <span id="S2.SS1.p1.5.3" class="ltx_text ltx_font_sansserif">1</span>, <span id="S2.SS1.p1.5.4" class="ltx_text ltx_font_sansserif">c</span>) and variables are denoted using upper case letters (e.g., <span id="S2.SS1.p1.5.5" class="ltx_text ltx_font_sansserif">W</span>, <span id="S2.SS1.p1.5.6" class="ltx_text ltx_font_sansserif">X</span>, <span id="S2.SS1.p1.5.7" class="ltx_text ltx_font_sansserif">Y</span>). A predicate is said to be grounded if all of its arguments are set to constants. Please refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> for more information on predicate logic.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">VQA Pipeline.</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Taking the scene graph and the question as the inputs, the proposed pipeline arrives at the answer by performing three separate steps, as in Figure <a href="#S2.F3" title="Figure 3 ‚Ä£ II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Representation of Scene Graph in the form of Background facts</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.3" class="ltx_p">In this step, the given scene graph is coverted into a set of facts encoded as groundings of predicates. To that end, object IDs between <math id="S2.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><mn id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><cn type="integer" id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">1</annotation></semantics></math> to <math id="S2.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS2.SSS1.p1.2.m2.1a"><mi id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.1b"><ci id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.1c">N</annotation></semantics></math> are assigned to each object in the scene, here <math id="S2.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS2.SSS1.p1.3.m3.1a"><mi id="S2.SS2.SSS1.p1.3.m3.1.1" xref="S2.SS2.SSS1.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.3.m3.1b"><ci id="S2.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.3.m3.1c">N</annotation></semantics></math> is the total number of objects in the scene. It is important to note that these IDs are local to the scene.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2111.04785/assets/x3.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="86" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The figure shows the conversion of scene graph to background knowledge. The scene contains two objects labelled 0 and 1. The attributes are listed in the table below the corresponding node and relation is indicated by the edge. Each attribute of the two objects are encoded as groundings of the predicate <span id="S2.F4.4.1" class="ltx_text ltx_font_sansserif">attribute</span> and similarly the relation <span id="S2.F4.5.2" class="ltx_text ltx_font_sansserif">behind</span> is encoded a grounding of the predicate <span id="S2.F4.6.3" class="ltx_text ltx_font_sansserif">relation</span>.</figcaption>
</figure>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">We define the following two predicates to aid in this conversion:</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_sansserif">attribute</span>(<span id="S2.I1.i1.p1.1.2" class="ltx_text ltx_font_sansserif">X, A, B</span>) - where the variable <span id="S2.I1.i1.p1.1.3" class="ltx_text ltx_font_sansserif">X</span> denotes ID of the object, <span id="S2.I1.i1.p1.1.4" class="ltx_text ltx_font_sansserif">A</span> denotes the attribute type (for example, <span id="S2.I1.i1.p1.1.5" class="ltx_text ltx_font_sansserif">color</span>, <span id="S2.I1.i1.p1.1.6" class="ltx_text ltx_font_sansserif">shape</span>) and <span id="S2.I1.i1.p1.1.7" class="ltx_text ltx_font_sansserif">B</span> denotes the specific attribute (for example, <span id="S2.I1.i1.p1.1.8" class="ltx_text ltx_font_sansserif">blue</span>, <span id="S2.I1.i1.p1.1.9" class="ltx_text ltx_font_sansserif">cube</span>). For instance, if object 1 in the scene was blue in color then the equivalent grounding would be <span id="S2.I1.i1.p1.1.10" class="ltx_text ltx_font_sansserif">attribute</span>(<span id="S2.I1.i1.p1.1.11" class="ltx_text ltx_font_sansserif">1, color, blue</span>).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_sansserif">relation</span>(<span id="S2.I1.i2.p1.1.2" class="ltx_text ltx_font_sansserif">X, Y, A</span>) - where <span id="S2.I1.i2.p1.1.3" class="ltx_text ltx_font_sansserif">X</span> and <span id="S2.I1.i2.p1.1.4" class="ltx_text ltx_font_sansserif">Y</span> denote the object IDs, and <span id="S2.I1.i2.p1.1.5" class="ltx_text ltx_font_sansserif">A</span> denotes the relation type (say <span id="S2.I1.i2.p1.1.6" class="ltx_text ltx_font_sansserif">behind</span>). For instance, if objects 1 is to the left of 2 then the equivalent grounding would be <span id="S2.I1.i2.p1.1.7" class="ltx_text ltx_font_sansserif">relation</span>(<span id="S2.I1.i2.p1.1.8" class="ltx_text ltx_font_sansserif">1</span>, <span id="S2.I1.i2.p1.1.9" class="ltx_text ltx_font_sansserif">2</span>, <span id="S2.I1.i2.p1.1.10" class="ltx_text ltx_font_sansserif">left</span>).</p>
</div>
</li>
</ol>
<p id="S2.SS2.SSS1.p3.1" class="ltx_p">We then extract the object attributes and the relations from the scene graph. Iterating through every object in the scene, attributes and relations are converted to groundings of the predicates <span id="S2.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_sansserif">attribute</span> and <span id="S2.SS2.SSS1.p3.1.2" class="ltx_text ltx_font_sansserif">relation</span> respectively. Finally, we obtain a list of groundings for the defined predicates which captures all the information available in the scene graph. We call this list the background knowledge corresponding to the scene. Figure <a href="#S2.F4" title="Figure 4 ‚Ä£ II-B1 Representation of Scene Graph in the form of Background facts ‚Ä£ II-B VQA Pipeline. ‚Ä£ II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the working for a toy example.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>Representation of Question in the form of first order logic rule</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">We now parse the question to generate a set of logic clauses. Given the background facts obtained from the previous step, the generated rules are such that running satisfiability check would yield in the answer. These sets of rules are formed using the predicates <span id="S2.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_sansserif">attribute</span>, <span id="S2.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_sansserif">relation</span> defined earlier along with a few other predicates that are necessary to capture the meaning of the question in terms of a formal logic statement. Table <a href="#S2.T1" title="TABLE I ‚Ä£ II-B2 Representation of Question in the form of first order logic rule ‚Ä£ II-B VQA Pipeline. ‚Ä£ II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> lists the complete set of predicates used for generating the rule. It is important to note that the choice of predicates is such that it can be used with most visual question answering datasets. The constants and the groundings of the predicates would be specific to the dataset at hand. In this paper we show how the proposed framework works for the CLEVR and the GQA dataset. With slight modifications it can be incorported to work with most other VQA datasets as well.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.18" class="ltx_p">Consider the question taken from the CLEVR data set ‚ÄúAre there more big green things than large purple shiny cubes?‚Äù the equivalent set of rules are shown below:</p>
<p id="S2.SS2.SSS2.p2.17.17.17" class="ltx_p ltx_minipage ltx_align_center ltx_align_middle" style="width:216.8pt;"><span id="S2.SS2.SSS2.p2.17.17.17.17" class="ltx_text ltx_font_sansserif" style="font-size:80%;">r<sub id="S2.SS2.SSS2.p2.17.17.17.17.6" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.6.1" class="ltx_text ltx_font_serif">0</span></sub>(W)<span id="S2.SS2.SSS2.p2.2.2.2.2.1" class="ltx_text ltx_font_serif"> <math id="S2.SS2.SSS2.p2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="S2.SS2.SSS2.p2.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S2.SS2.SSS2.p2.2.2.2.2.1.m1.1.1" xref="S2.SS2.SSS2.p2.2.2.2.2.1.m1.1.1.cmml">‚Üê</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.2.2.2.2.1.m1.1b"><ci id="S2.SS2.SSS2.p2.2.2.2.2.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.2.2.2.2.1.m1.1.1">‚Üê</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.2.2.2.2.1.m1.1c">\leftarrow</annotation></semantics></math> </span>attribute(W, size, large), attribute(W, color, green)<span id="S2.SS2.SSS2.p2.17.17.17.17.7" class="ltx_text ltx_font_serif">.
<br class="ltx_break"></span>r<sub id="S2.SS2.SSS2.p2.17.17.17.17.8" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.8.1" class="ltx_text ltx_font_serif">1</span></sub>(C)<span id="S2.SS2.SSS2.p2.4.4.4.4.2" class="ltx_text ltx_font_serif"> <math id="S2.SS2.SSS2.p2.4.4.4.4.2.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="S2.SS2.SSS2.p2.4.4.4.4.2.m1.1a"><mo stretchy="false" id="S2.SS2.SSS2.p2.4.4.4.4.2.m1.1.1" xref="S2.SS2.SSS2.p2.4.4.4.4.2.m1.1.1.cmml">‚Üê</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.4.4.4.4.2.m1.1b"><ci id="S2.SS2.SSS2.p2.4.4.4.4.2.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.4.4.4.4.2.m1.1.1">‚Üê</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.4.4.4.4.2.m1.1c">\leftarrow</annotation></semantics></math> </span>count(r<sub id="S2.SS2.SSS2.p2.17.17.17.17.9" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.9.1" class="ltx_text ltx_font_serif">0</span></sub>(W), C)<span id="S2.SS2.SSS2.p2.17.17.17.17.10" class="ltx_text ltx_font_serif">.
<br class="ltx_break"></span>r<sub id="S2.SS2.SSS2.p2.17.17.17.17.11" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.11.1" class="ltx_text ltx_font_serif">2</span></sub>(X)<span id="S2.SS2.SSS2.p2.7.7.7.7.3" class="ltx_text ltx_font_serif"> <math id="S2.SS2.SSS2.p2.7.7.7.7.3.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="S2.SS2.SSS2.p2.7.7.7.7.3.m1.1a"><mo stretchy="false" id="S2.SS2.SSS2.p2.7.7.7.7.3.m1.1.1" xref="S2.SS2.SSS2.p2.7.7.7.7.3.m1.1.1.cmml">‚Üê</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.7.7.7.7.3.m1.1b"><ci id="S2.SS2.SSS2.p2.7.7.7.7.3.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.7.7.7.7.3.m1.1.1">‚Üê</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.7.7.7.7.3.m1.1c">\leftarrow</annotation></semantics></math> </span>attribute(X, size, large), attribute(X, color, purple), attribute(X, material, metal), attribute(X, shape, cube)<span id="S2.SS2.SSS2.p2.17.17.17.17.12" class="ltx_text ltx_font_serif">.
<br class="ltx_break"></span>r<sub id="S2.SS2.SSS2.p2.17.17.17.17.13" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.13.1" class="ltx_text ltx_font_serif">3</span></sub>(C)<span id="S2.SS2.SSS2.p2.9.9.9.9.4" class="ltx_text ltx_font_serif"> <math id="S2.SS2.SSS2.p2.9.9.9.9.4.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="S2.SS2.SSS2.p2.9.9.9.9.4.m1.1a"><mo stretchy="false" id="S2.SS2.SSS2.p2.9.9.9.9.4.m1.1.1" xref="S2.SS2.SSS2.p2.9.9.9.9.4.m1.1.1.cmml">‚Üê</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.9.9.9.9.4.m1.1b"><ci id="S2.SS2.SSS2.p2.9.9.9.9.4.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.9.9.9.9.4.m1.1.1">‚Üê</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.9.9.9.9.4.m1.1c">\leftarrow</annotation></semantics></math> </span>count(r<sub id="S2.SS2.SSS2.p2.17.17.17.17.14" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.14.1" class="ltx_text ltx_font_serif">2</span></sub>(X), C)<span id="S2.SS2.SSS2.p2.17.17.17.17.15" class="ltx_text ltx_font_serif">.
<br class="ltx_break"></span>target<span id="S2.SS2.SSS2.p2.11.11.11.11.5" class="ltx_text ltx_font_serif"> <math id="S2.SS2.SSS2.p2.11.11.11.11.5.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="S2.SS2.SSS2.p2.11.11.11.11.5.m1.1a"><mo stretchy="false" id="S2.SS2.SSS2.p2.11.11.11.11.5.m1.1.1" xref="S2.SS2.SSS2.p2.11.11.11.11.5.m1.1.1.cmml">‚Üê</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.11.11.11.11.5.m1.1b"><ci id="S2.SS2.SSS2.p2.11.11.11.11.5.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.11.11.11.11.5.m1.1.1">‚Üê</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.11.11.11.11.5.m1.1c">\leftarrow</annotation></semantics></math> </span>r<sub id="S2.SS2.SSS2.p2.17.17.17.17.16" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.16.1" class="ltx_text ltx_font_serif">1</span></sub>(C<sub id="S2.SS2.SSS2.p2.17.17.17.17.17" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.17.1" class="ltx_text ltx_font_serif">1</span></sub>), r<sub id="S2.SS2.SSS2.p2.17.17.17.17.18" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.18.1" class="ltx_text ltx_font_serif">3</span></sub>(C<sub id="S2.SS2.SSS2.p2.17.17.17.17.19" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.19.1" class="ltx_text ltx_font_serif">2</span></sub>)<span id="S2.SS2.SSS2.p2.17.17.17.17.20" class="ltx_text ltx_font_serif">, </span>greater_than(C<sub id="S2.SS2.SSS2.p2.17.17.17.17.21" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.21.1" class="ltx_text ltx_font_serif">1</span></sub>, C<sub id="S2.SS2.SSS2.p2.17.17.17.17.22" class="ltx_sub"><span id="S2.SS2.SSS2.p2.17.17.17.17.22.1" class="ltx_text ltx_font_serif">2</span></sub>)<span id="S2.SS2.SSS2.p2.17.17.17.17.23" class="ltx_text ltx_font_serif">.
</span></span></p>
<p id="S2.SS2.SSS2.p2.19" class="ltx_p">The final rule is called the target rule as it yields the required answer to the question. In the rules stated above, the arguments of the predicates contain both variables and constants. In the final stage we check to see if there are valid groundings available in the background knowledge that satisfies the rule. Figure <a href="#S2.F5" title="Figure 5 ‚Ä£ II-B2 Representation of Question in the form of first order logic rule ‚Ä£ II-B VQA Pipeline. ‚Ä£ II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows a few examples of question target rule pair.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.2" class="ltx_p">To automate the process of generating the target rule given the question, we map the set of rules to a single sentence we define as the target sentence. The target sentence comprises of the predicates used in the rule along with characters to denote the conclusion of the first rule and the start of the next. For instance the target sentence for the rule mentioned previously is given by</p>
<div id="S2.SS2.SSS2.p3.1.1" class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" style="width:216.8pt;">
<p id="S2.SS2.SSS2.p3.1.1.1" class="ltx_p"><span id="S2.SS2.SSS2.p3.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">attribute(W, size, large),attribute(W, color, green)\C1 \attribute(X, size, large), attribute(X, color, purple),
attribute(X, material, metal), attribute(X, shape, cube)\C2 \<math id="S2.SS2.SSS2.p3.1.1.1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.SS2.SSS2.p3.1.1.1.1.m1.1a"><mo id="S2.SS2.SSS2.p3.1.1.1.1.m1.1.1" xref="S2.SS2.SSS2.p3.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p3.1.1.1.1.m1.1b"><gt id="S2.SS2.SSS2.p3.1.1.1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p3.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p3.1.1.1.1.m1.1c">&gt;</annotation></semantics></math>\<span id="S2.SS2.SSS2.p3.1.1.1.1.1" class="ltx_text ltx_font_serif">
</span></span></p>
</div>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Summary of all the predicates used in the proposed solution.</figcaption>
<table id="S2.T1.34" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.34.35.1" class="ltx_tr">
<th id="S2.T1.34.35.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="width:71.5pt;">
<span id="S2.T1.34.35.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.34.35.1.1.1.1" class="ltx_p">Predicate</span>
</span>
</th>
<th id="S2.T1.34.35.1.2" class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Definition</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.34.36.1" class="ltx_tr">
<th id="S2.T1.34.36.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" style="width:71.5pt;">
<span id="S2.T1.34.36.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.34.36.1.1.1.1" class="ltx_p"><span id="S2.T1.34.36.1.1.1.1.1" class="ltx_text ltx_font_sansserif" style="font-size:90%;">attribute(id,t,at)</span></span>
</span>
</th>
<td id="S2.T1.34.36.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" style="width:121.4pt;">
<span id="S2.T1.34.36.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.34.36.1.2.1.1" class="ltx_p">True if object <span id="S2.T1.34.36.1.2.1.1.1" class="ltx_text ltx_font_sansserif">id</span> has the attribute <span id="S2.T1.34.36.1.2.1.1.2" class="ltx_text ltx_font_sansserif">at</span> of type <span id="S2.T1.34.36.1.2.1.1.3" class="ltx_text ltx_font_sansserif">t</span>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<th id="S2.T1.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="width:71.5pt;">
<span id="S2.T1.2.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.2.2.2.2" class="ltx_p"><span id="S2.T1.2.2.2.2.2.2" class="ltx_text ltx_font_sansserif" style="font-size:90%;">relation(id<sub id="S2.T1.2.2.2.2.2.2.1" class="ltx_sub"><span id="S2.T1.2.2.2.2.2.2.1.1" class="ltx_text ltx_font_serif">1</span></sub>,id<sub id="S2.T1.2.2.2.2.2.2.2" class="ltx_sub"><span id="S2.T1.2.2.2.2.2.2.2.1" class="ltx_text ltx_font_serif">2</span></sub>,rl)</span></span>
</span>
</th>
<td id="S2.T1.4.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:121.4pt;">
<span id="S2.T1.4.4.4.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.4.2.2" class="ltx_p">True if object <span id="S2.T1.3.3.3.1.1.1" class="ltx_text ltx_font_sansserif">id<sub id="S2.T1.3.3.3.1.1.1.1" class="ltx_sub"><span id="S2.T1.3.3.3.1.1.1.1.1" class="ltx_text ltx_font_serif">1</span></sub></span> is related to object <span id="S2.T1.4.4.4.2.2.2" class="ltx_text ltx_font_sansserif">id<sub id="S2.T1.4.4.4.2.2.2.1" class="ltx_sub"><span id="S2.T1.4.4.4.2.2.2.1.1" class="ltx_text ltx_font_serif">2</span></sub></span> by the relation <span id="S2.T1.4.4.4.2.2.3" class="ltx_text ltx_font_sansserif">rl</span>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.8.8" class="ltx_tr">
<th id="S2.T1.6.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="width:71.5pt;">
<span id="S2.T1.6.6.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.6.2.2.2" class="ltx_p"><span id="S2.T1.6.6.2.2.2.2" class="ltx_text ltx_font_sansserif" style="font-size:90%;">same_size(id<sub id="S2.T1.6.6.2.2.2.2.1" class="ltx_sub"><span id="S2.T1.6.6.2.2.2.2.1.1" class="ltx_text ltx_font_serif">1</span></sub>,id<sub id="S2.T1.6.6.2.2.2.2.2" class="ltx_sub"><span id="S2.T1.6.6.2.2.2.2.2.1" class="ltx_text ltx_font_serif">2</span></sub>)</span></span>
</span>
</th>
<td id="S2.T1.8.8.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:121.4pt;">
<span id="S2.T1.8.8.4.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.8.8.4.2.2" class="ltx_p">True if objects <span id="S2.T1.7.7.3.1.1.1" class="ltx_text ltx_font_sansserif">id<sub id="S2.T1.7.7.3.1.1.1.1" class="ltx_sub"><span id="S2.T1.7.7.3.1.1.1.1.1" class="ltx_text ltx_font_serif">1</span></sub></span> and <span id="S2.T1.8.8.4.2.2.2" class="ltx_text ltx_font_sansserif">id<sub id="S2.T1.8.8.4.2.2.2.1" class="ltx_sub"><span id="S2.T1.8.8.4.2.2.2.1.1" class="ltx_text ltx_font_serif">2</span></sub></span> have the same size attribute.</span>
</span>
</td>
</tr>
<tr id="S2.T1.12.12" class="ltx_tr">
<th id="S2.T1.10.10.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="width:71.5pt;">
<span id="S2.T1.10.10.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.10.10.2.2.2" class="ltx_p"><span id="S2.T1.10.10.2.2.2.2" class="ltx_text ltx_font_sansserif" style="font-size:90%;">same_shape(id<sub id="S2.T1.10.10.2.2.2.2.1" class="ltx_sub"><span id="S2.T1.10.10.2.2.2.2.1.1" class="ltx_text ltx_font_serif">1</span></sub>,id<sub id="S2.T1.10.10.2.2.2.2.2" class="ltx_sub"><span id="S2.T1.10.10.2.2.2.2.2.1" class="ltx_text ltx_font_serif">2</span></sub>)</span></span>
</span>
</th>
<td id="S2.T1.12.12.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:121.4pt;">
<span id="S2.T1.12.12.4.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.12.12.4.2.2" class="ltx_p">True if objects <span id="S2.T1.11.11.3.1.1.1" class="ltx_text ltx_font_sansserif">id<sub id="S2.T1.11.11.3.1.1.1.1" class="ltx_sub"><span id="S2.T1.11.11.3.1.1.1.1.1" class="ltx_text ltx_font_serif">1</span></sub></span> and <span id="S2.T1.12.12.4.2.2.2" class="ltx_text ltx_font_sansserif">id<sub id="S2.T1.12.12.4.2.2.2.1" class="ltx_sub"><span id="S2.T1.12.12.4.2.2.2.1.1" class="ltx_text ltx_font_serif">2</span></sub></span> have the same shape attribute.</span>
</span>
</td>
</tr>
<tr id="S2.T1.16.16" class="ltx_tr">
<th id="S2.T1.14.14.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="width:71.5pt;">
<span id="S2.T1.14.14.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.14.14.2.2.2" class="ltx_p"><span id="S2.T1.14.14.2.2.2.2" class="ltx_text ltx_font_sansserif" style="font-size:90%;">same_color(id<sub id="S2.T1.14.14.2.2.2.2.1" class="ltx_sub"><span id="S2.T1.14.14.2.2.2.2.1.1" class="ltx_text ltx_font_serif">1</span></sub>,id<sub id="S2.T1.14.14.2.2.2.2.2" class="ltx_sub"><span id="S2.T1.14.14.2.2.2.2.2.1" class="ltx_text ltx_font_serif">2</span></sub>)</span></span>
</span>
</th>
<td id="S2.T1.16.16.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:121.4pt;">
<span id="S2.T1.16.16.4.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.16.16.4.2.2" class="ltx_p">True if objects <span id="S2.T1.15.15.3.1.1.1" class="ltx_text ltx_font_sansserif">id<sub id="S2.T1.15.15.3.1.1.1.1" class="ltx_sub"><span id="S2.T1.15.15.3.1.1.1.1.1" class="ltx_text ltx_font_serif">1</span></sub></span> and <span id="S2.T1.16.16.4.2.2.2" class="ltx_text ltx_font_sansserif">id<sub id="S2.T1.16.16.4.2.2.2.1" class="ltx_sub"><span id="S2.T1.16.16.4.2.2.2.1.1" class="ltx_text ltx_font_serif">2</span></sub></span> have the same color attribute.</span>
</span>
</td>
</tr>
<tr id="S2.T1.20.20" class="ltx_tr">
<th id="S2.T1.18.18.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="width:71.5pt;">
<span id="S2.T1.18.18.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.18.18.2.2.2" class="ltx_p"><span id="S2.T1.18.18.2.2.2.2" class="ltx_text ltx_font_sansserif" style="font-size:90%;">same_material(id<sub id="S2.T1.18.18.2.2.2.2.1" class="ltx_sub"><span id="S2.T1.18.18.2.2.2.2.1.1" class="ltx_text ltx_font_serif">1</span></sub>,id<sub id="S2.T1.18.18.2.2.2.2.2" class="ltx_sub"><span id="S2.T1.18.18.2.2.2.2.2.1" class="ltx_text ltx_font_serif">2</span></sub>)</span></span>
</span>
</th>
<td id="S2.T1.20.20.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:121.4pt;">
<span id="S2.T1.20.20.4.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.20.20.4.2.2" class="ltx_p">True if objects <span id="S2.T1.19.19.3.1.1.1" class="ltx_text ltx_font_sansserif">id<sub id="S2.T1.19.19.3.1.1.1.1" class="ltx_sub"><span id="S2.T1.19.19.3.1.1.1.1.1" class="ltx_text ltx_font_serif">1</span></sub></span> and <span id="S2.T1.20.20.4.2.2.2" class="ltx_text ltx_font_sansserif">id<sub id="S2.T1.20.20.4.2.2.2.1" class="ltx_sub"><span id="S2.T1.20.20.4.2.2.2.1.1" class="ltx_text ltx_font_serif">2</span></sub></span> have the same material attribute.</span>
</span>
</td>
</tr>
<tr id="S2.T1.24.24" class="ltx_tr">
<th id="S2.T1.22.22.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="width:71.5pt;">
<span id="S2.T1.22.22.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.22.22.2.2.2" class="ltx_p"><span id="S2.T1.22.22.2.2.2.2" class="ltx_text ltx_font_sansserif" style="font-size:90%;">greater_than(n<sub id="S2.T1.22.22.2.2.2.2.1" class="ltx_sub"><span id="S2.T1.22.22.2.2.2.2.1.1" class="ltx_text ltx_font_serif">1</span></sub>,n<sub id="S2.T1.22.22.2.2.2.2.2" class="ltx_sub"><span id="S2.T1.22.22.2.2.2.2.2.1" class="ltx_text ltx_font_serif">2</span></sub>)</span></span>
</span>
</th>
<td id="S2.T1.24.24.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:121.4pt;">
<span id="S2.T1.24.24.4.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.24.24.4.2.2" class="ltx_p">True if the number <span id="S2.T1.23.23.3.1.1.1" class="ltx_text ltx_font_sansserif">n<sub id="S2.T1.23.23.3.1.1.1.1" class="ltx_sub"><span id="S2.T1.23.23.3.1.1.1.1.1" class="ltx_text ltx_font_serif">1</span></sub></span> is greater than <span id="S2.T1.24.24.4.2.2.2" class="ltx_text ltx_font_sansserif">n<sub id="S2.T1.24.24.4.2.2.2.1" class="ltx_sub"><span id="S2.T1.24.24.4.2.2.2.1.1" class="ltx_text ltx_font_serif">2</span></sub></span>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.28.28" class="ltx_tr">
<th id="S2.T1.26.26.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="width:71.5pt;">
<span id="S2.T1.26.26.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.26.26.2.2.2" class="ltx_p"><span id="S2.T1.26.26.2.2.2.2" class="ltx_text ltx_font_sansserif" style="font-size:90%;">lesser_than(n<sub id="S2.T1.26.26.2.2.2.2.1" class="ltx_sub"><span id="S2.T1.26.26.2.2.2.2.1.1" class="ltx_text ltx_font_serif">1</span></sub>,n<sub id="S2.T1.26.26.2.2.2.2.2" class="ltx_sub"><span id="S2.T1.26.26.2.2.2.2.2.1" class="ltx_text ltx_font_serif">2</span></sub>)</span></span>
</span>
</th>
<td id="S2.T1.28.28.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:121.4pt;">
<span id="S2.T1.28.28.4.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.28.28.4.2.2" class="ltx_p">True if the number <span id="S2.T1.27.27.3.1.1.1" class="ltx_text ltx_font_sansserif">n<sub id="S2.T1.27.27.3.1.1.1.1" class="ltx_sub"><span id="S2.T1.27.27.3.1.1.1.1.1" class="ltx_text ltx_font_serif">1</span></sub></span> is lesser than <span id="S2.T1.28.28.4.2.2.2" class="ltx_text ltx_font_sansserif">n<sub id="S2.T1.28.28.4.2.2.2.1" class="ltx_sub"><span id="S2.T1.28.28.4.2.2.2.1.1" class="ltx_text ltx_font_serif">2</span></sub></span>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.32.32" class="ltx_tr">
<th id="S2.T1.30.30.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="width:71.5pt;">
<span id="S2.T1.30.30.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.30.30.2.2.2" class="ltx_p"><span id="S2.T1.30.30.2.2.2.2" class="ltx_text ltx_font_sansserif" style="font-size:90%;">same(c<sub id="S2.T1.30.30.2.2.2.2.1" class="ltx_sub"><span id="S2.T1.30.30.2.2.2.2.1.1" class="ltx_text ltx_font_serif">1</span></sub>,c<sub id="S2.T1.30.30.2.2.2.2.2" class="ltx_sub"><span id="S2.T1.30.30.2.2.2.2.2.1" class="ltx_text ltx_font_serif">2</span></sub>)</span></span>
</span>
</th>
<td id="S2.T1.32.32.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:121.4pt;">
<span id="S2.T1.32.32.4.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.32.32.4.2.2" class="ltx_p">True if the constant <span id="S2.T1.31.31.3.1.1.1" class="ltx_text ltx_font_sansserif">c<sub id="S2.T1.31.31.3.1.1.1.1" class="ltx_sub"><span id="S2.T1.31.31.3.1.1.1.1.1" class="ltx_text ltx_font_serif">1</span></sub></span> is same as the constant <span id="S2.T1.32.32.4.2.2.2" class="ltx_text ltx_font_sansserif">c<sub id="S2.T1.32.32.4.2.2.2.1" class="ltx_sub"><span id="S2.T1.32.32.4.2.2.2.1.1" class="ltx_text ltx_font_serif">2</span></sub></span>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.34.34" class="ltx_tr">
<th id="S2.T1.33.33.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:71.5pt;">
<span id="S2.T1.33.33.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.33.33.1.1.1" class="ltx_p"><span id="S2.T1.33.33.1.1.1.1" class="ltx_text ltx_font_sansserif" style="font-size:90%;">count(r<sub id="S2.T1.33.33.1.1.1.1.1" class="ltx_sub"><span id="S2.T1.33.33.1.1.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">i</span></sub>(X), c)</span></span>
</span>
</th>
<td id="S2.T1.34.34.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:121.4pt;">
<span id="S2.T1.34.34.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.34.34.2.1.1" class="ltx_p">True if there are <span id="S2.T1.34.34.2.1.1.2" class="ltx_text ltx_font_sansserif">c</span> number of solutions to <span id="S2.T1.34.34.2.1.1.3" class="ltx_text ltx_font_sansserif">X</span> that satisfy <span id="S2.T1.34.34.2.1.1.1" class="ltx_text ltx_font_sansserif">r<sub id="S2.T1.34.34.2.1.1.1.1" class="ltx_sub"><span id="S2.T1.34.34.2.1.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">i</span></sub>(X)</span>.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2111.04785/assets/x4.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="89" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration of a few examples of questions taken from the CLEVR dataset along with the converted target rule converted by the transformer.</figcaption>
</figure>
<div id="S2.SS2.SSS2.p4" class="ltx_para">
<p id="S2.SS2.SSS2.p4.1" class="ltx_p">Given the target sentence, the rules can then be reconstructed, details regarding the construction of the target sentence and the reconstruction of the rules can be found in the supplementary materials. The rule generation can now be viewed as a machine translation problem going from english sentences to the the target sentence. To that end, we implement a transformer based sequence to sequence model to translate the question to the target sentence. We use a BART <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> pretrained on sequence regeneration task and add a MLP + softmax layer at the end. The target sentence is generated for question in the training set and the transformer network is trained in a supervised to produce the target sentence given the question.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS3.4.1.1" class="ltx_text">II-B</span>3 </span>Logic Inference Engine.</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">Prolog is used to check for satisfiability of the variables present in the target rules. For binary questions (questions whose answers are either a ‚Äúyes‚Äù or a ‚Äúno‚Äù) the final satisfiability of the target rule is sufficient to answer the questions. For other types of questions, we output the specific grounding that satisfies the target rule as the answer to the question. The overall pipeline of the proposed pipeline is shown in Figure <a href="#S2.F3" title="Figure 3 ‚Ä£ II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In case the target rule produced after translation has syntax errors, then the inference engine outputs a NULL.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we show our model performance through experiments on the CLEVR and the GQA dataset.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">CLEVR Dataset</span>
</h3>

<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>The table summarizes the resuls obtained on the CLEVR validation set. For the proposed solution we report the results when the model was trained with 10% of training set and 100% of the training set.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Models</th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Count</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Exist</th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.4.1.1" class="ltx_p">Compare Number</span>
</span>
</th>
<th id="S3.T2.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.5.1.1" class="ltx_p">Compare Attribute</span>
</span>
</th>
<th id="S3.T2.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.6.1.1" class="ltx_p">Query ¬†Attribute</span>
</span>
</th>
<th id="S3.T2.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Overall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">Humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">86.7</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">96.6</td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" style="width:26.0pt;">
<span id="S3.T2.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.4.1.1" class="ltx_p">86.4</span>
</span>
</td>
<td id="S3.T2.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" style="width:26.0pt;">
<span id="S3.T2.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.5.1.1" class="ltx_p">96.0</span>
</span>
</td>
<td id="S3.T2.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" style="width:26.0pt;">
<span id="S3.T2.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.6.1.1" class="ltx_p">95.0</span>
</span>
</td>
<td id="S3.T2.1.2.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">92.6</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">CNN+LSTM+SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.7</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.9</td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.4.1.1" class="ltx_p">75.1</span>
</span>
</td>
<td id="S3.T2.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.5.1.1" class="ltx_p">70.8</span>
</span>
</td>
<td id="S3.T2.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.6.1.1" class="ltx_p">80.9</span>
</span>
</td>
<td id="S3.T2.1.3.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">73.2</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">IEP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.7</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">97.1</td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.4.1.1" class="ltx_p">98.7</span>
</span>
</td>
<td id="S3.T2.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.5.1.1" class="ltx_p">98.9</span>
</span>
</td>
<td id="S3.T2.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.6.1.1" class="ltx_p">98.1</span>
</span>
</td>
<td id="S3.T2.1.4.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">96.9</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<td id="S3.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">NS-CL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">98.2</td>
<td id="S3.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">98.8</td>
<td id="S3.T2.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.4.1.1" class="ltx_p">99.0</span>
</span>
</td>
<td id="S3.T2.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.5.1.1" class="ltx_p">99.1</span>
</span>
</td>
<td id="S3.T2.1.5.4.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.6.1.1" class="ltx_p">99.3</span>
</span>
</td>
<td id="S3.T2.1.5.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">98.9</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<td id="S3.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">NS-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.7</td>
<td id="S3.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.9</td>
<td id="S3.T2.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.5.4.1.1" class="ltx_p">99.9</span>
</span>
</td>
<td id="S3.T2.1.6.5.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.5.5.1.1" class="ltx_p">99.8</span>
</span>
</td>
<td id="S3.T2.1.6.5.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.5.6.1.1" class="ltx_p">99.8</span>
</span>
</td>
<td id="S3.T2.1.6.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">99.8</td>
</tr>
<tr id="S3.T2.1.7.6" class="ltx_tr">
<td id="S3.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Proposed Method (10%)</td>
<td id="S3.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.3</td>
<td id="S3.T2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.1</td>
<td id="S3.T2.1.7.6.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.6.4.1.1" class="ltx_p">99.1</span>
</span>
</td>
<td id="S3.T2.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.6.5.1.1" class="ltx_p">99.1</span>
</span>
</td>
<td id="S3.T2.1.7.6.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.7.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.6.6.1.1" class="ltx_p">98.7</span>
</span>
</td>
<td id="S3.T2.1.7.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">99.1</td>
</tr>
<tr id="S3.T2.1.8.7" class="ltx_tr">
<td id="S3.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Proposed Method (100%)</td>
<td id="S3.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">99.6</td>
<td id="S3.T2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">99.9</td>
<td id="S3.T2.1.8.7.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.8.7.4.1.1" class="ltx_p">98.9</span>
</span>
</td>
<td id="S3.T2.1.8.7.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.8.7.5.1.1" class="ltx_p">99.7</span>
</span>
</td>
<td id="S3.T2.1.8.7.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:26.0pt;">
<span id="S3.T2.1.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.8.7.6.1.1" class="ltx_p">99.5</span>
</span>
</td>
<td id="S3.T2.1.8.7.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">99.6</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">This dataset consists of rendered images containing objects of different attributes such as shape, color, size and materials. The objects are also characterized by their locations with respect to the other. These constitute the relations between the objects in the dataset, such as left, right, front and behind. Each image has a set of questions associated with it which are generated using 90 different program templates.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We operate under the perfect sight setting and use the scene graph provided by the dataset. For each scene in the dataset, the scene graph is converted to background knowledge as explained in Section <a href="#S2" title="II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. To generate the ground truth target rule, in order to train the transformer network, we make use of the functional form that the dataset provides for each question. The functional form contains a series of operations, which when performed on the image, provides the answer to the posed question. To go from the functional form to the target rule, we look at all possible operations that are a part of the functional form, and a map is created between the operations and the elements of the target rule.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The transformer model is trained with a variable length question as the input and the target sentence (defined in section <a href="#S2" title="II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>) as the output. In particular, we use a pretrained BART model for the transformer, in conjunction with a MLP and softmax layer. The final softmax layer outputs the necessary tokens to generate the target sentence. The use of transformer makes the training highly parallelizable. We train the model using Google Colab on a NVIDIA Tesla V100 GPU. The model was trained for a total of 4 epochs on the training set using ADAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> optimizer with a learning rate of <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msup id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mn id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">10</mn><mrow id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml"><mo id="S3.SS1.p3.1.m1.1.1.3a" xref="S3.SS1.p3.1.m1.1.1.3.cmml">‚àí</mo><mn id="S3.SS1.p3.1.m1.1.1.3.2" xref="S3.SS1.p3.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">10</cn><apply id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3"><minus id="S3.SS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3"></minus><cn type="integer" id="S3.SS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">10^{-4}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">At this point, we have the scenes converted into background knowledge and the questions translated to logic rules, which act as queries on the background knowledge. Then, prolog is used to run a satisfiability check on the rules to obtain the answer.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">The proposed method was evaluated on the validation set and we obtained a near perfect accuracy of 99.6%, close to the existing state of the art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> (99.8%). This shows that the proposed framework of using formal logic is capable of solving the task of visual question answering. Table <a href="#S3.T2" title="TABLE II ‚Ä£ III-A CLEVR Dataset ‚Ä£ III Experiments ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> summarizes the performance along with a comparison with the baselines.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">To further test the learning capabilities of our proposed method, we train our translation model on 10% of the questions from the training set (62,997 questions) equally distributed between the 90 question templates. Even with a fraction of training samples our proposed method attains 99.16% accuracy on the validation set. We chose the validation set over the test set to test our model as the scene graphs are readily available in the validation set. For the purpose of training the transformer, we split the training data into training plus validation to monitor the training process.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">Figure <a href="#S2.F5" title="Figure 5 ‚Ä£ II-B2 Representation of Question in the form of first order logic rule ‚Ä£ II-B VQA Pipeline. ‚Ä£ II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows a few examples of the questions and the corresponding target rule produced by the translator for the CLEVR dataset. From knowing the definition of the predicates (defined in Section <a href="#S2.SS2.SSS2" title="II-B2 Representation of Question in the form of first order logic rule ‚Ä£ II-B VQA Pipeline. ‚Ä£ II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span>2</span></a>), the target rule can easily be interpreted by a human. For example, considering the first example in Figure <a href="#S2.F5" title="Figure 5 ‚Ä£ II-B2 Representation of Question in the form of first order logic rule ‚Ä£ II-B VQA Pipeline. ‚Ä£ II Methodology ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we need to check if there are any other things with the same size as the brown shiny sphere. Looking at the corresponding target clause we see that it is a conjunction of four predicates where the first three predicates filter out objects with the attributes <span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_italic">brown, metal, sphere</span> and the last predicate checks if there are two objects with the same set of attributes mentioned earlier. Therefore looking at the translated rules we know exactly what is happening underneath and what the satisfiability check is looking for.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">CLEVR CoGenT.</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">CLEVR cogent was proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> to test the generalizing capabilities of VQA models when the dataset is biased, refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> for more information on the construction of the dataset.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We train our model on split A using the same training procedure as the CLEVR dataset and test on the validation sets of both the splits. Our proposed method again achieves identical near perfect validation accuracy of 99.52% on both the splits. The independent treatment of images and language information makes it easy for the model to learn separate disentangled representations for the individual attributes as evident from the final accuracy. As seen from Table <a href="#S3.T3" title="TABLE III ‚Ä£ III-B CLEVR CoGenT. ‚Ä£ III Experiments ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, the proposed model performs as well as the state of the art NS-VQA with a significant reduction in the training burden.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Performance comparison of the proposed method on the CLEVR CoGenT dataset when trained on split A.</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Models</th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Split A</th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Split B</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.2.1" class="ltx_tr">
<td id="S3.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">NS-CL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S3.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">98.8%</td>
<td id="S3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">98.9%</td>
</tr>
<tr id="S3.T3.1.3.2" class="ltx_tr">
<td id="S3.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">NS-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S3.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.8</td>
<td id="S3.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.7</td>
</tr>
<tr id="S3.T3.1.4.3" class="ltx_tr">
<td id="S3.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Proposed Method</td>
<td id="S3.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">99.5</td>
<td id="S3.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">99.5</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">GQA Dataset.</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">GQA is another visual question answering dataset proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, where questions are constructed using compositions of a set of operations. We follow the same pipeline used in the CLEVR dataset. Again, we operate under pure sight condition and use the scene graphs provided by the dataset for conversion into background knowledge.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Performance of the proposed method on the GQA validation set. For the proposed solution we report results when the model was tested on full validation set (FV) and the reduced validation set. (RV)</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.2.1" class="ltx_tr">
<td id="S3.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">GN+MAC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S3.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">96.3</td>
</tr>
<tr id="S3.T4.1.3.2" class="ltx_tr">
<td id="S3.T4.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Proposed Method-FV</td>
<td id="S3.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.3</td>
</tr>
<tr id="S3.T4.1.4.3" class="ltx_tr">
<td id="S3.T4.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Proposed Method-RV</td>
<td id="S3.T4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">93.1</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">To train the translation model, we make use of the functional form provided by the dataset for each question to produce the target sentence. Similar to CLEVR, every operation in the functional form is mapped to a predicate/rule in the target sentence. We found the functional forms provided by the dataset to be inconsistent in the semantics used for defining the arguments associated with the operations in the functional form, refer to the supplementary materials for more details. This resulted in incorrect target sentences for certain questions, therefore we restrict our training to only those questions having consistent functional forms.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">On the GQA validation set, we obtained an overall accuracy of (74%) but this contains several questions corresponding to inconsistent functional forms for which the model wasn‚Äôt trained for. When the validation set is limited to those questions which have a consistent functional form, then the validation accuracy becomes (93%), which is a better indicator of the model‚Äôs performance on the GQA datasets. This is comparable to the current state of the art for the GQA dataset under perfect site configuration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> (96.3%). Aside from the fact that the proposed solution is also highly interpretable and the reasoning steps are easy to follow, the training of the transformer is much more easier to perform than a Graph Neural Network used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Table <a href="#S3.T4" title="TABLE IV ‚Ä£ III-C GQA Dataset. ‚Ä£ III Experiments ‚Ä£ Visual Question Answering based on Formal Logic" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> summarizes the model performance on the GQA dataset. Again, since the scene graphs for the test set are not readily provided by the dataset we only evaluate the model on the validation set as we operate in the perfect sight setting.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Discussion and Conclusion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we showcased the feasibility of using an interpretable formal logic based approach to solve the visual question answering problem. We circumvent the inflexibility of formal logic systems to noisy inputs by using transformers to translate natural language questions to interpretable logic rules. From the conducted experiments, it can be seen that our proposed solution is competitive and even manages to achieve near perfect accuracy on the CLEVR dataset. It is also data efficient and there is only a slight drop in accuracy even when trained with just 10% data from the CLEVR dataset.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S.¬†Antol, A.¬†Agrawal, J.¬†Lu, M.¬†Mitchell, D.¬†Batra, C.¬†L. Zitnick, and
D.¬†Parikh, ‚ÄúVqa: Visual question answering,‚Äù in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE International Conference on Computer Vision (ICCV)</em>, December 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
D.¬†A. Hudson and C.¬†D. Manning, ‚ÄúGqa: A new dataset for real-world visual
reasoning and compositional question answering,‚Äù in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition</em>, 2019, pp.
6700‚Äì6709.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J.¬†Johnson, B.¬†Hariharan, L.¬†van¬†der Maaten, L.¬†Fei-Fei, C.¬†Lawrence¬†Zitnick,
and R.¬†Girshick, ‚ÄúClevr: A diagnostic dataset for compositional language and
elementary visual reasoning,‚Äù in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition</em>, 2017, pp. 2901‚Äì2910.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Q.¬†Wu, D.¬†Teney, P.¬†Wang, C.¬†Shen, A.¬†Dick, and A.¬†van den Hengel, ‚ÄúVisual
question answering: A survey of methods and datasets,‚Äù <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computer Vision
and Image Understanding</em>, vol. 163, pp. 21 ‚Äì 40, 2017, language in Vision.
[Online]. Available:
http://www.sciencedirect.com/science/article/pii/S1077314217300772

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A.¬†Jabri, A.¬†Joulin, and L.¬†Van Der¬†Maaten, ‚ÄúRevisiting visual question
answering baselines,‚Äù in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">European conference on computer
vision</em>.¬†¬†¬†Springer, 2016, pp. 727‚Äì739.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D.¬†Xu, Y.¬†Zhu, C.¬†B. Choy, and L.¬†Fei-Fei, ‚ÄúScene graph generation by
iterative message passing,‚Äù <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1701.02426, 2017.
[Online]. Available: http://arxiv.org/abs/1701.02426

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.¬†Li, W.¬†Ouyang, B.¬†Zhou, Y.¬†Cui, J.¬†Shi, and X.¬†Wang, ‚ÄúFactorizable net: An
efficient subgraph-based framework for scene graph generation,‚Äù <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/1806.11538, 2018. [Online]. Available:
http://arxiv.org/abs/1806.11538

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
N.¬†Xu, A.-A. Liu, J.¬†Liu, W.¬†Nie, and Y.¬†Su, ‚ÄúScene graph captioner: Image
captioning based on structural visual representation,‚Äù <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Journal of
Visual Communication and Image Representation</em>, vol.¬†58, pp. 477 ‚Äì 485,
2019. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S1047320318303535

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S.¬†Lee, J.¬†W. Kim, Y.¬†Oh, and J.¬†H. Jeon, ‚ÄúVisual Question Answering over
Scene Graph,‚Äù <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings - 2019 1st International Conference on
Graph Computing, GC 2019</em>, pp. 45‚Äì50, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J.¬†Andreas, M.¬†Rohrbach, T.¬†Darrell, and D.¬†Klein, ‚ÄúLearning to compose neural
networks for question answering,‚Äù in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016
Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies</em>.¬†¬†¬†San Diego, California: Association for Computational
Linguistics, Jun. 2016, pp. 1545‚Äì1554. [Online]. Available:
https://www.aclweb.org/anthology/N16-1181

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J.¬†Johnson, B.¬†Hariharan, L.¬†van¬†der Maaten, J.¬†Hoffman, L.¬†Fei-Fei,
C.¬†Lawrence¬†Zitnick, and R.¬†Girshick, ‚ÄúInferring and executing programs for
visual reasoning,‚Äù in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference
on Computer Vision (ICCV)</em>, Oct 2017.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J.¬†Mao, C.¬†Gan, P.¬†Kohli, J.¬†B. Tenenbaum, and J.¬†Wu, ‚ÄúThe neuro-symbolic
concept learner: Interpreting scenes, words, and sentences from natural
supervision,‚Äù in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>, 2019. [Online]. Available:
https://openreview.net/forum?id=rJgMlhRctm

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K.¬†Yi, J.¬†Wu, C.¬†Gan, A.¬†Torralba, P.¬†Kohli, and J.¬†Tenenbaum,
‚ÄúNeural-symbolic vqa: Disentangling reasoning from vision and language
understanding,‚Äù in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
31</em>, S.¬†Bengio, H.¬†Wallach, H.¬†Larochelle, K.¬†Grauman, N.¬†Cesa-Bianchi, and
R.¬†Garnett, Eds.¬†¬†¬†Curran Associates,
Inc., 2018, pp. 1031‚Äì1042. [Online]. Available:
http://papers.nips.cc/paper/7381-neural-symbolic-vqa-disentangling-reasoning-from-vision-and-language-understanding.pdf

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
R.¬†C. Moore, <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">The role of logic in knowledge representation and
commonsense reasoning</em>.¬†¬†¬†SRI
International. Artificial Intelligence Center, 1982.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A.¬†Vaswani, N.¬†Shazeer, N.¬†Parmar, J.¬†Uszkoreit, L.¬†Jones, A.¬†N. Gomez, L.¬†u.
Kaiser, and I.¬†Polosukhin, ‚ÄúAttention is all you need,‚Äù in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances
in Neural Information Processing Systems 30</em>, I.¬†Guyon, U.¬†V. Luxburg,
S.¬†Bengio, H.¬†Wallach, R.¬†Fergus, S.¬†Vishwanathan, and R.¬†Garnett, Eds.¬†¬†¬†Curran Associates, Inc., 2017, pp.
5998‚Äì6008. [Online]. Available:
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J.¬†Wielemaker, T.¬†Schrijvers, M.¬†Triska, and T.¬†Lager, ‚ÄúSWI-Prolog,‚Äù
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Theory and Practice of Logic Programming</em>, vol.¬†12, no. 1-2, pp.
67‚Äì96, 2012.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M.¬†Lewis, Y.¬†Liu, N.¬†Goyal, M.¬†Ghazvininejad, A.¬†Mohamed, O.¬†Levy, V.¬†Stoyanov,
and L.¬†Zettlemoyer, ‚ÄúBart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension,‚Äù <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1910.13461</em>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.¬†Muggleton and L.¬†de¬†Raedt, ‚ÄúInductive Logic Programming: Theory and
methods,‚Äù <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">The Journal of Logic Programming</em>, vol. 19-20, pp.
629‚Äì679, 1994. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/0743106694900353

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
D.¬†P. Kingma and J.¬†Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù in
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ICLR (Poster)</em>, 2015. [Online]. Available:
http://arxiv.org/abs/1412.6980

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2111.04783" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2111.04785" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2111.04785">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2111.04785" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2111.04786" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 16:37:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
